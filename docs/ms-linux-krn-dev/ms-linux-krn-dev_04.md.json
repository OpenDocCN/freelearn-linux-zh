["```\n/* arch/x86/boot/main.c */\nvoid main(void)\n{\n /* First, copy the boot header into the \"zeropage\" */\n copy_boot_params();\n\n /* Initialize the early-boot console */\n console_init();\n if (cmdline_find_option_bool(\"debug\"))\n puts(\"early console in setup coden\");\n\n /* End of heap check */\n init_heap();\n\n /* Make sure we have all the proper CPU support */\n if (validate_cpu()) {\n puts(\"Unable to boot - please use a kernel appropriate \"\n \"for your CPU.n\");\n die();\n }\n\n /* Tell the BIOS what CPU mode we intend to run in. */\n set_bios_mode();\n\n /* Detect memory layout */\n detect_memory();\n\n /* Set keyboard repeat rate (why?) and query the lock flags */\n keyboard_init();\n\n /* Query Intel SpeedStep (IST) information */\n query_ist();\n\n /* Query APM information */\n#if defined(CONFIG_APM) || defined(CONFIG_APM_MODULE)\n query_apm_bios();\n#endif\n\n /* Query EDD information */\n#if defined(CONFIG_EDD) || defined(CONFIG_EDD_MODULE)\n query_edd();\n#endif\n\n /* Set the video mode */\n set_video();\n\n /* Do the last things and invoke protected mode */\n go_to_protected_mode();\n}\n```", "```\n/*include/linux/mm-types.h */ \n/* The objects in struct page are organized in double word blocks in\n * order to allows us to use atomic double word operations on portions\n * of struct page. That is currently only used by slub but the arrangement\n * allows the use of atomic double word operations on the flags/mapping\n * and lru list pointers also.\n */\nstruct page {\n        /* First double word block */\n         unsigned long flags; /* Atomic flags, some possibly updated asynchronously */   union {\n          struct address_space *mapping; \n          void *s_mem; /* slab first object */\n          atomic_t compound_mapcount; /* first tail page */\n          /* page_deferred_list().next -- second tail page */\n   };\n  ....\n  ....\n\n}\n```", "```\n/*Macros to create function definitions for page flags */\n#define TESTPAGEFLAG(uname, lname, policy) \\\nstatic __always_inline int Page##uname(struct page *page) \\\n{ return test_bit(PG_##lname, &policy(page, 0)->flags); }\n\n#define SETPAGEFLAG(uname, lname, policy) \\\nstatic __always_inline void SetPage##uname(struct page *page) \\\n{ set_bit(PG_##lname, &policy(page, 1)->flags); }\n\n#define CLEARPAGEFLAG(uname, lname, policy) \\\nstatic __always_inline void ClearPage##uname(struct page *page) \\\n{ clear_bit(PG_##lname, &policy(page, 1)->flags); }\n\n#define __SETPAGEFLAG(uname, lname, policy) \\\nstatic __always_inline void __SetPage##uname(struct page *page) \\\n{ __set_bit(PG_##lname, &policy(page, 1)->flags); }\n\n#define __CLEARPAGEFLAG(uname, lname, policy) \\\nstatic __always_inline void __ClearPage##uname(struct page *page) \\\n{ __clear_bit(PG_##lname, &policy(page, 1)->flags); }\n\n#define TESTSETFLAG(uname, lname, policy) \\\nstatic __always_inline int TestSetPage##uname(struct page *page) \\\n{ return test_and_set_bit(PG_##lname, &policy(page, 1)->flags); }\n\n#define TESTCLEARFLAG(uname, lname, policy) \\\nstatic __always_inline int TestClearPage##uname(struct page *page) \\\n{ return test_and_clear_bit(PG_##lname, &policy(page, 1)->flags); }\n\n*....\n....* \n```", "```\n/* include/linux/mmzone.h */\nenum zone_type {\n#ifdef CONFIG_ZONE_DMA\nZONE_DMA,\n#endif\n#ifdef CONFIG_ZONE_DMA32\n ZONE_DMA32,\n#endif\n#ifdef CONFIG_HIGHMEM\n ZONE_HIGHMEM,\n#endif\n ZONE_MOVABLE,\n#ifdef CONFIG_ZONE_DEVICE\n ZONE_DEVICE,\n#endif\n __MAX_NR_ZONES\n};\n```", "```\nvoid *devm_memremap_pages(struct device *dev, struct resource *res,\n                        struct percpu_ref *ref, struct vmem_altmap *altmap); \n```", "```\n/* include/linux/mmzone.h */typedef struct pglist_data {\n  struct zone node_zones[MAX_NR_ZONES];\n struct zonelist node_zonelists[MAX_ZONELISTS];\n int nr_zones;\n\n#ifdef CONFIG_FLAT_NODE_MEM_MAP /* means !SPARSEMEM */\n  struct page *node_mem_map;\n#ifdef CONFIG_PAGE_EXTENSION\n  struct page_ext *node_page_ext;\n#endif\n#endif\n\n#ifndef CONFIG_NO_BOOTMEM\n  struct bootmem_data *bdata;\n#endif\n#ifdef CONFIG_MEMORY_HOTPLUG\n spinlock_t node_size_lock;\n#endif\n unsigned long node_start_pfn;\n unsigned long node_present_pages; /* total number of physical pages */\n unsigned long node_spanned_pages; \n int node_id;\n wait_queue_head_t kswapd_wait;\n wait_queue_head_t pfmemalloc_wait;\n struct task_struct *kswapd; \n int kswapd_order;\n enum zone_type kswapd_classzone_idx;\n\n#ifdef CONFIG_COMPACTION\n int kcompactd_max_order;\n enum zone_type kcompactd_classzone_idx;\n wait_queue_head_t kcompactd_wait;\n struct task_struct *kcompactd;\n#endif\n#ifdef CONFIG_NUMA_BALANCING\n spinlock_t numabalancing_migrate_lock;\n unsigned long numabalancing_migrate_next_window;\n unsigned long numabalancing_migrate_nr_pages;\n#endif\n unsigned long totalreserve_pages;\n\n#ifdef CONFIG_NUMA\n unsigned long min_unmapped_pages;\n unsigned long min_slab_pages;\n#endif /* CONFIG_NUMA */\n\n ZONE_PADDING(_pad1_)\n spinlock_t lru_lock;\n\n#ifdef CONFIG_DEFERRED_STRUCT_PAGE_INIT\n unsigned long first_deferred_pfn;\n#endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */\n\n#ifdef CONFIG_TRANSPARENT_HUGEPAGE\n spinlock_t split_queue_lock;\n struct list_head split_queue;\n unsigned long split_queue_len;\n#endif\n unsigned int inactive_ratio;\n unsigned long flags;\n\n ZONE_PADDING(_pad2_)\n struct per_cpu_nodestat __percpu *per_cpu_nodestats;\n atomic_long_t vm_stat[NR_VM_NODE_STAT_ITEMS];\n} pg_data_t;\n```", "```\nstruct zone {\n /* Read-mostly fields */\n\n /* zone watermarks, access with *_wmark_pages(zone) macros */\n unsigned long watermark[NR_WMARK];\n\n unsigned long nr_reserved_highatomic;\n\n /*\n * We don't know if the memory that we're going to allocate will be\n * freeable or/and it will be released eventually, so to avoid totally\n * wasting several GB of ram we must reserve some of the lower zone\n * memory (otherwise we risk to run OOM on the lower zones despite\n * there being tons of freeable ram on the higher zones). This array is\n * recalculated at runtime if the sysctl_lowmem_reserve_ratio sysctl\n * changes.\n */\n long lowmem_reserve[MAX_NR_ZONES];\n\n#ifdef CONFIG_NUMA\n int node;\n#endif\n struct pglist_data *zone_pgdat;\n struct per_cpu_pageset __percpu *pageset;\n\n#ifndef CONFIG_SPARSEMEM\n /*\n * Flags for a pageblock_nr_pages block. See pageblock-flags.h.\n * In SPARSEMEM, this map is stored in struct mem_section\n */\n unsigned long *pageblock_flags;\n#endif /* CONFIG_SPARSEMEM */\n\n /* zone_start_pfn == zone_start_paddr >> PAGE_SHIFT */\n unsigned long zone_start_pfn;\n\n /*\n * spanned_pages is the total pages spanned by the zone, including\n * holes, which is calculated as:\n * spanned_pages = zone_end_pfn - zone_start_pfn;\n *\n * present_pages is physical pages existing within the zone, which\n * is calculated as:\n * present_pages = spanned_pages - absent_pages(pages in holes);\n *\n * managed_pages is present pages managed by the buddy system, which\n * is calculated as (reserved_pages includes pages allocated by the\n * bootmem allocator):\n * managed_pages = present_pages - reserved_pages;\n *\n * So present_pages may be used by memory hotplug or memory power\n * management logic to figure out unmanaged pages by checking\n * (present_pages - managed_pages). And managed_pages should be used\n * by page allocator and vm scanner to calculate all kinds of watermarks\n * and thresholds.\n *\n * Locking rules:\n *\n * zone_start_pfn and spanned_pages are protected by span_seqlock.\n * It is a seqlock because it has to be read outside of zone->lock,\n * and it is done in the main allocator path. But, it is written\n * quite infrequently.\n *\n * The span_seq lock is declared along with zone->lock because it is\n * frequently read in proximity to zone->lock. It's good to\n * give them a chance of being in the same cacheline.\n *\n * Write access to present_pages at runtime should be protected by\n * mem_hotplug_begin/end(). Any reader who can't tolerant drift of\n * present_pages should get_online_mems() to get a stable value.\n *\n * Read access to managed_pages should be safe because it's unsigned\n * long. Write access to zone->managed_pages and totalram_pages are\n * protected by managed_page_count_lock at runtime. Idealy only\n * adjust_managed_page_count() should be used instead of directly\n * touching zone->managed_pages and totalram_pages.\n */\n unsigned long managed_pages;\n unsigned long spanned_pages;\n unsigned long present_pages;\n\n const char *name;// name of this zone\n\n#ifdef CONFIG_MEMORY_ISOLATION\n /*\n * Number of isolated pageblock. It is used to solve incorrect\n * freepage counting problem due to racy retrieving migratetype\n * of pageblock. Protected by zone->lock.\n */\n unsigned long nr_isolate_pageblock;\n#endif\n\n#ifdef CONFIG_MEMORY_HOTPLUG\n /* see spanned/present_pages for more description */\n seqlock_t span_seqlock;\n#endif\n\n int initialized;\n\n /* Write-intensive fields used from the page allocator */\n ZONE_PADDING(_pad1_)\n\n /* free areas of different sizes */\nstruct free_area free_area[MAX_ORDER];\n\n /* zone flags, see below */\n unsigned long flags;\n\n /* Primarily protects free_area */\n spinlock_t lock;\n\n /* Write-intensive fields used by compaction and vmstats. */\n ZONE_PADDING(_pad2_)\n\n /*\n * When free pages are below this point, additional steps are taken\n * when reading the number of free pages to avoid per-CPU counter\n * drift allowing watermarks to be breached\n */\n unsigned long percpu_drift_mark;\n\n#if defined CONFIG_COMPACTION || defined CONFIG_CMA\n /* pfn where compaction free scanner should start */\n unsigned long compact_cached_free_pfn;\n /* pfn where async and sync compaction migration scanner should start */\n unsigned long compact_cached_migrate_pfn[2];\n#endif\n\n#ifdef CONFIG_COMPACTION\n /*\n * On compaction failure, 1<<compact_defer_shift compactions\n * are skipped before trying again. The number attempted since\n * last failure is tracked with compact_considered.\n */\n unsigned int compact_considered;\n unsigned int compact_defer_shift;\n int compact_order_failed;\n#endif\n\n#if defined CONFIG_COMPACTION || defined CONFIG_CMA\n /* Set to true when the PG_migrate_skip bits should be cleared */\n bool compact_blockskip_flush;\n#endif\n\n bool contiguous;\n\n ZONE_PADDING(_pad3_)\n /* Zone statistics */\n atomic_long_t vm_stat[NR_VM_ZONE_STAT_ITEMS];\n} ____cacheline_internodealigned_in_smp;\n```", "```\nstatic inline struct page *alloc_pages(gfp_t gfp_mask, unsigned int order);\n```", "```\n#define alloc_page(gfp_mask) alloc_pages(gfp_mask, 0);\n```", "```\n/* allocates 2^(order) pages and returns start linear address */ unsigned long __get_free_pages(gfp_t gfp_mask, unsigned int order)\n{\nstruct page *page;\n/*\n* __get_free_pages() returns a 32-bit address, which cannot represent\n* a highmem page\n*/\nVM_BUG_ON((gfp_mask & __GFP_HIGHMEM) != 0);\n\npage = alloc_pages(gfp_mask, order);\nif (!page)\nreturn 0;\nreturn (unsigned long) page_address(page);\n}\n\n/* Returns start linear address to zero initialized page */\nunsigned long get_zeroed_page(gfp_t gfp_mask)\n{\nreturn __get_free_pages(gfp_mask | __GFP_ZERO, 0);\n}\n /* Allocates a page */\n#define __get_free_page(gfp_mask) \\\n__get_free_pages((gfp_mask), 0)\n\n/* Allocate page/pages from DMA zone */\n#define __get_dma_pages(gfp_mask, order) \\\n __get_free_pages((gfp_mask) | GFP_DMA, (order))\n\n```", "```\nvoid __free_pages(struct page *page, unsigned int order);\nvoid free_pages(unsigned long addr, unsigned int order);\nvoid free_page(addr);\n```", "```\n  struct zone {\n          ...\n          ...\n          struct free_area[MAX_ORDER];\n          ...\n          ...\n     };\n```", "```\n struct free_area {\n          struct list_head free_list[MIGRATE_TYPES];\n          unsigned long nr_free;\n    };\n```", "```\nenum {\n MIGRATE_UNMOVABLE,\n MIGRATE_MOVABLE,\n MIGRATE_RECLAIMABLE,\n MIGRATE_PCPTYPES, /* the number of types on the pcp lists */\n MIGRATE_HIGHATOMIC = MIGRATE_PCPTYPES,\n#ifdef CONFIG_CMA\n MIGRATE_CMA,\n#endif\n#ifdef CONFIG_MEMORY_ISOLATION\n MIGRATE_ISOLATE, /* can't allocate from here */\n#endif\n MIGRATE_TYPES\n};  \n```", "```\n/* include/linux/mmzone.h */\n\nstruct per_cpu_pages {\n int count; /* number of pages in the list */\n int high; /* high watermark, emptying needed */\n int batch; /* chunk size for buddy add/remove */\n\n /* Lists of pages, one per migrate type stored on the pcp-lists */\n struct list_head lists[MIGRATE_PCPTYPES];\n};\n\nstruct per_cpu_pageset {\n struct per_cpu_pages pcp;\n#ifdef CONFIG_NUMA\n s8 expire;\n#endif\n#ifdef CONFIG_SMP\n s8 stat_threshold;\n s8 vm_stat_diff[NR_VM_ZONE_STAT_ITEMS];\n#endif\n};\n\nstruct zone {\n ...\n ...\n struct per_cpu_pageset __percpu *pageset;\n ...\n ...\n};\n```", "```\nstatic int fallbacks[MIGRATE_TYPES][4] = {\n [MIGRATE_UNMOVABLE] = { MIGRATE_RECLAIMABLE, MIGRATE_MOVABLE, MIGRATE_TYPES },\n [MIGRATE_RECLAIMABLE] = { MIGRATE_UNMOVABLE, MIGRATE_MOVABLE, MIGRATE_TYPES },\n [MIGRATE_MOVABLE] = { MIGRATE_RECLAIMABLE, MIGRATE_UNMOVABLE, MIGRATE_TYPES },\n#ifdef CONFIG_CMA\n [MIGRATE_CMA] = { MIGRATE_TYPES }, /* Never used */\n#endif\n#ifdef CONFIG_MEMORY_ISOLATION\n [MIGRATE_ISOLATE] = { MIGRATE_TYPES }, /* Never used */\n#endif\n};\n```", "```\ntypedef unsigned __bitwise__ gfp_t;\n```", "```\n#define __GFP_DMA ((__force gfp_t)___GFP_DMA)\n#define __GFP_HIGHMEM ((__force gfp_t)___GFP_HIGHMEM)\n#define __GFP_DMA32 ((__force gfp_t)___GFP_DMA32)\n#define __GFP_MOVABLE ((__force gfp_t)___GFP_MOVABLE) /* ZONE_MOVABLE allowed */\n```", "```\n#define __GFP_RECLAIMABLE ((__force gfp_t)___GFP_RECLAIMABLE)\n#define __GFP_WRITE ((__force gfp_t)___GFP_WRITE)\n#define __GFP_HARDWALL ((__force gfp_t)___GFP_HARDWALL)\n#define __GFP_THISNODE ((__force gfp_t)___GFP_THISNODE)\n#define __GFP_ACCOUNT ((__force gfp_t)___GFP_ACCOUNT)\n```", "```\n#define __GFP_ATOMIC ((__force gfp_t)___GFP_ATOMIC)\n#define __GFP_HIGH ((__force gfp_t)___GFP_HIGH)\n#define __GFP_MEMALLOC ((__force gfp_t)___GFP_MEMALLOC)\n#define __GFP_NOMEMALLOC ((__force gfp_t)___GFP_NOMEMALLOC)\n```", "```\n#define __GFP_IO ((__force gfp_t)___GFP_IO)\n#define __GFP_FS ((__force gfp_t)___GFP_FS)\n#define __GFP_DIRECT_RECLAIM ((__force gfp_t)___GFP_DIRECT_RECLAIM) /* Caller can reclaim */\n#define __GFP_KSWAPD_RECLAIM ((__force gfp_t)___GFP_KSWAPD_RECLAIM) /* kswapd can wake */\n#define __GFP_RECLAIM ((__force gfp_t)(___GFP_DIRECT_RECLAIM|___GFP_KSWAPD_RECLAIM))\n#define __GFP_REPEAT ((__force gfp_t)___GFP_REPEAT)\n#define __GFP_NOFAIL ((__force gfp_t)___GFP_NOFAIL)\n#define __GFP_NORETRY ((__force gfp_t)___GFP_NORETRY)\n```", "```\n#define __GFP_COLD ((__force gfp_t)___GFP_COLD)\n#define __GFP_NOWARN ((__force gfp_t)___GFP_NOWARN)\n#define __GFP_COMP ((__force gfp_t)___GFP_COMP)\n#define __GFP_ZERO ((__force gfp_t)___GFP_ZERO)\n#define __GFP_NOTRACK ((__force gfp_t)___GFP_NOTRACK)\n#define __GFP_NOTRACK_FALSE_POSITIVE (__GFP_NOTRACK)\n#define __GFP_OTHER_NODE ((__force gfp_t)___GFP_OTHER_NODE)\n```", "```\n#define GFP_ATOMIC (__GFP_HIGH|__GFP_ATOMIC|__GFP_KSWAPD_RECLAIM)\n#define GFP_KERNEL (__GFP_RECLAIM | __GFP_IO | __GFP_FS)\n#define GFP_KERNEL_ACCOUNT (GFP_KERNEL | __GFP_ACCOUNT)\n#define GFP_NOWAIT (__GFP_KSWAPD_RECLAIM)\n#define GFP_NOIO (__GFP_RECLAIM)\n#define GFP_NOFS (__GFP_RECLAIM | __GFP_IO)\n#define GFP_TEMPORARY (__GFP_RECLAIM | __GFP_IO | __GFP_FS | __GFP_RECLAIMABLE)\n#define GFP_USER (__GFP_RECLAIM | __GFP_IO | __GFP_FS | __GFP_HARDWALL)\n#define GFP_DMA __GFP_DMA\n#define GFP_DMA32 __GFP_DMA32\n#define GFP_HIGHUSER (GFP_USER | __GFP_HIGHMEM)\n#define GFP_HIGHUSER_MOVABLE (GFP_HIGHUSER | __GFP_MOVABLE)\n#define GFP_TRANSHUGE_LIGHT ((GFP_HIGHUSER_MOVABLE | __GFP_COMP | __GFP_NOMEMALLOC | \\ __GFP_NOWARN) & ~__GFP_RECLAIM)\n#define GFP_TRANSHUGE (GFP_TRANSHUGE_LIGHT | __GFP_DIRECT_RECLAIM)\n```", "```\n#cat /proc/slabinfo \nslabinfo - version: 2.1\n# name <active_objs> <num_objs> <objsize> <objperslab> <pagesperslab> : tunables <limit> <batchcount> <sharedfactor> : slabdata <active_slabs> <num_slabs> <sharedavail>dma-kmalloc-8192 0 0 8192 4 8 : tunables 0 0 0 : slabdata 0 0 0\ndma-kmalloc-4096 0 0 4096 8 8 : tunables 0 0 0 : slabdata 0 0 0\ndma-kmalloc-2048 0 0 2048 16 8 : tunables 0 0 0 : slabdata 0 0 0\ndma-kmalloc-1024 0 0 1024 16 4 : tunables 0 0 0 : slabdata 0 0 0\ndma-kmalloc-512 0 0 512 16 2 : tunables 0 0 0 : slabdata 0 0 0\ndma-kmalloc-256 0 0 256 16 1 : tunables 0 0 0 : slabdata 0 0 0\ndma-kmalloc-128 0 0 128 32 1 : tunables 0 0 0 : slabdata 0 0 0\ndma-kmalloc-64 0 0 64 64 1 : tunables 0 0 0 : slabdata 0 0 0\ndma-kmalloc-32 0 0 32 128 1 : tunables 0 0 0 : slabdata 0 0 0\ndma-kmalloc-16 0 0 16 256 1 : tunables 0 0 0 : slabdata 0 0 0\ndma-kmalloc-8 0 0 8 512 1 : tunables 0 0 0 : slabdata 0 0 0\ndma-kmalloc-192 0 0 192 21 1 : tunables 0 0 0 : slabdata 0 0 0\ndma-kmalloc-96 0 0 96 42 1 : tunables 0 0 0 : slabdata 0 0 0\nkmalloc-8192 156 156 8192 4 8 : tunables 0 0 0 : slabdata 39 39 0\nkmalloc-4096 325 352 4096 8 8 : tunables 0 0 0 : slabdata 44 44 0\nkmalloc-2048 1105 1184 2048 16 8 : tunables 0 0 0 : slabdata 74 74 0\nkmalloc-1024 2374 2448 1024 16 4 : tunables 0 0 0 : slabdata 153 153 0\nkmalloc-512 1445 1520 512 16 2 : tunables 0 0 0 : slabdata 95 95 0\nkmalloc-256 9988 10400 256 16 1 : tunables 0 0 0 : slabdata 650 650 0\nkmalloc-192 3561 4053 192 21 1 : tunables 0 0 0 : slabdata 193 193 0\nkmalloc-128 3588 5728 128 32 1 : tunables 0 0 0 : slabdata 179 179 0\nkmalloc-96 3402 3402 96 42 1 : tunables 0 0 0 : slabdata 81 81 0\nkmalloc-64 42672 45184 64 64 1 : tunables 0 0 0 : slabdata 706 706 0\nkmalloc-32 15095 16000 32 128 1 : tunables 0 0 0 : slabdata 125 125 0\nkmalloc-16 6400 6400 16 256 1 : tunables 0 0 0 : slabdata 25 25 0\nkmalloc-8 6144 6144 8 512 1 : tunables 0 0 0 : slabdata 12 12 0\n```", "```\n/**\n * kmalloc - allocate memory. \n * @size: bytes of memory required.\n * @flags: the type of memory to allocate.\n */\n/**\n * kmalloc_array - allocate memory for an array.\n * @n: number of elements.\n * @size: element size.\n * @flags: the type of memory to allocate (see kmalloc).\n */ \n/**\n * kcalloc - allocate memory for an array. The memory is set to zero.\n * @n: number of elements.\n * @size: element size.\n * @flags: the type of memory to allocate (see kmalloc).\n *//**\n * krealloc - reallocate memory. The contents will remain unchanged.\n * @p: object to reallocate memory for.\n * @new_size: bytes of memory are required.\n * @flags: the type of memory to allocate.\n *\n * The contents of the object pointed to are preserved up to the \n * lesser of the new and old sizes. If @p is %NULL, krealloc()\n * behaves exactly like kmalloc(). If @new_size is 0 and @p is not a\n * %NULL pointer, the object pointed to is freed\n */\n void *krealloc(const void *p, size_t new_size, gfp_t flags) /**\n * kmalloc_node - allocate memory from a particular memory node.\n * @size: bytes of memory are required.\n * @flags: the type of memory to allocate.\n * @node: memory node from which to allocate\n */ void *kmalloc_node(size_t size, gfp_t flags, int node) /**\n * kzalloc_node - allocate zeroed memory from a particular memory node.\n * @size: how many bytes of memory are required.\n * @flags: the type of memory to allocate (see kmalloc).\n * @node: memory node from which to allocate\n */ void *kzalloc_node(size_t size, gfp_t flags, int node)\n```", "```\n/**\n * kfree - free previously allocated memory\n * @objp: pointer returned by kmalloc.\n *\n * If @objp is NULL, no operation is performed.\n *\n * Don't free memory not originally allocated by kmalloc()\n * or you will run into trouble.\n */\nvoid kfree(const void *objp) /**\n * kzfree - like kfree but zero memory\n * @p: object to free memory of\n *\n * The memory of the object @p points to is zeroed before freed.\n * If @p is %NULL, kzfree() does nothing.\n *\n * Note: this function zeroes the whole allocated buffer which can be a good\n * deal bigger than the requested buffer size passed to kmalloc(). So be\n * careful when using this function in performance sensitive code.\n */ void kzfree(const void *p)\n```", "```\n# cat /proc/slabinfo \nslabinfo - version: 2.1\n# name <active_objs> <num_objs> <objsize> <objperslab> <pagesperslab> : tunables <limit> <batchcount> <sharedfactor> : slabdata <active_slabs> <num_slabs> <sharedavail>\nsigqueue 100 100 160 25 1 : tunables 0 0 0 : slabdata 4 4 0\nbdev_cache 76 76 832 19 4 : tunables 0 0 0 : slabdata 4 4 0\nkernfs_node_cache 28594 28594 120 34 1 : tunables 0 0 0 : slabdata 841 841 0\nmnt_cache 489 588 384 21 2 : tunables 0 0 0 : slabdata 28 28 0\ninode_cache 15932 15932 568 28 4 : tunables 0 0 0 : slabdata 569 569 0\ndentry 89541 89817 192 21 1 : tunables 0 0 0 : slabdata 4277 4277 0\niint_cache 0 0 72 56 1 : tunables 0 0 0 : slabdata 0 0 0\nbuffer_head 53079 53430 104 39 1 : tunables 0 0 0 : slabdata 1370 1370 0\nvm_area_struct 41287 42400 200 20 1 : tunables 0 0 0 : slabdata 2120 2120 0\nfiles_cache 207 207 704 23 4 : tunables 0 0 0 : slabdata 9 9 0\nsignal_cache 420 420 1088 30 8 : tunables 0 0 0 : slabdata 14 14 0\nsighand_cache 289 315 2112 15 8 : tunables 0 0 0 : slabdata 21 21 0\ntask_struct 750 801 3584 9 8 : tunables 0 0 0 : slabdata 89 89 0\n```", "```\n/*\n * kmem_cache_create - Create a cache.\n * @name: A string which is used in /proc/slabinfo to identify this cache.\n * @size: The size of objects to be created in this cache.\n * @align: The required alignment for the objects.\n * @flags: SLAB flags\n * @ctor: A constructor for the objects.\n *\n * Returns a ptr to the cache on success, NULL on failure.\n * Cannot be called within a interrupt, but can be interrupted.\n * The @ctor is run when new pages are allocated by the cache.\n *\n */\nstruct kmem_cache * kmem_cache_create(const char *name, size_t size, size_t align,\n                                      unsigned long flags, void (*ctor)(void *))\n```", "```\nkmem_cache_create():\n```", "```\n/* net/core/skbuff.c */\n\nstruct kmem_cache *skbuff_head_cache;\nskbuff_head_cache = kmem_cache_create(\"skbuff_head_cache\",sizeof(struct sk_buff), 0, \n                                       SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);                                                                                                   \n```", "```\n SLAB_CONSISTENCY_CHECKS /* DEBUG: Perform (expensive) checks o alloc/free */\n SLAB_RED_ZONE /* DEBUG: Red zone objs in a cache */\n SLAB_POISON /* DEBUG: Poison objects */\n SLAB_HWCACHE_ALIGN  /* Align objs on cache lines */\n SLAB_CACHE_DMA  /* Use GFP_DMA memory */\n SLAB_STORE_USER  /* DEBUG: Store the last owner for bug hunting */\n SLAB_PANIC  /* Panic if kmem_cache_create() fails */\n\n```", "```\n/**\n * kmem_cache_alloc - Allocate an object\n * @cachep: The cache to allocate from.\n * @flags: GFP mask.\n *\n * Allocate an object from this cache. The flags are only relevant\n * if the cache has no available objects.\n */\nvoid *kmem_cache_alloc(struct kmem_cache *cachep, gfp_t flags);\n\n/**\n * kmem_cache_alloc_node - Allocate an object on the specified node\n * @cachep: The cache to allocate from.\n * @flags: GFP mask.\n * @nodeid: node number of the target node.\n *\n * Identical to kmem_cache_alloc but it will allocate memory on the given\n * node, which can improve the performance for cpu bound structures.\n *\n * Fallback to other node is possible if __GFP_THISNODE is not set.\n */\nvoid *kmem_cache_alloc_node(struct kmem_cache *cachep, gfp_t flags, int nodeid); /**\n * kmem_cache_free - Deallocate an object\n * @cachep: The cache the allocation was from.\n * @objp: The previously allocated object.\n *\n * Free an object which was previously allocated from this\n * cache.\n */ void kmem_cache_free(struct kmem_cache *cachep, void *objp);\n```", "```\nstruct kmem_cache {\n struct kmem_cache_cpu __percpu *cpu_slab;\n /* Used for retriving partial slabs etc */\n unsigned long flags;\n unsigned long min_partial;\n int size; /* The size of an object including meta data */\n int object_size; /* The size of an object without meta data */\n int offset; /* Free pointer offset. */\n int cpu_partial; /* Number of per cpu partial objects to keep around */\n struct kmem_cache_order_objects oo;\n\n /* Allocation and freeing of slabs */\n struct kmem_cache_order_objects max;\n struct kmem_cache_order_objects min;\n gfp_t allocflags; /* gfp flags to use on each alloc */\n int refcount; /* Refcount for slab cache destroy */\n void (*ctor)(void *);\n int inuse; /* Offset to metadata */\n int align; /* Alignment */\n int reserved; /* Reserved bytes at the end of slabs */\n const char *name; /* Name (only for display!) */\n struct list_head list; /* List of slab caches */\n int red_left_pad; /* Left redzone padding size */\n ...\n ...\n ...\n struct kmem_cache_node *node[MAX_NUMNODES];\n};\n```", "```\nstruct kmem_cache_node {\n spinlock_t list_lock;\n ...\n ...\n#ifdef CONFIG_SLUB\n unsigned long nr_partial;\n struct list_head partial;\n#ifdef CONFIG_SLUB_DEBUG\n atomic_long_t nr_slabs;\n atomic_long_t total_objects;\n struct list_head full;\n#endif\n#endif \n};\n```", "```\nstruct page {\n      ...\n      ...\n     union {\n      pgoff_t index; /* Our offset within mapping. */\n      void *freelist; /* sl[aou]b first free object */\n   };\n     ...\n     ...\n   struct {\n          union {\n                  ...\n                   struct { /* SLUB */\n                          unsigned inuse:16;\n                          unsigned objects:15;\n                          unsigned frozen:1;\n                     };\n                   ...\n                };\n               ...\n          };\n     ...\n     ...\n       union {\n             ...\n             ...\n             struct kmem_cache *slab_cache; /* SL[AU]B: Pointer to slab */\n         };\n    ...\n    ...\n};\n```", "```\nstruct kmem_cache_cpu {\n     void **freelist; /* Pointer to next available object */\n     unsigned long tid; /* Globally unique transaction id */\n     struct page *page; /* The slab from which we are allocating */\n     struct page *partial; /* Partially allocated frozen slabs */\n     #ifdef CONFIG_SLUB_STATS\n        unsigned stat[NR_SLUB_STAT_ITEMS];\n     #endif\n};\n```", "```\n/**\n  * vmalloc  -  allocate virtually contiguous memory\n  * @size:   -  allocation size\n  * Allocate enough pages to cover @size from the page level\n  * allocator and map them into contiguous kernel virtual space.\n  *\n  */\n    void *vmalloc(unsigned long size) \n/**\n  * vzalloc - allocate virtually contiguous memory with zero fill\n1 * @size:  allocation size\n  * Allocate enough pages to cover @size from the page level\n  * allocator and map them into contiguous kernel virtual space.\n  * The memory allocated is set to zero.\n  *\n  */\n void *vzalloc(unsigned long size) \n/**\n  * vmalloc_user - allocate zeroed virtually contiguous memory for userspace\n  * @size: allocation size\n  * The resulting memory area is zeroed so it can be mapped to userspace\n  * without leaking data.\n  */\n    void *vmalloc_user(unsigned long size) /**\n  * vmalloc_node  -  allocate memory on a specific node\n  * @size:          allocation size\n  * @node:          numa node\n  * Allocate enough pages to cover @size from the page level\n  * allocator and map them into contiguous kernel virtual space.\n  *\n  */\n    void *vmalloc_node(unsigned long size, int node) /**\n  * vfree  -  release memory allocated by vmalloc()\n  * @addr:          memory base address\n  * Free the virtually continuous memory area starting at @addr, as\n  * obtained from vmalloc(), vmalloc_32() or __vmalloc(). If @addr is\n  * NULL, no operation is performed.\n  */\n void vfree(const void *addr) /**\n  * vfree_atomic  -  release memory allocated by vmalloc()\n  * @addr:          memory base address\n  * This one is just like vfree() but can be called in any atomic context except NMIs.\n  */\n    void vfree_atomic(const void *addr)\n```"]
- en: Using GlusterFS on the Cloud Infrastructure
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在云基础设施上使用GlusterFS
- en: With a good understanding of the core concepts of GlusterFS, we can now dive
    into the installation, configuration, and optimization of a storage cluster.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在对GlusterFS的核心概念有了很好的理解之后，我们现在可以深入到存储集群的安装、配置和优化中。
- en: We will be installing GlusterFS on a three-node cluster using Azure as the cloud
    provider for this example. However, the concepts can also be applied to other
    cloud providers.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在Azure上使用云提供商搭建一个三节点集群的GlusterFS，作为本示例的云提供商。然而，这些概念也可以应用于其他云提供商。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Configuring GlusterFS backend storage
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置GlusterFS后端存储
- en: Installing and configuring GlusterFS
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和配置GlusterFS
- en: Setting up volumes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置卷
- en: Optimizing performance
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化性能
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Here''s the list of technical resources for this chapter:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的技术资源列表如下：
- en: 'A detailed view of Azure **virtual machine** (**VM**) sizes:'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure虚拟机（VM）大小的详细视图：
- en: '[https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes-storage](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes-storage)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes-storage](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes-storage)'
- en: 'A detailed view of Azure disk sizes and types:'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure磁盘大小和类型的详细视图：
- en: '[https://azure.microsoft.com/en-us/pricing/details/managed-disks/](https://azure.microsoft.com/en-us/pricing/details/managed-disks/)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://azure.microsoft.com/en-us/pricing/details/managed-disks/](https://azure.microsoft.com/en-us/pricing/details/managed-disks/)'
- en: 'The main page for the ZFS on Linux project:'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZFS on Linux项目的主页：
- en: '[https://github.com/zfsonlinux/zfs/wiki/RHEL-and-CentOS](https://github.com/zfsonlinux/zfs/wiki/RHEL-and-CentOS)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/zfsonlinux/zfs/wiki/RHEL-and-CentOS](https://github.com/zfsonlinux/zfs/wiki/RHEL-and-CentOS)'
- en: 'GlusterFS installation guide for CentOS:'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CentOS上的GlusterFS安装指南：
- en: '[https://wiki.centos.org/HowTos/GlusterFSonCentOS](https://wiki.centos.org/HowTos/GlusterFSonCentOS)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://wiki.centos.org/HowTos/GlusterFSonCentOS](https://wiki.centos.org/HowTos/GlusterFSonCentOS)'
- en: 'GlusterFS quick start guide on the Gluster website:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GlusterFS在Gluster网站上的快速入门指南：
- en: '[https://docs.gluster.org/en/latest/Quick-Start-Guide/Quickstart/](https://docs.gluster.org/en/latest/Quick-Start-Guide/Quickstart/)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.gluster.org/en/latest/Quick-Start-Guide/Quickstart/](https://docs.gluster.org/en/latest/Quick-Start-Guide/Quickstart/)'
- en: 'GlusterFS setting up volumes on the administrators guide:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在管理员指南上设置GlusterFS卷：
- en: '[https://docs.gluster.org/en/latest/Administrator%20Guide/Setting%20Up%20Volumes/](https://docs.gluster.org/en/latest/Administrator%20Guide/Setting%20Up%20Volumes/)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.gluster.org/en/latest/Administrator%20Guide/Setting%20Up%20Volumes/](https://docs.gluster.org/en/latest/Administrator%20Guide/Setting%20Up%20Volumes/)'
- en: 'GlusterFS tuning volumes for better performance:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GlusterFS调整卷以获得更好的性能：
- en: '[https://docs.gluster.org/en/latest/Administrator%20Guide/Managing%20Volumes/#tuning-options](https://docs.gluster.org/en/latest/Administrator%20Guide/Managing%20Volumes/#tuning-options)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.gluster.org/en/latest/Administrator%20Guide/Managing%20Volumes/#tuning-options](https://docs.gluster.org/en/latest/Administrator%20Guide/Managing%20Volumes/#tuning-options)'
- en: Setting up the bricks used for backend storage
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置用于后端存储的砖
- en: 'The following is the list of components that we''ll be using:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们将使用的组件列表：
- en: Azure L4s VM with 4vCPUs and 32 GB of RAM
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure L4s VM，具有4个vCPU和32GB的RAM
- en: Four S10 128 GB Disks per VM
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个VM有四个S10 128GB的磁盘
- en: CentOS 7.5
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CentOS 7.5
- en: ZFS on Linux as the filesystem for the bricks
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZFS on Linux作为砖的文件系统
- en: A single RAID 0 group with four disks
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个由四个磁盘组成的单个RAID 0组
- en: GlusterFS 4.1
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GlusterFS 4.1
- en: Azure deployment
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Azure部署
- en: Before going into the details of how to configure the bricks, we first need
    to deploy the nodes in Azure. For this example, we are using the storage optimized
    VM series, or L-series. One thing that is worth mentioning is that Azure has a
    30-day free trial that can be used for testing before committing to any deployment.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入讨论如何配置砖之前，我们首先需要在Azure中部署节点。在本示例中，我们使用存储优化的VM系列，即L系列。值得一提的是，Azure提供了一个30天的免费试用期，可以用于在承诺任何部署之前进行测试。
- en: In Azure, performance is defined on several levels. The first level is the VM
    limit, which is the maximum performance that the VM allows. The L-series family
    provides the correct balance of price versus performance as these VMs are optimized
    to deliver higher **input/output operations per second** (**IOPS**)and throughput
    rather than delivering high compute or memory resources. The second level on which
    performance is defined is through the disks that are attached to the VM. For this
    example, we will be using standard **hard disk drives** (**HDD**) for a cost-effective
    solution. If more performance is needed, the disks can always be migrated to premium
    **solid-state drives** (**SSD**) storage.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在Azure中，性能在几个级别上进行定义。第一级是VM限制，即VM允许的最大性能。L系列家族提供了价格与性能的正确平衡，因为这些VM经过优化，提供了更高的IOPS和吞吐量，而不是提供高计算或内存资源。性能定义的第二级是通过连接到VM的磁盘。在本示例中，我们将使用标准的硬盘驱动器作为一种经济实惠的解决方案。如果需要更高的性能，磁盘可以随时迁移到高级的固态硬盘存储。
- en: The exact VM size for this example will be L4s, which provides four vCPUs and
    32 GB of RAM, and is enough for a small storage cluster for general purposes.
    With a maximum of 125 MB/s and 5k IOPS, it still retains respectable performance
    when correctly configured.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的确切VM大小将是L4s，提供四个vCPU和32GB的RAM，足够用于一般用途的小型存储集群。在正确配置时，最大可达125MB/s和5k IOPS，仍然保持可观的性能。
- en: A new generation of storage optimized VMs has been recently released, offering
    a locally-accessible NVMe SSD of 2 TB. Additionally, it provides increased core
    count and memory, making these new VMs ideal for a GlusterFS setup with **Z file
    system** (**ZFS**). The new L8s_v2 VM can be used for this specific setup, and
    the sizes and specifications can be seen on the product page ([https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes-storage#lsv2-series](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes-storage#lsv2-series)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最近推出了一代新的面向存储优化的VM，提供了一个本地可访问的2 TB NVMe SSD。此外，它提供了增加的核心数和内存，使这些新的VM成为GlusterFS设置与**Z文件系统**（**ZFS**）的理想选择。新的L8s_v2
    VM可以用于这个特定的设置，产品页面上可以看到大小和规格（[https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes-storage#lsv2-series](https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes-storage#lsv2-series)）。
- en: 'The following screenshot shows the Availability set, Current fault domain,
    and Current update domain settings:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了可用性集、当前故障域和当前更新域的设置：
- en: '![](img/8b322180-550e-4559-ba84-4edffa7ad8cd.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8b322180-550e-4559-ba84-4edffa7ad8cd.png)'
- en: When deploying a GlusterFS setup in Azure, make sure that each node lands on
    a different update and fault domain. This is done through the use of availability
    sets (refer to the preceding screenshot). Doing so ensures that if the platform
    restarts a node, the others remain up and serving data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在Azure中部署GlusterFS设置时，请确保每个节点都落在不同的更新和故障域上。这是通过使用可用性集来实现的（参考前面的截图）。这样做可以确保如果平台重新启动一个节点，其他节点仍然保持运行并提供数据。
- en: 'Finally, for the Azure setup, we need **512 GB** per node for a total of 1.5
    TB raw, or 1 TB usable space. The most cost-effective way to achieve this is by
    using a single **S20 512 GB** disk, since the price per gigabyte per month is
    approximately **$0.04**. Going down the route of a single disk will impact on
    performance, as a single standard disk only provides a maximum of 500 IOPS and
    60 MB/s. Considering performance and accepting the fact that we will lose a bit
    of efficiency in the cost department, we will be using four **S10 128** GB disks
    in a single RAID0 group. The price per month per gigabyte of an **S10** disk is
    **$0.05**, compared to **$0.04** per month for an **S20** disk. You can refer
    to the following table, where the calculation is done based on the cost of the
    managed disk divided by its respective size:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于Azure设置，我们需要每个节点**512 GB**，总共1.5 TB原始空间，或1 TB可用空间。实现这一点的最具成本效益的方法是使用单个**S20
    512 GB**磁盘，因为每月每千兆字节的价格约为**$0.04**。选择单个磁盘会影响性能，因为单个标准磁盘只提供最大500 IOPS和60 MB/s。考虑性能并接受我们在成本方面会失去一些效率的事实，我们将使用四个**S10
    128** GB磁盘组成单个RAID0组。**S10**磁盘每月每千兆字节的价格为**$0.05**，而**S20**磁盘每月每千兆字节的价格为**$0.04**。您可以参考以下表格，其中计算是基于托管磁盘的成本除以其相应的大小进行的：
- en: '![](img/4bf1ef53-b1a8-4381-8d20-0db862ac1bc6.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bf1ef53-b1a8-4381-8d20-0db862ac1bc6.png)'
- en: Make sure that all three nodes are deployed on the same region and the same
    resource group for consistency.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 确保所有三个节点都部署在同一区域和相同的资源组中，以保持一致性。
- en: ZFS as the backend for the bricks
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ZFS作为砖的后端
- en: We spoke about ZFS in a [Chapter 3](cb87a072-b90e-4e96-9ba1-54424c2a0231.xhtml), *Architecting
    a Storage Cluster*. ZFS is a filesystem that was developed by Sun Microsystems
    and was later acquired by Oracle. The project was later made open source and was
    ported to Linux. Although the project is still in beta, most of the features work
    fine and the majority of the problems have been ruled out—the project is now focused
    on adding new features.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第3章](cb87a072-b90e-4e96-9ba1-54424c2a0231.xhtml)中讨论了ZFS，*构建存储集群*。ZFS是由Sun
    Microsystems开发的文件系统，后来被Oracle收购。该项目后来被开源，并移植到了Linux。尽管该项目仍处于测试阶段，但大多数功能都运行良好，大部分问题已经排除，该项目现在专注于添加新功能。
- en: ZFS is a software layer that combines disk management, logical volumes, and
    a filesystem all in one. Advanced features such as compression, **adaptive replacement
    cache** (**ARC**), deduplication, and snapshots make it ideal to work with GlusterFS
    as the backend for the bricks.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS是一个软件层，将磁盘管理、逻辑卷和文件系统合而为一。诸如压缩、**自适应替换缓存**（**ARC**）、重复数据删除和快照等高级功能使其成为与GlusterFS作为砖的后端一起使用的理想选择。
- en: Installing ZFS
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装ZFS
- en: Let's start by installing ZFS; there are some dependencies, such as **dynamic
    kernel modules** (**DKMS**), that live in the EPEL repository.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从安装ZFS开始；有一些依赖项，比如**动态内核模块**（**DKMS**），它们存放在EPEL存储库中。
- en: Note that most of the commands that run here are assumed to be running as root;
    the commands can be run as the non-root account by prefacing `sudo` before each.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里运行的大多数命令都假定是以root身份运行的；可以在每个命令之前加上`sudo`作为非root帐户运行命令。
- en: 'To install the required components, we can use the following commands:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装所需的组件，我们可以使用以下命令：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will use the following command:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用以下命令：
- en: '[PRE1]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following commands are used to enable the ZFS components:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令用于启用ZFS组件：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Configuring the zpools
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置zpools
- en: With ZFS installed and enabled, we can now create the zpools. Zpool is the name
    given to volumes that are created within ZFS.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 安装并启用ZFS后，我们现在可以创建zpools。Zpool是在ZFS中创建的卷的名称。
- en: 'Since we will be using a single RAID 0 group consisting of four disks, we can
    create a zpool named `brick1`; this needs to be done on all three nodes. Additionally,
    let''s create a directory named `bricks` that lives under the root (`/`); this
    directory houses the bricks under a directory with the brick name. The command
    required to do this is as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用由四个磁盘组成的单个RAID 0组，我们可以创建一个名为`brick1`的zpool；这需要在所有三个节点上完成。此外，让我们创建一个名为`bricks`的目录，位于根目录（`/`）下；这个目录包含了一个带有砖名称的目录下的砖。执行此操作所需的命令如下：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This creates the directory tree, as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建目录树，如下所示：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To further explain the command, `brick1` is the name of the zpool. Then, we
    indicate the path to the disks. In this example, we are using the ID of the disks
    since this avoids problems if the disks change order. While ZFS is not affected
    for disks in a different order, it is better to avoid problems by using an ID
    that will never change.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步解释该命令，`brick1`是zpool的名称。然后，我们指示磁盘的路径。在本例中，我们使用磁盘的ID，因为这样可以避免磁盘更改顺序时出现问题。虽然ZFS不受磁盘顺序不同的影响，但最好通过使用永远不会更改的ID来避免问题。
- en: ZFS can use the entire disk because it creates the required partitions automatically.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS可以使用整个磁盘，因为它会自动创建所需的分区。
- en: 'With the `zpool` instance created, we can check whether it has completed correctly
    by using the `zpool status` command:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 创建`zpool`实例后，我们可以使用`zpool status`命令检查是否已正确完成：
- en: '![](img/c86f5d0e-9ca7-4f0d-ad2c-804cbee4ff84.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c86f5d0e-9ca7-4f0d-ad2c-804cbee4ff84.png)'
- en: 'Let''s enable compression and change the mount point of the pool to the previously
    created directory. To do this, run the following command:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们启用压缩并将池的挂载点更改为先前创建的目录。要执行此操作，请运行以下命令：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You will also need to run the following command:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要运行以下命令：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The first command enables compression with the `lz4` algorithm, which has a
    low CPU overhead. The second command changes the mount point of the zpool. Make
    sure that you use the correct name of the pool when changing the settings.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令使用`lz4`算法启用压缩，其CPU开销较低。第二个命令更改了zpool的挂载点。在更改设置时，请确保使用正确的池名称。
- en: 'After doing this, we should have the ZFS volume mounted under `/bricks/brick1`,
    as shown in the `df` command:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此操作后，我们应该在`df`命令中看到ZFS卷已挂载在`/bricks/brick1`下：
- en: '![](img/f8102aba-efc0-42ba-95a7-62420a947695.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f8102aba-efc0-42ba-95a7-62420a947695.png)'
- en: 'We need to create a directory on the recently added mount point to use as the
    brick; the consensus is to use the name of the volume. In this case, we''ll name
    the volume `gvol1`, and simply create the directory:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在最近添加的挂载点上创建一个目录以用作brick；共识是使用卷的名称。在这种情况下，我们将卷命名为`gvol1`，然后简单地创建目录：
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This needs to be done on all the nodes.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要在所有节点上完成。
- en: Adding the ZFS cache to the pool (optional)
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将ZFS缓存添加到池中（可选）
- en: With Azure, every single VM has a temporary resource drive. The performance
    of this temporary resource drive is considerably higher than the data disks that
    are added to it. This drive is ephemeral, meaning the data is wiped once the VM
    is deallocated; this should work very well as a read cache drive since there is
    no need to persistently keep the data across reboots.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在Azure中，每个VM都有一个临时资源驱动器。这个临时资源驱动器的性能比添加到它的数据磁盘要高得多。这个驱动器是临时的，这意味着一旦VM被取消分配，数据就会被擦除；由于不需要在重新启动时持续保留数据，这应该非常适合作为读取缓存驱动器。
- en: Since the drive is wiped with every `stop/deallocate/start` cycle, we need to
    tweak some things with the unit files for ZFS to allow the disk to be added on
    every reboot. The drive will always be `/dev/sdb`, and since there is no need
    to create a partition on it, we can simply tell ZFS to add it as a new disk each
    time the system boots.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于驱动器在每次`stop/deallocate/start`周期中都会被擦除，因此我们需要调整一些ZFS的单元文件，以允许在每次重新启动时添加磁盘。驱动器将始终是`/dev/sdb`，并且由于不需要在其上创建分区，因此我们可以简单地告诉ZFS在系统引导时每次将其添加为新磁盘。
- en: 'This can be achieved by editing the `systemd` unit for `zfs-mount.service`,
    which is located under `/usr/lib/systemd/system/zfs-mount.service`. The problem
    with this approach is that the ZFS updates will overwrite the changes made to
    the preceding unit. One solution to this problem is to run `sudo systemctl edit
    zfs-mount` and add the following code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过编辑位于`/usr/lib/systemd/system/zfs-mount.service`下的`zfs-mount.service`的`systemd`单元来实现。这种方法的问题在于ZFS更新将覆盖对先前单元所做的更改。解决此问题的一种方法是运行`sudo
    systemctl edit zfs-mount`并添加以下代码：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To apply the changes, run the following command:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用更改，请运行以下命令：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now that we have ensured that the cache drive will be added after every reboot,
    we need to change an Azure-specific configuration with the Linux agent that runs
    on Azure VMs. This agent is in charge of creating the temporary resource drive,
    and since we''ll be using it for another purpose, we need to tell the agent not
    to create the ephemeral disk. To achieve this, we need to edit the file located
    in `/etc/waagent.conf` and look for the following line:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已确保缓存驱动器将在每次重新启动后添加，我们需要更改在Azure VM上运行的Linux代理的特定于Azure的配置。该代理负责创建临时资源驱动器，由于我们将用于其他目的，因此我们需要告诉代理不要创建临时磁盘。为实现此目的，我们需要编辑位于`/etc/waagent.conf`中的文件，并查找以下行：
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You will then need to change it to the following line:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您需要将其更改为以下行：
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After doing this, we can add the cache drive to the pool by running the following
    command:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此操作后，我们可以通过运行以下命令将缓存驱动器添加到池中：
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `-f` option must only be used the first time because it removes the previously
    created filesystem. Note that the `stop/deallocate/start` cycle of the VM is required
    to stop the agent from formatting the resource disk, as it gets an `ext4` filesystem
    by default.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`-f`选项只能在第一次使用时使用，因为它会删除先前创建的文件系统。请注意，需要对VM进行`stop/deallocate/start`周期以阻止代理程序默认格式化资源磁盘，因为它默认会得到一个`ext4`文件系统。'
- en: The previous process can also be applied to the newer Ls_v2 VMs, which use the
    much faster NVMe drives, such as the L8s_v2; simply replace `/dev /sdb` with `/dev/nvme0n1`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的过程也可以应用于使用更快的NVMe驱动器的新Ls_v2 VM，例如L8s_v2；只需将`/dev /sdb`替换为`/dev/nvme0n1`。
- en: 'You can verify that the cache disk was added as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以验证缓存磁盘是否已添加如下：
- en: '![](img/abbd1b75-a9b2-4338-92ea-3291dfa89a29.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/abbd1b75-a9b2-4338-92ea-3291dfa89a29.png)'
- en: As we'll be using a single RAID group, this will be used as a read cache for
    the entire brick, allowing better performance when reading the files of the GlusterFS
    volume.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用单个RAID组，因此这将用作整个brick的读取缓存，从而在读取GlusterFS卷的文件时提供更好的性能。
- en: Installing GlusterFS on the nodes
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在节点上安装GlusterFS
- en: With each node having the bricks already configured, we can finally install
    GlusterFS. The installation is relatively straightforward and requires just a
    couple of commands.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点已经配置了砖块后，我们最终可以安装GlusterFS。安装相对简单，只需要几个命令。
- en: Installing the packages
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装软件包
- en: 'We''ll be using the packages provided by CentOS. To install GlusterFS, we first
    install the repository as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用CentOS提供的软件包。要安装GlusterFS，我们首先安装存储库如下：
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we install the `glusterfs-server` package:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们安装`glusterfs-server`软件包：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We then make sure the `glusterd` service is enabled and started:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，确保启用并启动`glusterd`服务：
- en: '![](img/c41c81ee-4ef5-449c-8fe0-53dbf9fc8167.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c41c81ee-4ef5-449c-8fe0-53dbf9fc8167.png)'
- en: These commands need to be run on each of the nodes that will be part of the
    cluster; this is because each node requires the packages and services to be enabled.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令需要在将成为集群一部分的每个节点上运行；这是因为每个节点都需要启用软件包和服务。
- en: Creating the trusted pool
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建受信任的池
- en: Finally, we need to create a trusted pool. A trusted pool is a list of nodes
    that will be part of the cluster, where each Gluster node trusts the other, thus
    allowing for the creation of volumes.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要创建一个受信任的池。受信任的池是将成为集群一部分的节点列表，其中每个Gluster节点都信任其他节点，从而允许创建卷。
- en: 'To create the trusted pool, run the following code from the first node:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建受信任的池，请从第一个节点运行以下代码：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You can verify that the nodes show up as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按以下方式验证节点是否显示：
- en: '![](img/469305e9-615d-45a6-9031-742db5231517.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/469305e9-615d-45a6-9031-742db5231517.png)'
- en: The command can be run from any node, and the hostnames or IPs need to be modified
    to include the others. In this case, I have added the IP addresses of each of
    the nodes onto the `/etc/hosts` file to allow for easy configuration. Ideally,
    the hostnames should be registered with the DNS server for the name resolution
    within the network.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令可以从任何节点运行，并且主机名或IP地址需要修改以包括其他节点。在这种情况下，我已将每个节点的IP地址添加到`/etc/hosts`文件中，以便进行简单的配置。理想情况下，主机名应该在DNS服务器上注册，以便在网络内进行名称解析。
- en: After the installation, the `gluster` nodes should allow volumes to be created.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，`gluster`节点应允许创建卷。
- en: Creating the volumes
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建卷
- en: We have now reached the point where we can create the volumes; this is because
    we have the bricks configured and the necessary packages for GlusterFS to work.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经到达可以创建卷的阶段；这是因为我们已经配置了砖块并且安装了GlusterFS所需的软件包。
- en: Creating a dispersed volume
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建分散卷
- en: We'll be using a dispersed volume type across three nodes, giving a good balance
    of high availability and performance. The raw space of all of the nodes combined
    will be around 1.5 TB; however, the distributed volume will have a usable space
    of approximately 1 TB.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在三个节点之间使用分散卷类型，以实现高可用性和性能的良好平衡。所有节点的原始空间总共约为1.5 TB；但是，分布式卷的可用空间将约为1 TB。
- en: 'To create a dispersed volume, use the following code:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建分散卷，请使用以下代码：
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then, start the volume using the following code:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用以下代码启动卷：
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Make sure that it starts correctly by using the following code:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 确保使用以下代码正确启动：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The volume should show up now as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应该显示卷如下：
- en: '![](img/a87c65a6-747f-4151-92ce-23dc2caf1991.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a87c65a6-747f-4151-92ce-23dc2caf1991.png)'
- en: Mounting the volume
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挂载卷
- en: The volume is now created and can be mounted on the clients; the preferred method
    for doing this is by using the native `glusterfs-fuse` client, which allows for
    automatic failovers in the event that one of the nodes goes down.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 卷现在已创建，并且可以在客户端上挂载；最好的方法是使用本机的`glusterfs-fuse`客户端进行此操作，它允许在其中一个节点宕机时自动进行故障转移。
- en: 'To install the `gluster-fuse` client, use the following code:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装`gluster-fuse`客户端，请使用以下代码：
- en: '[PRE19]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, let''s create a directory under root called `gvol1`:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在根目录下创建一个名为`gvol1`的目录：
- en: '[PRE20]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, we can mount the GlusterFS volume on the client as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以按以下方式在客户端上挂载GlusterFS卷：
- en: '[PRE21]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: It doesn't matter which node you specify, as the volume can be accessed from
    any of them. In the event that one of the nodes goes down, the client will automatically
    redirect I/O requests to the remaining nodes.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您指定的节点并不重要，因为可以从任何节点访问卷。如果其中一个节点宕机，客户端将自动将I/O请求重定向到剩余的节点。
- en: Optimizing performance
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化性能
- en: With the volume created and mounted, we can tweak some parameters to get the
    best performance. Mainly, performance tuning can be done on the filesystem level
    (in this case, ZFS), and on the GlusterFS volume level.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 创建和挂载卷后，我们可以调整一些参数以获得最佳性能。主要是在文件系统级别（在本例中为ZFS）和GlusterFS卷级别上进行性能调整。
- en: GlusterFS tuning
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GlusterFS调优
- en: 'Here, the main variable is `performance.cache-size`. This setting specifies
    the amount of RAM to be allocated as a read cache for the GlusterFS volume. By
    default, it is set to 32 MB, which is fairly low. Given that the selected VM has
    enough RAM, this can be bumped to 4 GB using the following command:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，主要变量是`performance.cache-size`。此设置指定要分配为GlusterFS卷的读取缓存的RAM量。默认情况下，它设置为32
    MB，这相当低。鉴于所选的VM具有足够的RAM，可以使用以下命令将其提高到4 GB：
- en: '[PRE22]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Another essential parameter once the cluster starts growing is `performance.io-thread-count`.
    This controls how many I/O threads are spawned by the volume. The default is `16`
    threads, which are enough for small-to-medium clusters. However, once the cluster
    size starts growing, this can be doubled. To change the setting, use the following
    command:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦集群开始增长，另一个重要的参数是`performance.io-thread-count`。这控制卷生成多少I/O线程。默认值为`16`个线程，对于中小型集群来说足够了。但是，一旦集群规模开始增大，可以将其加倍。要更改此设置，请使用以下命令：
- en: '[PRE23]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This setting should be tested to check whether increasing the count improves
    the performance or not.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 应该测试此设置，以检查增加计数是否改善性能。
- en: ZFS
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ZFS
- en: 'We''ll be primarily changing two settings: ARC and the L2ARC feed performance.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将主要更改两个设置：ARC和L2ARC feed性能。
- en: ARC
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ARC
- en: The primary setting for ZFS is its read cache, called ARC. Allowing more RAM
    to be allocated to ZFS increases read performance substantially. Since we have
    already allocated 4 GB to the Gluster volume read cache and the VM has 32 GB available,
    we can allocate 26 GB of RAM to ZFS, which will leave approximately 2 GB for the
    OS.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS的主要设置是其读取缓存，称为ARC。允许分配更多的RAM给ZFS可以大大提高读取性能。由于我们已经为Gluster卷读取缓存分配了4GB，并且VM有32GB可用，我们可以为ZFS分配26GB的RAM，这将留下大约2GB给操作系统。
- en: 'To change the maximum size that is allowed for ARC, use the following code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 要更改ARC允许的最大大小，使用以下代码：
- en: '[PRE24]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here, the number is the amount of RAM in bytes, in this case, 26 GB. Doing
    this changes the setting on the fly but does not make it boot persistent. To have
    the settings applied on boot, create a file named `/etc/modprobe.d/zfs.conf` and
    add the following values:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，数字是以字节为单位的RAM数量，本例中为26GB。这样做会即时更改设置，但不会使其持久化。要在引导时应用设置，创建一个名为`/etc/modprobe.d/zfs.conf`的文件，并添加以下值：
- en: '[PRE25]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: By doing this, you can make the changes persist across boots.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做，您可以使更改在引导时持久化。
- en: L2ARC
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: L2ARC
- en: 'L2ARC refers to a second level of read cache; this is the cache disk that was
    previously added to the zpools. Changing the speed in which data is fed to the
    cache helps by decreasing the amount of time it takes to warm or fill up the cache
    with constantly accessed files. The setting is specified in bytes per second.
    To change it you can use the following command:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: L2ARC是指第二级读取缓存；这是先前添加到zpools的缓存磁盘。改变数据馈送到缓存的速度有助于减少将常访问文件填充到缓存中所需的时间。该设置以每秒字节为单位指定。要更改它，可以使用以下命令：
- en: '[PRE26]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As with the previous setting, this is applied to the running kernel. To make
    it boot-persistent, add the following line to the `/etc/modprobe.d/zfs.conf` file:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的设置一样，这适用于运行的内核。要使其持久化，将以下行添加到`/etc/modprobe.d/zfs.conf`文件中：
- en: '[PRE27]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This setting allows a maximum of 256 MB/s of L2ARC feed; the setting should
    be increased to at least double if the VM size is changed to a higher tier.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 此设置允许最大256MB/s的L2ARC feed；如果VM大小更改为更高级别，则应将设置至少增加一倍。
- en: 'In the end, you should end up with a file on each node that looks like this:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您应该在每个节点上得到一个类似这样的文件：
- en: '![](img/a1e838b2-e494-45bf-a92e-b0fa4e54513d.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1e838b2-e494-45bf-a92e-b0fa4e54513d.png)'
- en: Regarding ZFS, on other types of filesystems, changing the block size helps
    to gain some performance. ZFS has a variable block size, allowing for small and
    big files to achieve similar results, so there is no need to change this setting.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 关于ZFS，在其他类型的文件系统上，改变块大小有助于提高性能。ZFS具有可变的块大小，允许小文件和大文件实现类似的结果，因此无需更改此设置。
- en: Summary
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: After installing ZFS, creating the zpools, installing GlusterFS, and creating
    the volumes, we have ended up with a solution with respectable performance that
    can sustain a node failure and still serve data to its clients.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 安装ZFS、创建zpools、安装GlusterFS和创建卷后，我们得到了一个性能可观的解决方案，可以承受节点故障并仍然为其客户端提供数据。
- en: For the setup, we used Azure as the cloud provider. While each provider has
    their own set of configuration challenges, the core concepts can be used on other
    cloud providers as well.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对于设置，我们使用Azure作为云提供商。虽然每个提供商都有自己的一套配置挑战，但核心概念也可以在其他云提供商上使用。
- en: However, this design has a disadvantage. When adding new disks to the zpools,
    the stripes don't align, causing new reads and writes to yield lower performance.
    This problem can be avoided by adding an entire set of disks at once; lower read
    performance is mostly covered by the read cache on RAM (ARC) and the cache disk
    (L2ARC).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种设计有一个缺点。当向zpools添加新磁盘时，条带不对齐，导致新的读取和写入产生较低的性能。通过一次添加整套磁盘可以避免这个问题；较低的读取性能大部分由RAM上的读取缓存(ARC)和缓存磁盘(L2ARC)覆盖。
- en: For GlusterFS, we used a dispersed layout that balances performance with high
    availability. In this three-node cluster setup, we can sustain a node failure
    without holding I/O from the clients.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GlusterFS，我们使用了一个平衡性能和高可用性的分散布局。在这个三节点集群设置中，我们可以承受一个节点故障，而不会阻止来自客户端的I/O。
- en: The main takeaway is to have a critical mindset when designing a solution. In
    this example, we worked with the resources that we had available to achieve a
    configuration that would perform to specification and utilize what we provided.
    Make sure that you always ask yourself how this setting will impact the result,
    and how you can change it to be more efficient.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的要点是在设计解决方案时要有批判性的思维。在这个例子中，我们利用现有的资源来实现一个符合规格并利用我们提供的配置的解决方案。确保您始终问自己这个设置将如何影响结果，以及如何更改它以使其更有效。
- en: In the next chapter, we'll go through testing and validating the performance
    of the setup.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将测试和验证设置的性能。
- en: Questions
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What are GlusterFS bricks?
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是GlusterFS的bricks？
- en: What is ZFS?
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是ZFS？
- en: What is a zpool?
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是zpool？
- en: What is a cache disk?
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是缓存磁盘？
- en: How is GlusterFS installed?
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何安装GlusterFS？
- en: What is a trusted pool?
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是受信任的池？
- en: How is a GlusterFS volume created?
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何创建GlusterFS卷？
- en: What is performance.cache-size?
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是performance.cache-size？
- en: What is ARC?
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是ARC？
- en: Further reading
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Learning Microsoft Azure* by Geoff Webber-Cross: [https://www.packtpub.com/networking-and-servers/learning-microsoft-azure](https://www.packtpub.com/networking-and-servers/learning-microsoft-azure)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过Geoff Webber-Cross学习Microsoft Azure*：[https://www.packtpub.com/networking-and-servers/learning-microsoft-azure](https://www.packtpub.com/networking-and-servers/learning-microsoft-azure)'
- en: '*Implementing Azure Solutions* by Florian Klaffenbach, Jan-Henrik Damaschke,
    and Oliver Michalski: [https://www.packtpub.com/virtualization-and-cloud/implementing-azure-solutions](https://www.packtpub.com/virtualization-and-cloud/implementing-azure-solutions)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过Florian Klaffenbach、Jan-Henrik Damaschke和Oliver Michalski实施Azure解决方案*：[https://www.packtpub.com/virtualization-and-cloud/implementing-azure-solutions](https://www.packtpub.com/virtualization-and-cloud/implementing-azure-solutions)'
- en: '*Azure for Architects* by Ritesh Modi: [https://www.packtpub.com/virtualization-and-cloud/azure-architects](https://www.packtpub.com/virtualization-and-cloud/azure-architects)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《Azure架构师》作者Ritesh Modi：[https://www.packtpub.com/virtualization-and-cloud/azure-architects](https://www.packtpub.com/virtualization-and-cloud/azure-architects)

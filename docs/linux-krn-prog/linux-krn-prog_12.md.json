["```\nsudo perf top\nsudo perf top --sort comm,dso\nsudo perf top -r 90 --sort pid,comm,dso,symbol\n```", "```\nalias ptopv='sudo perf top -r 80 -f 99 --sort pid,comm,dso,symbol --demangle-kernel -v --call-graph dwarf,fractal'\n```", "```\nsudo perf timechart record\n```", "```\nsudo perf timechart \n```", "```\n$ sudo perf timechart record\n[sudo] password for <user>:\n^C[ perf record: Woken up 18 times to write data ] \n[ perf record: Captured and wrote 6.899 MB perf.data (196166 samples) ] \n$ ls -lh perf.data \n-rw------- 1 root root 7.0M Jun 18 12:57 perf.data \n$ sudo perf timechart\nWritten 7.1 seconds of trace to output.svg.\n```", "```\n# Show thread(s) running on cpu core 'n' - func c'n'\nfunction c0() \n{ \n    ps -eLF | awk '{ if($5==0) print $0}' \n} \nfunction c1() \n{ \n    ps -eLF | awk '{ if($5==1) print $0}' \n} \n```", "```\n// kernel/sched/sched.h\n[ ... ] \nextern const struct sched_class stop_sched_class; \nextern const struct sched_class dl_sched_class; \nextern const struct sched_class rt_sched_class; \nextern const struct sched_class fair_sched_class; \nextern const struct sched_class idle_sched_class;\n```", "```\nschedule() --> __schedule() --> pick_next_task() \n```", "```\n// kernel/sched/core.c\n /* \n  * Pick up the highest-prio task: \n  */ \nstatic inline struct task_struct * \npick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf) \n{ \n    const struct sched_class *class; \n    struct task_struct *p; \n\n    /* Optimization: [...] */\n    [...]\n\n   for_each_class(class) { \n        p = class->pick_next_task(rq, NULL, NULL);\n        if (p)\n            return p;\n    }\n\n    /* The idle class should always have a runnable task: */\n    BUG();\n}\n```", "```\n// kernel/sched/sched.h\n[...]\n#ifdef CONFIG_SMP\n#define sched_class_highest (&stop_sched_class)\n#else\n#define sched_class_highest (&dl_sched_class)\n#endif\n\n#define for_class_range(class, _from, _to) \\\n    for (class = (_from); class != (_to); class = class->next)\n\n#define for_each_class(class) \\\n    for_class_range(class, sched_class_highest, NULL)\n```", "```\n// kernel/sched/core.c\n[ ... ] \nstatic inline struct task_struct * \npick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf) \n{ \n    const struct sched_class *class;\n    struct task_struct *p;\n    [ ... ] \nfor_each_class(class){\n        p = class->pick_next_task(rq, NULL, NULL);\n        if (p)\n            return p;\n    }\n    [ ... ]\n\n```", "```\n// location: kernel/sched/sched.h\n[ ... ] \nstruct sched_class {\n    const struct sched_class *next;\n    [...]\n    void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags); \n    void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags);\n    [ ... ]\n    struct task_struct * (*pick_next_task)(struct rq *rq,\n                           struct task_struct *prev,\n                           struct rq_flags *rf);\n    [ ... ] \n    void (*task_tick)(struct rq *rq, struct task_struct *p, int queued); \n    void (*task_fork)(struct task_struct *p); \n    [ ... ]\n};\n```", "```\n// kernel/sched/fair.c\nconst struct sched_class fair_sched_class = {\n    .next = &idle_sched_class,\n    .enqueue_task = enqueue_task_fair,\n    .dequeue_task = dequeue_task_fair,\n    [ ... ]\n    .pick_next_task = pick_next_task_fair,\n    [ ... ]\n    .task_tick = task_tick_fair,\n    .task_fork = task_fork_fair,\n    .prio_changed = prio_changed_fair,\n    [ ... ]\n};\n```", "```\n$ chrt -p 1 \npid 1's current scheduling policy: SCHED_OTHER \npid 1's current scheduling priority: 0 \n$ \n```", "```\n$ strace chrt -p 1\n[ ... ] \nsched_getattr(1, {size=48, sched_policy=SCHED_OTHER, sched_flags=0, \nsched_nice=0, sched_priority=0, sched_runtime=0, sched_deadline=0, \nsched_period=0}, 48, 0) = 0 \nfstat(1, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 6), ...}) = 0 \nwrite(1, \"pid 1's current scheduling polic\"..., 47) = 47 \nwrite(1, \"pid 1's current scheduling prior\"..., 39) = 39 \n[ ... ] $ \n```", "```\nstatic void exit_to_usermode_loop(struct pt_regs *regs, u32 cached_flags)\n{\n[...]\n if (cached_flags & _TIF_NEED_RESCHED)\n schedule();\n```", "```\n// kernel/sched/core.c/*\n * __schedule() is the main scheduler function.\n * The main means of driving the scheduler and thus entering this function are:\n * 1\\. Explicit blocking: mutex, semaphore, waitqueue, etc.\n *\n * 2\\. TIF_NEED_RESCHED flag is checked on interrupt and user space return\n *    paths. For example, see arch/x86/entry_64.S.\n *\n *    To drive preemption between tasks, the scheduler sets the flag in timer\n *    interrupt handler scheduler_tick().\n *\n * 3\\. Wakeups don't really cause entry into schedule(). They add a\n *    task to the run-queue and that's it.\n *\n *    Now, if the new task added to the run-queue preempts the current\n *    task, then the wakeup sets TIF_NEED_RESCHED and schedule() gets\n *    called on the nearest possible occasion:\n *    - If the kernel is preemptible (CONFIG_PREEMPTION=y):\n *\n *    - in syscall or exception context, at the next outmost\n *      preempt_enable(). (this might be as soon as the wake_up()'s\n *      spin_unlock()!)\n *\n *    - in IRQ context, return from interrupt-handler to\n *      preemptible context\n *\n *    - If the kernel is not preemptible (CONFIG_PREEMPTION is not set)\n *      then at the next:\n *       - cond_resched() call\n *       - explicit schedule() call\n *       - return from syscall or exception to user-space\n *       - return from interrupt-handler to user-space\n * WARNING: must be called with preemption disabled!\n */\n```", "```\nstatic void __sched notrace __schedule(bool preempt)\n{\n    struct task_struct *prev, *next;\n    [...] struct rq *rq;\n    int cpu;\n\n    cpu = smp_processor_id();\n    rq = cpu_rq(cpu);\n    prev = rq->curr;                 *<< this is 'current' ! >>*\n\n    [ ... ]\n\n    next = pick_next_task(rq, prev, &rf);  *<< here we 'pick' the task to run next in an 'object-\n                                          oriented' manner, as discussed earlier in detail ... >>*\n    clear_tsk_need_resched(prev);\n    clear_preempt_need_resched();\n\n    if (likely(prev != next)) {\n        [ ... ]\n        /* Also unlocks the rq: */\n        rq = context_switch(rq, prev, next, &rf);\n    [ ... ]\n}\n```"]
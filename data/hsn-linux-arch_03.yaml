- en: Defining GlusterFS Storage
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义GlusterFS存储
- en: Every day, applications require faster storage that can sustain thousands of
    concurrent I/O requests. GlusterFS is a highly-scalable, redundancy filesystem
    that can deliver high-performance I/O to many clients simultaneously. We will
    define the core concept of a cluster and then introduce how GlusterFS plays an
    important role.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 每天，应用程序需要更快的存储，可以支持成千上万个并发I/O请求。GlusterFS是一个高度可扩展的冗余文件系统，可以同时向许多客户端提供高性能I/O。我们将定义集群的核心概念，然后介绍GlusterFS如何发挥重要作用。
- en: In the preceding chapter, we went through the different aspects of designing
    solutions to provide high availability and performance to applications that have
    many requirements. In this chapter, we'll go through solving a very specific problem,
    that is, storage.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们已经讨论了设计解决方案的不同方面，以提供高可用性和性能，以满足许多要求的应用程序。在本章中，我们将解决一个非常具体的问题，即存储。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding the core concept of a cluster
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解集群的核心概念
- en: The reason for choosing GlusterFS
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择GlusterFS的原因
- en: Explaining **software-defined storage** (**SDS**)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释**软件定义存储**（**SDS**）
- en: Exploring the differences between file, object, and block storage
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索文件、对象和块存储之间的区别
- en: Explaining the need for high performance and highly available storage
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释对高性能和高可用存储的需求
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter will focus on defining GlusterFS. You can refer to the project's
    home page at [https://github.com/gluster/glusterfs](https://github.com/gluster/glusterfs) or
    [https://www.gluster.org/](https://www.gluster.org/).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点介绍定义GlusterFS。您可以参考项目的主页[https://github.com/gluster/glusterfs](https://github.com/gluster/glusterfs)或[https://www.gluster.org/](https://www.gluster.org/)。
- en: Additionally, the project's documentation can be found at [https://docs.gluster.org/en/latest/](https://docs.gluster.org/en/latest/).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，项目的文档可以在[https://docs.gluster.org/en/latest/](https://docs.gluster.org/en/latest/)找到。
- en: What is a cluster?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是集群？
- en: We can leverage the many advantages of SDS, which allows for easy scalability
    and enhanced fault tolerance. GlusterFS is a piece of software that can create
    highly scalable storage clusters while providing maximum performance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用SDS的许多优势，它允许轻松扩展和增强容错能力。GlusterFS是一款软件，可以创建高度可扩展的存储集群，同时提供最大性能。
- en: Before we go through how we can solve this specific need, we first need to define
    what a cluster is, why it exists, and what problems a cluster might be able to
    solve.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们解决这个特定需求之前，我们首先需要定义集群是什么，为什么它存在，以及集群可能能够解决什么问题。
- en: Computing a cluster
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算集群
- en: Put simply, a cluster is a set of computers (often called nodes) that work in
    tandem on the same workload and can distribute loads across all available members
    of the cluster to increase performance, while, at the same time, allowing for
    self-healing and availability. Note that the term **server** wasn't used before
    as, in reality, any computer can be added to a cluster. Made from a simple Raspberry
    Pi to multiple CPU servers, clusters can be made from a small two-node configuration
    to thousands of nodes in a data center.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，集群是一组计算机（通常称为节点），它们协同工作在相同的工作负载上，并可以将负载分布到集群的所有可用成员上，以增加性能，同时允许自我修复和可用性。请注意，在现实中，任何计算机都可以添加到集群中，因此术语**服务器**之前并未使用。从简单的树莓派到多CPU服务器，集群可以由一个小型的双节点配置制成，也可以由数据中心中的数千个节点制成。
- en: 'Here is an example of a cluster:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个集群的例子：
- en: '![](img/d1b1a48f-f1dc-40ea-971b-5bc322251318.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1b1a48f-f1dc-40ea-971b-5bc322251318.png)'
- en: Technically speaking, clustering allows workloads to scale performance by adding
    servers of the same kind with similar resource characteristics. Ideally, a cluster
    will have homogeneous hardware to avoid problems where nodes have different performance
    characteristics and, at the same time, make maintenance reasonably identical—this
    means hardware with the same CPU family, memory configuration, and software. The
    idea of adding nodes to a cluster allows you to compute workloads to decrease
    their processing time. Depending on the application, compute times can sometimes
    even decrease linearly.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，集群允许工作负载通过添加具有相似资源特征的相同类型的服务器来扩展性能。理想情况下，集群将具有同质硬件，以避免节点具有不同性能特征的问题，并且同时使维护相对相同
    - 这意味着具有相同CPU系列、内存配置和软件的硬件。向集群添加节点的想法允许您计算工作负载以减少其处理时间。根据应用程序，计算时间有时甚至可以线性减少。
- en: To further understand the concept of a cluster, imagine that you have an application
    that takes historical financial data. The application then receives such data
    and creates a forecast based on the stored information. On a single node, the
    forecast process (processes on a cluster are typically named jobs) takes roughly
    six days to complete, as we're dealing with several **terabytes** (**TB**) of
    data. Adding an extra node with the same characteristics decreases the processing
    time to four days. Adding a third node further decreases the time it takes to
    complete to three days.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步理解集群的概念，想象一下您有一个应用程序，它获取历史财务数据。然后，该应用程序接收这些数据并根据存储的信息创建预测。在单个节点上，预测过程（集群上的进程通常被称为作业）大约需要六天才能完成，因为我们处理了数TB的数据。添加具有相同特征的额外节点将处理时间缩短到四天。再添加第三个节点将进一步缩短完成时间至三天。
- en: Note that while we added three times the number of compute resources, the compute
    time only decreased by approximately half. Some applications can scale performance
    linearly, while others don't have the same scalability, requiring more and more
    resources for fewer gains, up to the point of diminishing returns. Adding more
    resources to obtain minimal time gain is not cost-effective.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，虽然我们增加了三倍的计算资源，但计算时间仅减少了大约一半。一些应用程序可以线性扩展性能，而其他应用程序则没有相同的可扩展性，需要更多的资源来获得更少的收益，直至收益递减的点。为了获得最小的时间收益，添加更多资源是不划算的。
- en: 'With all this in mind, we can point out several characteristics that define
    a cluster:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有这些，我们可以指出定义集群的几个特征：
- en: It can help reduce processing times by adding compute resources
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以通过添加计算资源来帮助减少处理时间
- en: It can scale both vertically and horizontally
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以进行垂直和水平扩展
- en: It can be redundant, that is, if one node fails, others should take the workload
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以是冗余的，也就是说，如果一个节点失败，其他节点应该接管工作负载
- en: It can allow for increased resources to be available for applications
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以允许增加资源以供应用程序使用
- en: It is a single pool of resources rather than individual servers
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是一个资源池，而不是单独的服务器
- en: It has no single point of failure
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它没有单点故障
- en: Storage clusters
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储集群
- en: Now that we have an understanding of how to compute a cluster, let's move on
    to another application of clusters.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何计算集群，让我们继续讨论集群的另一个应用。
- en: Instead of aggregating compute resources to decrease processing times, a storage
    cluster's main functionality is to aggregate available space to provide maximum
    space utilization while, at the same time, providing some form of redundancy.
    With the increased need for storing large amounts of data comes the need of being
    able to do it at a lower cost, while still maintaining increased data availability.
    Storage clusters help to solve this problem by allowing single monolithic storage
    nodes to work together as a large pool of available storage space. Thus, it allows
    storage solutions to reach the petascale mark without the need to deploy specialized
    proprietary hardware.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 存储集群的主要功能不是聚合计算资源以减少处理时间，而是聚合可用空间以提供最大的空间利用率，同时提供某种形式的冗余。随着存储大量数据的需求增加，需要以更低的成本进行存储，同时仍然保持增加的数据可用性。存储集群通过允许单个单片式存储节点一起工作，形成一个大型的可用存储空间池，来解决这个问题。因此，它允许存储解决方案在不需要部署专门的专有硬件的情况下达到PB级别。
- en: For example, say that we have a single node with 500 TB of available space and
    we need to achieve the 1-**Petabyte** (**PB**) mark while providing redundancy.
    This individual node becomes a single point of failure because, if it goes down,
    then there's no way the data can be accessed. Additionally, we've reached the
    maximum **hard disk drive** (**HDD**) capacity available. In other words, we can't
    scale horizontally.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个单节点，可用空间为500 TB，我们需要实现1-**PB**标记，并提供冗余。这个单独的节点成为单点故障，因为如果它宕机，那么数据就无法访问。此外，我们已经达到了最大的**硬盘驱动器**（**HDD**）容量。换句话说，我们无法进行水平扩展。
- en: To solve this problem, we can add two more nodes with the same configuration,
    as the already existing one provides a total of 1 PB of available space. Now,
    let's do some math here, 500 TB times 3 should be approximately 1.5 PB, correct?
    The answer is most definitely yes. However, since we need to provide high availability
    to this solution, the third node acts as a backup, making the solution tolerate
    a single-node failure without interrupting the client's communication. This capability
    to allow node failures is all thanks to the power of SDS and storage clusters,
    such as GlusterFS, which we'll explore next.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以添加两个具有相同配置的节点，因为已经存在的节点提供了总共1 PB的可用空间。现在，让我们在这里做一些数学运算，500 TB乘以3应该大约是1.5
    PB，对吗？答案绝对是肯定的。然而，由于我们需要为这个解决方案提供高可用性，第三个节点充当备份，使解决方案能够容忍单节点故障而不中断客户端的通信。这种允许节点故障的能力完全归功于SDS和存储集群的强大功能，比如GlusterFS，接下来我们将探讨它。
- en: What is GlusterFS?
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是GlusterFS？
- en: GlusterFS is an open source project by Gluster, which was acquired by Red Hat,
    Inc. in 2011\. This acquisition does not mean that you have to acquire a Red Hat
    subscription or pay Red Hat to use it since, as previously mentioned, it is an
    open source project; therefore, you can freely install it, look at its source
    code, and even contribute to the project. Although Red Hat offers paid solutions
    based on GlusterFS, we will talk about the **open source software** (**OSS**)
    and project itself in this chapter.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: GlusterFS是Gluster的一个开源项目，该项目于2011年被Red Hat公司收购。这个收购并不意味着您必须获取Red Hat订阅或支付Red
    Hat才能使用它，因为正如前面提到的，它是一个开源项目；因此，您可以自由安装它，查看其源代码，甚至为项目做出贡献。尽管Red Hat提供基于GlusterFS的付费解决方案，但我们将在本章中讨论**开源软件**（**OSS**）和项目本身。
- en: 'The following diagram is the number of **Contributors** and **Commits** in
    the Gluster project:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了Gluster项目中的**贡献者**和**提交**数量：
- en: '![](img/a6c3ccf4-03a9-4fef-b25f-b5ec04db0748.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6c3ccf4-03a9-4fef-b25f-b5ec04db0748.png)'
- en: To understand GlusterFS, we must understand how it differs from traditional
    storage. To do this, we need to understand the concepts behind SDS, including
    what GlusterFS is.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解GlusterFS，我们必须了解它与传统存储的区别。为了做到这一点，我们需要了解SDS背后的概念，包括GlusterFS是什么。
- en: 'Traditional storage is an industry-standard storage array with proprietary
    software in it that is bound to the hardware vendor. All this restricts you to
    the following set of rules,set by your storage provider:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 传统存储是一个行业标准的存储阵列，其中包含专有软件，与硬件供应商绑定。所有这些都限制了您遵循存储提供商设定的一组规则：
- en: Scalability limitations
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可扩展性限制
- en: Hardware compatibility limitations
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 硬件兼容性限制
- en: Client-operating system limitations
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户端操作系统限制
- en: Configuration limitations
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置限制
- en: Vendor lock-in
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 供应商锁定
- en: SDS
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SDS
- en: With SDS, many, if not all, of the preceding limitations are gone, since it
    provides impressive scalability by not depending on any hardware. You can fundamentally
    take an industry-standard server from any vendor that contains the storage you
    require and add it to your storage pool. By only doing this one simple step, you
    already overcome four of the preceding limitations.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 有了SDS，许多，如果不是所有，前面的限制都消失了，因为它通过不依赖于任何硬件提供了令人印象深刻的可扩展性。您基本上可以从任何供应商那里获取包含您所需存储的行业标准服务器，并将其添加到您的存储池中。只需执行这一简单步骤，您就已经克服了前面四个限制。
- en: Cost reduction
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本降低
- en: The example from *SDS* section, highly reduces the **operating expense** (**OPEX**) costs,
    as you do not have to buy additional highly-priced expansion shelves for an existing
    vendor storage array that can take weeks to arrive and be installed. You can quickly
    grab a server that you have stored in the corner of your data center, and use
    it to provide storage space for your existing applications. This process is called
    plugin scalability and is present in most of the open source SDS projects out
    there. In theory, the sky is the limit when it comes to scalability with SDS.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*SDS*部分的示例大大降低了**运营费用**（**OPEX**）成本，因为你不必为现有供应商存储阵列购买昂贵的扩展架，这可能需要数周才能到货并安装。你可以快速获取存储在数据中心角落的服务器，并用它为现有应用程序提供存储空间。这个过程称为插件可扩展性，并且存在于大多数开源SDS项目中。从理论上讲，SDS在可扩展性方面的潜力是无限的。'
- en: Scalability
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可扩展性
- en: SDS scales when you add new servers to your storage pools and also increases
    the resilience of your storage cluster. Depending on what configuration you have,
    data is spread across multiple member nodes providing additional high availability
    by mirroring or creating parity for your data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当你向存储池添加新服务器时，SDS会扩展，并增加存储集群的弹性。根据你的配置，数据会分布在多个成员节点上，通过镜像或创建数据的奇偶校验提供额外的高可用性。
- en: Control
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制
- en: You also need to understand that SDS does not create space out of nothing, nor
    does it separate the concept of storage from hardware—such as hard drives, **solid
    state drives** (**SSD**), or any hardware device that is designed to store information.
    These hardware devices will always be where the actual data is stored. SDS adds
    a logical layer that allows you to control where and how you store this data.
    It leverages this with its most fundamental components, that is, with an **application
    programming interface** (**API**) that allows you to manage and maintain your
    storage cluster and logical volumes, which provide the storage capacity to your
    other servers, applications, and even monitoring agents that self-heal the cluster
    in the event of degradation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要明白，SDS并不是从虚无中创造空间，也不会将存储的概念与硬件（如硬盘、**固态硬盘**（**SSD**）或任何旨在存储信息的硬件设备）分离。这些硬件设备将始终是实际数据存储的地方。SDS添加了一个逻辑层，允许你控制数据的存储位置和方式。它利用了最基本的组件，即**应用程序编程接口**（**API**），允许你管理和维护存储集群和逻辑卷，为其他服务器、应用程序甚至自我修复集群的监控代理提供存储容量。
- en: The market is moving toward SDS
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 市场正在向SDS发展
- en: SDS is the future, and this is where the storage industry is moving. In fact,
    it is predicted that, in the next few years, approximately 70% of all current
    storage arrays will be available as software-only solutions or **virtual storage
    appliances** (**VSAs**). Traditional **network-attached storage** (**NAS**) solutions
    are 30% more expensive than current SDS implementations, and mid-range disk arrays
    are even more costly. Taking all this into account alongside the fact that data
    consumption is growing approximately 40% in enterprise every year, with a cost
    decline of only 25%, you can see why we are moving toward an SDS world in the
    near future.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: SDS是未来，这就是存储行业的发展方向。事实上，预测未来几年，大约70%的当前存储阵列将作为纯软件解决方案或**虚拟存储设备**（**VSAs**）提供。传统的**网络附加存储**（**NAS**）解决方案比当前的SDS实施要贵30%，中档磁盘阵列甚至更昂贵。考虑到每年企业数据消耗约增长40%，成本仅下降25%，你可以看到为什么我们在不久的将来会向SDS世界迈进。
- en: 'With the number of applications that are running public, private, and hybrid
    clouds, consumer and business data consumption is growing exponentially and ceaselessly.
    This data is usually mission-critical and requires a high level of resiliency.
    The following is a list of some of these applications:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 随着在公共、私人和混合云中运行的应用数量不断增长，消费者和企业数据的消耗呈指数级增长。这些数据通常是使命关键的，并且需要高水平的弹性。以下是一些这些应用的列表：
- en: E-commerce and online storefronts
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子商务和在线商店
- en: Financial applications
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金融应用
- en: Enterprise resource planning
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 企业资源规划
- en: Health care
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 医疗保健
- en: Big data
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据
- en: Customer relationship management
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户关系管理
- en: When companies store this type of data (called **bulk data**), they not only
    need to archive it, but they also need to access it, and with the lowest latency
    possible. Imagine a scenario where you are sent to take X-rays during your doctor's
    appointment, and when you arrive, they tell you that you have to wait for a week
    to get your scans because they have no storage space available to save your images.
    Naturally, this scenario will not happen because every hospital has a highly efficient
    procurement process, where they can predict usage based on their storage consumption
    and decide when to start the purchase and installation of new hardware—but you
    get the idea. It is much faster and more efficient to install a POSIX-standard
    server into your SDS layer and be ready to go.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当公司存储这种类型的数据（称为**大数据**）时，他们不仅需要对其进行归档，还需要访问它，并且尽可能地降低延迟。想象一种情景，你在医生的预约中被要求进行X光检查，当你到达时，他们告诉你需要等一周才能得到你的扫描结果，因为他们没有存储空间来保存你的图像。当然，这种情况不会发生，因为每家医院都有一个高效的采购流程，他们可以根据存储消耗来预测使用情况，并决定何时开始购买和安装新的硬件，但你明白我的意思。在你的SDS层中安装一个符合POSIX标准的服务器并准备好使用，速度更快、更高效。
- en: Massive storage
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大规模存储
- en: Many other companies also require data lakes as secondary storage, mainly to
    store data in its raw form for analysis, real-time analytics, machine learning,
    and more. SDS is excellent for this type of storage, mainly because the maintenance
    required is minimal and also for the economic reasons that we discussed previously.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 许多其他公司也需要数据湖作为辅助存储，主要是为了以原始形式存储数据以进行分析、实时分析、机器学习等。SDS非常适合这种类型的存储，主要是因为所需的维护很少，还有我们之前讨论过的经济原因。
- en: We have been talking mainly about how economic and scalable SDS is, but it is
    also important to mention the high flexibility that it brings to the table. SDS
    can be used for everything from archiving data and storing reach media to providing
    storage for **virtual machines** (**VMs**), as an endpoint for object storage
    in your private cloud, and even in containers. It can be deployed on any of the
    previously mentioned infrastructures. It can run on your public cloud of choice,
    in your current on-premises virtual infrastructure, and even in a Docker container
    or Kubernetes pod. In fact, it's so flexible that you can even integrate Kubernetes
    with GlusterFS using a RESTful management interface called *heketi* that dynamically
    provisions volumes every time you require persistent volumes for your pods.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要讨论了SDS的经济性和可扩展性，但也很重要提到它带来的高灵活性。SDS可用于从存档数据和存储丰富媒体到为虚拟机（VM）提供存储，作为私有云中对象存储的端点，甚至在容器中。它可以部署在前面提到的任何基础设施上。它可以在您选择的公共云中运行，在您当前的本地虚拟基础设施中运行，甚至在Docker容器或Kubernetes
    pod中运行。事实上，它非常灵活，以至于您甚至可以使用称为heketi的RESTful管理接口将Kubernetes与GlusterFS集成，每当您需要为您的pod提供持久卷时，它都会动态分配卷。
- en: Block, file, and object storage
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 块、文件和对象存储
- en: Now that we have gone through why SDS is the future of next-generation workloads,
    it is time to dig a little deeper into the types of storage that we can achieve
    using SDS.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了为什么SDS是下一代工作负载的未来，是时候深入了解使用SDS可以实现的存储类型了。
- en: Traditional **storage area network** (**SAN**) and NAS solutions more commonly
    serve storage using protocols such as **internet small computer systems interface** (**iSCSI**),
    **fibre channel** (**FC**), **fibre** **channel over ethernet** (**FCoE**), **network
    file system** (**NFS**), and **server message block** (**SMB**)/**common internet
    file system** (**CIFS**). However, because we are moving more toward the cloud,
    our storage needs change and this is where object storage comes into play. We
    will explore what object storage is and how it compares to block and file storage.
    GlusterFS is also a file storage solution, but it has block and object storage
    capabilities that can be configured further down the line.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的存储区域网络（SAN）和NAS解决方案更常用的是使用诸如Internet小型计算机系统接口（iSCSI）、光纤通道（FC）、以太网上的光纤通道（FCoE）、网络文件系统（NFS）和服务器消息块（SMB）/通用互联网文件系统（CIFS）等协议提供存储。然而，由于我们更多地向云端迁移，我们的存储需求也发生变化，这就是对象存储发挥作用的地方。我们将探讨对象存储是什么，以及它与块和文件存储的比较。GlusterFS也是一种文件存储解决方案，但它具有可以进一步配置的块和对象存储功能。
- en: 'The following diagram displays block, file, and object storage:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了块、文件和对象存储：
- en: '![](img/21326729-1160-4c53-b272-9745456b4f9a.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21326729-1160-4c53-b272-9745456b4f9a.png)'
- en: Block storage, file storage, and object storage work in very different ways
    when it comes to how the client stores data in them—causing their use cases to
    be completely different.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 块存储、文件存储和对象存储在客户端存储数据的方式上有很大不同，因此它们的用途也完全不同。
- en: Block storage
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 块存储
- en: A SAN is where block storage is mainly utilized, using protocols such as FC, or iSCSI,
    which are essentially mappings of the **Small Computer System Interface** (**SCSI**) protocol
    over FC and TCP/IP, respectively.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: SAN主要是利用块存储的地方，使用诸如FC或iSCSI之类的协议，这些协议本质上是在FC和TCP/IP上映射SCSI协议。
- en: 'A typical FC SAN looks like the following diagram:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的FC SAN如下图所示：
- en: '![](img/37d1c291-1ddc-4951-ad4e-035dda119178.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37d1c291-1ddc-4951-ad4e-035dda119178.png)'
- en: 'A typical iSCSI SAN looks like the following diagram:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的iSCSI SAN如下图所示：
- en: '![](img/45a4ac04-e210-4c3b-bb83-57316dabb29a.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/45a4ac04-e210-4c3b-bb83-57316dabb29a.png)'
- en: Data is stored in logical block addresses. When retrieving data, the application
    usually says—*I want X number of blocks from address XXYYZZZZ*. This process tends
    to be very fast (less than a millisecond), making this type of storage very low
    on latency, a very transactional-oriented type of storage form, and ideal for
    random access. However, it also has its disadvantages when it comes to sharing
    across multiple systems. This is because block storage usually presents itself
    in its raw form, and you require a filesystem on top of it that can support multiple
    writes across different systems without corruption—in other words, a clustered
    filesystem.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储在逻辑块地址中。在检索数据时，应用程序通常会说：“我要从地址XXYYZZZZ获取X个块。”这个过程往往非常快（不到一毫秒），使得这种类型的存储在延迟上非常低，是一种非常事务性导向的存储形式，非常适合随机访问。然而，当涉及跨多个系统共享时，它也有其缺点。这是因为块存储通常以原始形式呈现，您需要在其上方使用一个文件系统，该文件系统可以支持在不同系统之间进行多次写入而不会损坏，换句话说，是一个集群文件系统。
- en: This type of storage also has some downsides when it comes to high availability
    or disaster recovery; because it is presented in its raw form, the storage controllers
    and managers are, therefore, not aware of how this storage is being used. So,
    when it comes to replicate its data to a recovery point, it only takes blocks
    into account, and some filesystems are terrible at reclaiming or zeroing blocks,
    which leads to unused blocks being replicated as well, thus leading to deficient
    storage utilization.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到高可用性或灾难恢复时，这种类型的存储也有一些缺点；因为它以原始形式呈现，存储控制器和管理器因此不知道如何使用这种存储。因此，当涉及将其数据复制到恢复点时，它只考虑块，并且一些文件系统在回收或清零块时表现糟糕，这导致未使用的块也被复制，从而导致存储利用不足。
- en: Because of its advantages and low latency, block storage is perfect for structured
    databases, random read/write operations, and to store multiple VM images that
    query disks with hundreds, if not thousands, of I/O requests. For this, clustered
    filesystems are designed to support multiple reads and writes from different hosts.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其优势和低延迟，块存储非常适合结构化数据库、随机读/写操作以及存储多个虚拟机镜像，这些镜像查询磁盘时可能有数百甚至数千次的I/O请求。为此，集群文件系统被设计为支持来自不同主机的多次读写。
- en: However, due to its advantages and disadvantages, block storage requires quite
    a lot of care and feeding—you need to take care of the filesystem and partitioning
    that you are going to put on top of your block devices. Additionally, you have
    to make sure that the filesystem is kept consistent and secure, with correct permissions,
    and without corruption across all the systems that are accessing it. VMs have
    other filesystems stored in their virtual disks that also add another layer of
    complexity—data can be written to the VM's filesystem and into the hypervisor's
    filesystem. Both filesystems have files that come and go, and they need to be
    adequately zeroed for blocks to be reclaimed in a thinly provisioned replication
    scenario, and, as we mentioned before, most storage arrays are not aware of the
    actual data being written to them.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于其优点和缺点，块存储需要相当多的维护和管理——您需要关注要放在块设备上的文件系统和分区。此外，您还需要确保文件系统保持一致和安全，具有正确的权限，并且在访问它的所有系统中没有损坏。虚拟机中还有其他文件系统存储在其虚拟磁盘中，这也增加了另一层复杂性——数据可以写入虚拟机的文件系统和超级管理程序的文件系统。两个文件系统都有文件的进出，它们需要适当地清零以便在薄量复制场景中回收块，正如我们之前提到的，大多数存储阵列并不知道实际写入它们的数据。
- en: File storage
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文件存储
- en: On the other hand, file storage or NAS is far more straightforward. You don't
    have to worry about partitioning, or about selecting and formatting a filesystem
    that suits your multi-host environment.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，文件存储或NAS要简单得多。您不必担心分区，也不必担心选择和格式化适合多主机环境的文件系统。
- en: NAS is usually NFS or SMB/CIFS protocols, which are mainly used for storing
    data in shared folders as unstructured data. These protocols are not very good
    at scaling or meeting the high media demands that we face in the cloud, such as
    social media serving and creating/uploading thousands of images or videos each
    day. This is where object storage saves the day, but we will be covering object
    storage later in this chapter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: NAS通常使用NFS或SMB/CIFS协议，主要用于将数据存储在共享文件夹中作为非结构化数据。这些协议在扩展性或满足云中面临的高媒体需求方面并不是很好，比如社交媒体服务以及每天创建/上传成千上万张图片或视频。这就是对象存储发挥作用的地方，但我们将在本章后面讨论对象存储。
- en: File storage, as the name suggests, works at the file level of storage when
    you perform a request to NAS; you are requesting a file or a piece of a file from
    the filesystem, not a series of logical addresses. With NAS, this process is abstracted
    from the host (where the storage is mounted), and your storage array or SDS is
    in charge of accessing the disks on the backend and retrieving the file that you
    are requesting. File storage also comes with native features, such as file-locking,
    user and group integration (when we are talking about OSS, we are talking about
    NFS mainly here), security, and encryption.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 文件存储，顾名思义，是在文件级别的存储上工作，当您向NAS发出请求时，您请求的是文件或文件系统中的一部分，而不是一系列逻辑地址。对于NAS，这个过程是从主机（存储被挂载的地方）中抽象出来的，您的存储阵列或SDS负责访问后端的磁盘并检索您请求的文件。文件存储还具有本地功能，如文件锁定、用户和组集成（当我们谈论OSS时，主要是指NFS），安全性和加密。
- en: Even though NAS abstracts and makes things simple for the client, it also has
    its downsides, as NAS relies heavily, if not entirely, on the network. It also
    has an additional filesystem layer with much higher latency than block storage.
    Many factors can cause latency or increase **round-trip time** (**RTT**). You
    need to consider things such as how many hops your NAS is away from your clients,
    TCP window scaling, or having no jumbo frames enabled on devices accessing your
    file shares. Also, all these factors not only affect latency but are key players
    when it comes to the throughput of your NAS solution, which is where file storage
    excels the most.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管NAS为客户端提供了抽象和简化，但它也有其缺点，因为NAS在很大程度上，如果不是完全地，依赖于网络。它还有一个额外的文件系统层，延迟远高于块存储。许多因素可能导致延迟或增加往返时间（RTT）。您需要考虑诸如您的NAS距离客户端有多少跳、TCP窗口缩放，或者设备访问文件共享时未启用巨帧。此外，所有这些因素不仅影响延迟，而且在涉及NAS解决方案的吞吐量时起着关键作用，这也是文件存储最擅长的地方。
- en: 'The following diagram demonstrates how versatile file storage sharing is:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了文件存储共享的多功能性：
- en: '![](img/e1d8ff10-cd79-433d-9875-70b65d8ec7c1.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e1d8ff10-cd79-433d-9875-70b65d8ec7c1.png)'
- en: Object storage
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对象存储
- en: Object storage is entirely different from NAS (file storage) and SAN (block
    storage). Although data is still accessed through the network, the way that data
    is retrieved is uniquely different. You will not access files through a filesystem,
    but through RESTful APIs using HTTP methods.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对象存储与NAS（文件存储）和SAN（块存储）完全不同。尽管数据仍然通过网络访问，但数据检索的方式是独特不同的。您不会通过文件系统访问文件，而是通过使用HTTP方法的RESTful
    API访问。
- en: 'Objects are stored in a flat namespace, which can store millions or billions
    of them; this is the key to its high scalability, as it is not restrained by the
    number of nodes as it is in regular filesystems, such as XFS and EXT4\. It is
    important to know that the namespaces can have partitions (often called buckets),
    but they cannot be nested as regular folders in a filesystem because the namespace
    is flat:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对象存储在一个扁平的命名空间中存储，可以存储数百万甚至数十亿个对象；这是其高可扩展性的关键，因为它不受节点数量的限制，就像常规文件系统（如XFS和EXT4）一样。重要的是要知道，命名空间可以有分区（通常称为存储桶），但它们不能像文件系统中的常规文件夹那样嵌套，因为命名空间是扁平的：
- en: '![](img/d81a09ae-c01d-46d8-aa33-f29999148167.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d81a09ae-c01d-46d8-aa33-f29999148167.png)'
- en: When comparing object storage with traditional storage, the self-parking versus
    valet-parking analogy is often used. Why is this similar? Well, because, in traditional
    filesystems, when you store your file you store it in a folder or directory, and
    it is your responsibility to know where that file was stored, just like parking
    a car in a parking spot—you need to remember the number and floor of where you
    left your car. With object storage, on the other hand, when you upload your data
    or put a file in a bucket, you are granted a unique identifier that you can later
    use to retrieve it; you don't need to remember where it was stored. Just like
    a valet, who will go and get the car for you, you simply need to give them the
    ticket you received when you left your car.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在将对象存储与传统存储进行比较时，经常使用自动停车与代客泊车的类比。为什么这类似呢？因为在传统文件系统中，当你存储文件时，你将其存储在一个文件夹或目录中，你需要记住文件存储的位置，就像把车停在一个停车位上一样，你需要记住你的车停在哪个号码和楼层。另一方面，使用对象存储时，当你上传数据或将文件放入一个存储桶时，你会得到一个唯一的标识符，以后可以用来检索它；你不需要记住它存储在哪里。就像代客泊车员会为你取车一样，你只需要给他们你离开车时收到的票。
- en: Continuing with the valet-parking reference, you usually give your valet information
    about the car they need to get to you, not because they need it, but because they
    can identify your car better in this way—for instance, the color, plate number,
    or model of the car will help them a lot. With object storage, the process is
    the same. Each object has its own metadata, its unique ID, and the file itself,
    which are all part of the stored object.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用代客泊车的比喻，通常你会给代客泊车提供关于他们需要为你取车的信息，不是因为他们需要，而是因为这样可以更好地识别你的车，比如车的颜色、车牌号或车型将对他们有很大帮助。对象存储的过程也是一样的。每个对象都有自己的元数据、唯一ID和文件本身，这些都是存储对象的一部分。
- en: 'The following diagram shows what comprises an object in object storage:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了对象存储中包含的对象的组成部分：
- en: '![](img/9055589c-e121-4f79-b61c-773602d62904.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9055589c-e121-4f79-b61c-773602d62904.png)'
- en: 'As we have mentioned several times, object storage is accessed through RESTful
    APIs. So, in theory, any device that supports HTTP protocols can access your object
    storage buckets via HTTP methods such as `PUT` or `GET`. This sounds insecure,
    but, in fact, most software-defined object storage has some type of authentication
    method, and you require an authentication token in order to retrieve or upload
    files. A simple request using the Linux `curl` tool may look like this:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经多次提到的，对象存储是通过RESTful API访问的。因此，理论上，任何支持HTTP协议的设备都可以通过HTTP方法（如PUT或GET）访问你的对象存储桶。这听起来不安全，但事实上，大多数软件定义的对象存储都有某种类型的身份验证方法，你需要一个身份验证令牌才能检索或上传文件。使用Linux的curl工具的简单请求可能如下所示：
- en: '[PRE0]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, we can see how multiple distinct devices can connect to object storage
    buckets in the cloud through the HTTP protocol:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到多个不同的设备如何通过HTTP协议连接到云中的对象存储桶：
- en: '![](img/08e53f1f-9861-4a8b-b7f1-4f566628aefa.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08e53f1f-9861-4a8b-b7f1-4f566628aefa.png)'
- en: Why choose GlusterFS?
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择GlusterFS？
- en: Now that we understand the core concepts of SDS, storage clusters, and the differences
    between block, file, and object storage, we can go through some of the reasons
    why enterprise customers choose GlusterFS for their storage needs.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了SDS、存储集群以及块、文件和对象存储之间的区别，我们可以看一下企业客户选择GlusterFS作为他们存储需求的一些原因。
- en: As previously stated, GlusterFS is an SDS, that is, a layer that sits on top
    of traditional local storage mount points, allowing the aggregation of storage
    space between multiple nodes into a single storage entity or a storage cluster.
    GlusterFS can run on shelf-commodity hardware to private, public, or hybrid clouds.
    Although its primary usage is file storage (NAS), several plugins allow it to
    be used as a backend for block storage through the gluster-block plugin and for
    object storage with the gluster-swift plugin.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前所述，GlusterFS是一个SDS，它是一个位于传统本地存储挂载点之上的层，允许将多个节点之间的存储空间聚合成单个存储实体或存储集群。GlusterFS可以在私有、公共或混合云上运行。尽管其主要用途是文件存储（NAS），但几个插件允许它通过gluster-block插件用作块存储的后端，通过gluster-swift插件用作对象存储的后端。
- en: 'Some of the main features that define GlusterFS are as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 定义GlusterFS的一些主要特点如下：
- en: Commodity hardware
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 商品硬件
- en: Can be deployed on a private, public, or hybrid cloud
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以部署在私有、公共或混合云上
- en: No single point of failure
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有单点故障
- en: Scalability
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展性
- en: Asynchronous geo-replication
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异步地理复制
- en: Performance
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能
- en: Self-healing
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自愈
- en: Flexibility
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 灵活性
- en: GlusterFS features
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GlusterFS特点
- en: Let's go through each one of these features to understand why GlusterFS is so
    attractive to enterprise customers.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个了解这些特点，以了解为什么GlusterFS对企业客户如此有吸引力。
- en: Commodity hardware – GlusterFS runs on pretty much anything
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 商品硬件- GlusterFS几乎可以运行在任何东西上
- en: From **Advanced RISC Machines** (**ARM**) on a Raspberry Pi to any variety of
    x86 hardware, Gluster merely requires local storage used as a brick, which lays
    the foundation storage for volumes. There is no need for dedicated hardware or
    specialized storage controllers.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 从树莓派上的ARM（高级精简指令集计算机）到各种x86硬件，Gluster只需要作为砖块使用的本地存储，为卷奠定基础存储。不需要专用硬件或专门的存储控制器。
- en: In its most basic configuration, a single disk formatted as XFS can be used
    with a single node. While not the best configuration, it allows for further growth
    by adding more bricks or more nodes.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最基本的配置中，可以使用单个格式为XFS的磁盘与单个节点一起使用。虽然不是最佳配置，但可以通过添加更多的砖块或更多的节点来实现进一步的增长。
- en: GlusterFS can be deployed on private, public, or hybrid clouds
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GlusterFS可以部署在私有、公共或混合云上
- en: From a container image to a full VM dedicated to GlusterFS, one of the main
    points of interest for cloud customers is that since GlusterFS is merely software,
    it can be deployed on private, public, or hybrid clouds. Because there is no vendor,
    locking volumes that span different cloud providers is entirely possible. Allowing
    for multi-cloud provider volumes with high availability setups is done so that
    when one cloud provider has problems, the volume traffic can be moved to an entirely
    different provider with minimal-to-no downtime, depending on the configuration.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从容器镜像到专用于GlusterFS的完整VM，对云客户来说，一个主要的吸引点是，由于GlusterFS仅仅是软件，它可以部署在私有、公共或混合云上。因为没有供应商，跨不同云提供商的卷是完全可能的。允许使用高可用性设置的多云提供商卷，这样当一个云提供商出现问题时，卷流量可以转移到完全不同的提供商，几乎没有或没有停机时间，这取决于配置。
- en: No single point of failure
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无单点故障
- en: Depending on the volume configuration, data is distributed across multiple nodes
    in the cluster, removing a single point of failure, as no `head`or `master`node
    controls the cluster.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 根据卷配置，数据分布在集群中的多个节点上，消除了单点故障，因为没有“head”或“master”节点控制集群。
- en: Scalability
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可扩展性
- en: GlusterFS allows for the smooth scaling of resources by vertically adding new
    bricks, or by horizontally adding new nodes to the cluster.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: GlusterFS允许通过垂直添加新的砖块或通过水平添加新的节点来平稳扩展资源。
- en: All this can be done online while the cluster serves data, without any disruption
    to the client's communication.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都可以在线完成，而集群提供数据，而不会对客户的通信造成任何中断。
- en: Asynchronous geo-replication
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异步地理复制
- en: GlusterFS takes the no-single-point-of-failure concept, which provides geo-replication,
    allowing data to be asynchronously replicated to clusters in entirely different
    geophysical data centers.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: GlusterFS采用无单点故障概念，提供地理复制，允许数据异步复制到完全不同地理数据中心的集群中。
- en: 'The following diagram shows geo-replication across multiple sites:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了跨多个站点的地理复制：
- en: '![](img/6a4c90ba-0cb9-499b-928a-86e6aeaff77e.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a4c90ba-0cb9-499b-928a-86e6aeaff77e.png)'
- en: Performance
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能
- en: Since data is distributed across multiple nodes, we can also have multiple clients
    accessing the cluster at the same time. This process of accessing data from multiple
    sources simultaneously is called parallelism, and GlusterFS allows for increased
    performance by directing clients to different nodes. Additionally, performance
    can be increased by adding bricks or nodes—effectively, by scaling horizontally
    or vertically.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据分布在多个节点上，我们还可以同时有多个客户端访问集群。从多个来源同时访问数据的过程称为并行性，GlusterFS通过将客户端指向不同的节点来实现增加性能。此外，通过添加砖块或节点，可以增加性能，有效地通过水平或垂直扩展。
- en: Self-healing
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自愈
- en: In the case of unexpected downtime, the remaining nodes can still serve traffic.
    If new data is added to the cluster while one of the nodes is down, this data
    needs to be synchronized once the node is brought back up.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在意外停机的情况下，剩余节点仍然可以提供流量服务。如果在其中一个节点宕机时向集群添加了新数据，则需要在节点恢复后同步这些数据。
- en: GlusterFS will automatically self-heal these new files once they're accessed,
    triggering a self-heal operation between the nodes and copying the missing data.
    This is transparent to the users and clients.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: GlusterFS将在访问这些新文件时自动自愈，触发节点之间的自愈操作并复制丢失的数据。这对用户和客户是透明的。
- en: Flexibility
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 灵活性
- en: GlusterFS can be deployed on-premises on already existing hardware or existing
    virtual infrastructure, on the cloud as a VM, or as a container. There is no lock-in
    as to how it needs to be deployed, and customers can decide what suits their needs
    best.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: GlusterFS可以部署在现有硬件或虚拟基础设施上，也可以作为VM在云上部署，或作为容器。它的部署方式并不受限制，客户可以决定最适合他们需求的方式。
- en: Remote direct memory access (RDMA)
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 远程直接内存访问（RDMA）
- en: RDMA allows for ultra-low latency and extremely high-performance network communication
    between the Gluster server and the Gluster clients. GlusterFS can leverage RDMA
    for **high-performance computing** (**HPC**) applications and highly-concurrent
    workloads.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: RDMA允许Gluster服务器和Gluster客户端之间进行超低延迟和极高性能的网络通信。GlusterFS可以利用RDMA进行高性能计算（HPC）应用和高并发工作负载。
- en: Gluster volume types
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gluster卷类型
- en: Having gained an understanding of the core features of GlusterFS, we can now
    define the different types of volumes that GlusterFS provides. This will help
    in the next chapters as we dive into the actual design of a GlusterFS solution.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通过了解GlusterFS的核心特性，我们现在可以定义GlusterFS提供的不同类型的卷。这将有助于我们在接下来的章节中深入讨论GlusterFS解决方案的实际设计。
- en: GlusterFS provides the flexibility of choosing the type of volume that best
    suits the needs of the workload; for example, for a high-availability requirement,
    we can use replicated volume. This type of volume replicates the data between
    two or more nodes, resulting in exact copies of each of the nodes.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: GlusterFS提供了选择最适合工作负载需求的卷类型的灵活性；例如，对于高可用性要求，我们可以使用复制卷。这种类型的卷在两个或更多节点之间复制数据，导致每个节点的精确副本。
- en: 'Let''s quickly list the types of volumes that are available, and later, we''ll
    discuss each of their advantages and disadvantages:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速列出可用的卷类型，稍后我们将讨论它们各自的优缺点：
- en: Distributed
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式
- en: Replicated
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制
- en: Distributed replicated
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式复制
- en: Dispersed
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分散
- en: Distributed dispersed
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式分散
- en: Distributed
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式
- en: As the name implies, data is distributed across the bricks in the volume and
    across the nodes. This type of volume allows for a seamless and low-cost increase
    in available space. The main drawback is that there is no data redundancy since
    files are allocated between bricks that could be on the same node or different
    nodes. It is mainly used for high-storage-capacity and concurrency applications.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，数据分布在卷和节点的砖块之间。这种类型的卷允许可用空间的无缝和低成本增加。主要缺点是没有数据冗余，因为文件分配在可能在同一节点或不同节点上的砖块之间。它主要用于高存储容量和并发应用。
- en: Think of this volume type as **just a bunch of disks** (**JBOD**) or a linear **logical
    volume manager** (**LVM**) where space is just aggregated without any stripping
    or parity.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 将这种卷类型视为**一堆磁盘**（JBOD）或线性**逻辑卷管理器**（LVM），其中空间只是聚合而没有任何分割或奇偶校验。
- en: 'The following diagram shows a distributed volume:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了一个分布式卷：
- en: '![](img/54bad647-9417-49c4-ae05-6402334472b3.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/54bad647-9417-49c4-ae05-6402334472b3.png)'
- en: Replicated
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复制
- en: In a replicated volume, data is copied across bricks on different nodes. Expanding
    a replicated volume requires the same number of replicas to be added. For example,
    if I have a volume with two replicas and I want to expand it, I require a total
    of four replicas.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在复制卷中，数据被复制到不同节点上的砖块上。扩展复制卷需要添加相同数量的副本。例如，如果我有一个带有两个副本的卷，我想要扩展它，我需要总共四个副本。
- en: A replicated volume can be compared to a RAID1, where data is mirrored between
    all available nodes. One of its shortcomings is that scalability is relatively
    limited. On the other hand, its main characteristic is high availability, as data
    is served even in the event of unexpected downtime.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 复制卷可以与RAID1相比，其中数据在所有可用节点之间进行镜像。它的缺点之一是可扩展性相对有限。另一方面，它的主要特点是高可用性，因为即使在意外停机的情况下，数据也可以被访问。
- en: With this type of volume, mechanisms to avoid split-brain situations must be
    implemented. A split-brain occurs when new data is written to the volume, and
    different sets of nodes are allowed to process writes separately. Server quorum
    is such a mechanism, as it allows for a tiebreaker to exist.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种类型的卷，必须实施机制来避免脑裂情况。当新数据被写入卷时，不同的节点集被允许分别处理写入时就会发生脑裂。服务器仲裁是这样一种机制，因为它允许存在一个决胜者。
- en: 'The following diagram shows a replicated volume:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了一个复制卷：
- en: '![](img/f74ac36a-4b5e-4784-8cb0-db549f1313fc.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f74ac36a-4b5e-4784-8cb0-db549f1313fc.png)'
- en: Distributed replicated
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式复制
- en: A distributed replicated volume is similar to a replicated volume, with the
    main difference being that replicated volumes are distributed. To explain this,
    consider having two separate replicated volumes, each with 10 TB of space. When
    both are distributed, the volume ends up with a total of 20 TB of space.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式复制卷类似于复制卷，其主要区别在于复制卷是分布式的。为了解释这一点，考虑有两个单独的复制卷，每个卷有10TB的空间。当两者都被分布时，卷最终拥有总共20TB的空间。
- en: This type of volume is mainly used when both high availability and redundancy
    are needed, as the cluster can tolerate node failures.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要高可用性和冗余性时，主要使用这种类型的卷，因为集群可以容忍节点故障。
- en: 'The following diagram shows a distributed replicated volume:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了一个分布式复制卷：
- en: '![](img/1c01cd67-92b1-4cc1-8a91-93d2597d0327.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c01cd67-92b1-4cc1-8a91-93d2597d0327.png)'
- en: Dispersed
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分散
- en: Dispersed volumes take the best of both distributed and replicated volumes by
    stripping the data across all available bricks and, at the same time, allowing
    redundancy. Bricks should be of the same size, as the volume suspends all writes
    once the smallest brick becomes full. For example, imagine a dispersed volume
    such as a RAID 5 or 6, where data is stripped and parity is created, allowing
    data to be reconstructed from the parity. While the analogy helps us to understand
    this type of volume, the actual process is entirely different as it uses erasure
    codes where data is broken into fragments. Dispersed volumes provide the right
    balance of performance, space, and high availability.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 分散卷通过在所有可用的砖块上分割数据，并同时允许冗余性，兼具分布式和复制卷的优点。砖块应该是相同大小的，因为一旦最小的砖块满了，卷就会暂停所有写入。例如，想象一个分散卷，比如RAID
    5或6，数据被分割并创建奇偶校验，允许从奇偶校验中重建数据。虽然这个类比帮助我们理解这种类型的卷，但实际过程完全不同，因为它使用了数据被分成片段的纠错码。分散卷提供了性能、空间和高可用性的正确平衡。
- en: Distributed dispersed
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式分散
- en: In a distributed dispersed volume, data is distributed across volumes of a dispersed
    type. Redundancy is provided at the dispersed volume level, having similar advantages
    to a distributed replicated volume.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式分散卷中，数据分布在分散类型的卷中。冗余性是在分散卷级别提供的，具有与分布式复制卷相似的优势。
- en: Imagine a JBOD on top of two RAID 5 arrays—growing this type of volume requires
    an additional dispersed volume. While not necessarily the same size, ideally,
    it should maintain the same characteristics to avoid complications.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下在两个RAID 5阵列之上的JBOD上存储数据——扩展这种类型的卷需要额外的分散卷。虽然不一定是相同的大小，但理想情况下，它应该保持相同的特性以避免复杂性。
- en: The need for highly redundant storage
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对高度冗余存储的需求
- en: With an increase in the available space for applications comes an increased
    demand on the storage. Applications may require access to their information all
    of the time without any disruption that could cause the entire business continuity
    to be at risk. No company wants to have to deal with an outage, let alone an interruption
    in the central infrastructure that leads to money being lost, customers not being
    served, and users not being able to log in to their accounts because of bad decisions.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 随着应用程序可用空间的增加，对存储的需求也在增加。应用程序可能需要随时访问它们的信息，而不会出现任何可能导致整个业务连续性受到威胁的中断。没有公司希望出现停机，更不用说中央基础设施的中断导致损失、客户无法服务和用户因错误决策而无法登录到他们的帐户。
- en: Let's consider storing data on a traditional monolithic storage array—doing
    this can cause significant risks as everything is in a single place. A single
    massive storage array containing all of the company's information signifies an
    operational risk as the array is predisposed to fail. Every single type of hardware—no
    matter how good—fails at some point.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑将数据存储在传统的单片存储阵列上——这样做可能会带来重大风险，因为一切都集中在一个地方。一个包含公司所有信息的大型存储阵列意味着操作风险，因为该阵列容易出现故障。每一种硬件——无论多么好——都会在某个时候出现故障。
- en: Monolithic arrays tend to handle failures by providing some form of redundancy
    through the use of traditional RAID methods used on the disk level. While this
    is good for small local storage that serves a couple of hundred users, this might
    not be a good idea when we reach the petascale and storage space and active concurrent
    users increase drastically. In specific scenarios, a RAID recovery can cause the
    entire storage system to go down or degrade performance to the point that the
    application doesn't work as expected. Additionally, with increased disk sizes
    and single-disk performance being the same over the past couple of years, recovering
    a single disk now takes a more substantial amount of time; rebuilding 1 TB disks
    is not the same as rebuilding 10 TB disks.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 单体阵列倾向于通过在磁盘级别使用传统的RAID方法来提供某种形式的冗余来处理故障。虽然这对为数百用户提供服务的小型本地存储来说是不错的，但是当我们达到PB级别并且存储空间和活跃并发用户大幅增加时，这可能不是一个好主意。在特定情况下，RAID恢复可能导致整个存储系统崩溃或性能下降到应用程序无法正常工作的程度。此外，随着磁盘容量的增加和单个磁盘性能在过去几年内保持不变，现在恢复单个磁盘需要更长的时间；重建1TB磁盘不同于重建10TB磁盘。
- en: Storage clusters, such as GlusterFS, handle redundancy differently by providing
    methods that best fit the workload. For example, when using a replicated volume,
    data is mirrored from one node to another. If a node goes down, then traffic is
    seamlessly directed to the remaining nodes, being utterly transparent to the users.
    Once the problematic node is serviced, it can be quickly put back into the cluster,
    where it will go through self-healing of the data. In comparison to traditional
    storage, a storage cluster removes the single point of failure by distributing
    data to multiple members of the clusters.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 存储集群，如GlusterFS，通过提供最适合工作负载的方法来处理冗余。例如，当使用复制卷时，数据从一个节点镜像到另一个节点。如果一个节点宕机，那么流量会无缝地转向剩余的节点，对用户来说完全透明。一旦问题节点得到处理，它可以迅速重新加入集群，在那里它将进行数据的自我修复。与传统存储相比，存储集群通过将数据分发到集群的多个成员来消除单点故障。
- en: Having increased availability means that we can reach the application service-level
    agreements and maintain the desired uptime.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 提高可用性意味着我们可以达到应用程序服务级别协议并保持所需的正常运行时间。
- en: Disaster recovery
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 灾难恢复
- en: There's no escaping from it—disasters happen, whether it's natural or human
    error. What counts is how well-prepared we are for them, and how fast and efficiently
    we can recover.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 逃不过灾难——无论是自然灾害还是人为错误。重要的是我们为此做好了多少准备，以及我们能多快多高效地恢复。
- en: 'Implementing disaster recovery protocols is of utmost importance for business
    continuity. There are two terms that we need to understand before proceeding:
    **recovery time objective** (**RTO**) and **recovery point objective** (**RPO**).
    Let''s take a quick glance at each. RTO is the time it takes to recover from a
    failure or event that causes a disruption. Put simply, it refers to how fast we
    can get the application back up. RPO, on the other hand, refers to how far the
    data can go back in time without affecting business continuity, that is, how much
    data you can lose.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 实施灾难恢复协议对业务连续性至关重要。在继续之前，有两个术语我们需要了解：**恢复时间目标**（**RTO**）和**恢复点目标**（**RPO**）。让我们快速浏览一下每个术语。RTO是从故障或事件导致中断开始恢复所需的时间。简单地说，它指的是我们可以多快将应用程序恢复正常。另一方面，RPO指的是数据可以在不影响业务连续性的情况下回溯多远的时间，也就是说，你可以丢失多少数据。
- en: 'The concept of RPO looks something like this:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: RPO的概念看起来像这样：
- en: '![](img/13880b4a-7116-4947-bb98-e1a8bbeb6aa9.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13880b4a-7116-4947-bb98-e1a8bbeb6aa9.png)'
- en: RTO
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RTO
- en: As previously stated, this is the amount of time it takes to recover a functionality
    after a failure. Depending on the complexity of the solution, RTO might take a
    considerable amount of time.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，这是在故障后恢复功能所需的时间。根据解决方案的复杂性，RTO可能需要相当长的时间。
- en: Depending on the business requirements, RTO might be as short as a couple of
    hours. This is where designing a highly redundant solution comes into play—by
    decreasing the amount of time that is required to be operational again.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 根据业务需求，RTO可能只有几个小时。这就是设计高度冗余解决方案的作用所在——通过减少恢复所需的时间。
- en: RPO
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RPO
- en: This is the amount of time data that can be lost and still go back to a recovery
    point, in other words, this is how often recovery points are taken; in the case
    of backups, how often a backup is taken (it could be hourly, daily, or weekly),
    and in the case of a storage cluster, how often changes are replicated.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这是可以丢失的数据量，仍然可以返回到恢复点的时间，换句话说，这是恢复点被采取的频率；在备份的情况下，备份的频率（可以是每小时、每天或每周），以及在存储集群的情况下，更改的复制频率。
- en: One thing to take into consideration is the speed at which changes can be replicated,
    as we want changes to be replicated almost immediately; however, due to bandwidth
    constraints, the real time replication is not possible most of the time.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑的一点是变化可以被复制的速度，因为我们希望变化几乎可以立即被复制；然而，由于带宽限制，实时复制大多数时候是不可能的。
- en: 'Finally, an essential factor to consider is how data is replicated. Generally,
    there are two types of replication: synchronous and asynchronous.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，需要考虑的一个重要因素是数据如何被复制。一般来说，有两种类型的复制：同步和异步。
- en: Synchronous replication
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 同步复制
- en: Synchronous replication means that data is replicated immediately after it is
    written. This is useful for minimizing RPO, as there is no wait or drift between
    the data from one node to another. A GlusterFS-replicated volume provides this
    kind of replication. Bandwidth should also be considered, as changes need to be
    committed immediately.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 同步复制意味着数据在写入后立即复制。这对于最小化RPO是有用的，因为从一个节点到另一个节点的数据之间没有等待或漂移。GlusterFS复制的卷提供了这种复制。带宽也应该被考虑，因为更改需要立即提交。
- en: Asynchronous replication
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异步复制
- en: Asynchronous replication means that the data is replicated into fragments of
    time, for example, every 10 minutes. During set up, the RPO is chosen based on
    several factors, including the business need and the bandwidth that is available.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 异步复制意味着数据在时间片段内复制，例如，每10分钟。在设置过程中，根据业务需求和可用带宽等多个因素选择RPO。
- en: Bandwidth is the primary consideration; this is because, depending on the size
    of the changes, the real time replication might not fit in the RPO window, requiring
    a more considerable replication time and directly affecting RPO times. If unlimited
    bandwidth is available, synchronous replication should be chosen.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 带宽是主要考虑因素；这是因为根据更改的大小，实时复制可能无法适应RPO窗口，需要更长的复制时间，直接影响RPO时间。如果有无限带宽可用，应选择同步复制。
- en: In hindsight, we, as IT architects, spend a significant amount of time trying
    to figure out how to make our systems more resilient. Indeed, successfully decreasing
    RTO and RPO times can mark the difference between a partially thought out solution
    and an entirely architected design.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾过去，作为IT架构师，我们花费了大量时间试图找出如何使我们的系统更具弹性。事实上，成功降低RTO和RPO时间可能标志着部分考虑周到的解决方案和完全设计良好的设计之间的差异。
- en: The need for high performance
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高性能的需求
- en: With more and more users accessing the same resources, response times get slower
    and applications start taking longer to process. The performance of traditional
    storage has not changed in the last couple of years—a single HDD yields about
    150 MB/s with response times of several milliseconds. With the introduction of
    flash media and protocols such as **non-volatile memory express** (**NVMe**),
    a single SSD can easily achieve gigabytes-per-second and sub-millisecond response
    times; SDS can leverage these new technologies to provide increased performance
    and significantly reduce response times.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 随着越来越多的用户访问相同的资源，响应时间变慢，应用程序开始处理时间变长。传统存储的性能在过去几年没有改变——单个HDD的吞吐量约为150 MB/s，响应时间为几毫秒。随着闪存介质和诸如非易失性内存扩展（NVMe）之类的协议的引入，单个SSD可以轻松实现每秒几十亿字节的吞吐量和亚毫秒的响应时间；SDS可以利用这些新技术提供增加的性能并显著减少响应时间。
- en: Enterprise storage is designed to handle multiple concurrent requests for hundreds
    of clients who are trying to get their data as fast as possible, but when the
    performance limits are reached, traditional monolithic storage starts slowing
    down, causing applications to fail as requests are not completed in time. Increasing
    the performance of this type of storage comes at a high price and, in most cases,
    it can't be done while the storage is still serving data.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 企业存储被设计为处理数百个客户的多个并发请求，这些客户都希望尽快获取他们的数据，但当性能限制达到时，传统的单体存储开始变慢，导致应用程序失败，因为请求未能及时完成。提高这种类型存储的性能代价高，并且在大多数情况下，无法在存储仍在提供数据的情况下完成。
- en: The need for increased performance comes from the increased load in storage
    servers; with the explosion in data consumption, users are storing much more information
    and require it much faster than before.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 增加性能的需求来自存储服务器负载的增加；随着数据消费的激增，用户存储的信息量比以往更多，并且需要比以前更快地获取。
- en: Applications also require data to be delivered to them as quickly as possible;
    for example, consider the stock market, where data is requested multiple times
    a second by thousands of users. At the same time, another thousand users are continuously
    writing new data. If a single transaction is not committed in time, people will
    not be able to make the correct decision when buying or selling stocks because
    the wrong information is displayed.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序还需要尽快将数据传递给它们；例如，考虑股票市场，数千用户每秒多次请求数据。与此同时，另外一千用户不断地写入新数据。如果单个交易未能及时提交，人们将无法根据正确的信息做出买卖股票的决定。
- en: The previous problem is something that architects have to face when designing
    a solution that can deliver the expected performance that is necessary for the
    application to work as expected. Taking the right amount of time to size storage
    solutions correctly makes the entire process flow smoother with less back and
    forth between design and implementation.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的问题是架构师在设计能够提供应用所需性能的解决方案时必须面对的问题。花费适当的时间正确规划存储解决方案可以使整个流程更加顺畅，减少设计和实施之间的来回。
- en: Storage systems, such as GlusterFS, can serve thousands of concurrent users
    simultaneously without a significant decrease in performance, as data is spread
    across multiple nodes in the cluster. This approach is considerably better than
    accessing a single storage location, such as with traditional arrays.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如GlusterFS之类的存储系统可以同时为数千个并发用户提供服务，而不会出现性能显著下降，因为数据分布在集群中的多个节点上。这种方法比访问单个存储位置（例如传统阵列）要好得多。
- en: Parallel I/O
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行I/O
- en: I/O refers to the process of requesting and writing data to a storage system.
    The process is done through I/O streams, where data is requested one block, file,
    or object at a time.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: I/O是指向存储系统请求和写入数据的过程。该过程通过I/O流进行，数据一次请求一个块、文件或对象。
- en: Parallel I/O refers to the process where multiple streams perform operations
    concurrently on the same storage system. This increases performance and reduces
    access times, as various files or blocks are read or written at the same time.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 并行I/O是指多个流同时在同一存储系统上执行操作的过程。这增加了性能并减少了访问时间，因为各种文件或块同时读取或写入。
- en: In comparison, serial I/O is the process of performing a single stream of I/O,
    which could lead to reduced performance and increased latency or access times.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，串行I/O是执行单个I/O流的过程，这可能会导致性能降低和延迟或访问时间增加。
- en: Storage clusters, such as GlusterFS, take advantage of parallel I/O, since data
    is spread through multiple nodes, allowing for numerous clients to access data
    at the same time without any drop in latency or throughput.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 存储集群（如GlusterFS）利用并行I/O的优势，因为数据分布在多个节点上，允许多个客户端同时访问数据，而不会出现延迟或吞吐量下降。
- en: Summary
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we went through the core concepts of what a cluster is and
    defined it as a set of computers called nodes working together in the same type
    of workload. A compute cluster's primary function is to perform tasks that run
    CPU-intensive workloads, which are designed to reduce processing time. A storage
    cluster's function is to aggregate available storage resources into a single storage
    space that simplifies management and allows you to efficiently reach the petascale
    or go beyond the 1-PB available space. Then, we explored how SDS is changing the
    way that data is stored and how GlusterFS is one of the projects that is leading
    this change. SDS allows for the simplified management of storage resources, while
    at the same time adding features that were impossible with traditional monolithic
    storage arrays.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入了解了集群的核心概念，并将其定义为一组计算机节点，它们一起处理相同类型的工作负载。计算集群的主要功能是执行运行CPU密集型工作负载的任务，旨在减少处理时间。存储集群的功能是将可用的存储资源聚合到一个单一的存储空间中，简化管理，并允许您有效地达到PB级别或超过1PB的可用空间。然后，我们探讨了SDS如何改变数据存储方式，以及GlusterFS是领导这一变革的项目之一。SDS允许简化存储资源的管理，同时添加了传统的单片式存储阵列无法实现的功能。
- en: To further understand how applications interact with storage, we defined the
    core differences between block, file, and object storage. Primarily, block storage
    deals with logical blocks of data in a storage device, file storage works by reading
    or writing actual files from a storage space, and object storage provides metadata
    to each object for further interaction. With these concepts of different interactions
    with storage in mind, we went on to point out the characteristics of GlusterFS
    that make it attractive for enterprise customers and how these features tie into
    what SDS stands for.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步了解应用程序与存储的交互方式，我们定义了块、文件和对象存储之间的核心区别。主要地，块存储处理存储设备中的逻辑数据块，文件存储通过从存储空间读取或写入实际文件来工作，对象存储为每个对象提供元数据以进行进一步交互。有了这些不同存储交互方式的概念，我们继续指出了使GlusterFS对企业客户具有吸引力的特征，以及这些特性如何与SDS的定义联系起来。
- en: Finally, we delved into the main reasons why high availability and high performance
    are a must for every storage design and how performing parallel, or serial, I/O
    can affect application performance.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们深入探讨了为什么高可用性和高性能对于每个存储设计都是必不可少的主要原因，以及如何执行并行或串行I/O会影响应用程序性能。
- en: In the next chapter, we will dive into the actual process of architecting a
    GlusterFS storage cluster.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨架构GlusterFS存储集群的实际过程。
- en: Questions
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: How can I optimize my storage performance?
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我如何优化我的存储性能？
- en: What type of workload is GlusterFS better suited for?
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GlusterFS更适合哪种工作负载？
- en: Which cloud providers offer object storage?
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪些云提供商提供对象存储？
- en: What types of storage does GlusterFS offer?
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GlusterFS提供哪些类型的存储？
- en: Does Red Hat own GlusterFS?
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 红帽是否拥有GlusterFS？
- en: Do I have to pay to use GlusterFS?
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我使用GlusterFS需要付费吗？
- en: Does Gluster offer disaster recovery or replication?
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gluster是否提供灾难恢复或复制？
- en: Further reading
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Ceph Cookbook – Second Edition* by Vikhyat Umrao and Michael Hackett: [https://prod.packtpub.com/in/virtualization-and-cloud/ceph-cookbook-second-edition](https://prod.packtpub.com/in/virtualization-and-cloud/ceph-cookbook-second-edition)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '* Ceph Cookbook – Second Edition * by Vikhyat Umrao and Michael Hackett: [https://prod.packtpub.com/in/virtualization-and-cloud/ceph-cookbook-second-edition](https://prod.packtpub.com/in/virtualization-and-cloud/ceph-cookbook-second-edition)'
- en: '*Mastering Ceph* by Nick Fisk: [https://prod.packtpub.com/in/big-data-and-business-intelligence/mastering-ceph](https://prod.packtpub.com/in/big-data-and-business-intelligence/mastering-ceph)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '* Mastering Ceph * by Nick Fisk: [https://prod.packtpub.com/in/big-data-and-business-intelligence/mastering-ceph](https://prod.packtpub.com/in/big-data-and-business-intelligence/mastering-ceph)'
- en: '*Learning Ceph – Second Edition *by Anthony D''Atri and Vaibhav Bhembre: [https://prod.packtpub.com/in/virtualization-and-cloud/learning-ceph-second-edition](https://prod.packtpub.com/in/virtualization-and-cloud/learning-ceph-second-edition)'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '* Learning Ceph – Second Edition * by Anthony D''Atri and Vaibhav Bhembre:
    [https://prod.packtpub.com/in/virtualization-and-cloud/learning-ceph-second-edition](https://prod.packtpub.com/in/virtualization-and-cloud/learning-ceph-second-edition)'

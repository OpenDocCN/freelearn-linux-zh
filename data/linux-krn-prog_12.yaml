- en: The CPU Scheduler - Part 1
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: CPU调度器 - 第1部分
- en: In this chapter and the next, you will dive into the details regarding a key
    OS topic – that is, CPU scheduling on the Linux OS. I will try and keep the learning
    more hands-on, by asking (and answering) typical questions and performing common
    tasks related to scheduling. Understanding how scheduling works at the level of
    the OS is not only important from a kernel (and driver) developer viewpoint, but
    it will also automatically make you a better system architect (even for user space
    applications).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章和下一章中，您将深入了解一个关键的操作系统主题 - 即Linux操作系统上的CPU调度。我将尝试通过提出（并回答）典型问题并执行与调度相关的常见任务来使学习更加实际。了解操作系统级别的调度工作不仅对于内核（和驱动程序）开发人员来说很重要，而且还会自动使您成为更好的系统架构师（甚至对于用户空间应用程序）。
- en: We shall begin by covering essential background material; this will include
    the **Kernel Schedulable Entity** (**KSE**) on Linux, as well as the POSIX scheduling
    policies that Linux implements. We will then move on to using tools – `perf` and
    others – to visualize the flow of control as the OS runs tasks on CPUs and switches
    between them. This is useful to know when profiling apps as well! After that,
    we will dive deeper into the details of how exactly CPU scheduling works on Linux,
    covering modular scheduling classes, **Completely Fair Scheduling** (**CFS**),
    the running of the core schedule function, and so on. Along the way, we will also
    cover how you can programmatically (and dynamically) query and set the scheduling
    policy and priority of any thread on the system.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍基本背景材料；这将包括Linux上的**内核可调度实体**（**KSE**），以及Linux实现的POSIX调度策略。然后，我们将使用工具
    - `perf`和其他工具 - 来可视化操作系统在CPU上运行任务并在它们之间切换的控制流。这对于应用程序的性能分析也很有用！之后，我们将更深入地了解Linux上CPU调度的工作原理，包括模块化调度类、**完全公平调度**（**CFS**）、核心调度函数的运行等。在此过程中，我们还将介绍如何以编程方式（动态地）查询和设置系统上任何线程的调度策略和优先级。
- en: 'In this chapter, we will cover the following areas:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下领域：
- en: Learning about the CPU scheduling internals – part 1 – essential background
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习CPU调度内部 - 第1部分 - 基本背景
- en: Visualizing the flow
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化流程
- en: Learning about the CPU scheduling internals – part 2
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习CPU调度内部 - 第2部分
- en: Threads – which scheduling policy and priority
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程 - 调度策略和优先级
- en: Learning about the CPU scheduling internals – part 3
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习CPU调度内部 - 第3部分
- en: Now, let's get started with this interesting topic!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始这个有趣的话题吧！
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: I assume that you have gone through [Chapter 1](ad75db43-a1a2-4f3f-92c7-a544f47baa23.xhtml), *Kernel
    Workspace Setup*, and have appropriately prepared a guest **Virtual Machine**
    (**VM**) running Ubuntu 18.04 LTS (or a later stable release) and installed all
    the required packages. If not, I highly recommend you do this first.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设您已经阅读了[第1章](ad75db43-a1a2-4f3f-92c7-a544f47baa23.xhtml) *内核工作区设置*，并已经适当地准备了一个运行Ubuntu
    18.04 LTS（或更高稳定版本）的客户**虚拟机**（**VM**）并安装了所有必需的软件包。如果没有，我强烈建议您首先这样做。
- en: To get the most out of this book, I strongly recommend you first set up the
    workspace environment, including cloning this book's GitHub repository for the
    code and working on it in a hands-on fashion. The repository can be found here: [https://github.com/PacktPublishing/Linux-Kernel-Programming](https://github.com/PacktPublishing/Linux-Kernel-Programming).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用本书，我强烈建议您首先设置工作环境，包括克隆本书的GitHub存储库以获取代码并进行实际操作。存储库可以在这里找到：[https://github.com/PacktPublishing/Linux-Kernel-Programming](https://github.com/PacktPublishing/Linux-Kernel-Programming)。
- en: Learning about the CPU scheduling internals – part 1 – essential background
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习CPU调度内部 - 第1部分 - 基本背景
- en: Let's take a quick look at the essential background information we require to
    understand CPU scheduling on Linux.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速了解一下我们需要了解Linux上CPU调度的基本背景信息。
- en: 'Note that in this book, we do not intend to cover material that competent system
    programmers on Linux should already be well aware of; this includes basics such
    as process (or thread) states, the state machine and transitions on it, and more
    information on what real time is, the POSIX scheduling policies, and so on. This
    (and more) has been covered in some detail in my earlier book: *Hands-On System
    Programming with Linux*, published by Packt in October 2018.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在本书中，我们不打算涵盖Linux上熟练的系统程序员应该已经非常了解的材料；这包括基础知识，如进程（或线程）状态，状态机及其转换，以及更多关于实时性、POSIX调度策略等的信息。这些内容（以及更多内容）已经在我之前的一本书中详细介绍过：《Linux系统编程实战》，由Packt于2018年10月出版。
- en: What is the KSE on Linux?
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Linux上的KSE是什么？
- en: As you learned in [Chapter 6](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml), *Kernel
    Internals Essentials – Processes and Threads*, in the *Organizing processes, threads,
    and their stacks – user and kernel space* section, every process – in fact, every
    thread alive on the system – is bestowed with a task structure (`struct task_struct`)
    and both a user-mode as well as a kernel-mode stack.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[第6章](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml)中所学到的，在*内核内部要点 - 进程和线程*一节中，每个进程
    - 实际上是系统上存活的每个线程 - 都被赋予一个任务结构（`struct task_struct`）以及用户模式和内核模式堆栈。
- en: 'Here, the key question to ask is: when scheduling is performed, *what object
    does it act upon*, in other words, what is the **Kernel Schedulable Entity**,
    the **KSE**? On Linux, **the KSE is a thread**, not a process (of course, every
    process contains a minimum of one thread). Thus, the thread is the granularity
    level at which scheduling is performed.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，需要问的关键问题是：在进行调度时，*它作用于哪个对象*，换句话说，什么是**内核可调度实体**，**KSE**？在Linux上，**KSE是一个线程**，而不是一个进程（当然，每个进程至少包含一个线程）。因此，线程是进行调度的粒度级别。
- en: 'An example will help explain this: if we have a hypothetical situation where
    we have one CPU core and 10 user space processes, consisting of three threads
    each, plus five kernel threads, then we have a total of (10 x 3) + 5, which equals
    35 threads. Each of them, except for the five kernel threads, has a user and kernel
    stack and a task structure (the kernel threads only have kernel stacks and task
    structures; all of this has been thoroughly explained in [Chapter 6](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml),
    *Kernel Internals Essentials – Processes and Threads*, in the *Organizing processes,
    threads, and their stacks – user and kernel space* section). Now, if all these
    35 threads are runnable, they then  compete for the single processor (though it''s
    unlikely that they''re all runnable simultaneously, but let''s just consider it
    for the sake of discussion), then we now have 35 *threads* in competition for
    the CPU resource, not 10 processes and five kernel threads.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子来解释这一点：如果我们假设有一个CPU核心和10个用户空间进程，每个进程包括三个线程，再加上五个内核线程，那么我们总共有（10 x 3）+ 5，等于35个线程。除了五个内核线程外，每个线程都有用户和内核栈以及一个任务结构（内核线程只有内核栈和任务结构；所有这些都在[第6章](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml)中得到了详细解释，*内核内部要点-进程和线程*，在*组织进程、线程及其栈-用户空间和内核空间*部分）。现在，如果所有这35个线程都是可运行的，那么它们将竞争单个处理器（尽管它们不太可能同时都是可运行的，但为了讨论的完整性，让我们假设它们都是可运行的），那么现在有35个*线程*竞争CPU资源，而不是10个进程和五个内核线程。
- en: Now that we understand that the KSE is a thread, we will (almost) always refer
    to the thread in the context of scheduling. Now that this is understood, let's
    move on to the scheduling policies Linux implements.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了KSE是一个线程，我们（几乎）总是在调度上下文中引用线程。既然这一点已经理解，让我们继续讨论Linux实现的调度策略。
- en: The POSIX scheduling policies
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: POSIX调度策略
- en: It's important to realize that the Linux kernel does not have just one algorithm
    that implements CPU scheduling; the fact is, the POSIX standard specifies a minimal
    three scheduling policies (algorithms, in effect) that a POSIX-compliant OS must
    adhere to. Linux goes above and beyond, implementing these three as well as more,
    with a powerful design called scheduling classes (more on this in the *Understanding
    modular scheduling classes* section later in this chapter).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要意识到Linux内核不仅实现了一个实现CPU调度的算法；事实上，POSIX标准规定了一个POSIX兼容的操作系统必须遵循的最少三种调度策略（实际上是算法）。Linux不仅实现了这三种，还实现了更多，采用了一种称为调度类的强大设计（稍后在本章的*理解模块化调度类*部分中详细介绍）。
- en: Again, information on the POSIX scheduling policies on Linux (and more) is covered
    in more detail in my earlier book, *Hands-On System Programming with Linux*, published
    by Packt in October 2018.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '关于Linux上的POSIX调度策略（以及更多）的信息在我早期的书籍*Hands-On System Programming with Linux*中有更详细的介绍，该书于2018年10月由Packt出版。 '
- en: 'For now, let''s just briefly summarize the POSIX scheduling policies and what
    effect they have in the following table:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们简要总结一下POSIX调度策略以及它们在下表中的影响：
- en: '| **Scheduling policy** | **Key points** | **Priority scale** |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| **调度策略** | **关键点** | **优先级范围** |'
- en: '| `SCHED_OTHER` or `SCHED_NORMAL` | Always the default; threads with this policy
    are non-real-time; internally implemented as a **Completely Fair Scheduling**
    (**CFS**) class (seen later in the *A word on CFS and the vruntime value* section).The
    motivation behind this schedule policy is fairness and overall throughput. | Real-time priority
    is `0`; the non-real-time priority is called the nice value: it ranges from -20
    to +19 (a lower number implies superior priority) with a base of 0 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| `SCHED_OTHER`或`SCHED_NORMAL` | 始终是默认值；具有此策略的线程是非实时的；在内部实现为**完全公平调度**（CFS）类（稍后在*关于CFS和vruntime值*部分中看到）。这种调度策略背后的动机是公平性和整体吞吐量。|
    实时优先级为`0`；非实时优先级称为nice值：范围从-20到+19（较低的数字意味着更高的优先级），基数为0 |'
- en: '| `SCHED_RR` | The motivation behind this schedule policy is a (soft) real-time
    policy that''s moderately aggressive. Has a finite timeslice (typically defaulting
    to 100 ms).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '| `SCHED_RR` | 这种调度策略背后的动机是一种（软）实时策略，相对积极。具有有限时间片（通常默认为100毫秒）。'
- en: 'A `SCHED_RR` thread will yield the processor IFF (if and only if):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`SCHED_RR`线程将在以下情况下让出处理器（如果且仅如果）：'
- en: '- It blocks on I/O (goes to sleep).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '- 它在I/O上阻塞（进入睡眠状态）。'
- en: '- It stops or dies.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '- 它停止或终止。'
- en: '- A higher-priority real-time thread becomes runnable (which will preempt this
    one).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '- 更高优先级的实时线程变为可运行状态（将抢占此线程）。'
- en: '- Its timeslice expires. | (Soft) real-time: 1 to 99 (a higher number'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '- 它的时间片到期。|（软）实时：1到99（较高的数字'
- en: implies superior priority) |
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 意味着更高的优先级）|
- en: '| `SCHED_FIFO` | The motivation behind this schedule policy is a (soft) real-time
    policy that''s (by comparison, very) aggressive. A `SCHED_FIFO` thread will yield
    the processor IFF:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '| `SCHED_FIFO` | 这种调度策略背后的动机是一种（软）实时策略，相对来说非常积极。`SCHED_FIFO`线程将在以下情况下让出处理器：'
- en: '- It blocks on I/O (goes to sleep).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '- 它在I/O上阻塞（进入睡眠状态）。'
- en: '- It stops or dies.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '- 它停止或终止。'
- en: '- A higher-priority real-time thread becomes runnable (which will preempt this
    one).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '- 更高优先级的实时线程变为可运行状态（将抢占此线程）。'
- en: It has, in effect, infinite timeslice. |  (same as `SCHED_RR`) |
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 它实际上有无限的时间片。|（与`SCHED_RR`相同）|
- en: '| `SCHED_BATCH` | The motivation behind this schedule policy is a scheduling
    policy that''s suitable for non-interactive batch jobs, less preemption. |  Nice
    value range (-20 to +19) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| `SCHED_BATCH` | 这种调度策略背后的动机是适用于非交互式批处理作业的调度策略，较少的抢占。| Nice值范围（-20到+19）|'
- en: '| `SCHED_IDLE` | Special case: typically the PID `0` kernel thread (traditionally
    called the `swapper`; in reality, it''s the per CPU idle thread) uses this policy.
    It''s always guaranteed to be the lowest-priority thread on the system and only
    runs when no other thread wants the CPU. | The lowest priority of all (think of
    it as being below the nice value +19) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| `SCHED_IDLE` | 特殊情况：通常PID`0`内核线程（传统上称为`swapper`；实际上是每个CPU的空闲线程）使用此策略。它始终保证是系统中优先级最低的线程，并且只在没有其他线程想要CPU时运行。|
    所有优先级中最低的（可以认为低于nice值+19）|'
- en: It's important to note that when we say real-time in the preceding table, we
    really mean *soft* (or at best, *firm*) real time and *not* hard real time as
    in an **Real-Time Operating System** (**RTOS**). Linux is a **GPOS**, a **general-purpose
    OS**, not an RTOS. Having said that, you can convert vanilla Linux into a true
    hard real-time RTOS by applying an external patch series (called the RTL, supported
    by the Linux Foundation); you'll learn how to do precisely this in the following
    chapter in the *Converting mainline Linux into an RTOS* section.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，当我们在上表中说实时时，我们实际上指的是*软*实时（或者最好是*硬*实时），而不是**实时操作系统**（**RTOS**）中的硬实时。Linux是一个**GPOS**，一个**通用操作系统**，而不是RTOS。话虽如此，您可以通过应用外部补丁系列（称为RTL，由Linux基金会支持）将普通的Linux转换为真正的硬实时RTOS；您将在以下章节*将主线Linux转换为RTOS*中学习如何做到这一点。
- en: Notice that a `SCHED_FIFO` thread in effect has infinite timeslice and runs
    until it wishes or one of the preceding mentioned conditions comes true. At this
    point, it's important to understand that we're only concerned with thread (KSE)
    scheduling here; on an OS such as Linux, the reality is that hardware (and software) *interrupts *are
    always superior and will always preempt even (kernel or user space) `SCHED_FIFO`
    threads! Do refer back to Figure 6.1 to see this. Also, we will cover hardware
    interrupts in detail in Chapter 14, *Handling Hardware Interrupts*. For our discussion
    here, we will ignore interrupts for the time being.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`SCHED_FIFO`线程实际上具有无限的时间片，并且运行直到它希望停止或前面提到的条件之一成立。在这一点上，重要的是要理解我们只关注线程（KSE）调度；在诸如Linux的操作系统上，现实情况是硬件（和软件）*中断*总是优先的，并且甚至会抢占（内核或用户空间）`SCHED_FIFO`线程！请参考图6.1以了解这一点。此外，我们将在第14章*处理硬件中断*中详细介绍硬件中断。在我们的讨论中，我们暂时将忽略中断。
- en: 'The priority scaling is simple:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 优先级缩放很简单：
- en: Non-real-time threads (`SCHED_OTHER`) have a real-time priority of `0`; this
    ensures that they cannot even compete with real-time threads. They use an (old
    UNIX-style) priority value called the **nice value**, which ranges from -20 to
    +19 (-20 being the highest priority and +19 the worst).
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非实时线程（`SCHED_OTHER`）具有`0`的实时优先级；这确保它们甚至不能与实时线程竞争。它们使用一个（旧的UNIX风格）称为**nice value**的优先级值，范围从-20到+19（-20是最高优先级，+19是最差的）。
- en: The way it's implemented on modern Linux, each nice level corresponds to an
    approximate 10% change (or delta, plus or minus) in CPU bandwidth, which is a
    significant amount.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代Linux上的实现方式是，每个nice级别对应于CPU带宽的大约10%的变化（或增量，加或减），这是一个相当大的数量。
- en: 'Real-time threads (`SCHED_FIFO / SCHED_RR`) have a real-time priority scale
    from 1 to 99, 1 being the least and 99 being the highest priority. Think of it
    this way: on a non-preemptible Linux system with one CPU, a `SCHED_FIFO` priority
    99 thread spinning in an unbreakable infinite loop will effectively hang the machine!
    (Of course, even this will be preempted by interrupts – both hard and soft; see
    Figure 6.1.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时线程（`SCHED_FIFO / SCHED_RR`）具有1到99的实时优先级范围，1是最低优先级，99是最高优先级。可以这样理解：在一个不可抢占的Linux系统上，一个`SCHED_FIFO`优先级为99的线程在一个无法中断的无限循环中旋转，实际上会使机器挂起！（当然，即使这样也会被中断
    - 包括硬中断和软中断；请参见图6.1。）
- en: 'The scheduling policy and priorities (both the static nice value and real-time
    priority) are members of the task structure, of course. The scheduling class that
    a thread belongs to is exclusive: a thread can only belong to one scheduling policy
    at a given point in time (worry not, we''ll cover scheduling classes in some detail
    later in the *CPU scheduling internals – part 2* section).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 调度策略和优先级（静态nice值和实时优先级）当然是任务结构的成员。线程所属的调度类是独占的：一个线程在特定时间点只能属于一个调度策略（不用担心，我们稍后将在*CPU调度内部
    - 第2部分*中详细介绍调度类）。
- en: 'Also, you should realize that on a modern Linux kernel, there are other scheduling
    classes (stop-schedule and deadline) that are in fact superior (in priority) to
    the FIFO/RR ones we mentioned earlier. Now that you have an idea of the basics,
    let''s move on to something pretty interesting: how we can actually *visualize*
    the flow of control. Read on!'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您应该意识到在现代Linux内核上，还有其他调度类（stop-schedule和deadline），它们实际上比我们之前提到的FIFO/RR更优先（优先级更高）。既然您已经了解了基础知识，让我们继续看一些非常有趣的东西：我们实际上如何*可视化*控制流。继续阅读！
- en: Visualizing the flow
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化流程
- en: Multicore systems have led to processes and threads executing concurrently on
    different processors. This is useful for gaining higher throughput and thus performance,
    but also causes synchronization headaches with shared writable data. So, for example,
    on a hardware platform with, say, four processor cores, we can expect processes
    (and threads) to execute in parallel on them. This is nothing new; is there a
    way, though, to actually see which processes or threads are executing on which
    CPU core – that is, a way to visualize a processor timeline? It turns out there
    are indeed a few ways to do so. In the following sections, we will look at one
    interesting way with `perf`, followed later by others (with LTTng, Trace Compass,
    and Ftrace).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 多核系统导致进程和线程在不同处理器上并发执行。这对于获得更高的吞吐量和性能非常有用，但也会导致共享可写数据的同步问题。因此，例如，在一个具有四个处理器核心的硬件平台上，我们可以期望进程（和线程）在它们上面并行执行。这并不是什么新鲜事；不过，有没有一种方法可以实际上看到哪些进程或线程在哪个CPU核心上执行
    - 也就是说，有没有一种可视化处理器时间线的方法？事实证明确实有几种方法可以做到这一点。在接下来的章节中，我们将首先使用`perf`来看一种有趣的方法，然后再使用其他方法（使用LTTng、Trace
    Compass和Ftrace）。
- en: Using perf to visualize the flow
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用perf来可视化流程
- en: Linux, with its vast arsenal of developer and **Quality Assurance** (**QA**)
    tools, has a really powerful one in `perf(1)`. In a nutshell, the `perf` toolset
    is the modern way to perform CPU profiling on a Linux box. (Besides a few tips,
    we do not cover `perf` in detail in this book.)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Linux拥有庞大的开发人员和**质量保证**（**QA**）工具库，其中`perf(1)`是一个非常强大的工具。简而言之，`perf`工具集是在Linux系统上执行CPU性能分析的现代方式。（除了一些提示外，我们在本书中不会详细介绍`perf`。）
- en: Akin to the venerable `top(1)` utility, to get a thousand-foot view of what's
    eating the CPU (in a lot more detail than `top(1)`), the **`perf(1)`** set of
    utilities is excellent. Do note, though, that, quite unusually for an app, `perf`
    is tightly coupled with the kernel that it runs upon. It's important that you
    install the `linux-tools-$(uname -r)` package first. Also, the distribution package
    will not be available for the custom 5.4 kernel we have built; so, when using
    `perf`, I suggest you boot your guest VM with one of the standard (or distro)
    kernels, install the `linux-tools-$(uname -r)` package, and then try using `perf`.
    (Of course, you can always manually build perf from within the kernel source tree,
    under the `tools/perf/` folder.)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于古老的`top(1)`实用程序，要详细了解正在占用CPU的情况（比`top(1)`更详细），**`perf(1)`**一系列实用程序非常出色。不过，请注意，与应用程序相比，`perf`与其运行的内核紧密耦合，这是相当不寻常的。首先，你需要安装`linux-tools-$(uname
    -r)`包。此外，自定义的5.4内核包将不可用；因此，在使用`perf`时，我建议你使用标准（或发行版）内核引导你的虚拟机，安装`linux-tools-$(uname
    -r)`包，然后尝试使用`perf`。（当然，你也可以在内核源代码树中的`tools/perf/`文件夹下手动构建`perf`。）
- en: 'With `perf` installed and running, do try out these `perf` commands:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 安装并运行`perf`后，请尝试这些`perf`命令：
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '(By the way, `comm` implies the name of the command/process, `**dso**` is an abbreviation for **dynamic
    shared object**). Using an `alias` makes it easier; try this one (in one line)
    for even more verbose details (the call stack can be expanded too!):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: （顺便说一句，`comm`意味着命令/进程的名称，`**dso**`是**动态共享对象**的缩写）。使用`alias`会更容易；尝试这个（一行）以获得更详细的信息（调用堆栈也可以展开！）：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `man` page on `perf(1)` provides the details; use the `man perf-<foo>` notation
    – for example, `man perf-top` – to get help with `perf top`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`perf(1)`的`man`页面提供了详细信息；使用`man perf-<foo>`表示法 - 例如，`man perf-top` - 以获取有关`perf
    top`的帮助。'
- en: 'One way to use `perf` is to obtain an idea of what task is running on what
    CPU; this is done via the `timechart` sub-command in `perf`. You can record events
    using `perf`, both system-wide as well as for a specific process. To record events
    system-wide, run the following command:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`perf`的一种方法是了解在哪个CPU上运行了什么任务；这是通过`perf`中的`timechart`子命令完成的。你可以使用`perf`记录系统范围的事件，也可以记录特定进程的事件。要记录系统范围的事件，请运行以下命令：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Terminate the recording session with a signal (`^C`). This will generate a
    binary data file named `perf.data` by default. It can now be examined with the
    following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过信号（`^C`）终止记录会话。这将默认生成一个名为`perf.data`的二进制数据文件。现在可以使用以下命令进行检查：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This command generates a **Scalable Vector Graphics** (**SVG**) file! It can
    be viewed with vector drawing utilities (such as Inkscape, or via the `display`
    command in ImageMagick) or simply within a web browser. It can be quite fascinating
    to study the time chart; I urge you to try it out. Do note, though, that the vector
    images can be quite large and therefore take a while to open.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令生成了一个**可伸缩矢量图形**（**SVG**）文件！它可以在矢量绘图工具（如Inkscape，或通过ImageMagick中的`display`命令）中查看，或者直接在Web浏览器中查看。研究时间表可能会很有趣；我建议你试试。不过，请注意，矢量图像可能会很大，因此打开需要一段时间。
- en: 'A system-wide sampling run on a native Linux x86_64 laptop running Ubuntu 18.10
    is shown as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在运行Ubuntu 18.10的本机Linux x86_64笔记本电脑上进行的系统范围采样运行：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: It is possible to configure `perf` to work with non-root access. Here, we don't;
    we just run `perf` as root via `sudo(8)`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 可以配置`perf`以使用非root访问权限。在这里，我们不这样做；我们只是通过`sudo(8)`以root身份运行`perf`。
- en: 'A screenshot of the SVG file generated by `perf` is seen in the following screenshot.
    To view the SVG file, you can simply drag and drop it into your web browser:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`perf`生成的SVG文件的屏幕截图如下所示。要查看SVG文件，你可以简单地将其拖放到你的Web浏览器中：'
- en: '![](img/20bb069c-c381-4463-92d8-477e983b6122.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20bb069c-c381-4463-92d8-477e983b6122.png)'
- en: Figure 10.1 – (Partial) screenshot showing the SVG file generated by sudo perf
    timechart
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 - （部分）屏幕截图显示由sudo perf timechart生成的SVG文件
- en: In the preceding screenshot, as one example, you can see that the `EMT-0` thread
    is busy and takes maximum CPU cycles (the phrase CPU 3 is unfortunately unclear;
    look closely in the purple bar below CPU 2). This makes sense; it's the thread
    representing the **Virtual CPU** (**VCPU**) of VirtualBox where we are running
    Fedora 29 (**EMT** stands for **emulator thread**)!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的屏幕截图中，举个例子，你可以看到`EMT-0`线程很忙，占用了最大的CPU周期（不幸的是，CPU 3这个短语不太清楚；仔细看看紫色条下面的CPU
    2）。这是有道理的；它是代表我们运行Fedora 29的VirtualBox的**虚拟CPU**（**VCPU**）的线程（**EMT**代表**模拟器线程**）！
- en: 'You can zoom in and out of this SVG file, studying the scheduling and CPU events
    that are recorded by default by `perf`. The following figure, a partial screenshot
    when zoomed in 400% to the CPU 1 region of the preceding screenshot, shows `htop` running
    on CPU #1 (the purple band literally shows the slice when it executed):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以放大和缩小这个SVG文件，研究`perf`默认记录的调度和CPU事件。下图是前面截图的部分屏幕截图，放大400%至CPU 1区域，显示了在CPU＃1上运行的`htop`（紫色条实际上显示了它执行时的时间段）：
- en: '![](img/effe3bb9-475a-46e0-8746-2d2aedd84371.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/effe3bb9-475a-46e0-8746-2d2aedd84371.png)'
- en: Figure 10.2 – Partial screenshot of perf timechart's SVG file, when zoomed in
    400% to the CPU 1 region
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 - 对perf timechart的SVG文件的部分屏幕截图，放大400%至CPU 1区域
- en: What else? By using the `-I` option switch to `perf timechart record`, you can
    request only system-wide disk I/O (and network, apparently) events be recorded.
    This could be especially useful as, often, the real performance bottlenecks are
    caused by I/O activity (and not the CPU; I/O is usually the culprit!). The `man`
    page on `perf-timechart(1)` details further useful options; for example, `--callchain`
    to perform stack backtrace recording. As another example, the `--highlight <name>` option
    switch will highlight all tasks with the name `<name>`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 还有什么？通过使用`-I`选项切换到`perf timechart record`，你可以请求仅记录系统范围的磁盘I/O（和网络，显然）事件。这可能特别有用，因为通常真正的性能瓶颈是由I/O活动引起的（而不是CPU；I/O通常是罪魁祸首！）。`perf-timechart(1)`的`man`页面详细介绍了更多有用的选项；例如，`--callchain`用于执行堆栈回溯记录。另一个例子是，`--highlight
    <name>`选项将突出显示所有名称为`<name>`的任务。
- en: You can convert `perf`'s binary `perf.data` record file into the popular **Common
    Trace Format** (**CTF**) file format, using `perf data convert -- all --to-ctf`,
    where the last argument is the directory where the CTF file(s) get stored. Why
    is this useful? CTF is the native data format used by powerful GUI visualizers
    and analyzer tools such as Trace Compass (seen later in [Chapter 11](d6e5ebd3-1f04-40e8-a240-2607c58b1299.xhtml),
    *The CPU Scheduler – Part 2*, under the *Visualization with LTTng and Trace Compass*
    section).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`perf data convert -- all --to-ctf`将`perf`的二进制`perf.data`记录文件转换为流行的**通用跟踪格式**（**CTF**）文件格式，其中最后一个参数是存储CTF文件的目录。这有什么用呢？CTF是强大的GUI可视化器和分析工具（例如Trace
    Compass）使用的本机数据格式（稍后在[第11章](d6e5ebd3-1f04-40e8-a240-2607c58b1299.xhtml)中的*CPU调度程序-第2部分*中可以看到）。
- en: 'However, there is a catch, as mentioned in the Trace Compass Perf Profiling
    user guide ([https://archive.eclipse.org/tracecompass.incubator/doc/org.eclipse.tracecompass.incubator.perf.profiling.doc.user/User-Guide.html](https://archive.eclipse.org/tracecompass.incubator/doc/org.eclipse.tracecompass.incubator.perf.profiling.doc.user/User-Guide.html)):
    "*Not all Linux distributions have the ctf conversion builtin. One needs to compile
    perf (thus linux) with environment variables LIBBABELTRACE=1 and LIBBABELTRACE_DIR=/path/to/libbabeltrace
    to enable that support*."'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如Trace Compass Perf Profiling用户指南中所提到的那样（[https://archive.eclipse.org/tracecompass.incubator/doc/org.eclipse.tracecompass.incubator.perf.profiling.doc.user/User-Guide.html](https://archive.eclipse.org/tracecompass.incubator/doc/org.eclipse.tracecompass.incubator.perf.profiling.doc.user/User-Guide.html)）：“并非所有Linux发行版都具有内置的ctf转换。需要使用环境变量LIBBABELTRACE=1和LIBBABELTRACE_DIR=/path/to/libbabeltrace来编译perf（因此linux）以启用该支持。”
- en: Unfortunately, as of the time of writing, this is the case with Ubuntu.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在撰写本文时，Ubuntu就是这种情况。
- en: Visualizing the flow via alternate (CLI) approaches
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过替代（CLI）方法来可视化流程
- en: 'There are, of course, alternate ways to visualize what''s running on each processor;
    we mention a couple here and have saved one other interesting one (LTTng) for
    [Chapter 11](d6e5ebd3-1f04-40e8-a240-2607c58b1299.xhtml), *The CPU Scheduler –
    Part 2*, under the *Visualization with LTTng and Trace Compass* section):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，还有其他方法可以可视化每个处理器上正在运行的内容；我们在这里提到了一些，并保存了另一个有趣的方法（LTTng），将在[第11章](d6e5ebd3-1f04-40e8-a240-2607c58b1299.xhtml)中的*CPU调度程序-第2部分*中的*使用LTTng和Trace
    Compass进行可视化*部分中介绍。
- en: With `perf(1)`, again, run the `sudo perf sched record` command; this records
    activity. Stop by terminating it with the `^C` signal, followed by `sudo perf
    sched map` to see a (CLI) map of execution on the processor(s).
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 再次使用`perf(1)`运行`sudo perf sched record`命令；这将记录活动。通过使用`^C`信号终止它，然后使用`sudo perf
    sched map`来查看处理器上的执行情况（CLI地图）。
- en: 'Some simple Bash scripting can show what''s executing on a given core (a simple
    wrapper over `ps(1)`). In the following snippet, we show sample Bash functions;
    for example, the following `c0()` function shows what is currently executing on
    CPU core `#0`, while `c1()` does the same for core `#1`:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些简单的Bash脚本可以显示在给定核心上正在执行的内容（这是对`ps(1)`的简单封装）。在下面的片段中，我们展示了一些示例Bash函数；例如，以下`c0()`函数显示了当前在CPU核心`#0`上正在执行的内容，而`c1()`则对`#1`核心执行相同的操作。
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'While on the broad topic of `perf`, Brendan Gregg has a very useful series
    of scripts that perform a lot of the hard work required when monitoring production
    Linux systems using `perf`; do take a look at them here: [https://github.com/brendangregg/perf-tools](https://github.com/brendangregg/perf-tools) (some
    distributions include them as a package called `perf-tools[-unstable]`).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在广泛讨论`perf`的话题上，Brendan Gregg有一系列非常有用的脚本，可以在使用`perf`监视生产Linux系统时执行许多必要的工作；请在这里查看它们：[https://github.com/brendangregg/perf-tools](https://github.com/brendangregg/perf-tools)（一些发行版将它们作为名为`perf-tools[-unstable]`的软件包包含在内）。
- en: Do give these alternatives (including the `perf-tools[-unstable] package`) a
    try!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试使用这些替代方案（包括`perf-tools[-unstable]`包）！
- en: Learning about the CPU scheduling internals – part 2
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解CPU调度内部工作原理-第2部分
- en: This section delves into kernel CPU scheduling internals in some detail, the
    emphasis being on the core aspect of the modern design, modular scheduler classes.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细介绍了内核CPU调度的内部工作原理，重点是现代设计的核心部分，即模块化调度类。
- en: Understanding modular scheduling classes
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解模块化调度类
- en: Ingo Molnar, a key kernel developer, (along with others) redesigned the internal
    structure of the kernel scheduler, introducing a new approach called **scheduling
    classes** (this was back in October 2007 with the release of the 2.6.23 kernel).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 内核开发人员Ingo Molnar（以及其他人）重新设计了内核调度程序的内部结构，引入了一种称为**调度类**的新方法（这是在2007年10月发布2.6.23内核时的情况）。
- en: As a side note, the word *class* here isn't a coincidence; many Linux kernel
    features are intrinsically, and quite naturally, designed with an **object-oriented**
    nature. The C language, of course, does not allow us to express this directly
    in code (hence the preponderance of structures with both data and function pointer
    members, emulating a class). Nevertheless, the design is very often object-oriented
    (as you shall again see with the driver model in the *Linux Kernel Programming
    Part 2* book). Please see the *Further reading* section of this chapter for more
    details on this.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，这里的“类”一词并非巧合；许多Linux内核功能本质上都是以**面向对象**的方式设计的。当然，C语言不允许我们直接在代码中表达这一点（因此结构中有数据和函数指针成员的比例很高，模拟了一个类）。然而，设计往往是面向对象的（您将在*Linux内核编程第2部分*书中再次看到这一点）。有关此内容的更多详细信息，请参阅本章的*进一步阅读*部分。
- en: A layer of abstraction was introduced under the core scheduling code, the `schedule()`
    function. This layer under `schedule()` is generically called the scheduling classes
    and is modular in design. Note that the word *modular* here implies that scheduler
    classes can be added or removed from the inline kernel code; it has nothing to
    do with the **Loadable Kernel Module** (**LKM**) framework.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在核心调度代码下引入了一层抽象，即`schedule()`函数。`schedule()`下的这一层通常称为调度类，设计上是模块化的。这里的“模块化”意味着调度类可以从内联内核代码中添加或删除；这与**可加载内核模块**（**LKM**）框架无关。
- en: 'The basic idea is this: when the core scheduler code (encapsulated by the `schedule()`
    function) is invoked, understanding that there are various available scheduling
    classes under it, it iterates over each of the classes in a predefined priority
    order, asking each if it has a thread (or process) that requires scheduling onto
    a processor (how exactly, we shall soon see).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想是：当核心调度程序代码（由`schedule()`函数封装）被调用时，了解到它下面有各种可用的调度类别，它按照预定义的优先级顺序迭代每个类别，询问每个类别是否有一个需要调度到处理器上的线程（或进程）（我们很快就会看到具体是如何做的）。
- en: 'As of the 5.4 Linux kernel, these are the scheduler classes within the kernel,
    listed in priority order, with the highest priority first:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 截至Linux内核5.4，这些是内核中的调度类别，按优先级顺序列出，优先级最高的排在前面：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: There we have it, the five scheduler classes – stop-schedule, deadline, (soft)
    real time, fair, and idle – in priority order, highest to lowest. The data structures
    that abstracts these scheduling classes, `struct sched_class`, are strung together
    on a singly linked list, which the core scheduling code iterates over. (You will
    come to what the `sched_class` structure is later; ignore it for now).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们所拥有的五个调度程序类别 - 停止调度、截止时间、（软）实时、公平和空闲 - 按优先级顺序排列，从高到低。抽象这些调度类别的数据结构`struct
    sched_class`被串联在一个单链表上，核心调度代码对其进行迭代。（稍后您将了解`sched_class`结构是什么；现在请忽略它）。
- en: Every thread is associated with it's own unique task structure (`task_struct`);
    within the task structure, the `policy` member specifies the scheduling policy
    that the thread adheres to (typically one of `SCHED_FIFO`, `SCHED_RR`, or `SCHED_OTHER`).
    It's exclusive - a thread can only adhere to one scheduling policy at any given
    point in time (it can be changed though). Similarly, another member of the task
    structure, `struct sched_class`, holds the modular scheduling class that the thread
    belongs to (which is also exclusive). Both the scheduling policy and priority
    are dynamic and can be queried and set programmatically (or via utilities; you
    will soon see this).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 每个线程都与其自己独特的任务结构（`task_struct`）相关联；在任务结构中，`policy`成员指定线程遵循的调度策略（通常是`SCHED_FIFO`、`SCHED_RR`或`SCHED_OTHER`中的一个）。它是独占的
    - 一个线程在任何给定时间点只能遵循一个调度策略（尽管它可以改变）。类似地，任务结构的另一个成员`struct sched_class`保存线程所属的模块化调度类别（也是独占的）。调度策略和优先级都是动态的，可以通过编程查询和设置（或通过实用程序；您很快就会看到这一点）。
- en: So knowing this, you will now realize that all threads that adhere to either the `SCHED_FIFO` or `SCHED_RR` scheduling
    policy, map to the `rt_sched_class` (for their `sched_class` within the task structure),
    all threads that are `SCHED_OTHER` (or `SCHED_NORMAL`) map to the `fair_sched_class`,
    and the idle thread (`swapper/n`, where `n` is the CPU number starting from `0`)
    always maps to `idle_sched_class` scheduling class.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您现在将意识到，所有遵循`SCHED_FIFO`或`SCHED_RR`调度策略的线程都映射到`rt_sched_class`（在其任务结构中的`sched_class`），所有遵循`SCHED_OTHER`（或`SCHED_NORMAL`）的线程都映射到`fair_sched_class`，而空闲线程（`swapper/n`，其中`n`是从`0`开始的CPU编号）始终映射到`idle_sched_class`调度类别。
- en: 'When the kernel needs to schedule, this is the essential call sequence:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当内核需要进行调度时，这是基本的调用顺序：
- en: '[PRE7]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The actual iteration over the preceding scheduling classes occurs here; see
    the (partial) code of `pick_next_task()`, as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的调度类别的实际迭代发生在这里；请参见`pick_next_task()`的（部分）代码，如下：
- en: '[PRE8]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding `for_each_class()` macro sets up a `for` loop to iterate over
    all scheduling classes. Its implementation is as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的`for_each_class()`宏设置了一个`for`循环，用于迭代所有调度类别。其实现如下：
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can see from the preceding implementation that the code results in each
    class, from `sched_class_highest` to `NULL` (implying the end of the linked list
    they're on), being asked, via the `pick_next_task()` "method", who to schedule
    next. Now, the scheduling class code determines whether it has any candidates
    that want to execute. How? That's simple actually; it merely looks up its **runqueue**
    data structure.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的实现中可以看出，代码导致每个类都被要求通过`pick_next_task()`"方法"来安排下一个调度的任务，从`sched_class_highest`到`NULL`（意味着它们所在的链表的末尾）。现在，调度类代码确定它是否有任何想要执行的候选者。怎么做？实际上很简单；它只是查找它的**runqueue**数据结构。
- en: 'Now, this is a key point: *the kernel maintains one runqueue for every processor
    core and for every scheduling class*! So, if we have a system with, say, eight
    CPU cores, then we will have *8 cores * 5 sched classes  = 40 runqueues*! Runqueues
    are in fact implemented as per-CPU variables, an interesting lock-free technique (exception:
    on **Uniprocessor** (**UP**) systems, the `stop-sched` class does not exist):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这是一个关键点：*内核为每个处理器核心和每个调度类别维护一个运行队列*！因此，如果我们有一个有8个CPU核心的系统，那么我们将有*8个核心 * 5个调度类别
    = 40个运行队列*！运行队列实际上是作为每个CPU变量实现的，这是一种有趣的无锁技术（例外情况：在**单处理器**（**UP**）系统上，`stop-sched`类别不存在）：
- en: '![](img/abb1b8ad-48fa-4554-80ac-1c6e13d443ae.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/abb1b8ad-48fa-4554-80ac-1c6e13d443ae.png)'
- en: Figure 10.3 – There is a runqueue per CPU core per scheduling class
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 - 每个CPU核心每个调度类都有一个运行队列
- en: Please note that in the preceding diagram, the way I show the runqueues makes
    them perhaps appear as arrays. That isn't the intention at all, it's merely a
    conceptual diagram. The actual runqueue data structure used depends on the scheduling
    class (the class code implements the runqueue after all). It could be an array
    of linked lists (as with the real-time class), a tree - a **red-black (rb) tree**
    -as with the fair class), and so on.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的图中，我展示运行队列的方式可能让它们看起来像数组。这并不是本意，这只是一个概念图。实际使用的运行队列数据结构取决于调度类别（类别代码实现了运行队列）。它可以是一个链表数组（就像实时类别一样），也可以是一棵树
    - 一棵**红黑（rb）树** - 就像公平类别一样，等等。
- en: 'To help better understand the scheduler class model, we will devise an example:
    let''s say, on an **Symmetric Multi Processor** (**SMP**) or multicore) system,
    we have 100 threads alive (in both user and kernel space). Among them, we have
    a few competing for the CPUs; that is, they are in the ready-to-run (run) state,
    implying they are runnable and thus enqueued on runqueue data structures:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解调度器类模型，我们将设计一个例子：假设在对称多处理器（**SMP**）或多核系统上，我们有100个线程处于活动状态（在用户空间和内核空间）。其中，有一些线程在竞争CPU；也就是说，它们处于准备运行（run）状态，意味着它们是可运行的，因此被排队在运行队列数据结构上：
- en: 'Thread S1: Scheduler class, `stop-sched` (**SS**)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程S1：调度器类，`stop-sched`（**SS**）
- en: 'Threads D1 and D2: Scheduler class, **Deadline** (**DL**)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程D1和D2：调度器类，**Deadline**（**DL**）
- en: 'Threads RT1 and RT2: Scheduler class, **Real Time** (**RT**)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程RT1和RT2：调度器类，**Real Time**（**RT**）
- en: 'Threads F1, F2, and F3: Scheduler class, CFS (or fair)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程F1、F2和F3：调度器类，CFS（或公平）
- en: 'Thread I1: Scheduler class, idle.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程I1：调度器类，空闲。
- en: 'Imagine that, to begin with, thread F2 is on a processor core, happily executing
    code. At some point, the kernel wishes to context switch to some other task on
    that CPU (what triggers this? You shall soon see). On the scheduling code path,
    the kernel code ultimately ends up in the `kernel/sched/core.c:void schedule(void)` kernel
    routine (again, code-level details follow later). What''s important to understand
    for now is that the `pick_next_task()` routine, invoked by `schedule()`, iterates
    over the linked list of scheduler classes, asking each whether it has a candidate
    to run. It''s code path (conceptually, of course) looks something like this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，一开始，线程F2正在处理器核心上，愉快地执行代码。在某个时刻，内核希望在该CPU上切换到其他任务（是什么触发了这个？你很快就会看到）。在调度代码路径上，内核代码最终进入`kernel/sched/core.c:void
    schedule(void)`内核例程（稍后会跟进代码级细节）。现在重要的是要理解`pick_next_task()`例程，由`schedule()`调用，遍历调度器类的链表，询问每个类是否有候选者可以运行。它的代码路径（概念上，当然）看起来像这样：
- en: 'Core scheduler code (`schedule()`): "*H**ey, SS, do you have any threads that
    want to run?*"'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 核心调度器代码（`schedule()`）：“*嘿，SS，你有任何想要运行的线程吗？*”
- en: 'SS class code: Iterates over its runqueue and does find a runnable thread;
    it thus replies: "*Yes, I do, it''s thread S**1.*"'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SS类代码：遍历其运行队列并找到一个可运行的线程；因此它回答：“*是的，我有，它是线程S1*”
- en: 'Core scheduler code (`schedule()`): "*Okay, let''s context switch to S1.*"'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 核心调度器代码（`schedule()`）：“*好的，让我们切换到S1上下文*”
- en: 'And the job is done. But what if there is no runnable thread S1 on the SS runqueue
    for that processor (or it has gone to sleep, or is stopped, or it''s on another
    CPU''s runqueue). Then, SS will say "*no*" and the next most important scheduling
    class, DL, will be asked. If it has potential candidate threads that want to run
    (D1 and D2, in our example), its class code will identify which of D1 or D2 should
    run, and the kernel scheduler will faithfully context switch to it. This process
    continues for the RT and fair (CFS) scheduling classes. (A picture''s worth a
    thousand words, right: see Figure 10.4).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 工作完成了。但是，如果在该处理器的SS运行队列上没有可运行的线程S1（或者它已经进入睡眠状态，或者已经停止，或者它在另一个CPU的运行队列上）。那么，SS会说“*不*”，然后会询问下一个最重要的调度类DL。如果它有潜在的候选线程想要运行（在我们的例子中是D1和D2），它的类代码将确定D1或D2中应该运行的线程，并且内核调度器将忠实地上下文切换到它。这个过程会继续进行RT和公平（CFS）调度类。（一图胜千言，对吧：参见图10.4）。
- en: 'In all likelihood (on your typical moderately loaded Linux system), there will
    be no SS, DL, or RT candidate threads that want to run on the CPU in question,
    and there often will be at least one fair (CFS) thread that will want to run;
    hence, it will be picked and context-switched to. If there''s none that wants
    to run (no SS/DL/RT/CFS class thread wants to run), it implies that the system
    is presently idle (lazy chap). Now, the idle class is asked whether it wants to
    run: it always says yes! This makes sense: after all, it is the CPU idle thread''s
    job to run on the processor when no one else needs to. Hence, in such a case,
    the kernel switches context to the idle thread (typically labelled `swapper/n`,
    where `n` is the CPU number that it''s executing upon (starting from `0`)).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能（在您典型的中度负载的Linux系统上），在问题CPU上没有SS、DL或RT候选线程想要运行，通常至少会有一个公平（CFS）线程想要运行；因此，它将被选择并进行上下文切换。如果没有想要运行的线程（没有SS/DL/RT/CFS类线程想要运行），这意味着系统目前处于空闲状态（懒惰的家伙）。现在，空闲类被问及是否想要运行：它总是说是！这是有道理的：毕竟，当没有其他人需要时，CPU空闲线程的工作就是在处理器上运行。因此，在这种情况下，内核将上下文切换到空闲线程（通常标记为`swapper/n`，其中`n`是它正在执行的CPU编号（从`0`开始））。
- en: 'Also, note that the `swapper/n` (CPU idle) kernel thread does not show up in
    the `ps(1)` listing, though it''s always present (recall the code we demonstrated
    in [Chapter 6](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml), *Kernel Internals
    Essentials – Processes and Threads*, here: `ch6/foreach/thrd_showall/thrd_showall.c`.
    There, we wrote a `disp_idle_thread()` routine to show some details of the CPU
    idle thread as even the kernel''s `do_each_thread() { ... } while_each_thread()`
    loop that we employed there does not show the idle thread).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，`swapper/n`（CPU空闲）内核线程不会出现在`ps(1)`列表中，尽管它一直存在（回想一下我们在[第6章](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml)中展示的代码，*内核内部要点-进程和线程*，这里：`ch6/foreach/thrd_showall/thrd_showall.c`。在那里，我们编写了一个`disp_idle_thread()`例程来显示CPU空闲线程的一些细节，即使我们在那里使用的内核的`do_each_thread()
    { ... } while_each_thread()`循环也不显示空闲线程）。
- en: 'The following diagram neatly sums up the way the core scheduling code invokes
    the scheduling classes in priority order, context switching to the ultimately
    selected next thread:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表清楚地总结了核心调度代码如何按优先顺序调用调度类，切换到最终选择的下一个线程：
- en: '![](img/566496e5-f85b-4568-897a-ed2ca202fdf4.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/566496e5-f85b-4568-897a-ed2ca202fdf4.png)'
- en: Figure 10.4 – Iterating over every scheduling class to pick the task that will
    run next
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4-遍历每个调度类以选择下一个要运行的任务
- en: In the following chapter, you shall learn, among other things, how to visualize
    kernel flow via some powerful tools. There, precisely this work of iterating over
    modular scheduler classes is actually seen.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将学习如何通过一些强大的工具来可视化内核流程。在那里，实际上可以看到对模块化调度器类进行迭代的工作。
- en: Asking the scheduling class
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 询问调度类
- en: 'How exactly does the core scheduler code (`pick_next_task()`) ask the scheduling
    classes whether they have any threads that want to run? We have already seen this,
    but I feel it''s worthwhile repeating the following code fragment for clarity
    (called mostly from `__schedule()` and also from the thread migration code path):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 核心调度器代码（`pick_next_task()`）如何询问调度类是否有任何想要运行的线程？我们已经看到了这一点，但我觉得值得为了清晰起见重复以下代码片段（大部分从`__schedule()`调用，也从线程迁移代码路径调用）：
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Notice the object orientation in action: the `class->pick_next_task()` code,
    for all practical purposes, is invoking a method, `pick_next_task()`, of the scheduling
    class, `class`! The return value, conveniently, is the pointer to the task structure
    of the picked task, which the code now context switches to.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在这里的面向对象的特性：`class->pick_next_task()`代码，实际上是调用调度类`class`的`pick_next_task()`方法！方便的返回值是选定任务的任务结构的指针，现在代码切换到该任务。
- en: 'The preceding paragraph implies, of course, that there is a `class` structure,
    embodying what we really mean by the scheduling class. Indeed, this is the case:
    it contains all possible operations, as well as useful hooks, that you might require
    in a scheduling class. It''s (surprisingly) called the `sched_class` structure:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的段落当然意味着，有一个`class`结构，体现了我们对调度类的真正意思。确实如此：它包含了所有可能的操作，以及有用的挂钩，你可能在调度类中需要。它（令人惊讶地）被称为`sched_class`结构：
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: (There are many more members to this structure than we've shown here; do look
    it up in the code). As should be obvious by now, each scheduling class instantiates
    this structure, appropriately populating it with methods (function pointers, of
    course). The core scheduling code, iterating over the linked list of scheduling
    classes (as well as elsewhere in the kernel), invokes - as long as it's not `NULL`-
    the methods and hook functions as required.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: （这个结构的成员比我们在这里展示的要多得多；在代码中查找它）。显然，每个调度类都实例化了这个结构，并适当地填充了它的方法（当然是函数指针）。核心调度代码在调度类的链接列表上进行迭代（以及内核的其他地方），根据需要调用方法和挂钩函数，只要它不是`NULL`。
- en: 'As an example, let''s consider how the fair scheduling class (CFS) implements
    its scheduling class:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，让我们看看公平调度类（CFS）如何实现其调度类的调度算法：
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'So now you see it: the code used by the fair sched class to pick the next task
    to run (when asked by the core scheduler), is the function `pick_next_task_fair()`.
    FYI, the `task_tick` and `task_fork` members are good examples of scheduling class
    hooks; these functions will be invoked by the scheduler core on every timer tick
    (that is, each timer interrupt, which fires – in theory, at least – `CONFIG_HZ`
    times a second) and when a thread belonging to this scheduling class forks, respectively.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你看到了：公平调度类用于选择下一个要运行的任务的代码（当核心调度器询问时）是函数`pick_next_task_fair()`。FYI，`task_tick`和`task_fork`成员是调度类挂钩的很好的例子；这些函数将分别在每个定时器滴答（即每个定时器中断，理论上至少每秒触发`CONFIG_HZ`次）和当属于这个调度类的线程fork时，由调度器核心调用。
- en: 'An interesting in-depth Linux kernel project, perhaps: create your own scheduling
    class with its particular methods and hooks, implementing its internal scheduling
    algorithm(s). Link all the bits and pieces as required (into the scheduling classes-linked
    list, inserted at the desired priority, and so on) and test! Now you can see why
    they''re called modular scheduling classes.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 也许一个有趣的深入研究的Linux内核项目：创建你自己的调度类，具有特定的方法和挂钩，实现其内部调度算法。根据需要链接所有的部分（插入到所需优先级的调度类链接列表中等），并进行测试！现在你可以看到为什么它们被称为模块化调度类了。
- en: Great – now that you understand the architecture behind how the modern modular
    CPU scheduler works, let's take a brief look at the algorithm behind CFS, perhaps
    the most used scheduling class on generic Linux.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你了解了现代模块化CPU调度器工作背后的架构，让我们简要地看一下CFS背后的算法，也许是通用Linux上最常用的调度类。
- en: A word on CFS and the vruntime value
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于CFS和vruntime值
- en: Since version 2.6.23, CFS has been the de facto kernel CPU scheduling code for
    regular threads; the majority of threads are `SCHED_OTHER`, which is driven by
    CFS. The driver *behind CFS is fairness and overall throughput*. In a nutshell,
    within its implementation, the kernel keeps track of the actual CPU runtime (at
    nanosecond granularity) of every runnable CFS (`SCHED_OTHER`) thread; the thread
    with the smallest runtime is the thread that most deserves to run and will be
    awarded the processor on the next scheduling switch. Conversely, threads that
    continually hammer on the processor will accumulate a large amount of runtime
    and will thus be penalized (it's quite karmic, really)!
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 自2.6.23版本以来，CFS一直是常规线程的事实内核CPU调度代码；大多数线程都是`SCHED_OTHER`，由CFS驱动。CFS背后的驱动力是公平性和整体吞吐量。简而言之，在其实现中，内核跟踪每个可运行的CFS（`SCHED_OTHER`）线程的实际CPU运行时间（以纳秒为粒度）；具有最小运行时间的线程最值得运行，并将在下一个调度切换时被授予处理器。相反，不断占用处理器的线程将累积大量运行时间，因此将受到惩罚（这实际上是相当具有因果报应的）！
- en: Without delving into too many details regarding the internals of the CFS implementation,
    embedded within the task structure is another data structure, `struct sched_entity`,
    which contains within it an unsigned 64-bit value called `vruntime`. This is,
    at a simplistic level, the monotonic counter that keeps track of the amount of
    time, in nanoseconds, that the thread has accumulated (run) on the processor.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 不深入讨论CFS实现的内部细节，任务结构中嵌入了另一个数据结构`struct sched_entity`，其中包含一个名为`vruntime`的无符号64位值。在简单的层面上，这是一个单调计数器，用于跟踪线程在处理器上累积（运行）的时间，以纳秒为单位。
- en: In practice, here, a lot of code-level tweaks, checks, and balances are required.
    For example, often, the kernel will reset the `vruntime` value to `0`, triggering
    another scheduling epoch. Also, there are various tunables under `/proc/sys/kernel/sched_*`,
    to help better fine-tune the CPU scheduler behavior.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这里需要大量的代码级调整、检查和平衡。例如，通常情况下，内核会将`vruntime`值重置为`0`，触发另一个调度纪元。此外，还有各种可调参数在`/proc/sys/kernel/sched_*`下，以帮助更好地微调CPU调度器的行为。
- en: 'How CFS picks the next task to run is encapsulated in the `kernel/sched/fair.c:pick_next_task_fair()` function.
    In theory, the way CFS works is simplicity itself: enqueue all runnable tasks
    (for that CPU) onto the runqueue, which is an rb-tree (a type of self-balancing
    binary search tree), in such a manner that the task that has spent the least amount
    of time on the processor is the leftmost leaf node on the tree, with succeeding
    nodes to the right representing the next task to run, then the one after that.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: CFS如何选择下一个要运行的任务被封装在`kernel/sched/fair.c:pick_next_task_fair()`函数中。从理论上讲，CFS的工作方式非常简单：将所有可运行的任务（对于该CPU）排队到运行队列上，这是一个rb-tree（一种自平衡二叉搜索树），使得在树上花费最少处理器时间的任务是树上最左边的叶节点，其后的节点表示下一个要运行的任务，然后是下一个。
- en: In effect, scanning the tree from left to right gives a timeline of future task
    execution. How is this assured? By using the aforementioned `vruntime` value as
    the key via which tasks are enqueued onto the rb-tree!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，从左到右扫描树可以给出未来任务执行的时间表。这是如何保证的？通过使用前面提到的`vruntime`值作为任务排队到rb-tree上的关键！
- en: When the kernel needs to schedule, and it asks CFS, the CFS class code - we've
    already mentioned it, the `pick_next_task_fair()` function - *simply picks the
    leftmost leaf node on the tree*, returning the pointer to the task structure embedded
    there; it's, by definition, the task with the lowest `vruntime` value, effectively,
    the one that has run the least! (Traversing a tree is a *O(log n)* time-complexity
    algorithm, but due to some code optimization and a clever caching of the leftmost
    leaf node in effect render it into a very desirable *O(1)* algorithm!) Of course,
    the actual code is a lot more complex than is let on here; it requires several
    checks and balances. We won't delve into the gory details here.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 当内核需要调度并询问CFS时，CFS类代码 - 我们已经提到过了，`pick_next_task_fair()`函数 - *简单地选择树上最左边的叶节点*，返回嵌入其中的任务结构的指针；根据定义，它是具有最低`vruntime`值的任务，实际上是运行时间最短的任务！（遍历树是一个*O(log
    n)*时间复杂度算法，但由于一些代码优化和对最左边叶节点的巧妙缓存，实际上将其转换为一个非常理想的*O(1)*算法！）当然，实际代码比这里透露的要复杂得多；它需要进行多个检查和平衡。我们不会在这里深入讨论细节。
- en: We refer those of you that are interested in learning more on CFS to the kernel
    documentation on the topic, at [https://www.kernel.org/doc/Documentation/scheduler/sched-design-CFS.txt](https://www.kernel.org/doc/%20Documentation/scheduler/sched-design-CFS.txt).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议那些对CFS更多了解的人参考有关该主题的内核文档，网址为[https://www.kernel.org/doc/Documentation/scheduler/sched-design-CFS.txt](https://www.kernel.org/doc/%20Documentation/scheduler/sched-design-CFS.txt)。
- en: Also, the kernel contains several tunables under `/proc/sys/kernel/sched_*`
    that have a direct impact on scheduling. Notes on these and how to use them can
    be found on the *Tuning the Task Scheduler* page ([https://documentation.suse.com/sles/12-SP4/html/SLES-all/cha-tuning-taskscheduler.html](https://documentation.suse.com/sles/12-SP4/html/SLES-all/cha-tuning-taskscheduler.html)),
    and an excellent real-world use case can be found in the article at [https://www.scylladb.com/2016/06/10/read-latency-and-scylla-jmx-process/](https://www.scylladb.com/2016/06/10/read-latency-and-scylla-jmx-process/).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，内核包含了一些在`/proc/sys/kernel/sched_*`下的可调参数，对调度产生直接影响。关于这些参数以及如何使用它们的说明可以在*Tuning
    the Task Scheduler*页面找到（[https://documentation.suse.com/sles/12-SP4/html/SLES-all/cha-tuning-taskscheduler.html](https://documentation.suse.com/sles/12-SP4/html/SLES-all/cha-tuning-taskscheduler.html)），而在文章[https://www.scylladb.com/2016/06/10/read-latency-and-scylla-jmx-process/](https://www.scylladb.com/2016/06/10/read-latency-and-scylla-jmx-process/)中可以找到一个出色的实际用例。
- en: Now let's move onto learning how to query the scheduling policy and priority
    of any given thread.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续学习如何查询任何给定线程的调度策略和优先级。
- en: Threads – which scheduling policy and priority
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线程 - 调度策略和优先级
- en: In this section, you'll learn how to query the scheduling policy and priority
    of any given thread on the system. (But what about programmatically querying and
    setting the same? We defer that discussion to the following chapter, in the *Querying
    and setting a thread’s scheduling policy and priority* section.)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何查询系统上任何给定线程的调度策略和优先级。（但是关于以编程方式查询和设置相同的讨论我们推迟到下一章，在*查询和设置线程的调度策略和优先级*部分。）
- en: We learned that, on Linux, the thread is the KSE; it's what actually gets scheduled
    and runs on the processor. Also, Linux has several choices for the scheduling
    policy (or algorithm) to use. The policy, as well as the priority to allocate
    to a given task (process or thread), is assigned on a per-thread basis, with the
    default always being the `SCHED_OTHER` policy with real-time priority `0`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们了解到，在Linux上，线程就是KSE；它实际上是被调度并在处理器上运行的东西。此外，Linux有多种选择可供使用的调度策略（或算法）。策略以及分配给给定任务（进程或线程）的优先级是基于每个线程的，其中默认值始终是`SCHED_OTHER`策略，实时优先级为`0`。
- en: On a given Linux system, we can always see all processes alive (via a simple
    `ps -A`), or, with GNU `ps`, even every thread alive (`ps -LA`). This does not
    tell us, though, what scheduling policy and priority these tasks are running under;
    how do we query that?
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的Linux系统上，我们总是可以看到所有活动的进程（通过简单的`ps -A`），甚至可以看到每个活动的线程（使用GNU `ps`，`ps -LA`）。但这并不告诉我们这些任务正在运行的调度策略和优先级；我们如何查询呢？
- en: 'This turns out to be pretty simple: on the shell, the `chrt(1)` utility is
    admirably suited to query and set a given process'' scheduling policy and/or priority.
    Issuing `chrt` with the `-p` option switch and providing the PID as a parameter
    has it display both the scheduling policy as well as the real-time priority of
    the task in question; for example, let''s query this for the `init` process (or
    systemd) PID `1`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这其实很简单：在shell上，`chrt(1)`实用程序非常适合查询和设置给定进程的调度策略和/或优先级。使用`-p`选项开关发出`chrt`并提供PID作为参数，它将显示所讨论任务的调度策略以及实时优先级；例如，让我们查询`init`进程（或systemd）的PID`1`的情况：
- en: '[PRE13]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As usual, the `man` page on `chrt(1)` provides all the option switches and their
    usage; do take a peek at it.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，`chrt(1)`的`man`页面提供了所有选项开关及其用法；请查看一下。
- en: 'In the following (partial) screenshot, we show a run of a simple Bash script
    (`ch10/query_task_sched.sh`, a wrapper over `chrt`, essentially) that queries
    and displays the scheduling policy and real-time priority of all the alive threads
    (at the point they''re run):'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下（部分）屏幕截图中，我们展示了一个简单的Bash脚本（`ch10/query_task_sched.sh`，本质上是`chrt`的包装器），它查询并显示了所有活动线程（在运行时）的调度策略和实时优先级：
- en: '![](img/be65cbf3-e0cf-4dd8-a670-caca37a35397.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/be65cbf3-e0cf-4dd8-a670-caca37a35397.png)'
- en: Figure 10.5 – (Partial) screenshot of our ch10/query_task_sched.sh Bash script
    in action
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 - 我们的ch10/query_task_sched.sh Bash脚本的（部分）屏幕截图
- en: 'A few things to notice:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一些需要注意的事项：
- en: 'In our script, by using GNU `ps(1)`, with `ps -LA`, we''re able to capture
    all the threads that are alive on the system; their PID and TID are displayed.
    As you learned in [Chapter 6](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml), *Kernel
    Internals Essentials – Processes and Threads*, the PID is the user space equivalent
    of the kernel TGID and the TID is the user space equivalent of the kernel PID.
    We can thus conclude the following:'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的脚本中，通过使用GNU `ps(1)`，使用`ps -LA`，我们能够捕获系统上所有活动的线程；它们的PID和TID都会显示出来。正如您在[第6章](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml)中学到的，*内核内部基础知识-进程和线程*，PID是内核TGID的用户空间等价物，而TID是内核PID的用户空间等价物。因此，我们可以得出以下结论：
- en: If the PID and TID match, it - the thread seen in that row (the third column
    has its name) - is the main thread of the process.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果PID和TID匹配，那么它 - 在该行中看到的线程（第三列有它的名称） - 是该进程的主线程。
- en: If the PID and TID match and the PID shows up only once, it's a single-threaded
    process.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果PID和TID匹配，并且PID只出现一次，那么它是一个单线程进程。
- en: If we have the same PID multiple times (leftmost column) with varying TIDs (second
    column), those are the child (or worker) threads of the process. Our script shows
    this by indenting the TID number a bit to the right.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们在左侧列中多次具有相同的PID（最左侧列）和不同的TID（第二列），那么这些是该进程的子线程（或工作线程）。我们的脚本通过将TID号稍微向右缩进来显示这一点。
- en: Notice how the vast majority of threads on a typical Linux box (even embedded)
    will tend to be non real-time (the `SCHED_OTHER` policy). On a typical desktop,
    server, or even embedded Linux, the majority of threads will be `SCHED_OTHER`
    (the default policy), with a few real-time threads (FIFO/RR). **Deadline** (**DL**)
    and **Stop-Sched** (**SS**) threads are very rare indeed.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意，在典型的Linux系统（甚至是嵌入式系统）上，绝大多数线程都倾向于是非实时的（`SCHED_OTHER`策略）。在典型的桌面、服务器甚至嵌入式Linux上，大多数线程都是`SCHED_OTHER`（默认策略），只有少数实时线程（FIFO/RR）。**Deadline**（**DL**）和**Stop-Sched**（**SS**）线程确实非常罕见。
- en: 'Do notice the following observations regarding the real-time threads that showed
    up in the preceding output:'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请注意以下关于前述输出中出现的实时线程的观察：
- en: Our script highlights any real-time threads (one with policy: `SCHED_FIFO` or
    `SCHED_RR`) by displaying an asterisk on the extreme right.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的脚本通过在极右边显示一个星号来突出显示任何实时线程（具有策略：`SCHED_FIFO`或`SCHED_RR`）。
- en: Moreover, any real-time threads with a real-time priority of 99 (the maximum
    possible value) will have three asterisks on the extreme right (these tend to
    be specialized kernel threads).
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，任何实时优先级为99（最大可能值）的实时线程将在极右边有三个星号（这些往往是专门的内核线程）。
- en: The `SCHED_RESET_ON_FORK` flag, when Boolean ORed with the scheduling policy,
    has the effect of disallowing any children (via `fork(2)`) to inherit a privileged
    scheduling policy (a security measure).
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当与调度策略进行布尔OR运算时，`SCHED_RESET_ON_FORK`标志会禁止任何子进程（通过`fork(2)`）继承特权调度策略（这是一项安全措施）。
- en: Changing the scheduling policy and/or priority of a thread can be performed
    with `chrt(1)`; however, you should realize that this is a sensitive operation
    requiring root privileges (or, nowadays, the preferred mechanism should be the
    capabilities model, the `CAP_SYS_NICE` capability being the capability bit in
    question).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更改线程的调度策略和/或优先级可以使用`chrt(1)`来执行；但是，您应该意识到这是一个需要root权限的敏感操作（或者，现在应该是首选机制的能力模型，`CAP_SYS_NICE`能力位是相关的能力）。
- en: We will leave it to you to examine the code of the script (`ch10/query_task_sched.sh`).
    Also, be aware (beware!) that performance and shell scripting do not really go
    together (so don't expect much in terms of performance here). Think about it,
    every external command issued within a shell script (and we have several here,
    such as `awk`, `grep`, and `cut`) involves a fork-exec-wait semantic and context
    switching. Also, these are all executing within a loop.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将让您自行查看脚本（`ch10/query_task_sched.sh`）的代码。另外，请注意（注意！）性能和shell脚本实际上并不搭配（所以在性能方面不要期望太多）。想一想，shell脚本中的每个外部命令（我们这里有几个，如`awk`、`grep`和`cut`）都涉及到fork-exec-wait语义和上下文切换。而且，这些都是在循环中执行的。
- en: The `tuna(8)` program can be used to both query and set various attributes;
    this includes process-/thread-level scheduling policy/priority and a CPU affinity
    mask, as well as IRQ affinity.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`tuna(8)`程序可用于查询和设置各种属性；这包括进程/线程级别的调度策略/优先级和CPU亲和力掩码，以及中断请求（IRQ）亲和力。'
- en: 'You might ask, will the (few) threads with the `SCHED_FIFO` policy and a real-time
    priority of `99` always hog the system''s processors? No, not really; the reality
    is that these threads are asleep most of the time. When the kernel does require
    them to perform some work, it wakes them up. Now, precisely due to their real-time
    policy and priority, it''s pretty much guaranteed that they will get a CPU and
    execute for as long as is required (going back to sleep once the work is done).
    The key point: when they require the processor, they will get it (somewhat akin
    to an RTOS, but without the iron-clad guarantees and determinism that an RTOS
    delivers).'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，具有`SCHED_FIFO`策略和实时优先级`99`的（少数）线程是否总是占用系统的处理器？实际上并不是；事实是这些线程大部分时间都是睡眠的。当内核确实需要它们执行一些工作时，它会唤醒它们。由于它们的实时策略和优先级，几乎可以保证它们将获得CPU并执行所需的时间（工作完成后再次进入睡眠状态）。关键是：当它们需要处理器时，它们将得到（类似于实时操作系统，但没有实时操作系统提供的铁定保证和确定性）。
- en: 'How exactly does the `chrt(1)` utility query (and set) the real-time scheduling
    policy/priority? Ah, that should be obvious: as they reside within the task structure
    in kernel **Virtual Address Space** (**VAS**), the `chrt` process must issue a
    system call. There are several system call variations that perform these tasks:
    the one used by `chrt(1)` is the `sched_getattr(2)` to query, and the `sched_setattr(2)` system
    call is to set the scheduling policy and priority. (Be sure to look up the `man`
    page on `sched(7)` for details on these and more scheduler-related system calls.)
    A quick `strace(1)` on `chrt` will indeed verify this!'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '`chrt(1)`实用程序如何查询（和设置）实时调度策略/优先级？嗯，这显而易见：由于它们驻留在内核虚拟地址空间（VAS）中的任务结构中，`chrt`进程必须发出系统调用。有几种系统调用变体执行这些任务：`chrt(1)`使用的是`sched_getattr(2)`进行查询，`sched_setattr(2)`系统调用用于设置调度策略和优先级。（务必查阅`sched(7)`手册页，了解更多与调度程序相关的系统调用的详细信息。）对`chrt`进行快速的`strace(1)`将验证这一点！'
- en: '[PRE14]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now that you have the practical knowledge to query (and even set) a thread's
    scheduling policy/priority, it's time to dig a bit deeper. In the following section,
    we delve further into the internal workings of Linux's CPU scheduler. We figure
    out who runs the code of the scheduler and when it runs. Curious? Read on!
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经掌握了查询（甚至设置）线程调度策略/优先级的实际知识，是时候深入一点了。在接下来的部分中，我们将进一步探讨Linux CPU调度程序的内部工作原理。我们将弄清楚谁运行调度程序的代码以及何时运行。好奇吗？继续阅读！
- en: Learning about the CPU scheduling internals – part 3
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解CPU调度内部 - 第3部分
- en: 'In the preceding sections, you learned that the core kernel scheduling code
    is anchored within the `void schedule(void)` function, and that the modular scheduler
    classes are iterated over, ending up with a thread picked to be context-switched
    to. All of this is fine; a key question now is: who and when, exactly, is the `schedule()`
    code path run?'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，你学到了核心内核调度代码嵌入了`void schedule(void)`函数中，并且模块化调度器类被迭代，最终选择一个线程进行上下文切换。这一切都很好；现在一个关键问题是：`schedule()`代码路径由谁和何时运行？
- en: Who runs the scheduler code?
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谁运行调度器代码？
- en: 'A subtle yet key misconception regarding how scheduling works is unfortunately
    held by many: we imagine that some kind of kernel thread (or some such entity)
    called the "scheduler" is present, that periodically runs and schedules tasks.
    This is just plain wrong; in a monolithic OS such as Linux, scheduling is carried
    out by the process contexts themselves, the regular threads that run on the CPU!'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 关于调度工作方式的一个微妙但关键的误解不幸地被许多人持有：我们想象存在一种称为“调度程序”的内核线程（或类似实体），定期运行和调度任务。这是完全错误的；在像Linux这样的单内核操作系统中，调度是由进程上下文自身执行的，即在CPU上运行的常规线程！
- en: In fact, the scheduling code is always run by the process context that is currently
    executing the code of the kernel, in other words, by **`current`.**
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，调度代码总是由当前执行内核代码的进程上下文运行，换句话说，由`current`运行。
- en: 'This may also be an appropriate time to remind you of what we shall call one
    of the *golden rules* of the Linux kernel: *scheduling code must never ever run
    in any kind of atomic or interrupt context*. In other words, interrupt context
    code must be guaranteed to be non-blocking; this is why you cannot call `kmalloc()`
    with the `GFP_KERNEL` flag in an interrupt context – it might block! But with
    the `GFP_ATOMIC` flag, it''s all right as that instructs the kernel memory management
    code to never block. Also, kernel preemption is disabled while the schedule code
    runs; this makes sense.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可能是一个适当的时机来提醒您Linux内核的一个“黄金规则”：调度代码绝对不能在任何原子或中断上下文中运行。换句话说，中断上下文代码必须保证是非阻塞的；这就是为什么你不能在中断上下文中使用`GFP_KERNEL`标志调用`kmalloc()`
    - 它可能会阻塞！但是使用`GFP_ATOMIC`标志就可以，因为这指示内核内存管理代码永远不会阻塞。此外，当调度代码运行时，内核抢占被禁用；这是有道理的。
- en: When does the scheduler run?
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调度程序何时运行？
- en: 'The job of the OS scheduler is to arbitrate access to the processor (CPU) resource,
    sharing it between competing entities (threads) that want to use it. But what
    if the system is busy, with many threads continually competing for and acquiring
    the processor? More correctly, what we''re really getting at is: in order to ensure
    fair sharing of the CPU resource between tasks, you must ensure that the policeman
    in the picture, the scheduler itself, runs periodically on the processor. Sounds
    good, but how exactly can you ensure that?'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统调度程序的工作是在竞争使用处理器（CPU）资源的线程之间进行仲裁，共享处理器资源。但是如果系统很忙，有许多线程不断竞争和获取处理器呢？更准确地说，为了确保任务之间公平共享CPU资源，必须确保图片中的警察，即调度程序本身，定期在处理器上运行。听起来不错，但你究竟如何确保呢？
- en: 'Here''s a (seemingly) logical way to go about it: invoke the scheduler when
    the timer interrupt fires; that is, it gets a chance to run `CONFIG_HZ` times
    a second (which is often set to the value 250)! Hang on, though, we learned a
    golden rule in [Chapter 8](e78245d1-5a99-4b9e-a98c-cb16b15f3bee.xhtml), *Kernel
    Memory Allocation for Module Authors – Part 1*, in the *Never sleep in interrupt
    or atomic contexts* section: you cannot invoke the scheduler in any kind of atomic
    or interrupt context; thus invoking it within the timer interrupt code path is 
    certainly disqualified. So, what does the OS do?'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个（看似）合乎逻辑的方法：当定时器中断触发时调用调度程序；也就是说，它每秒有`CONFIG_HZ`次运行的机会（通常设置为值250）！不过，我们在[第8章](e78245d1-5a99-4b9e-a98c-cb16b15f3bee.xhtml)中学到了一个黄金法则，*模块作者的内核内存分配
    - 第1部分*，在*永远不要在中断或原子上下文中休眠*部分：你不能在任何类型的原子或中断上下文中调用调度程序；因此，在定时器中断代码路径中调用它是被明确禁止的。那么，操作系统该怎么办呢？
- en: The way it's actually done is that both the timer interrupt context, and the
    process context code paths, are used to make scheduling work. We will briefly
    describe the details in the following section.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上的做法是，定时器中断上下文和进程上下文代码路径都用于进行调度工作。我们将在下一节简要描述详细信息。
- en: The timer interrupt part
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定时器中断部分
- en: Within the timer interrupt (in the code of `kernel/sched/core.c:scheduler_tick()`,
    wherein interrupts are disabled), the kernel performs the meta work necessary
    to keep scheduling running smoothly; this involves the constant updating of the
    per CPU runqueues as appropriate, load balancing work, and so on. Please be aware
    that the actual `schedule()` function is *never called here*. At best, the scheduling
    class hook function (for the process context `current` that was interrupted), `sched_class:task_tick()`, if
    non-null, is invoked. For example, for any thread belonging to the fair (CFS)
    class, the update of the `vruntime` member (the virtual runtime, the (priority-biased)
    time spent on the processor by the task) is done here in `task_tick_fair()`.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在定时器中断中（在`kernel/sched/core.c:scheduler_tick()`的代码中，其中中断被禁用），内核执行必要的元工作，以保持调度平稳运行；这涉及到适当地不断更新每个CPU的运行队列，负载平衡工作等。请注意，实际上*从不*在这里调用`schedule()`函数。最多，调度类钩子函数（对于被中断的进程上下文`current`）`sched_class:task_tick()`，如果非空，将被调用。例如，对于属于公平（CFS）类的任何线程，在`task_tick_fair()`中会在这里更新`vruntime`成员（虚拟运行时间，任务在处理器上花费的（优先级偏置）时间）。
- en: More technically, all this work described in the preceding paragraph occurs
    within the timer interrupt soft IRQ, `TIMER_SOFTIRQ`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，前面段落中描述的所有工作都发生在定时器中断软中断`TIMER_SOFTIRQ`中。
- en: 'Now, a key point, it''s the scheduling code that decides: do we need to preempt
    `current`? Within this timer interrupt code path, if the kernel detects that the
    current task has exceeded its time quantum or must, for any reason, be preempted
    (perhaps there is another runnable thread now on the runqueue with higher priority
    than it), the code sets a "global" flag called `need_resched`. (The reason we
    put the word global within quotes is that it''s not really a kernel-wide global;
    it''s actually simply a bit within `current` instance''s `thread_info->flags` bitmask
    named `TIF_NEED_RESCHED`. Why? It''s actually faster to access the bit that way!)
    It''s worth emphasizing that, in the typical (likely) case, there will be no need
    to preempt `current`, thus the `thread_info.flags:TIF_NEED_RESCHED` bit will remain
    clear. If set, scheduler activation will occur soon; but when exactly? Do read
    on...'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一个关键点，就是调度代码决定：我们是否需要抢占`current`？在定时器中断代码路径中，如果内核检测到当前任务已超过其时间量子，或者出于任何原因必须被抢占（也许现在运行队列上有另一个具有更高优先级的可运行线程），代码会设置一个名为`need_resched`的“全局”标志。（我们在“全局”一词中加引号的原因是它实际上并不是真正的全局内核；它实际上只是`current`实例的`thread_info->flags`位掩码中的一个位，名为`TIF_NEED_RESCHED`。为什么？这样访问位实际上更快！）值得强调的是，在典型（可能）情况下，不会有必要抢占`current`，因此`thread_info.flags:TIF_NEED_RESCHED`位将保持清除。如果设置，调度程序激活将很快发生；但具体何时发生？请继续阅读...
- en: The process context part
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进程上下文部分
- en: 'Once the just-described timer interrupt portion of the scheduling housekeeping
    work is done (and, of course, these things are done very quickly indeed), control
    is handed back to the process context (the thread, `current`) that was rudely
    interrupted. It will now be running what we think of as the exit path from the
    interrupt. Here, it checks whether the `TIF_NEED_RESCHED` bit is set – the `need_resched()` helper
    routine performs this task. If it returns `True`, this indicates the immediate
    need for a reschedule to occur: the kernel calls `schedule()`! Here, it''s fine
    to do so, as we are now running in process context. (Always keep in mind: all
    this code we''re talking about here is being run by `current`, the process context
    in question.)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦刚刚描述的调度工作的定时器中断部分完成（当然，这些事情确实非常迅速地完成），控制权就会交回到进程上下文（线程`current`）中，它会运行我们认为是从中断中退出的路径。在这里，它会检查`TIF_NEED_RESCHED`位是否已设置
    - `need_resched()`辅助例程会执行此任务。如果返回`True`，这表明需要立即进行重新调度：内核将调用`schedule()`！在这里，这样做是可以的，因为我们现在正在运行进程上下文。（请牢记：我们在这里谈论的所有代码都是由`current`，即相关的进程上下文运行的。）
- en: 'Of course, now the key question becomes where exactly is the code that will
    recognize whether the `TIF_NEED_RESCHED` bit has been set (by the previously described
    timer interrupt part)? Ah, this becomes the crux of it: the kernel arranges for
    several **scheduling opportunity points** to be present within the kernel code
    base. Two scheduling opportunity points are as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，现在关键问题是代码的确切位置，该代码将识别`TIF_NEED_RESCHED`位是否已被设置（由先前描述的定时器中断部分）？啊，这就成了问题的关键：内核安排了内核代码基础中存在几个**调度机会点**。两个调度机会点如下：
- en: Return from the system call code path.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从系统调用代码路径返回。
- en: Return from the interrupt code path.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从中断代码路径返回。
- en: 'So, think about it: every time any thread running in user space issues a system
    call, that thread is (context) switched to kernel mode and now runs code within
    the kernel, with kernel privilege. Of course, system calls are finite in length;
    when done, there is a well-known return path that they will follow in order to
    switch back to user mode and continue execution there. On this return path, a
    scheduling opportunity point is introduced: a check is made to see whether the `TIF_NEED_RESCHED` bit
    within its `thread_info` structure is set. If yes, the scheduler is activated.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，请考虑一下：每当运行在用户空间的任何线程发出系统调用时，该线程就会（上下文）切换到内核模式，并在内核中以内核特权运行代码。当然，系统调用是有限长度的；完成后，它们将遵循一个众所周知的返回路径，以便切换回用户模式并在那里继续执行。在这个返回路径上，引入了一个调度机会点：检查其`thread_info`结构中的`TIF_NEED_RESCHED`位是否设置。如果是，调度器就会被激活。
- en: 'FYI, the code to do this is arch-dependent; on x86 it''s here: `arch/x86/entry/common.c:exit_to_usermode_loop()`.
    Within it, the section relevant to us here is:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，执行此操作的代码是与体系结构相关的；在x86上是这里：`arch/x86/entry/common.c:exit_to_usermode_loop()`。在其中，与我们相关的部分是：
- en: '[PRE15]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Similarly, after handling an (any) hardware interrupt (and any associated soft
    IRQ handlers that needed to be run), after the switch back to process context
    within the kernel (an artifact within the kernel – `irq_exit()`), but before restoring
    context to the task that was interrupted, the kernel checks the `TIF_NEED_RESCHED` bit:
    if it is set, `schedule()` is invoked.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在处理（任何）硬件中断之后（和任何需要运行的相关软中断处理程序），在内核中的进程上下文切换回之后（内核中的一个工件——`irq_exit()`），但在恢复中断的上下文之前，内核检查`TIF_NEED_RESCHED`位：如果设置了，就调用`schedule()`。
- en: 'Let''s summarize the preceding discussion on the setting and recognition of
    the `TIF_NEED_RESCHED` bit:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下关于设置和识别`TIF_NEED_RESCHED`位的前面讨论：
- en: 'The timer interrupt (soft IRQ) sets the `thread_info:flags TIF_NEED_RESCHED`
    bit in the following cases:'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定时器中断（软中断）在以下情况下设置`thread_info:flags TIF_NEED_RESCHED`位：
- en: If preemption is required by the logic within the scheduling class's `scheduler_tick()`
    hook function; for example, on CFS, if the current task's `vruntime` value exceeds
    that of another runnable thread by a given threshold (typically 2.25 ms; the relevant
    tunable is `/proc/sys/kernel/sched_min_granularity_ns`).
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果调度类的`scheduler_tick()`钩子函数内的逻辑需要抢占；例如，在CFS上，如果当前任务的`vruntime`值超过另一个可运行线程的给定阈值（通常为2.25毫秒；相关的可调参数是`/proc/sys/kernel/sched_min_granularity_ns`）。
- en: If a higher-priority thread becomes runnable (on the same CPU and thus runqueue;
    via `try_to_wake_up()`).
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个更高优先级的线程可运行（在同一个CPU和运行队列上；通过`try_to_wake_up()`）。
- en: 'In process context, this is what occurs: on both the interrupt return and system
    call return path, check the value of `TIF_NEED_RESCHED`:'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在进程上下文中，发生了这样的事情：在中断返回和系统调用返回路径上，检查`TIF_NEED_RESCHED`的值：
- en: If it's set (`1`), call `schedule()`; otherwise, continue processing.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果设置为（`1`），则调用`schedule()`；否则，继续处理。
- en: As an aside, these scheduling opportunity points – the return from a hardware
    interrupt or a system call – also serve as signal recognition points. If a signal
    is pending on `current`, it is serviced before restoring context or returning
    to user space.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，这些调度机会点——从硬件中断返回或系统调用——也用作信号识别点。如果`current`上有信号挂起，它会在恢复上下文或返回到用户空间之前得到处理。
- en: Preemptible kernel
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可抢占内核
- en: 'Let''s take a hypothetical situation: you''re running on a system with one
    CPU. An analog clock app is running on the GUI along with a C program, `a.out`,
    whose one line of code is (groan) `while(1);`. So, what do you think: will the
    CPU hogger *while 1* process indefinitely hog the CPU, thus causing the GUI clock
    app to stop ticking (will its second hand stop moving altogether)?'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个假设的情况：你在一个只有一个CPU的系统上运行。一个模拟时钟应用程序在GUI上运行，还有一个C程序`a.out`，它的一行代码是（呻吟）`while(1);`。那么，你认为：CPU占用者*while
    1*进程会无限期地占用CPU，从而导致GUI时钟应用程序停止滴答（它的秒针会完全停止移动吗）？
- en: 'A little thought (and experimentation) will reveal that, indeed, the GUI clock
    app keeps ticking in spite of the naughty CPU hogger app! Actually, this is really
    the whole point of having an OS-level scheduler: it can, and does, preempt the
    CPU-hogging user space process. (We briefly discussed the CFS algorithm previously;
    CFS will cause the aggressive CPU hogger process to accumulate a huge `vruntime` value
    and thus move more to the right on its rb-tree runqueue, thus penalizing itself!)
    All modern OSes support this type of preemption – it''s called **user-mode preemption**.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 稍加思考（和实验）就会发现，尽管有一个占用CPU的应用程序，GUI时钟应用程序仍在继续滴答！实际上，这才是操作系统级调度器的全部意义：它可以并且确实抢占占用CPU的用户空间进程。（我们之前简要讨论了CFS算法；CFS将导致侵占CPU的进程累积一个巨大的`vruntime`值，从而在其rb-tree运行队列上向右移动更多，从而对自身进行惩罚！）所有现代操作系统都支持这种类型的抢占——它被称为**用户模式抢占**。
- en: 'But now, consider this: what if you write a kernel module that performs the
    same `while(1)` infinite loop on a single processor system? This could be a problem:
    the system will now simply hang. How will the OS preempt itself (as we understand
    that kernel modules run in kernel mode at kernel privilege)? Well, guess what:
    for many years now, Linux has provided a build-time configuration option to make
    the kernel preemptible, `CONFIG_PREEMPT`. (Actually, this is merely evolution
    toward the long-term goal of cutting down latencies and improving the kernel and
    scheduler response. A large body of this work came from earlier, and some ongoing,
    efforts: the **Low Latency** (**LowLat**) patches, (the old) RTLinux work, and
    so on. We will cover more on real-time (RTOS) Linux - RTL - in the following chapter.)
    Once this `CONFIG_PREEMPT` kernel config option is turned on and the kernel is
    built and booted into, we''re now running on a preemptible kernel – where the
    OS has the ability to preempt itself.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 但是现在，请考虑这样一个问题：如果你在单处理器系统上编写一个执行相同`while(1)`无限循环的内核模块会怎样？这可能是一个问题：系统现在将会简单地挂起。操作系统如何抢占自己（因为我们知道内核模块以内核特权在内核模式下运行）？好吧，你猜怎么着：多年来，Linux一直提供了一个构建时配置选项来使内核可抢占，`CONFIG_PREEMPT`。（实际上，这只是朝着减少延迟和改进内核和调度器响应的长期目标的演变。这项工作的大部分来自早期和一些持续的努力：**低延迟**（LowLat）补丁，（旧的）RTLinux工作等等。我们将在下一章中更多地介绍实时（RTOS）Linux
    - RTL。）一旦打开了`CONFIG_PREEMPT`内核配置选项并构建并引导内核，我们现在运行的是一个可抢占的内核——操作系统有能力抢占自己。
- en: To check out this option, within `make menuconfig`, navigate to General Setup
    | Preemption Model.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看此选项，在`make menuconfig`中，导航到General Setup | Preemption Model。
- en: 'There are essentially three available kernel config options as far as preemption
    goes:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上有三个可用的内核配置选项，就抢占而言：
- en: '| **Preemption type** | **Characteristics** | **Appropriate for** |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| **抢占类型** | **特点** | **适用于** |'
- en: '| `CONFIG_PREEMPT_NONE` | Traditional model, geared toward high overall throughput.
    | Server/enterprise-class and compute-intensive systems |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| `CONFIG_PREEMPT_NONE` | 传统模型，面向高整体吞吐量。 | 服务器/企业级和计算密集型系统 |'
- en: '| `CONFIG_PREEMPT_VOLUNTARY` | Preemptible kernel (desktop); more explicit
    preemption opportunity points within the OS; leads to lower latencies, better
    app response. Typically the default for distros. | Workstations/desktops, laptops
    running Linux for the desktop |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| `CONFIG_PREEMPT_VOLUNTARY` | 可抢占内核（桌面）；操作系统内更明确的抢占机会点；导致更低的延迟，更好的应用程序响应。通常是发行版的默认设置。
    | 用于桌面的工作站/台式机，运行Linux的笔记本电脑 |'
- en: '| `CONFIG_PREEMPT` | LowLat kernel; (almost) the entire kernel is preemptible;
    implies involuntary preemption of even kernel code paths is now possible; yields
    even lower latencies (tens of us to low hundreds us range on average) at the cost
    of slightly lower throughput and slight runtime overhead. | Fast multimedia systems
    (desktops, laptops, even modern embedded products: smartphones, tablets, and so
    on) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| `CONFIG_PREEMPT` | LowLat内核；（几乎）整个内核都是可抢占的；意味着甚至内核代码路径的非自愿抢占现在也是可能的；以稍微降低吞吐量和略微增加运行时开销为代价，产生更低的延迟（平均为几十微秒到低百微秒范围）。
    | 快速多媒体系统（桌面，笔记本电脑，甚至现代嵌入式产品：智能手机，平板电脑等） |'
- en: The `kernel/Kconfig.preempt` kbuild configuration file contains the relevant
    menu entries for the preemptible kernel options. (As you will see in the following
    chapter, when building Linux as an RTOS, a fourth choice for kernel preemption
    appears.)
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '`kernel/Kconfig.preempt` kbuild配置文件包含了可抢占内核选项的相关菜单条目。（正如你将在下一章中看到的，当将Linux构建为RTOS时，内核抢占的第四个选择出现了。）'
- en: CPU scheduler entry points
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CPU调度器入口点
- en: 'The detailed comments present in (just before) the core kernel scheduling function `kernel/sched/core.c:__schedule()` are
    well worth reading through; they specify all the possible entry points to the
    kernel CPU scheduler. We have simply reproduced them here directly from the 5.4
    kernel code base, so do take a look. Keep in mind: the following code is being
    run in process context by the process (thread, really) that''s going to kick itself
    off the CPU by ultimately context-switching to some other thread! And this thread
    is who? Why, it''s `current`, of course!'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在核心内核调度函数`kernel/sched/core.c:__schedule()`之前的详细注释非常值得一读；它们指定了内核CPU调度器的所有可能入口点。我们在这里直接从5.4内核代码库中复制了它们，所以一定要看一下。请记住：以下代码是由即将通过上下文切换到其他线程的进程上下文中运行的！这个线程是谁？当然是`current`！
- en: 'The `__schedule()` function has (among others) two local variables, pointer
    to struct `task_struct` named `prev` and `next`. The pointer named `prev` is set
    to `rq->curr`, which is nothing but `current`! The pointer named `next` will be
    set to the task that''s going to be context-switched to, that''s going to run
    next! So, you see: `current` runs the scheduler code, performing the work and
    then kicking itself off the processor by context-switching to `next`! Here''s
    the large comment we mentioned:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`__schedule()`函数有（其他）两个本地变量，指向名为`prev`和`next`的`task_struct`结构体的指针。名为`prev`的指针设置为`rq->curr`，这只是`current`！名为`next`的指针将设置为即将进行上下文切换的任务，即将运行的任务！所以，你看：`current`运行调度器代码，执行工作，然后通过上下文切换到`next`将自己从处理器中踢出！这是我们提到的大评论：'
- en: '[PRE16]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The preceding code is a large comment detailing how exactly the kernel CPU
    core scheduling code – `__schedule()` – can be invoked. Small relevant snippets
    of `__schedule()` itself can be seen in the following code, reiterating the points
    we have been discussing:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码是一个大评论，详细说明了内核CPU核心调度代码`__schedule()`如何被调用。`__schedule()`本身的一些相关片段可以在以下代码中看到，重申了我们一直在讨论的要点：
- en: '[PRE17]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: A quick word on the actual context switch follows.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是关于实际上下文切换的简短说明。
- en: The context switch
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 上下文切换
- en: 'To finish this discussion, a quick word on the (scheduler) context switch.
    The job of the context switch (in the context of the CPU scheduler) is quite obvious:
    before simply switching to the next task, the OS must save the state of the previous,
    that is, the currently executing, task; in other words, the state of `current`.
    You will recall from [Chapter 6](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml),
    *Kernel Internals Essentials – Processes and Threads*, that the task structure
    holds an inline structure to store/retrieve the thread''s hardware context; it''s
    the member `struct thread_struct thread` (on the x86, it''s always the very last
    member of the task struct). In Linux, an inline function, `kernel/sched/core.c:context_switch()`,
    performs the job, switching from the `prev` task (that is, from `current`) to
    the `next` task, the winner of this scheduling round or preemption. This switch
    is essentially performed in two (arch-specific) stages:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，简要介绍一下（调度程序）上下文切换。上下文切换（在CPU调度程序的上下文中）的工作非常明显：在简单地切换到下一个任务之前，操作系统必须保存先前任务的状态，也就是当前正在执行的任务的状态；换句话说，`current`的状态。您会回忆起[第6章](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml)中所述，*内核内部要点-进程和线程*，任务结构包含一个内联结构，用于存储/检索线程的硬件上下文；它是成员`struct
    thread_struct thread`（在x86上，它始终是任务结构的最后一个成员）。在Linux中，一个内联函数，`kernel/sched/core.c:context_switch()`，执行了这项工作，从`prev`任务（也就是从`current`）切换到`next`任务，即本次调度轮或抢占的赢家。这个切换基本上是在两个（特定于体系结构）阶段中完成的。
- en: '**The memory (MM) switch**: Switch an arch-specific CPU register to point to
    the memory descriptor structure (`struct mm_struct`) of `next`. On the x86[_64],
    this register is called `CR3` (**Control Register 3**); on ARM, it''s called the
    `TTBR0` (**Translation Table Base Register `0`**) register.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存（MM）切换**：将特定于体系结构的CPU寄存器切换到`next`的内存描述符结构（`struct mm_struct`）。在x86[_64]上，此寄存器称为`CR3`（**控制寄存器3**）；在ARM上，它称为`TTBR0`（**翻译表基址寄存器`0`**）寄存器。'
- en: '**The actual CPU switch**: Switch from `prev` to `next` by saving the stack
    and CPU register state of `prev` and restoring the stack and CPU register state
    of `next` onto the processor; this is done within the `switch_to()` macro.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实际的CPU切换**：通过保存`prev`的堆栈和CPU寄存器状态，并将`next`的堆栈和CPU寄存器状态恢复到处理器上，从`prev`切换到`next`；这是在`switch_to()`宏内完成的。'
- en: A detailed implementation of the context switch is not something we shall cover
    here; do check out the *Further reading *section for more resources.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文切换的详细实现不是我们将在这里涵盖的内容；请查看*进一步阅读*部分以获取更多资源。
- en: Summary
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned about several areas and facets of the versatile
    Linux kernel's CPU scheduler. Firstly, you saw how the actual KSE is a thread
    and not a process, followed by gaining an appreciation of the available scheduling
    policies that the OS implements. Next, you understood that to support multiple
    CPUs in a superbly scalable fashion, the kernel powerfully mirrors this with a
    design that employs one runqueue per CPU core per scheduling class. How to query
    any given thread's scheduling policy and priority, and deeper details on the internal
    implementation of the CPU scheduler, were then covered. We focused on how the
    modern scheduler leverages the modular scheduling classes design, who exactly
    runs the actual scheduler code and when, and ended with a brief note on the context
    switch.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您了解了多功能Linux内核CPU调度程序的几个领域和方面。首先，您看到实际的KSE是一个线程而不是一个进程，然后了解了操作系统实现的可用调度策略。接下来，您了解到为了以出色的可扩展方式支持多个CPU，内核使用了每个调度类别每个CPU核心一个运行队列的设计。然后介绍了如何查询任何给定线程的调度策略和优先级，以及CPU调度程序的内部实现的更深层细节。我们重点介绍了现代调度程序如何利用模块化调度类别设计，实际运行调度程序代码的人员以及何时运行，最后简要介绍了上下文切换。
- en: The next chapter has you continue on this journey, gaining more insight and
    details on the workings of the kernel-level CPU scheduler. I suggest you first
    fully digest this chapter's content, work on the questions given, and then move
    on to the next chapter. Great going!
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将让您继续这个旅程，更深入地了解内核级CPU调度程序的工作原理。我建议您首先充分消化本章的内容，解决所提出的问题，然后再继续下一章。干得好！
- en: Questions
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'As we conclude, here is a list of questions for you to test your knowledge
    regarding this chapter''s material: [https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions).
    You will find some of the questions answered in the book''s GitHub repo: [https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn).'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这里有一些问题供您测试对本章材料的了解：[https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions)。您会发现一些问题的答案在书的GitHub存储库中：[https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn)。
- en: Further reading
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: To help you delve deeper into the subject with useful materials, we provide
    a rather detailed list of online references and links (and at times, even books)
    in a Further reading document in this book's GitHub repository. The *Further reading*
    document is available here: [https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md](https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助您深入研究有用的材料，我们在本书的GitHub存储库的*进一步阅读*文档中提供了一个相当详细的在线参考和链接列表（有时甚至包括书籍）。*进一步阅读*文档在这里可用：[https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md](https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md)。

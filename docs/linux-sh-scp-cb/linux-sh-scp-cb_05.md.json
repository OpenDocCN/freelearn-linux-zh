["```\n$ wget URL\n\n```", "```\n$ wget http://slynux.org \n--2010-08-01 07:51:20--  http://slynux.org/ \nResolving slynux.org... 174.37.207.60 \nConnecting to slynux.org|174.37.207.60|:80... connected. \nHTTP request sent, awaiting response... 200 OK \nLength: 15280 (15K) [text/html] \nSaving to: \"index.html\" \n\n100%[======================================>] 15,280      75.3K/s   in 0.2s \n\n2010-08-01 07:51:21 (75.3 KB/s) - \"index.html\" saved [15280/15280]\n\n```", "```\n$ wget URL1 URL2 URL3 ..\n\n```", "```\n$ wget ftp://example_domain.com/somefile.img\n\n```", "```\n$ wget ftp://example_domain.com/somefile.img -O dloaded_file.img -o log\n\n```", "```\n$ wget -t 5 URL\n\n```", "```\n$ wget  --limit-rate 20k http://example.com/file.iso\n\n```", "```\n$ wget -Q 100m http://example.com/file1 http://example.com/file2\n\n```", "```\n$ wget -c URL\n\n```", "```\n$ curl http://slynux.org > index.html\n\n```", "```\n$ wget --mirror exampledomain.com\n\n```", "```\n$ wget -r -N -l DEPTH URL\n\n```", "```\n$ wget \u2013-user username \u2013-password pass URL\n\n```", "```\n$ lynx -dump URL > webpage_as_text.txt\n\n```", "```\n$ lynx -dump http://google.com > plain_text_page.txt\n\n```", "```\n$ cat plain_text_page.txt\n\n```", "```\n$ curl URL --silent\n\n```", "```\n$ curl URL \u2013-silent -O\n\n```", "```\n$ curl http://slynux.org/index.html --silent -O\n\n```", "```\n$ curl URL \u2013-silent -o new_filename\n\n```", "```\n$ curl http://slynux.org -o index.html --progress\n################################## 100.0% \n\n```", "```\n$ curl URL/file -C offset\n\n```", "```\n$ curl -C - URL\n\n```", "```\n$ curl \u2013-referer Referer_URL target_URL\n\n```", "```\n$ curl \u2013-referer http://google.com http://slynux.org\n\n```", "```\n$ curl http://example.com \u2013-cookie \"user=slynux;pass=hack\"\n\n```", "```\n$ curl URL \u2013-cookie-jar cookie_file\n\n```", "```\n$ curl URL \u2013-user-agent \"Mozilla/5.0\"\n\n```", "```\n$ curl -H \"Host: www.slynux.org\" -H \"Accept-language: en\" URL\n\n```", "```\n$ curl URL --limit-rate 20k\n\n```", "```\n$ curl URL --max-filesize bytes\n\n```", "```\n$ curl -u user:pass http://test_auth.com\n\n```", "```\n$ curl -u user http://test_auth.com \n\n```", "```\n$ curl -I http://slynux.org\nHTTP/1.1 200 OK \nDate: Sun, 01 Aug 2010 05:08:09 GMT \nServer: Apache/1.3.42 (Unix) mod_gzip/1.3.26.1a mod_log_bytes/1.2 mod_bwlimited/1.4 mod_auth_passthrough/1.8 FrontPage/5.0.2.2635 mod_ssl/2.8.31 OpenSSL/0.9.7a \nLast-Modified: Thu, 19 Jul 2007 09:00:58 GMT \nETag: \"17787f3-3bb0-469f284a\" \nAccept-Ranges: bytes \nContent-Length: 15280 \nConnection: close \nContent-Type: text/html\n\n```", "```\n#!/bin/bash\nFilename: fetch_gmail.sh\n#Description: Fetch gmail tool\n\nusername=\"PUT_USERNAME_HERE\"\npassword=\"PUT_PASSWORD_HERE\"\n\nSHOW_COUNT=5 # No of recent unread mails to be shown\n\necho\n\ncurl  -u $username:$password --silent \"https://mail.google.com/mail/feed/atom\" | \\\ntr -d '\\n' | sed 's:</entry>:\\n:g' |\\\n sed 's/.*<title>\\(.*\\)<\\/title.*<author><name>\\([^<]*\\)<\\/name><email>\\([^<]*\\).*/Author: \\2 [\\3] \\nSubject: \\1\\n/' | \\\nhead -n $(( $SHOW_COUNT * 3 ))\n```", "```\n$ ./fetch_gmail.sh\nAuthor: SLYNUX [ slynux@slynux.com ]\nSubject: Book release - 2\n\nAuthor: SLYNUX [ slynux@slynux.com ]\nSubject: Book release - 1\n.\n\u2026 5 entries\n\n```", "```\n sed 's/.*<title>\\(.*\\)<\\/title.*<author><name>\\([^<]*\\)<\\/name><email>\\([^<]*\\).*/Author: \\2 [\\3] \\nSubject: \\1\\n/'\n```", "```\n$ lynx -dump http://www.johntorres.net/BoxOfficefemaleList.html  | \\ grep -o \"Rank-.*\" | \\\nsed 's/Rank-//; s/\\[[0-9]\\+\\]//' | \\\nsort -nk 1 |\\\n awk ' \n{\n for(i=3;i<=NF;i++){ $2=$2\" \"$i } \n printf \"%-4s %s\\n\", $1,$2 ; \n}' > actresslist.txt\n\n```", "```\n# Only 3 entries shown. All others omitted due to space limits\n1   Keira Knightley \n2   Natalie Portman \n3   Monica Bellucci \n\n```", "```\nsed 's/Rank-//; s/\\[[0-9]\\+\\]//'\n```", "```\n#!/bin/bash\n#Description: Images downloader\n#Filename: img_downloader.sh\n\nif [ $# -ne 3 ];\nthen\n  echo \"Usage: $0 URL -d DIRECTORY\"\n  exit -1\nfi\n\nfor i in {1..4}\ndo\n  case $1 in\n  -d) shift; directory=$1; shift ;;\n   *) url=${url:-$1}; shift;;\nesac\ndone\n\nmkdir -p $directory;\nbaseurl=$(echo $url | egrep -o \"https?://[a-z.]+\")\n\ncurl \u2013s $url | egrep -o \"<img src=[^>]*>\" | sed 's/<img src=\\\"\\([^\"]*\\).*/\\1/g' > /tmp/$$.list\n\nsed -i \"s|^/|$baseurl/|\" /tmp/$$.list\n\ncd $directory;\n\nwhile read filename;\ndo\n  curl \u2013s -O \"$filename\" --silent\n\ndone < /tmp/$$.list\n```", "```\n$ ./img_downloader.sh http://www.flickr.com/search/?q=linux -d images\n\n```", "```\nfor i in {1..4}\ndo \n case $1 in\n -d) shift; directory=$1; shift ;;\n *) url=${url:-$1}; shift;;\nesac\ndone\n\n```", "```\n$ ./img_downloader.sh -d DIR URL\n\n```", "```\n$ ./img_downloader.sh URL -d DIR\n\n```", "```\n#!/bin/bash\n#Filename: generate_album.sh\n#Description: Create a photo album using images in current directory\n\necho \"Creating album..\"\nmkdir -p thumbs\ncat <<EOF > index.html\n<html>\n<head>\n<style>\n\nbody \n{ \n  width:470px;\n  margin:auto;\n  border: 1px dashed grey;\n  padding:10px; \n} \n\nimg\n{ \n  margin:5px;\n  border: 1px solid black;\n\n} \n</style>\n</head>\n<body>\n<center><h1> #Album title </h1></center>\n<p>\nEOF\n\nfor img in *.jpg;\ndo \n  convert \"$img\" -resize \"100x\" \"thumbs/$img\"\n  echo \"<a href=\\\"$img\\\" ><img src=\\\"thumbs/$img\\\" title=\\\"$img\\\" /></a>\" >> index.html\ndone\n\ncat <<EOF >> index.html\n\n</p>\n</body>\n</html>\nEOF \n\necho Album generated to index.html\n```", "```\n$ ./generate_album.sh\nCreating album..\nAlbum generated to index.html\n\n```", "```\ncat <<EOF > index.html\ncontents...\nEOF\n```", "```\necho \"<a href=\\\"$img\\\" ><img src=\\\"thumbs/$img\\\" title=\\\"$img\\\" /></a>\" >> index.html\n```", "```\n#!/bin/bash\n#Filename: tweets.sh\n#Description: Basic twitter client\n\nUSERNAME=\"PUT_USERNAME_HERE\"\nPASSWORD=\"PUT_PASSWORD_HERE\"\nCOUNT=\"PUT_NO_OF_TWEETS\"\n\nif [[ \"$1\" != \"read\" ]] && [[ \"$1\" != \"tweet\" ]];\nthen \n  echo -e \"Usage: $0 send status_message\\n   OR\\n      $0 read\\n\"\n  exit -1;\nfi\n\nif [[ \"$1\" = \"read\" ]];\nthen \n  curl --silent -u $USERNAME:$PASSWORD  http://twitter.com/statuses/friends_timeline.rss | \\\ngrep title | \\\ntail -n +2 | \\\nhead -n $COUNT | \\\n  sed 's:.*<title>\\([^<]*\\).*:\\n\\1:'\n\nelif [[ \"$1\" = \"tweet\" ]];\nthen \n  status=$( echo $@ | tr -d '\"' | sed 's/.*tweet //')\n  curl --silent -u $USERNAME:$PASSWORD -d status=\"$status\" http://twitter.com/statuses/update.xml > /dev/null\n  echo 'Tweeted :)'\nfi\n```", "```\n$ ./tweets.sh tweet Thinking of writing a X version of wall command \"#bash\"\nTweeted :)\n\n$ ./tweets.sh read\nbot: A tweet line\nt3rm1n4l: Thinking of writing a X version of wall command #bash\n\n```", "```\n#!/bin/bash\n#Filename: define.sh\n#Description: A Google define: frontend\n\nlimit=0\nif  [ ! $# -ge 1 ];\nthen\n  echo -e \"Usage: $0 WORD [-n No_of_definitions]\\n\"\n  exit -1;\nfi\n\nif [ \"$2\" = \"-n\" ];\nthen\n  limit=$3;\n  let limit++\nfi\n\nword=$1\n\nlynx -dump http://www.google.co.in/search?q=define:$word | \\\nawk '/Defini/,/Find defini/' | head -n -1 | sed 's:*:\\n*:; s:^[ ]*::' | \\\ngrep -v \"[[0-9]]\" | \\\nawk '{\nif ( substr($0,1,1) == \"*\" )\n{ sub(\"*\",++count\".\") } ;\nprint\n} ' >  /tmp/$$.txt\n\necho\n\nif [ $limit -ge 1 ];\nthen\n\ncat /tmp/$$.txt | sed -n \"/^1\\./, /${limit}/p\" | head -n -1\n\nelse\n\ncat /tmp/$$.txt;\n\nfi\n```", "```\n$ ./define.sh hack -n 2\n1\\. chop: cut with a hacking tool\n2\\. one who works hard at boring tasks\n\n```", "```\n#!/bin/bash \n#Filename: find_broken.sh\n#Description: Find broken links in a website\n\nif [ $# -eq 2 ]; \nthen \n  echo -e \"$Usage $0 URL\\n\" \n  exit -1; \nfi \n\necho Broken links: \n\nmkdir /tmp/$$.lynx \n\ncd /tmp/$$.lynx \n\nlynx -traversal $1 > /dev/null \ncount=0; \n\nsort -u reject.dat > links.txt \n\nwhile read link; \ndo \n  output=`curl -I $link -s | grep \"HTTP/.*OK\"`; \n  if [[ -z $output ]]; \n  then \n    echo $link; \n    let count++ \n  fi \n\ndone < links.txt \n\n[ $count -eq 0 ] && echo No broken links found.\n```", "```\n#!/bin/bash\n#Filename: change_track.sh\n#Desc: Script to track changes to webpage\n\nif [ $# -eq 2 ];\nthen \n  echo -e \"$Usage $0 URL\\n\"\n  exit -1;\nfi\n\nfirst_time=0\n# Not first time\n\nif [ ! -e \"last.html\" ];\nthen\n  first_time=1\n  # Set it is first time run\nfi\n\ncurl --silent $1 -o recent.html\n\nif [ $first_time -ne 1 ];\nthen\n  changes=$(diff -u last.html recent.html)\n  if [ -n \"$changes\" ];\n  then\n    echo -e \"Changes:\\n\"\n    echo \"$changes\"\n  else\n    echo -e \"\\nWebsite has no changes\"\n  fi\nelse\n  echo \"[First run] Archiving..\"\n\nfi\n\ncp recent.html last.html\n```", "```\n    $ ./track_changes.sh http://web.sarathlakshman.info/test.html\n    [First run] Archiving..\n\n    ```", "```\n    $ ./track_changes.sh http://web.sarathlakshman.info/test.html\n    Website has no changes \n\n    ```", "```\n    $ ./test.sh http://web.sarathlakshman.info/test_change/test.html \n    Changes: \n\n    --- last.html\t2010-08-01 07:29:15.000000000 +0200 \n    +++ recent.html\t2010-08-01 07:29:43.000000000 +0200 \n    @@ -1,3 +1,4 @@ \n    <html>\n    +added line :)\n    <p>data</p>\n    </html>\n\n    ```", "```\n$ curl URL -d \"postvar=postdata2&postvar2=postdata2\"\n\n```", "```\n<form action=\"http://book.sarathlakshman.com/lsc/mlogs/submit.php\" method=\"post\" >\n\n<input type=\"text\" name=\"host\" value=\"HOSTNAME\" >\n<input type=\"text\" name=\"user\" value=\"USER\" >\n<input type=\"submit\" >\n</form>\n```", "```\n$ curl http://book.sarathlakshman.com/lsc/mlogs/submit.php -d \"host=test-host&user=slynux\"\n<html>\nYou have entered :\n<p>HOST : test-host</p>\n<p>USER : slynux</p>\n<html>\n\n```", "```\n$ curl \u2013-data \"name=value\" URL -o output.html\n\n```", "```\n$ curl -d \"name1=val1&name2=val2\" URL -o output.html\n\n```", "```\n$ wget URL \u2013post-data \"name=value\" -O output.html\n\n```"]
- en: Chapter 12. Root Cause Analysis of an Unexpected Reboot
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章。意外重启的根本原因分析
- en: 'In this last chapter, we will put the troubleshooting methods and skills that
    you learned in previous chapters to the test. We will perform a root cause analysis
    of one of the most difficult real-world scenarios: an unexpected reboot.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将对您在之前章节中学到的故障排除方法和技能进行测试。我们将对最困难的真实场景之一进行根本原因分析：意外重启。
- en: As we discussed in [Chapter 1](part0014_split_000.html#DB7S1-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 1. Troubleshooting Best Practices"), *Troubleshooting Best Practices*,
    a root cause analysis is a bit more involved than simply troubleshooting and resolving
    an issue. In Enterprise environments, you will find that every issue that causes
    a significant impact will require a root cause analysis (RCA). The reason for
    this is because Enterprise environments often have well-established processes
    of how incidents are supposed to be handled.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第1章](part0014_split_000.html#DB7S1-8ae10833f0c4428b9e1482c7fee089b4 "第1章。故障排除最佳实践")中讨论的，*故障排除最佳实践*，根本原因分析比简单的故障排除和解决问题要复杂一些。在企业环境中，您会发现每个导致重大影响的问题都需要进行根本原因分析（RCA）。这是因为企业环境通常有关于应该如何处理事件的成熟流程。
- en: In general, when a significant incident occurs, the organization impacted by
    it wants to avoid it from happening again. You can see this in many industries
    even outside of technical environments.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，当发生重大事件时，受到影响的组织希望避免再次发生。即使在技术环境之外的许多行业中也可以看到这一点。
- en: 'As we discussed in [Chapter 1](part0014_split_000.html#DB7S1-8ae10833f0c4428b9e1482c7fee089b4
    "Chapter 1. Troubleshooting Best Practices"), *Troubleshooting Best Practices*,
    a useful RCA has the following characteristics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第1章](part0014_split_000.html#DB7S1-8ae10833f0c4428b9e1482c7fee089b4 "第1章。故障排除最佳实践")中讨论的，*故障排除最佳实践*，一个有用的根本原因分析具有以下特征：
- en: The problem as it was reported
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题的报告方式
- en: The actual root cause of the problem
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题的实际根本原因
- en: A timeline of events and actions taken
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 事件和采取的行动的时间线
- en: Any key data points
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何关键数据点
- en: A plan of action to prevent the incident from re-occurring
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防止事件再次发生的行动计划
- en: For today's issue, we will use an incident to build a sample root cause analysis
    document. To do this, we will use the information gathering and troubleshooting
    steps you learned in previous chapters. While doing all of this, you will also
    learn to handle unexpected reboots, one of the worst incidents to identify the
    root cause for.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于今天的问题，我们将使用一个事件来构建一个样本根本原因分析文档。为此，我们将使用您在之前章节中学到的信息收集和故障排除步骤。在做所有这些的同时，您还将学会处理意外重启，这是确定根本原因的最糟糕的事件之一。
- en: The reason unexpected reboots are difficult is that when the system reboots
    you often lose the information you need to identify the root cause of the issue.
    As we have seen in previous chapters, the more data we can collect during an issue,
    the more likely we are to identify the cause of the issue.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 意外重启困难的原因在于系统重启时通常会丢失您需要识别问题根本原因的信息。正如我们在之前的章节中所看到的，我们在问题发生期间收集的数据越多，我们就越有可能确定问题的原因。
- en: The information lost during reboots can often be the difference between identifying
    the root cause and not identifying the root cause.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在重启期间丢失的信息往往是确定根本原因和未确定根本原因之间的区别。
- en: A late night alert
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深夜警报
- en: As we have been progressing through the chapters and solving many issues for
    our recent employer, we have also been gaining their confidence in our abilities.
    Recently, we were even placed on the **on call** rotation, which means that if
    issues occur after hours an alert will be sent to our phone by SMS.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们在章节中的进展和为最近的雇主解决了许多问题，我们也在获得他们对我们能力的信任。最近，我们甚至被放在了**值班**轮换中，这意味着如果在工作时间之后出现问题，我们的手机将通过短信收到警报。
- en: Of course, the first night of being on call we get an alert; the alert is not
    a good one.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，值班的第一个晚上我们收到了一个警报；这个警报不是一个好消息。
- en: '*ALERT: blog.example.com is no longer responding to ICMP Pings*'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*警报：blog.example.com不再响应ICMP Ping*'
- en: When we were added to the on call rotation, our team lead informed us that any
    major incident that occurs after hours must also have an RCA performed. The reason
    for this is so that others in our group can learn and understand what we did to
    resolve the issue and how to prevent it from happening again.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们被加入到值班轮换中时，我们的团队负责人告诉我们，任何在工作时间之后发生的重大事件都必须进行根本原因分析。这样做的原因是为了让我们组中的其他人学习和了解我们是如何解决问题以及如何防止再次发生的。
- en: As we discussed earlier one of the key components to a useful RCA is listing
    when things happen. A major event in our timeline is when we received the alert;
    based on our SMS message we can see that we received the alert on July 05th, 2015
    at 01:52 or rather; 1:52 A.M. on the fifth of July (welcome to on call!).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，有用的根本原因分析的关键组成部分之一是列出事情发生的时间。我们时间线中的一个重大事件是我们收到警报的时间；根据我们的短信消息，我们可以看到我们在2015年7月5日01:52收到了警报，或者说；7月5日凌晨1:52（欢迎来到值班！）。
- en: Identifying the issue
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定问题
- en: 'From the alert, we can see that our monitoring system was unable to perform
    `ICMP` pings to our company blog server. The first thing we should do is determine
    whether or not we can `ping` the server:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从警报中，我们可以看到我们的监控系统无法对我们公司的博客服务器执行`ICMP` ping。我们应该做的第一件事是确定我们是否可以`ping`服务器：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'It seems that we are able to ping the server in question, so maybe this is
    a false alert? Just in case, let''s attempt to log in to the system:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们能够ping通相关服务器，所以也许这是一个虚警？以防万一，让我们尝试登录系统：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Looks like we were able to log in and the system is up and running; let's start
    taking a look around to check whether we can identify any issues.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们能够登录，系统正在运行；让我们开始四处看看，检查是否能够确定任何问题。
- en: 'As covered in a previous chapter, the first command we always run is `w`:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在之前的章节中介绍的，我们总是运行的第一个命令是`w`：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In this instance, this little habit has actually paid off quite well. With the
    output of the `w` command, we can see that this server has only been up for `9`
    minutes. It seems our monitoring system could not ping our server because it was
    rebooting.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，这个小习惯实际上效果很好。通过“w”命令的输出，我们可以看到这台服务器只运行了“9”分钟。看来我们的监控系统无法ping通我们的服务器，因为它正在重新启动。
- en: Tip
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: We should take note that we were able to identify that the server was rebooted
    after logging in; this will be a critical event in our timeline.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意到我们能够确定服务器在登录后重新启动；这将是我们时间表中的一个关键事件。
- en: Did someone reboot this server?
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 有人重新启动了这台服务器吗？
- en: 'While we have just identified the root cause of the alert, this is not the
    root cause of the issue. We need to identify why the server rebooted. It''s not
    often (at least shouldn''t be) that servers reboot themselves; sometimes it can
    simply be someone performing maintenance on this server without letting others
    know. We can see if anyone has been logged into this server recently using the
    `last` command:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们刚刚确定了警报的根本原因，但这并不是问题的根本原因。我们需要确定服务器为什么重新启动。服务器不经常（至少不应该）自行重新启动；有时可能只是有人在未告知其他人的情况下对该服务器进行维护。我们可以使用“last”命令查看最近是否有人登录到该服务器：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `last` command''s output starts with the latest logins at the top. This
    data is pulled from `/var/log/wtmp`, which is used to store login details. At
    the end of the last command''s output, we see the following line:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: “last”命令的输出从顶部开始显示最新的登录。这些数据来自“/var/log/wtmp”，用于存储登录详细信息。在“last”命令的输出末尾，我们看到以下行：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This tells us how far back the `wtmp` log file goes; a pretty useful piece of
    information. If we want to see a specific number of logins, we could simply add
    the `–n` flag followed by the number of logins we wish to see.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们“wtmp”日志文件的历史记录；这是一个非常有用的信息。如果我们想查看特定数量的登录，我们可以简单地添加“-n”标志，后面跟上我们希望看到的登录数量。
- en: This can be pretty useful in general; however, since we don't know how many
    logins there have been lately on this machine we will just use the default.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是非常有用的；但是，由于我们不知道最近在这台机器上有多少次登录，我们将使用默认设置。
- en: From the output we received, we can see that there haven't been any logins on
    this server recently. Outside of someone physically pressing the power button
    or unplugging this system, we can assume that a person did not reboot the server.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们收到的输出中，我们可以看到最近没有人登录到这台服务器。除非有人亲自按下电源按钮或拔掉系统，否则我们可以假设没有人重新启动服务器。
- en: Tip
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: This is another fact/event that we should use in our timeline.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们时间表中应该使用的另一个事实/事件。
- en: What do the logs tell us?
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日志告诉我们什么？
- en: 'Since a person didn''t reboot this server, our next hypothesis is that this
    server was rebooted by either a software or hardware problem. The next logical
    step for us is to look through the system log files to determine what happened:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于没有人重新启动这台服务器，我们的下一个假设是这台服务器是由软件或硬件问题重新启动的。我们下一个合乎逻辑的步骤是查看系统日志文件，以确定发生了什么事情：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Since there is quite a bit of information here, let's break down what we see
    a little bit.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这里有相当多的信息，让我们稍微分解一下我们看到的内容。
- en: 'The first task is finding a log message that is clearly written on boot. By
    identifying a log message that is written on boot, we will be able to identify
    which logs were written prior to and after the reboot. We will also be able to
    identify a boot time for our root cause documentation:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个任务是找到一个清楚写在启动时的日志消息。通过识别写在启动时的日志消息，我们将能够确定在重新启动之前和之后写入了哪些日志。我们还将能够确定我们的根本原因文档的启动时间：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The first log entry that looks promising is the message from `NetworkManager`
    at `01:50:32`. This message is stating that the `NetworkManager` service has started
    `dhclient`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来有希望的第一个日志条目是“NetworkManager”在“01:50:32”的消息。这条消息说明“NetworkManager”服务已启动“dhclient”。
- en: 'The `dhclient` process is used to make DHCP requests and configure network
    settings based on the reply. This process is generally only called when the network
    is being reconfigured or at boot time:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: “dhclient”进程用于发出DHCP请求并根据回复配置网络设置。这个过程通常只在网络被重新配置或在启动时调用：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If we look at the preceding line, we can see that at 01:50:12, the `rsyslogd`
    process is `exiting on signal 15`. This means, the `rsyslogd` process was sent
    a signal to terminate, a pretty standard process during shutdown.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看前一行，我们可以看到在01:50:12，“rsyslogd”进程正在“退出信号15”。这意味着在关机期间发送了终止信号给“rsyslogd”进程，这是一个非常标准的过程。
- en: We can determine that at 01:50:12 the server was in the shutdown process and
    at 01:50:32 the server was in the boot process. This means, we should be looking
    at everything before 01:50:12 to determine why the system rebooted.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以确定在01:50:12服务器正在关机过程中，在01:50:32服务器正在启动过程中。这意味着我们应该查看01:50:12之前的所有内容，以确定系统为什么重新启动。
- en: Tip
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The shutdown time and boot time will also be needed for our root cause timelines.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 关机时间和启动时间也将需要用于我们的根本原因时间表。
- en: From the preceding captured logs, we can see two processes wrote to `/var/log/messages`
    before 01:50; the `auditd` and watchdog processes.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前捕获的日志中，我们可以看到在01:50之前有两个进程写入了“/var/log/messages”；“auditd”和看门狗进程。
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s first take a look at the `auditd` process. We can see a "low on disk
    space" message in the first line. Could our system have run into an issue due
    to low disk space? It''s possible, and it is something we can check right now:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看一下“auditd”进程。我们可以在第一行看到“磁盘空间不足”的消息。我们的系统是否因为磁盘空间不足而遇到问题？这是可能的，我们现在可以检查一下：
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'It does seem like the filesystem is at 100 percent but something like that
    in itself would not typically cause a reboot. Considering the second `auditd`
    message displays **the daemon is suspending logging**; this would also not seem
    like a reboot procedure. Let''s keep looking and see what else we can identify:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来文件系统已经满了，但这本身通常不会导致重新启动。考虑到第二个“auditd”消息显示**守护程序正在暂停记录**；这也不像是重新启动过程。让我们继续查看，看看我们还能识别出什么：
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The next two messages from the `watchdog` process are interesting. The first
    one states that the `loadavg` for the server is higher than a specified threshold.
    The second message is very interesting as it specifically states, "shutting down
    the system".
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`看门狗`进程的接下来两条消息很有趣。第一条消息指出服务器的`loadavg`高于指定的阈值。第二条消息非常有趣，因为它明确指出了“关闭系统”。'
- en: Could the `watchdog` process have rebooted this server? Maybe, but the first
    question is, what is the `watchdog` process?
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`看门狗`进程可能重新启动了这台服务器吗？也许是的，但首要问题是，`看门狗`进程是什么？'
- en: Learning about new processes and services
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解新的进程和服务
- en: 'It''s not uncommon when digging through the `messages` log to find a process
    you have never used or seen before:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看`messages`日志时发现一个从未使用或见过的进程并不罕见：
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Even on our basic example system, there are 115 unique commands in the process
    list. This is especially true when you add in a newer release such as Red Hat
    Enterprise Linux 7 (newer at the time of writing this). Each new release brings
    in new functionality, which might even mean new processes running by default.
    It's very hard to keep up with it all.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在我们的基本示例系统上，进程列表中有115个独特的命令。特别是当你加入一个新版本，比如写作时的Red Hat Enterprise Linux 7（较新的版本）。每个新版本都带来新的功能，甚至可能意味着默认运行新的进程。要跟上这一切是非常困难的。
- en: 'For the sake of our example, `watchdog` is one of those cases. At this point,
    outside of inferring from the name that it watches things, we have no idea what
    this process does. So how do we learn more about it? Well, we either Google it,
    or `man` it:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 就我们的例子而言，`看门狗`就是这种情况之一。在这一点上，除了从名称中推断出它是观察事物之外，我们不知道这个进程的作用。那么我们如何了解更多关于它的信息呢？好吧，我们要么谷歌一下，要么查看`man`：
- en: '[PRE14]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Based on the `man` page, we have identified that the `watchdog` service is
    actually used to determine whether the server is healthy. If the `watchdog` is
    unable to do this, it might reboot the server:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 根据`man`页面，我们已经确定`看门狗`服务实际上用于确定服务器是否健康。如果`看门狗`无法做到这一点，它可能会重新启动服务器：
- en: '[PRE15]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: It seems from this log message that the watchdog software is the one that caused
    the reboot. Could it be that watchdog rebooted the system because the filesystems
    are full?
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从这条日志消息中看来，`看门狗`软件是导致重新启动的原因。是不是因为文件系统已满，`看门狗`才重新启动了系统？
- en: 'If we go further down the `man` page, we will see another piece of useful information,
    as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们继续阅读`man`页面，我们将看到另一条有用的信息，如下所示：
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'On the last "`test`" in this list, it states that the `watchdog` daemon can
    check whether the average work load is too high:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个列表的最后一个“测试”中，它指出`看门狗`守护程序可以检查平均工作负载是否过高：
- en: '[PRE17]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Given the `man` page and the preceding log message, it seems that `watchdog`
    didn't reboot the server because of the filesystem, but rather due to the load
    average of the server.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 根据`man`页面和前面的日志消息，似乎`看门狗`并不是因为文件系统而重新启动服务器，而是因为服务器的负载平均值。
- en: Tip
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Before going further, let's note that at 01:50:02 the `watchdog` process kicked
    off the reboot.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们注意到在01:50:02，`看门狗`进程启动了重新启动。
- en: What caused the high load average?
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 是什么导致了高负载平均值？
- en: While we have identified what rebooted the server, we still have not gotten
    to the root cause of the issue. We still need to figure out what caused the high
    load average. Unfortunately, this would classify as information that is lost during
    a reboot.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经确定了重新启动服务器的原因，但我们仍然没有找到问题的根本原因。我们仍然需要弄清楚是什么导致了高负载平均值。不幸的是，这被归类为重新启动期间丢失的信息。
- en: If the system was still experiencing a high load average, we would simply be
    able to use `top` or `ps` to figure out which processes are using the most CPU
    time. Once the system was rebooted however, any process that was causing a high
    load average would have been restarted.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系统仍然经历着高负载平均值，我们可以简单地使用`top`或`ps`来找出哪些进程正在使用最多的CPU时间。然而，一旦系统重新启动，任何导致高负载平均值的进程都将被重新启动。
- en: Unless these processes started causing a high load average again, we have no
    way of identifying the source.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 除非这些进程再次导致高负载平均值，否则我们无法确定来源。
- en: '[PRE18]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: However, we are able to identify when the load average started to increase and
    how high it went. This information might be useful as we investigate further,
    as we can use it to identify what time things started to go wrong.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们能够确定负载平均值开始增加的时间和增加到多高。随着我们进一步调查，这些信息可能会有用，因为我们可以用它来确定问题开始出现的时间。
- en: 'To look at a historical view of the load average, we can use the `sar` command:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看负载平均值的历史视图，我们可以使用`sar`命令：
- en: '[PRE19]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Lucky for us, it seems the `sar` commands collection interval is set to every
    `2` minutes. The default is 10 minutes, which means we would normally see a line
    for every 10 minutes:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，看起来`sar`命令的收集间隔设置为每`2`分钟。默认值为10分钟，这意味着我们通常会看到每10分钟的一行：
- en: '[PRE20]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Looking at the output, we can see that at `01:46`, this system has hardly any
    CPU usage. However, starting at `01:48`, there was a `33` percent utilization
    of the CPU in the user space.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中可以看出，在`01:46`，这个系统几乎没有CPU使用率。然而，从`01:48`开始，用户空间的CPU利用率达到了`33`％。
- en: It also seems that at `01:50`, `sar` was able to capture the CPU utilization
    that was being used at `99.99` percent, with `87.8` percent being used by the
    user, and `12.19` percent being used by the system.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，似乎在`01:50`，`sar`能够捕获到CPU利用率达到`99.99`％，其中用户使用了`87.8`％，系统使用了`12.19`％。
- en: Tip
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The above are all good facts to use during our root cause summary.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以上都是我们在根本原因总结中可以使用的好事实。
- en: With this, we now know that our issue started sometime between `01:44` and `01:46`,
    we can see this from the CPU usage.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，我们现在知道我们的问题是在`01:44`和`01:46`之间开始的，我们可以从CPU使用情况中看出。
- en: 'Let''s take a look at the load average with the `–q` flag to see if the load
    averages match the CPU utilization:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`-q`标志来查看负载平均值，看看负载平均值是否与CPU利用率匹配：
- en: '[PRE21]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: With the **load average** measurements, we can see that all was quiet at `01:46`
    even though the CPU was high. However, in the next run at `01:48`, we could see
    the **run queue** at 14 and the 1 minute load average at 4.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**负载平均**的测量，我们可以看到即使在`01:46`时CPU利用率很高，一切都很平静。然而，在接下来的`01:48`运行中，我们可以看到**运行队列**为14，1分钟负载平均值为4。
- en: What are the run queue and load average?
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行队列和负载平均值是什么？
- en: Since we are looking at the run queue and load average, let's take a second
    to get an understanding of what these values mean.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在查看运行队列和负载平均值，让我们花一点时间来理解这些值的含义。
- en: In a very basic concept, the run queue value shows the number of processes in
    an active state waiting to be executed.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个非常基本的概念中，运行队列值显示了处于等待执行状态的进程数量。
- en: For more details, let's think about a CPU and how it works. A single CPU is
    able to perform only one task at a time. Most servers these days have multiple
    cores and sometimes multiple processors per server. On Linux, each core and thread
    (for hyper threaded CPUs) are seen as a single CPU.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 更多细节，请考虑一下CPU及其工作原理。单个CPU一次只能执行一个任务。如今大多数服务器都有多个核心，有时每台服务器还有多个处理器。在Linux上，每个核心和线程（对于超线程CPU）都被视为单个CPU。
- en: Each one of these CPUs is able to execute one task at a time. If we had two
    CPU servers, our server could execute two tasks at a time.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 每个CPU都能一次执行一个任务。如果我们有两个CPU服务器，我们的服务器可以同时执行两个任务。
- en: Let's assume for a second that our 2 CPU system needs to execute four tasks
    at the same time. The system can execute two of those tasks but the other two
    tasks must wait until the first two are finished. When situations like this happen,
    the processes that are waiting are placed into a "run queue". When the system
    has processes in the run queue, they will be prioritized and executed once CPU's
    become available.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们的双CPU系统需要同时执行四个任务。系统可以执行其中两个任务，但另外两个任务必须等到前两个任务完成后才能执行。当出现这种情况时，等待的进程将被放入“运行队列”。当系统中有进程在运行队列中时，它们将被优先处理，并在CPU可用时执行。
- en: In our `sar` capture, we can see the run queue value was 14 at 01:48; this means
    that at that moment, there were 14 tasks waiting in the run queue for CPU.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`sar`捕获中，我们可以看到01:48时运行队列值为14；这意味着在那一刻，有14个任务在运行队列中等待CPU。
- en: Load average
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负载平均值
- en: The load average is a bit different from the run queue, but not very. The load
    average is the average run queue value over a given amount of time. In our preceding
    example, we can see `ldavg-1` (this column is the average run queue length for
    the last minute).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 负载平均值与运行队列有些不同，但并不完全相同。负载平均值是在一定时间内的平均运行队列值。在我们前面的例子中，我们可以看到`ldavg-1`（这一列是最近一分钟的平均运行队列长度）。
- en: The run queue value and the 1-minute load average can be different because the
    run queue value, as reported by `sar` is at the time of execution where the 1-minute
    load average is the run queue averaged over 60 seconds.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 运行队列值和1分钟负载平均值可能会有所不同，因为由`sar`报告的运行队列值是在执行时的值，而1分钟负载平均值是60秒内的运行队列平均值。
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: A single capture of a high run queue might not necessarily mean there is an
    issue, especially if the 1-minute load average is not high. However, in our example,
    we can see that at `01:48`, our run queue had 14 tasks in queue, and at `01:50`,
    our run queue had 37 tasks in queue.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 高运行队列的单次捕获未必意味着存在问题，特别是如果1分钟负载平均值不高的话。然而，在我们的例子中，我们可以看到在`01:48`时，我们的运行队列中有14个任务在队列中，在`01:50`时，我们的运行队列中有37个任务在队列中。
- en: On top of that, we can see that at `01:50`, our 1-minute load average was 25.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们可以看到在`01:50`时，我们的1分钟负载平均值为25。
- en: Given the overlap with the CPU utilization, it seems that roughly around 01:46
    - 01:48, something happened to cause a high CPU utilization. Along with this high
    utilization, there were also a lot of tasks that needed to be executed but could
    not be.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 根据与CPU利用率的重叠，似乎大约在01:46 - 01:48左右，发生了导致CPU利用率高的事件。除了这种高利用率外，还有许多需要执行但无法执行的任务。
- en: Tip
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: We should take a second and note down the times and values we saw in `sar`,
    as these will be necessary details for the root cause summary.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该花一点时间记录下我们在`sar`中看到的时间和值，因为这些将是根本原因总结所必需的细节。
- en: Investigating the filesystem being full
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调查文件系统是否已满
- en: 'Earlier, we noticed that the filesystem was 100 percent full. Unfortunately,
    the version of `sysstat` we have installed doesn''t capture disk space usage.
    A useful thing to identify is when the filesystem filled up as compared to when
    our run queue started to increase:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 早些时候，我们注意到文件系统已经满了。不幸的是，我们安装的`sysstat`版本没有捕获磁盘空间使用情况。一个有用的事情是确定文件系统填满的时间与我们的运行队列开始增加的时间相比：
- en: '[PRE23]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: From the log messages we saw earlier, we could see the `auditd` process identified
    the low disk space at `01:48`. This is extremely close to the time our run queue
    spike was seen.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们之前看到的日志消息中，我们可以看到`auditd`进程在`01:48`识别出低磁盘空间。这与我们看到运行队列急剧增加的时间非常接近。
- en: This is building towards a hypothesis that the problem's root cause was a filesystem
    filling up, which caused a process to either launch many CPU intensive tasks or
    block the CPU for other tasks.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这正建立在一个假设的基础上，即问题的根本原因是文件系统填满，导致一个进程要么启动了许多CPU密集型任务，要么阻塞了CPU以执行其他任务。
- en: 'While this is a sound theory, we have to prove it to be true. One way we can
    get closer to proving this is to identify what is utilizing the majority of disk
    space on this system:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是一个合理的理论，但我们必须证明它是真实的。我们可以更接近证明这一点的方法之一是确定在这个系统上利用了大部分磁盘空间的是什么：
- en: '[PRE24]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The preceding one-liner is a very useful method for identifying which directories
    or files are using the most space.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的一行代码是一个非常有用的方法，用于识别哪些目录或文件使用了最多的空间。
- en: The du command
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: du命令
- en: The preceding one-liner uses the `sort` command, which you learned about in
    [Chapter 11](part0074_split_000.html#26I9K2-8ae10833f0c4428b9e1482c7fee089b4 "Chapter 11. Recovering
    from Common Failures"), *Recovering from Common Failures* to sort the output of
    `du`. The `du` command is a very useful command that can estimate the amount of
    space a given directory is using.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的一行命令使用了`sort`命令，你在[第11章](part0074_split_000.html#26I9K2-8ae10833f0c4428b9e1482c7fee089b4
    "第11章。从常见故障中恢复")中学到了有关`sort`命令的知识，*从常见故障中恢复*，对`du`的输出进行排序。`du`命令是一个非常有用的命令，可以估算给定目录使用的空间量。
- en: 'For example, if we wanted to know how much space the `/var/tmp` directory was
    using, we could easily identify that with the following `du` command:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们想知道`/var/tmp`目录使用了多少空间，我们可以很容易地通过以下`du`命令来确定：
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: A useful attribute of `du` is that, by default, it will not only list `/var/tmp`
    but also the directories within it. We can see that there are a few directories
    with nothing in them but the `/var/tmp/` directory contains 160 kb of data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`du`的一个有用属性是，默认情况下，它不仅会列出`/var/tmp`，还会列出其中的目录。我们可以看到有几个目录里面什么都没有，但`/var/tmp/`目录包含了160
    kb的数据。'
- en: '[PRE26]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: It is important to know that the size of `/var/tmp` is the size of the contents
    within `/var/tmp`, which includes the other subdirectories.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要知道`/var/tmp`的大小是`/var/tmp`中的内容的大小，其中包括其他子目录。
- en: To illustrate the preceding point, I created a directory named "`somedir`" and
    put a 4 kb file within it. We can see from this subsequent `du` command that the
    `/var/tmp` directory is now showing 164 kb used.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明前面的观点，我创建了一个名为“somedir”的目录，并在其中放了一个4 kb的文件。我们可以从随后的`du`命令中看到，`/var/tmp`目录现在显示已使用164
    kb。
- en: 'The `du` command has quite a number of flags that allow us to change how it
    outputs disk usage. In the preceding examples, the values are being printed in
    a human-readable format, thanks to the `–h` flag. In the one liner, these values
    are being represented in kilobytes due to the `–k` flag:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '`du`命令有很多标志，可以让我们改变它输出磁盘使用情况的方式。在前面的例子中，由于`-h`标志的存在，这些值以人类可读的格式打印出来。在一行命令中，由于`-k`标志的存在，这些值以千字节表示：'
- en: '[PRE27]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: If we go back to the one-liner, we can see from the output that from the 38
    GB used in `/`, 34 GB is in the `/opt/myapp/queue` directory. This directory is
    pretty familiar to us, as we were troubleshooting issues with this directory in
    previous chapters.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们回到一行命令，我们可以从输出中看到，在`/`中使用的38 GB中，有34 GB在`/opt/myapp/queue`目录中。这个目录对我们来说非常熟悉，因为我们在之前的章节中曾解决过这个目录的问题。
- en: From our previous experience, we know that this directory is used to queue messages
    received via a custom application.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们以往的经验，我们知道这个目录用于排队接收自定义应用程序接收的消息。
- en: Given the size of this directory, it's possible that before the reboot, the
    custom application was running on this server and filled up the filesystem.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这个目录的大小，有可能在重新启动之前，自定义应用程序在这台服务器上运行，并填满了文件系统。
- en: 'We already know that this directory is consuming the majority of the space
    on this system. It would be useful to determine when the last file in this directory
    was created as this will give us a rough timeframe of when this application was
    running last:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道这个目录占用了系统上大部分的空间。确定这个目录中最后一个文件的创建时间将会很有用，因为这将给我们一个大致的应用上次运行的时间范围：
- en: '[PRE28]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We can actually do this by performing an `ls` in the `/opt/myapp` directory.
    We can see from the preceding output that the `queue/` directory was last modified
    on July 5th at 01:50\. This correlates very nicely with our issues and at minimum,
    proves that the custom application was running prior to the reboot.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上可以通过在`/opt/myapp`目录中执行`ls`来做到这一点。从前面的输出中，我们可以看到`queue/`目录上次修改是在7月5日01:50。这与我们的问题非常吻合，至少证明了在重新启动之前自定义应用程序是在运行的。
- en: Tip
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: The timestamp of when this directory was last updated and the fact that this
    application was running are both items we will notate in our summary.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这个目录上次更新的时间戳以及这个应用程序运行的事实都是我们在总结中要记录的项目。
- en: Based on the preceding information we can, at this point, safely say that at
    the time of the incident, the custom application was running, and had created
    enough files to fill up the filesystem.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的信息，我们可以在这一点上安全地说，在事故发生时，自定义应用程序正在运行，并且已经创建了足够的文件来填满文件系统。
- en: We can also say that around the time the filesystem reached 100 percent utilized,
    the load average of the server spiked suddenly.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以说，在文件系统达到100％利用率时，服务器的负载平均值突然飙升。
- en: From these facts, we can create a hypothesis; our current working theory is
    that once the application filled the filesystem, it was no longer able to create
    files. This might have caused the same application to block CPU time or spawn
    many CPU tasks, which caused a high load average.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些事实，我们可以提出一个假设；我们目前的工作理论是，一旦应用程序填满了文件系统，它就不再能创建文件。这可能导致相同的应用程序阻塞CPU时间或产生许多CPU任务，从而导致负载平均值升高。
- en: Why wasn't the queue directory processed?
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么队列目录没有被处理？
- en: Since we know the custom application was the source of the filesystem issue,
    we also need to answer why.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道自定义应用程序是文件系统问题的根源，我们还需要回答为什么。
- en: 'In earlier chapters, you learned that this application''s queue directory is
    processed by a `cronjob` that runs as the "`vagrant`" user. Let''s take a look
    at when that cron job last ran by looking through the `/var/log/cron` log file:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，你学到了这个应用程序的队列目录是由作为“vagrant”用户运行的`cronjob`处理的。让我们通过查看`/var/log/cron`日志文件来看一下上次运行该cron作业的时间：
- en: '[PRE29]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: According to the `/var/log/cron` directory, the last time the job ran was `June
    6th`. This timeline coincides roughly when this process was moved to another system,
    after this the server ran out of memory.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 根据`/var/log/cron`目录的记录，作业上次运行的时间是`6月6日`。这个时间线大致与这个进程被移动到另一个系统的时间相吻合，之后服务器就没有内存了。
- en: Could it be that the processor job was stopped but the application was not?
    Possibly, we know the application was running but let's check on the `processor`
    job.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器作业是否停止了但应用程序没有停止？可能是，我们知道应用程序正在运行，但让我们检查一下`processor`作业。
- en: 'We can check if the processor job has been removed with the `crontab` command:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`crontab`命令检查处理器作业是否已被删除：
- en: '[PRE30]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The `–l` (list) flag will cause the `crontab` command to print or list the cronjobs
    defined for the user executing it. When the `-u` (user) flag is added, it allows
    us to specify a user to list the cronjobs for, in this case, the `vagrant` user.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`-l`（列出）标志将导致`crontab`命令打印或列出为执行它的用户定义的cronjobs。当添加`-u`（用户）标志时，它允许我们指定要列出cronjobs的用户，在这种情况下是`vagrant`用户。'
- en: It appears from the list that the `processor` job hasn't been removed, but rather,
    it has been disabled. We can see that it has been disabled because the line starts
    with an `#`, which is used to specify comments in the `crontab` file.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 从列表中看来，`processor`作业并没有被删除，而是被禁用了。我们可以看到它已被禁用，因为该行以`#`开头，这用于在`crontab`文件中指定注释。
- en: This essentially turns the job into a comment, rather than a scheduled job.
    This means that the `crond` process will not execute this job.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上将工作变成了一条注释，而不是一个计划任务。这意味着`crond`进程不会执行这项工作。
- en: A checkpoint on what you learned
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 您所学到的内容的检查点
- en: At this point, let's do a checkpoint on what we were able to identify and gather.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，让我们对我们能够确定和收集的内容进行一次检查点。
- en: 'After logging into the system, we were able to determine that the server had
    rebooted. We were able to see in `/var/log/messages` that the `watchdog` process
    was responsible for rebooting the server:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 登录系统后，我们能够确定服务器已经重新启动。我们能够在`/var/log/messages`中看到`watchdog`进程负责重新启动服务器：
- en: '[PRE31]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Based on the log messages in `/var/log/messages`, the watchdog process rebooted
    the server because of a high load. From `sar`, we could see that the load average
    went from 0 to 25 in a matter of a few minutes.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 根据`/var/log/messages`中的日志消息，看门狗进程因负载过高而重新启动了服务器。从`sar`中，我们可以看到负载平均值在几分钟内从0上升到25。
- en: While performing our investigation, we were also able to identify that the server's
    `/` (root) filesystem is full. Not only is it full but also interestingly enough
    it was roughly 100 percent utilized just a few minutes before the system rebooted.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行调查时，我们还能够确定服务器的`/`（根）文件系统已满。不仅满了，而且有趣的是，在系统重新启动前几分钟它大约使用了100％。
- en: The reason the filesystem was in this condition was because the custom application
    in `/opt/myapp` was still running and creating files in `/opt/myapp/queue`. However,
    the job to clear this queue was not running as it has been commented out in the
    vagrant user's `crontab`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 文件系统处于这种状态的原因是因为`/opt/myapp`中的自定义应用程序仍在运行并在`/opt/myapp/queue`中创建文件。然而，清除此队列的作业未运行，因为它已在vagrant用户的`crontab`中被注释掉。
- en: Based on this, we can say that the root cause of our issue is most likely due
    to the filesystem filling up, which is due to the application running but not
    processing messages.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，我们可以说我们问题的根本原因很可能是由于文件系统填满，这是由于应用程序正在运行但未处理消息造成的。
- en: Sometimes you cannot prove everything
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有时你不能证明一切
- en: At this point, we have identified about everything we can as to what caused
    the high load average. Since we don't have a snapshot of what processes were running
    at the time of the incident, we cannot say for certain that it was the custom
    application. We also cannot say for certain based on the information we could
    gather that it was triggered because of the filesystem filling up.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们已经确定了导致负载平均值升高的几乎所有原因。由于我们没有在事件发生时运行的进程的快照，我们无法确定是自定义应用程序。根据我们能够收集到的信息，我们也无法确定是因为文件系统填满而触发的。
- en: We could test this theory by duplicating this scenario in another system, but
    that is not necessarily something to take on at 2:00 A.M. on a weekend. Duplicating
    an issue to that degree is usually something to perform as a follow up activity.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过在另一个系统中复制此场景来测试这个理论，但这不一定是在周末凌晨2:00要做的事情。通常，将问题复制到这个程度通常是作为后续活动来执行的。
- en: At this point given the data we could find, we can be reasonably certain as
    to the root cause. In many cases, this is as close as you will get as you might
    run out of time to gather or simply not have data to base your root cause on.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，根据我们找到的数据，我们可以相当肯定地确定根本原因。在许多情况下，这是你能得到的最接近的，因为你可能没有时间收集数据，或者根本没有数据来确定根本原因。
- en: Preventing reoccurrence
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 防止再次发生
- en: Since we feel pretty confident about our hypothesis as to what happened, we
    now can move on to the final step of our root cause analysis; preventing the issue
    from reoccurring.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们对发生的原因有了相当自信的假设，现在我们可以继续进行我们根本原因分析的最后一步；防止问题再次发生。
- en: As we discussed in the beginning of our chapter, all useful root cause analysis
    reports include a plan of action. Sometimes, this plan of action is something
    to be performed immediately at the time of the issue. Sometimes, this plan is
    to be performed later as a long-term resolution.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章开头讨论的那样，所有有用的根本原因分析报告都包括一个行动计划。有时，这个行动计划是在问题发生时立即执行的。有时，这个计划是作为长期解决方案稍后执行的。
- en: For our issue, we are going to have both, immediate actions and long-term actions.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的问题，我们将采取即时行动和长期行动。
- en: Immediate action
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 即时行动
- en: The first immediate action we need to take is to ensure that the systems primary
    function is healthy. In this case, the server's primary function is to serve the
    company's blog.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要采取的第一个即时行动是确保系统的主要功能健康。在这种情况下，服务器的主要功能是为公司的博客提供服务。
- en: '![Immediate action](img/00009.jpeg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![即时行动](img/00009.jpeg)'
- en: 'This is easy enough to check by going to the blog address in a browser. We
    can see from the preceding screenshot that the blog is working as expected. Just
    to be sure, we can validate that the Apache service is running as well:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在浏览器中访问博客地址很容易检查。从前面的截图中我们可以看到博客正在正常工作。为了确保，我们也可以验证Apache服务是否正在运行：
- en: '[PRE32]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: From this, it looks like our web server has been online since the reboot, this
    is good as it means the blog has been working since the reboot as well.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个情况来看，我们的Web服务器自重启以来一直在线，这很好，因为这意味着博客自重启以来一直在工作。
- en: Tip
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Sometimes, depending on the criticality of the system, it might be important
    to first validate that the system is up and running before even investigating
    the issue. As with anything, this really depends on the environment as there are
    hard and fast rules about which comes first.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，根据系统的重要性，甚至在调查问题之前，首先验证系统是否正常运行可能是很重要的。与任何事情一样，这实际上取决于环境，因为关于哪个先来的硬性规定并不是绝对的。
- en: Now that we know the blog is working as expected, we need to resolve the disk
    being full.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道博客正在正常工作，我们需要解决磁盘已满的问题。
- en: '[PRE33]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As with earlier chapters, it seems the `queue` directory has quite a few messages
    waiting to be processed. In order to clear this properly, we will need to run
    the `processor` command manually, but there are a few extra steps we must take
    as well:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的章节一样，似乎`queue`目录中有很多等待处理的消息。为了正确清除这些消息，我们需要手动运行`processor`命令，但还需要进行一些额外的步骤：
- en: '[PRE34]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The first step we must take is to increase the number of files this system can
    have open at a time. We know this from past experience with the processor application
    and large amounts of messages.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须采取的第一步是增加系统一次可以打开的文件数量。我们根据过去使用processor应用程序和大量消息的经验得知这一点。
- en: '[PRE35]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The second step is to increase the user limitations imposed on the `vagrant`
    user; specifically, the number of open files limitation. This step needs to be
    performed in the same shell session that we will execute the `processor` command
    in. Once the step is complete, we can manually execute the `processor` command
    to process the queued messages:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是增加对`vagrant`用户施加的用户限制；具体来说，是增加打开文件数量的限制。这一步需要在我们执行`processor`命令的同一个shell会话中执行。完成这一步后，我们可以手动执行`processor`命令来处理排队的消息：
- en: '[PRE36]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now that the messages have been processed, we can recheck the filesystem utilization
    with the `df` command:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在消息已经被处理，我们可以使用`df`命令重新检查文件系统利用率：
- en: '[PRE37]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As we can see, the `/` filesystem is down to `10` percent utilization.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，`/`文件系统的利用率已降至`10%`。
- en: 'To ensure that we do not fill up this filesystem again, we validate that the
    custom application is currently stopped:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们不会再次填满这个文件系统，我们验证自定义应用程序当前是否已停止：
- en: '[PRE38]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Since we cannot see any processes running under the name application, we can
    be assured that the application is not running currently.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们看不到以应用程序命名的任何进程在运行，我们可以确信该应用程序当前未运行。
- en: Long-term actions
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 长期行动
- en: This brings us to our **long-term actions**. The long-term actions are actions
    that we will recommend in our root cause summary, but aren't taken at this moment.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这带我们来到我们的**长期行动**。长期行动是我们将在根本原因总结中推荐的行动，但此刻不会采取的行动。
- en: The first long-term action to recommend is that the custom application be permanently
    removed from this system. Since we know that the application has been migrated
    to another system, it should no longer be needed on this server. However, the
    removal of this application is not something we should take on at 2 A.M. or without
    validating that it is truly no longer required.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 建议的第一个长期行动是永久删除该系统中的自定义应用程序。由于我们知道该应用程序已迁移到另一个系统，因此在这台服务器上不再需要。但是，删除该应用程序不是我们应该在凌晨2点或在验证它是否真的不再需要之前就进行的事情。
- en: The second long-term action would be to investigate adding monitoring solutions,
    which can take periodic snapshots of running processes and the CPU/state of those
    processes. If we had that information available to us during this RCA investigation,
    we would be able to prove, without a doubt, which process was causing a high load.
    Since that information is not available, we are left to make an educated guess.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个长期行动是调查添加监控解决方案，可以定期对运行中的进程和这些进程的CPU/状态进行快照。如果在这次根本原因分析调查中有这些信息，我们将能够毫无疑问地证明哪个进程导致了高负载。由于这些信息不可用，我们只能做出合理的猜测。
- en: Again, this would not be a task that we would want to take on during a late
    night call but rather something for a standard work day.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这不是我们想在深夜电话中处理的任务，而是标准工作日的事情。
- en: A sample Root Cause Analysis
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 根本原因分析示例
- en: Now that we have all of the information we need, let's create a root cause analysis
    report. This report can be in any format, really, but I've found that something
    along the following lines works well.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经获得了所有需要的信息，让我们创建一个根本原因分析报告。实际上，这份报告可以是任何格式，但我发现以下内容比较有效。
- en: Problem summary
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题总结
- en: At approximately 1:50 A.M. on July 5, 2015 the server `blog.example.com` unexpectedly
    rebooted. The `watchdog` process initiated the reboot process due to a high load
    average on the server.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年7月5日凌晨1:50左右，服务器`blog.example.com`意外重启。由于服务器负载平均值过高，`watchdog`进程启动了重启过程。
- en: After investigation, the high load average appears to be caused by a custom
    e-mail application, which was left in a running state even though it has been
    migrated to another server.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 经过调查，高负载平均值似乎是由一个自定义的电子邮件应用程序引起的，尽管它已经迁移到另一台服务器，但仍处于运行状态。
- en: From the data available, it seems the application consumed 100 percent of the
    root filesystem.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 根据可用数据，似乎应用程序占用了根文件系统的100%。
- en: While I was unable to obtain process states from before the reboot, it appears
    the high load average might have also been due to the same application being unable
    to write to the disk.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我无法获得重启前的进程状态，但似乎高负载平均值也可能是由于同一应用程序无法写入磁盘而引起的。
- en: Problem details
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题详情
- en: The time at which the incident was reported—07/05/2015 at `01:52`
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 事件报告的时间为2015年7月5日`01:52`
- en: 'The timeline of the incident would be:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 事件的时间线将是：
- en: An SMS alert came through at `01:52` stating `blog.example.com` was unreachable
    via the ICMP ping.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`01:52`收到了一条短信警报，说明`blog.example.com`通过ICMP ping不可访问。
- en: 'The first troubleshooting step performed was a ping of the server:'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行的第一步故障排除是对服务器进行ping：
- en: The ping showed that the server was online
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ping显示服务器在线
- en: Logged into the server at `01:59` and determined that the server had rebooted.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`01:59`登录服务器并确定服务器已重新启动。
- en: 'Searched the `/var/log/messages` file and identified that the watchdog process
    had rebooted the server at `01:50:12`:'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索`/var/log/messages`文件，并确定`watchdog`进程在`01:50:12`重新启动了服务器：
- en: Watchdog started the reboot process due to the high load average at `01:50:02`
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`watchdog`在`01:50:02`开始了重新启动过程'
- en: During investigation, we found that no users were logged in at the time of the
    incident
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在调查过程中，我们发现在事件发生时没有用户登录
- en: The server started the boot process at `01:50:32`
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器在`01:50:32`开始了引导过程
- en: During the investigation, it was identified that the server had run out of available
    disk space at `01:48:01`.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在调查过程中，发现服务器在`01:48:01`已经没有可用的磁盘空间。
- en: The load average of this system started to increase at approximately the same
    time reaching 25 at `01:50:05`.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该系统的负载平均值在大约相同的时间开始增加，达到`01:50:05`时为25。
- en: 'We identified that the `/opt/myapp/queue` directory was last modified at `01:50`
    and contained roughly 34 GB of data creating 100 percent disk utilization:'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们确定`/opt/myapp/queue`目录在`01:50`最后修改，并包含大约34GB的数据，导致100%的磁盘利用率：
- en: This suggests that the custom e-mail application was running until the server
    rebooted
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这表明自定义电子邮件应用程序一直在服务器重新启动之前运行
- en: We found that the `processor` job has not run since June 6th, which means that
    the messages were not processed.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们发现自6月6日以来`processor`作业没有运行，这意味着消息没有被处理。
- en: Root cause
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 根本原因
- en: The filesystem reached 100 percent utilization due to the custom application
    running without the `processor` job being executed via cron. The data collected
    suggests this caused a high load average, which trigged the `watchdog` process
    to reboot the server.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 由于自定义应用程序在未通过cron执行`processor`作业的情况下运行，文件系统达到100%利用率。收集的数据表明这导致了高负载平均值，触发了`watchdog`进程重新启动服务器。
- en: Action plan
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 行动计划
- en: 'We should have the following steps in place:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该采取以下步骤：
- en: Validated that Apache is running and `Blog` is accessible
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证Apache正在运行并且`Blog`是可访问的
- en: Validated that the custom application is not running after system reboot
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证系统重新启动后自定义应用程序未在运行
- en: Executed the processor job manually at 02:15 resolving disk space issues
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在02:15手动执行了处理器作业，解决了磁盘空间问题
- en: Further actions to be taken
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 需要采取进一步行动
- en: Remove the custom application from the server to prevent the application from
    accidently starting
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从服务器中删除自定义应用程序，以防止应用程序意外启动
- en: 'Investigate the addition of process list monitoring to capture which processes
    are utilizing the CPU time during similar issues:'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调查添加进程列表监视，以捕获在类似问题期间利用CPU时间的进程：
- en: Will help in resolution of any similar situations should they occur
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将有助于解决类似情况
- en: As you can see in the preceding report, we have a high-level timeline showing
    what we were able to identify, how we identified it, and the actions we took to
    resolve the issue. All key components of a good root cause analysis.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前面的报告中所看到的，我们有一个高层次的时间线，显示了我们能够确定的内容，我们如何确定的，以及我们采取的解决问题的行动。这是一个良好的根本原因分析的所有关键组成部分。
- en: Summary
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we covered how to respond to a very difficult issue: an unexpected
    reboot. We used the tools and methodologies we saw throughout this book to identify
    the root cause and create a root cause report.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了如何应对一个非常困难的问题：意外的重新启动。我们使用了本书中看到的工具和方法来确定根本原因并创建根本原因报告。
- en: We used log files heavily throughout this book; in this chapter, we were able
    to use these logs to identify the process that rebooted the server. We also identified
    the reason `watchdog` decided to reboot the server, which was due to a high load
    average.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在整本书中大量使用日志文件；在本章中，我们能够使用这些日志文件来识别重新启动服务器的进程。我们还确定了`watchdog`决定重新启动服务器的原因，这是由于高负载平均值。
- en: We were able to use tools such as `sar`, `df`, `du`, and `ls` to determine the
    timing and cause of the high load average. All of these tools are commands you
    learned about throughout this book.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够使用`sar`、`df`、`du`和`ls`等工具来确定高负载平均值的时间和原因。这些工具都是您在整本书中学到的命令。
- en: With this last chapter, we covered quite a few examples that were covered earlier
    in this book. You learned how to troubleshoot web applications, performance issues,
    custom applications, and hardware problems. We did all of these using real-world
    examples with real-world solutions.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章，我们涵盖了本书中早期涵盖的许多示例。您学会了如何解决Web应用程序、性能问题、自定义应用程序和硬件问题。我们使用了真实世界的示例和解决方案。
- en: While this book covers quite a few topics, the goal of this book was to show
    you the concepts of troubleshooting issues with Red Hat Enterprise Linux systems.
    The examples might be commonplace or somewhat rare but the commands used in these
    examples are commands that are used daily during troubleshooting. The topics covered
    all provide a core competency with Linux and will provide you with the knowledge
    necessary to troubleshoot issues not directly covered in this book.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书涵盖了相当多的主题，但本书的目标是向您展示如何解决红帽企业Linux系统的故障排除问题。示例可能很常见，也可能有些罕见，但这些示例中使用的命令是在故障排除过程中日常使用的命令。所涵盖的主题都提供了与Linux相关的核心能力，并将为您提供解决本书未直接涵盖的问题所需的知识。

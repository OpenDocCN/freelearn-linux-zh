# 第六章：内核同步 - 第一部分

任何熟悉在多线程环境中编程的开发人员（甚至在多个进程共享内存或中断可能发生的单线程环境中）都知道，当两个或更多个线程（一般的代码路径）可能会竞争时，需要**同步**；也就是说，它们的结果是无法预测的。纯代码本身从来不是问题，因为它的权限是读/执行（`r-x`）；在多个 CPU 核心上同时读取和执行代码不仅完全正常和安全，而且是受鼓励的（它会提高吞吐量，这就是为什么多线程是一个好主意）。然而，当你开始处理共享可写数据时，你就需要开始非常小心了！

围绕并发性及其控制 - 同步 - 的讨论是多种多样的，特别是在像 Linux 内核这样的复杂软件环境中（其子系统和相关区域，如设备驱动程序），这也是我们在本书中要处理的。因此，为了方便起见，我们将把这个大主题分成两章，本章和下一章。

在本章中，我们将涵盖以下主题：

+   关键部分、独占执行和原子性

+   Linux 内核中的并发性问题

+   互斥锁还是自旋锁？在什么情况下使用

+   使用互斥锁

+   使用自旋锁

+   锁定和中断

让我们开始吧！

# 关键部分、独占执行和原子性

想象一下，你正在为一个多核系统编写软件（嗯，现在，通常情况下，你会在多核系统上工作，即使是在大多数嵌入式项目中）。正如我们在介绍中提到的，同时运行多个代码路径不仅是安全的，而且是可取的（否则，为什么要花那些钱呢，对吧？）。另一方面，在其中**共享可写数据**（也称为**共享状态**）**被访问**的并发（并行和同时）代码路径是需要你保证，在任何给定的时间点，只有一个线程可以同时处理该数据！这真的很关键；为什么？想想看：如果你允许多个并发代码路径在共享可写数据上并行工作，你实际上是在自找麻烦：**数据损坏**（"竞争"）可能会发生。

## 什么是关键部分？

可以并行执行并且可以处理（读取和/或写入）共享可写数据（共享状态）的代码路径被称为关键部分。它们需要保护免受并行性的影响。识别和保护关键部分免受同时执行是你 - 设计师/架构师/开发人员 - 必须处理的隐含要求，以确保正确的软件。

关键部分是必须要么独占地运行；也就是说，单独运行（串行化），要么是原子地；也就是说，不可分割地，一直运行到完成，没有中断。

通过“独占”，我们暗示在任何给定的时间点，一个线程正在运行关键部分的代码；这显然是出于数据安全的原因而需要的。

这个概念也提出了*原子性*的重要概念：单个原子操作是不可分割的。在任何现代处理器上，两个操作被认为总是**原子的**；也就是说，它们不能被中断，并且会一直运行到完成：

+   单个机器语言指令的执行。

+   对齐的原始数据类型的读取或写入，它在处理器的字长（通常为 32 位或 64 位）内；例如，在 64 位系统上读取或写入 64 位整数是有保证的。读取该变量的线程永远不会看到中间、撕裂或脏的结果；它们要么看到旧值，要么看到新值。

因此，如果您有一些代码行处理共享（全局或静态）可写数据，那么在没有任何显式同步机制的情况下，不能保证其独占运行。请注意，有时需要以原子方式运行临界区的代码，以及独占运行，但并非始终如此。

当临界区的代码在安全睡眠的进程上下文中运行时（例如通过用户应用程序对驱动程序进行典型文件操作（打开，读取，写入，ioctl，mmap 等），或者内核线程或工作队列的执行路径），也许可以接受临界区不是真正原子的。但是，当其代码在非阻塞原子上下文中运行时（例如硬中断，tasklet 或 softirq），*它必须以原子方式运行以及独占运行*（我们将在*互斥锁还是自旋锁？何时使用*部分中更详细地讨论这些问题）。

一个概念性的例子将有助于澄清事情。假设三个线程（来自用户空间应用程序）在多核系统上几乎同时尝试打开并从您的驱动程序读取。在没有任何干预的情况下，它们可能会并行运行临界区的代码，从而并行地处理共享可写数据，从而很可能损坏它！现在，让我们看一个概念图示，看看临界区代码路径内的非独占执行是错误的（我们甚至不会在这里谈论原子性）：

![](img/4daabc10-0ddf-4879-96c2-49eeb6aa96e3.png)

图 6.1 - 一个概念图示，显示了临界区代码路径如何被同时运行的多个线程违反

如前图所示，在您的设备驱动程序中，在其（比如）读取方法中，您正在运行一些代码以执行其工作（从硬件中读取一些数据）。让我们更深入地看一下这个图示*在不同时间点进行的数据访问*：

+   从时间`t0`到`t1`：没有或只有本地变量数据被访问。这是并发安全的，不需要保护，并且可以并行运行（因为每个线程都有自己的私有堆栈）。

+   从时间`t1`到`t2`：访问全局/静态共享可写数据。这是*不*并发安全的；它是**临界区**，因此必须受到**保护**，以免并发访问。它应该只包含以独占方式运行的代码（独自，每次只有一个线程，串行），也许还是原子的。

+   从时间`t2`到`t3`：没有或只有本地变量数据被访问。这是并发安全的，不需要保护，并且可以并行运行（因为每个线程都有自己的私有堆栈）。

在本书中，我们假设您已经意识到需要同步临界区；我们将不再讨论这个特定的主题。有兴趣的人可以参考我早期的书，*Linux 系统编程实战（Packt，2018 年 10 月）*，其中详细介绍了这些问题（特别是*第十五章*，*使用 Pthreads 进行多线程编程第二部分-同步*）。

因此，了解这一点，我们现在可以重新阐述临界区的概念，同时提到情况何时出现（在项目符号和斜体中显示在项目符号中）。临界区是必须按以下方式运行的代码：

+   （始终）*独占地*：独自（串行）

+   （在原子上下文中）*原子地*：不可分割地，完整地，没有中断

在下一节中，我们将看一个经典的场景 - 全局整数的增量。

## 一个经典的例子 - 全局 i ++

想象一下这个经典的例子：在并发代码路径中递增一个全局整数`i`，其中多个执行线程可以同时执行。对计算机硬件和软件的天真理解会让您相信这个操作显然是原子的。然而，现实是，现代硬件和软件（编译器和操作系统）要比您想象的复杂得多，因此会引起各种看不见的（对应用程序开发人员来说）性能驱动的优化。

我们不会在这里深入讨论太多细节，但现实是，现代处理器非常复杂：它们采用许多技术来提高性能，其中一些是超标量和超流水线执行，以便并行执行多个独立指令和各种指令的几个部分（分别），进行即时指令和/或内存重排序，在复杂的 CPU 缓存中缓存内存，虚假共享等等！我们将在第七章中的*内核同步-第二部分*中的*缓存效应-虚假共享*和*内存屏障*部分中深入探讨其中的一些细节。

Matt Kline 于 2020 年 4 月撰写的论文*《每个系统程序员都应该了解的并发知识》*（[`assets.bitbashing.io/papers/concurrency-primer.pdf`](https://assets.bitbashing.io/papers/concurrency-primer.pdf)）非常出色，是这个主题上的必读之作；一定要阅读！

所有这些使得情况比起初看起来更加复杂。让我们继续讨论经典的`i ++`：

```
static int i = 5;
[ ... ]
foo()
{
    [ ... ]
    i ++;     // is this safe? yes, if truly atomic... but is it truly atomic??
}
```

这个递增本身安全吗？简短的答案是否定的，您必须保护它。为什么？这是一个关键部分——我们正在访问共享的可写数据进行读取和/或写入操作。更长的答案是，这实际上取决于递增操作是否真正是原子的（不可分割的）；如果是，那么`i ++`在并行性的情况下不会造成危险——如果不是，就会有危险！那么，我们如何知道`i ++`是否真正是原子的呢？有两件事决定了这一点：

+   处理器的**指令集架构**（ISA），它决定了（与处理器低级相关的几件事情之一）在运行时执行的机器指令。

+   编译器。

如果 ISA 具有使用单个机器指令执行整数递增的功能，并且编译器具有使用它的智能，那么它就是真正原子的——它是安全的，不需要锁定。否则，它是不安全的，需要锁定！

尝试一下：将浏览器导航到这个精彩的编译器探索网站：[`godbolt.org/`](https://godbolt.org/)。选择 C 作为编程语言，然后在左侧窗格中声明全局整数`i`并在函数内递增。在右侧窗格中使用适当的编译器和编译器选项进行编译。您将看到为 C 高级`i ++;`语句生成的实际机器代码。如果确实是单个机器指令，那么它将是安全的；如果不是，您将需要锁定。总的来说，您会发现您实际上无法判断：实际上，您*不能*假设事情——您将不得不默认假设它是不安全的并加以保护！这可以在以下截图中看到：

![](img/a6f1659c-346b-40c8-b5e0-f0e4033381ef.png)

图 6.2——即使是最新的稳定 gcc 版本，但没有优化，x86_64 gcc 为 i ++生成了多个指令

前面的截图清楚地显示了这一点：左右两个窗格中的黄色背景区域分别是 C 源代码和编译器生成的相应汇编代码（基于 x86_64 ISA 和编译器的优化级别）。默认情况下，没有优化，`i ++`变成了三条机器指令。这正是我们所期望的：它对应于*获取*（内存到寄存器）、*增量*和*存储*（寄存器到内存）！现在，这*不是*原子的；完全有可能，在其中一条机器指令执行后，控制单元干扰并将指令流切换到不同的位置。这甚至可能导致另一个进程或线程被上下文切换！

好消息是，通过在`编译器选项...`窗口中快速加上`-O2`，`i ++`就变成了一条机器指令 - 真正的原子操作！然而，我们无法预测这些事情；有一天，你的代码可能会在一个相当低端的 ARM（RISC）系统上执行，增加了`i ++`需要多条机器指令的可能性。（不用担心 - 我们将在*使用原子整数操作符*部分专门介绍针对整数的优化锁技术）。

现代语言提供了本地原子操作符；对于 C/C++来说，这是相当近期的（从 2011 年起）；ISO C++11 和 ISO C11 标准提供了现成的和内置的原子变量。稍微搜索一下就可以快速找到它们。现代的 glibc 也在使用它们。举个例子，如果你在用户空间使用信号，你会知道要使用`volatile sig_atomic_t`数据类型来安全地访问和/或更新信号处理程序中的原子整数。那么内核呢？在下一章中，你将了解 Linux 内核对这个关键问题的解决方案。我们将在*使用原子整数操作符*和*使用原子位操作符*部分进行介绍。

Linux 内核当然是一个并发环境：多个执行线程在多个 CPU 核心上并行运行。不仅如此，即使在单处理器（UP/单 CPU）系统上，硬件中断、陷阱、故障、异常和软件信号的存在也可能导致数据完整性问题。毋庸置疑，保护代码路径中必要的并发性是易说难做的；识别和保护关键部分使用诸如锁等技术的同步原语和技术是绝对必要的，这也是为什么这是本章和下一章的核心主题。

## 概念 - 锁

我们需要同步是因为，没有任何干预，线程可以同时执行关键部分，其中共享可写数据（共享状态）正在被处理。为了打败并发性，我们需要摆脱并行性，我们需要*串行化*关键部分内的代码 - 共享数据正在被处理的地方（用于读取和/或写入）。

为了强制一个代码路径变成串行化，一个常见的技术是使用**锁**。基本上，锁通过保证在任何给定时间点上只有一个执行线程可以“获取”或拥有锁来工作。因此，在代码中使用锁来保护关键部分将给我们想要的东西 - 专门运行关键部分的代码（也许是原子的；更多内容即将到来）：

![](img/5ccf6307-e970-4b7f-bcaa-566fb4acfb80.png)

图 6.3 - 一个概念图示，展示了如何使用锁来保证关键部分代码路径的独占性

前面的图示展示了解决前面提到的情况的一种方法：使用锁来保护关键部分！锁（和解锁）在概念上是如何工作的呢？

锁的基本前提是，每当有争用时（即多个竞争线程（比如`n`个线程）尝试获取锁（`LOCK`操作）时），只有一个线程会成功。这被称为锁的“赢家”或“所有者”。它将*lock* API 视为非阻塞调用，因此在执行关键部分的代码时会继续运行 - 并且是独占的（关键部分实际上是*lock*和*unlock*操作之间的代码！）。那么剩下的`n-1`个“失败者”线程会发生什么呢？它们（也许）会将锁 API 视为阻塞调用；它们实际上会等待。等待什么？当然是锁的*unlock*操作，这是由锁的所有者（“赢家”线程）执行的！一旦解锁，剩下的`n-1`个线程现在会竞争下一个“赢家”位置；当然，它们中的一个会“赢”并继续前进；在此期间，`n-2`个失败者现在会等待（新的）赢家的*unlock*；这种情况会重复，直到所有`n`个线程（最终和顺序地）获取锁。

当然，锁定是有效的，但 - 这应该是相当直观的 - 它会导致（相当大的！）**开销，因为它破坏了并行性并串行化了**执行流！为了帮助您可视化这种情况，想象一个漏斗，狭窄的部分是只有一个线程可以一次进入的关键部分。所有其他线程都会被堵住；锁定会创建瓶颈：

![](img/4f476235-4b35-4d76-8d49-694b0095c1be.png)

图 6.4 - 锁创建了一个瓶颈，类似于一个物理漏斗

另一个经常提到的物理类比是一条高速公路，有几条车道汇入一条非常繁忙 - 交通拥挤 - 的车道（也许是一个设计不佳的收费站）。同样，并行性 - 车辆（线程）在不同车道上与其他车辆并行行驶（CPU） - 丢失，并且需要串行行为 - 车辆被迫排队排队。

因此，作为软件架构师，我们必须努力设计我们的产品/项目，以尽量减少对锁的需求。虽然在大多数实际项目中完全消除全局变量是不可行的，但优化和最小化它们的使用是必需的。我们将在以后更详细地介绍这一点，包括一些非常有趣的无锁编程技术。

另一个非常关键的点是，新手程序员可能天真地认为在共享可写数据对象上执行读取是完全安全的，因此不需要显式保护（除了在处理器总线大小范围内的对齐原始数据类型的情况下）；这是不正确的。这种情况可能导致所谓的**脏读或破碎读**，即在另一个写入线程同时写入时可能读取到过时的数据，而你在没有锁定的情况下错误地读取了相同的数据项。

既然我们谈到了原子性，正如我们刚刚了解的那样，在典型的现代微处理器上，唯一保证原子性的是单个机器语言指令或者在处理器总线宽度内对齐的原始数据类型的读/写。那么，我们如何标记几行“C”代码，使其真正原子化呢？在用户空间中，这甚至是不可能的（我们可以接近，但无法保证原子性）。

在用户空间应用程序中如何“接近”原子性？您可以始终构建一个用户线程来使用`SCHED_FIFO`策略和实时优先级为`99`。这样，当它想要运行时，除了硬件中断/异常之外，几乎没有其他东西可以抢占它。（旧的音频子系统实现在很大程度上依赖于此。）

在内核空间中，我们可以编写真正原子的代码。怎么做呢？简短的答案是，我们可以使用自旋锁！我们很快将更详细地了解自旋锁。

### 关键点总结

让我们总结一些关于临界区的关键点。仔细审查这些内容非常重要，保持这些内容方便，并确保在实践中使用它们：

+   **临界区**是一个可以并行执行并且可以操作（读和/或写）共享可写数据（也称为“共享状态”）的代码路径。

+   由于它处理共享可写数据，临界区需要保护免受以下影响：

+   并行性（也就是说，它必须单独运行/串行运行/以互斥的方式运行）

+   在原子（中断）非阻塞上下文中运行 - 原子地：不可分割地，完全地，没有中断。一旦受保护，你可以安全地访问你的共享状态，直到“解锁”。

+   代码库中的每个临界区都必须被识别和保护：

+   识别临界区至关重要！仔细审查你的代码，确保你没有漏掉它们。

+   可以通过各种技术来保护它们；一个非常常见的技术是*锁定*（还有无锁编程，我们将在下一章中看到）。

+   一个常见的错误是只保护对全局可写数据的*写*的临界区；你还必须保护对全局可写数据的*读*的临界区；否则，你会面临**破碎或脏读！**为了帮助澄清这一关键点，想象一下在 32 位系统上读取和写入无符号 64 位数据项；在这种情况下，操作不能是原子的（需要两次加载/存储操作）。因此，如果在一个线程中读取数据项的值的同时，另一个线程正在同时写入它，会怎么样！？写入线程以某种方式“锁定”，但因为你认为读取是安全的，读取线程没有获取锁；由于不幸的时间巧合，你最终可能会执行部分/破碎/脏读！我们将在接下来的章节和下一章中学习如何通过使用各种技术来克服这些问题。

+   另一个致命的错误是不使用相同的锁来保护给定的数据项。

+   未保护临界区会导致**数据竞争**，即实际值的结果 - 被读/写的数据的实际值 - 是“竞争的”，这意味着它会根据运行时环境和时间而变化。这被称为一个 bug。（一旦在“现场”中，这是极其难以看到、重现、确定其根本原因和修复的 bug。我们将在下一章中涵盖一些非常强大的内容，以帮助你解决这个问题，在*内核中的锁调试*部分；一定要阅读！）

+   **例外**：在以下情况下，你是安全的（隐式地，没有显式保护）：

+   当你在处理局部变量时。它们分配在线程的私有堆栈上（或者，在中断上下文中，分配在本地 IRQ 堆栈上），因此，根据定义，是安全的。

+   当你在代码中处理共享可写数据时，这段代码不可能在另一个上下文中运行；也就是说，它是串行化的。在我们的情况下，LKM 的*init*和*cleanup*方法符合条件（它们仅在`insmod`和`rmmod`上一次串行运行）。

+   当你在处理真正常量和只读的共享数据时（不要让 C 的`const`关键字误导你）。

+   锁定本质上是复杂的；你必须仔细思考、设计和实现，以避免*死锁*。我们将在*锁定指南和死锁*部分中更详细地介绍这一点。

# Linux 内核中的并发性问题

在内核代码中识别临界区至关重要；如果你甚至看不到它，你怎么保护它呢？以下是一些建议，可以帮助你作为一个新手内核/驱动程序开发人员，识别并发性问题的地方 - 因此可能出现临界区的地方：

+   **对称多处理器**（**SMP**）系统的存在（`CONFIG_SMP`）

+   可抢占内核的存在

+   阻塞 I/O

+   硬件中断（在 SMP 或 UP 系统上）

这些都是需要理解的关键点，我们将在本节中讨论每一个。

## 多核 SMP 系统和数据竞争

第一个点是非常明显的；看一下以下截图中显示的伪代码：

![](img/79357d73-c814-478c-b463-1951621f15e2.png)

图 6.5 - 伪代码 - 在（虚构的）驱动程序读取方法中的一个关键部分；由于没有锁定，这是错误的

这与我们在*图 6.1*和*6.3*中展示的情况类似；只是这里，我们用伪代码来展示并发。显然，从时间`t2`到时间`t3`，驱动程序正在处理一些全局共享的可写数据，因此这是一个关键部分。

现在，想象一个具有四个 CPU 核心（SMP 系统）的系统；两个用户空间进程，P1（运行在 CPU 0 上）和 P2（运行在 CPU 2 上），可以同时打开设备文件并同时发出`read(2)`系统调用。现在，两个进程将同时执行驱动程序的读取“方法”，因此同时处理共享的可写数据！这（在`t2`和`t3`之间的代码）是一个关键部分，由于我们违反了基本的排他性规则 - 关键部分必须由单个线程在任何时间点执行 - 我们很可能会破坏数据、应用程序，甚至更糟。

换句话说，这现在是一个**数据竞争**；取决于微妙的时间巧合，我们可能会或可能不会产生错误（bug）。这种不确定性 - 微妙的时间巧合 - 正是使得像这样找到和修复错误变得极其困难的原因（它可能逃脱了您的测试努力）。

这句格言太不幸地是真的：*测试可以检测到错误的存在，但不能检测到它们的缺失。*更糟糕的是，如果您的测试未能捕捉到竞争（和错误），那么它们将在现场自由发挥。

您可能会觉得，由于您的产品是运行在单个 CPU 核心（UP）上的小型嵌入式系统，因此关于控制并发性（通常通过锁定）的讨论对您不适用。我们不这么认为：几乎所有现代产品，如果尚未，都将转向多核（也许是在它们的下一代阶段）。更重要的是，即使是 UP 系统也存在并发性问题，我们将在接下来的部分中探讨。

## 可抢占内核，阻塞 I/O 和数据竞争

想象一下，您正在运行配置为可抢占的 Linux 内核的内核模块或驱动程序（即`CONFIG_PREEMPT`已打开；我们在配套指南*Linux 内核编程*的*第十章* *CPU 调度器-第一部分*中涵盖了这个主题）。考虑一个进程 P1，在进程上下文中运行驱动程序的读取方法代码，正在处理全局数组。现在，在关键部分内（在时间`t2`和`t3`之间），如果内核*抢占*了进程 P1 并上下文切换到另一个进程 P2，后者正好在等待执行这个代码路径？这是危险的，同样是数据竞争。这甚至可能发生在 UP 系统上！

另一个有些类似的情景（同样，可能发生在单核（UP）或多核系统上）：进程 P1 正在通过驱动程序方法的关键部分运行（在时间`t2`和`t3`之间；再次参见*图 6.5*）。这一次，如果在关键部分中遇到了阻塞调用呢？

**阻塞调用**是一个导致调用进程上下文进入休眠状态，等待事件发生的函数；当事件发生时，内核将“唤醒”任务，并从上次中断的地方恢复执行。这也被称为 I/O 阻塞，非常常见；许多 API（包括几个用户空间库和系统调用，以及几个内核 API）天生就是阻塞的。在这种情况下，进程 P1 实际上是从 CPU 上上下文切换并进入休眠状态，这意味着`schedule()`的代码运行并将其排队到等待队列。

在 P1 被切换回来之前，如果另一个进程 P2 被调度运行怎么办？如果该进程也在运行这个特定的代码路径怎么办？想一想-当 P1 回来时，共享数据可能已经在“它下面”发生了变化，导致各种错误；再次，数据竞争，一个错误！

## 硬件中断和数据竞争

最后，设想这种情况：进程 P1 再次无辜地运行驱动程序的读取方法代码；它进入了临界区（在时间`t2`和`t3`之间；再次参见*图 6.5*）。它取得了一些进展，但然后，哎呀，硬件中断触发了（在同一个 CPU 上）！在 Linux 操作系统上，硬件（外围）中断具有最高优先级；它们默认情况下会抢占任何代码（包括内核代码）。因此，进程（或线程）P1 将至少暂时被搁置，从而失去处理器；中断处理代码将抢占它并运行。

你可能会想，那又怎样呢？确实，这是一个非常普遍的情况！在现代系统上，硬件中断非常频繁地触发，有效地（字面上）中断了各种任务上下文（在你的 shell 上快速执行`vmstat 3`；`system`标签下的列显示了你的系统在过去 1 秒内触发的硬件中断的数量！）。要问的关键问题是：中断处理代码（无论是硬中断的顶半部分还是所谓的任务 let 或软中断的底半部分，无论哪个发生了），*是否共享并处理了它刚刚中断的进程上下文的相同共享可写数据？*

如果这是真的，那么，*休斯顿，我们有一个问题*-数据竞争！如果不是，那么你中断的代码对于中断代码路径来说不是一个临界区，那就没问题。事实上，大多数设备驱动程序确实处理中断；因此，驱动程序作者（你！）有责任确保没有全局或静态数据-实际上，没有临界区-在进程上下文和中断代码路径之间共享。如果有（这确实会发生），你必须以某种方式保护这些数据，以防数据竞争和可能的损坏。

这些情景可能会让你觉得，在面对这些并发问题时保护数据安全是一个非常艰巨的任务；你究竟如何在存在临界区的情况下确保数据安全，以及各种可能的并发问题？有趣的是，实际的 API 并不难学习使用；我们再次强调**识别临界区**是关键。

关于锁（概念上）的工作原理，锁定指南（非常重要；我们很快会对它们进行总结），以及死锁的类型和如何预防死锁，都在我早期的书籍《Linux 系统编程实践（Packt，2018 年 10 月）》中有所涉及。这本书在第十五章“使用 Pthreads 进行多线程编程第二部分-同步”中详细介绍了这些要点。

话不多说，让我们深入探讨主要的同步技术，以保护我们的临界区-锁定。

## 锁定指南和死锁

锁定本质上是一个复杂的问题；它往往会引发复杂的交叉锁定场景。不充分理解它可能会导致性能问题和错误-死锁、循环依赖、中断不安全的锁定等。以下锁定指南对确保使用锁定时编写正确的代码至关重要：

+   **锁定粒度**：锁定和解锁之间的“距离”（实际上是临界区的长度）不应该是粗粒度的（临界区太长），它应该是“足够细”; 这是什么意思？下面的要点解释了这一点：

+   在这里你需要小心。在处理大型项目时，保持过少的锁是一个问题，保持过多的锁也是一个问题！过少的锁可能会导致性能问题（因为相同的锁被重复使用，因此很容易受到高度争用）。

+   拥有大量锁实际上对性能有好处，但对复杂性控制不利。这也导致另一个关键点的理解：在代码库中有许多锁时，您应该非常清楚哪个锁保护哪个共享数据对象。如果您在代码路径中使用，例如`lockA`来保护`mystructX`，但在远处的代码路径（也许是中断处理程序）中忘记了这一点，并尝试在相同的结构上使用其他锁，`lockB`来保护！现在这些事情可能听起来很明显，但（有经验的开发人员知道），在足够的压力下，即使明显的事情也不总是明显的！

+   尝试平衡事物。在大型项目中，使用一个锁来保护一个全局（共享）数据结构是典型的。(*命名*好锁变量本身可能成为一个大问题！这就是为什么我们将保护数据结构的锁放在其中作为成员。)

+   **锁定顺序**至关重要；**锁必须以相同的顺序获取**，并且其顺序应该由所有参与项目开发的开发人员记录和遵循（注释锁也很有用；在下一章节关于*lockdep*的部分中会更多介绍）。不正确的锁定顺序经常导致死锁。

+   尽量避免递归锁定。

+   注意防止饥饿；验证一旦获取锁，确实会“足够快”释放。

+   **简单是关键**：尽量避免复杂性或过度设计，特别是涉及锁的复杂情况。

在锁定的话题上，（危险的）死锁问题出现了。**死锁**是无法取得任何进展；换句话说，应用程序和/或内核组件似乎无限期地挂起。虽然我们不打算在这里深入研究死锁的可怕细节，但我会快速提到一些可能发生的常见死锁情况类型：

+   简单情况，单个锁，进程上下文：

+   我们尝试两次获取相同的锁；这会导致**自死锁**。

+   简单情况，多个（两个或更多）锁，进程上下文 - 一个例子：

+   在 CPU `0`上，线程 A 获取锁 A，然后想要获取锁 B。

+   同时，在 CPU `1`上，线程 B 获取锁 B，然后想要获取锁 A。

+   结果是死锁，通常称为**AB-BA** **死锁**。

+   它可以被扩展；例如，AB-BC-CA **循环依赖**（A-B-C 锁链）会导致死锁。

+   复杂情况，单个锁，进程和中断上下文：

+   锁 A 在中断上下文中获取。

+   如果发生中断（在另一个核心上），并且处理程序试图获取锁 A，会发生死锁！因此，在中断上下文中获取的锁必须始终与中断禁用一起使用。（如何？当我们涵盖自旋锁时，我们将更详细地讨论这个问题。）

+   更复杂的情况，多个锁，进程和中断（硬中断和软中断）上下文

在更简单的情况下，始终遵循*锁定顺序指南*就足够了：始终以有记录的顺序获取和释放锁（我们将在内核代码中的*使用互斥锁*部分提供一个示例）。然而，这可能变得非常复杂；复杂的死锁情况甚至会让经验丰富的开发人员感到困惑。幸运的是，***lockdep*** - Linux 内核的运行时锁依赖验证器 - 可以捕捉每一个死锁情况！（不用担心 - 我们会到那里的：我们将在下一章节详细介绍 lockdep）。当我们涵盖自旋锁（*使用自旋锁*部分）时，我们将遇到类似于先前提到的进程和/或中断上下文情况；在那里明确了要使用的自旋锁类型。

关于死锁，Steve Rostedt 在 2011 年的 Linux Plumber's Conference 上对 lockdep 进行了非常详细的介绍；相关幻灯片内容丰富，探讨了简单和复杂的死锁场景，以及 lockdep 如何检测它们（[`blog.linuxplumbersconf.org/2011/ocw/sessions/153`](https://blog.linuxplumbersconf.org/2011/ocw/sessions/153)）。

另外，现实情况是，不仅是死锁，甚至**活锁**情况也可能同样致命！活锁本质上是一种类似于死锁的情况；只是参与任务的状态是运行而不是等待。例如，中断“风暴”可能导致活锁；现代网络驱动程序通过关闭中断（在中断负载下）并采用一种称为**新 API；切换中断**（**NAPI**）的轮询技术来减轻这种效应（在适当时重新打开中断；好吧，实际情况比这更复杂，但我们就到此为止）。

对于那些生活在石头下的人，你会知道 Linux 内核有两种主要类型的锁：互斥锁和自旋锁。实际上，还有几种类型，包括其他同步（和“无锁”编程）技术，所有这些都将在本章和下一章中涵盖。

# 互斥锁还是自旋锁？在何时使用

学习使用互斥锁和自旋锁的确切语义非常简单（在内核 API 集中有适当的抽象，使得对于典型的驱动程序开发人员或模块作者来说更容易）。在这种情况下的关键问题是一个概念性的问题：两种锁之间的真正区别是什么？更重要的是，在什么情况下应该使用哪种锁？你将在本节中找到这些问题的答案。

以前的驱动程序读取方法的伪代码（*图 6.5*）作为基本示例，假设三个线程 - **tA**，**tB**和**tC** - 在并行运行（在 SMP 系统上）通过这段代码。我们将通过在关键部分开始之前获取锁或获取锁来解决这个并发问题，同时避免任何数据竞争，并在关键部分代码路径结束后释放锁（解锁）（时间**t3**）。让我们再次看一下伪代码，这次使用锁定以确保它是正确的：

![](img/a0db53d6-0c64-4377-90a2-bdb95a2fab16.png)

图 6.6 - 伪代码 - 驱动程序读取方法中的关键部分；正确，带锁

当三个线程尝试同时获取锁时，系统保证只有一个线程会获得它。假设**tB**（线程 B）获得了锁：现在它是“获胜者”或“所有者”线程。这意味着线程**tA**和**tC**是“失败者”；他们会等待解锁！一旦“获胜者”（**tB**）完成关键部分并解锁锁，之前的失败者之间的战斗就会重新开始；其中一个将成为下一个获胜者，进程重复。

两种锁类型之间的关键区别 - 互斥锁和自旋锁 - 基于失败者等待解锁的方式。使用互斥锁，失败者线程会进入睡眠；也就是说，它们通过睡眠等待。一旦获胜者执行解锁，内核就会唤醒失败者（所有失败者）并重新运行，再次竞争锁。（事实上，互斥锁和信号量有时被称为睡眠锁。）

然而，使用**自旋锁**，没有睡眠的问题；失败者会在锁上自旋等待，直到它被解锁。从概念上看，情况如下：

```
while (locked) ;
```

请注意，这仅仅是*概念性的*。想一想——这实际上是轮询。然而，作为一个优秀的程序员，你会明白，轮询通常被认为是一个不好的主意。那么，自旋锁为什么会这样工作呢？嗯，它并不是这样的；它只是以这种方式呈现出来是为了概念上的目的。正如你很快会明白的，自旋锁只在多核（SMP）系统上才有意义。在这样的系统上，当获胜的线程离开并运行关键部分的代码时，失败者会在其他 CPU 核上旋转等待！实际上，在实现层面，用于实现现代自旋锁的代码是高度优化的（并且特定于体系结构），并不是通过简单地“自旋”来工作（例如，许多 ARM 的自旋锁实现使用**等待事件**（**WFE**）机器语言指令，这使得 CPU 在低功耗状态下等待；请参阅*进一步阅读*部分，了解有关自旋锁内部实现的几个资源）。

## 在理论上确定使用哪种锁

自旋锁的实现方式实际上并不是我们关心的重点；自旋锁的开销比互斥锁更低对我们来说是有兴趣的。为什么呢？实际上很简单：为了使互斥锁工作，失败者线程必须休眠。为了做到这一点，内部调用了`schedule()`函数，这意味着失败者将互斥锁 API 视为一个阻塞调用！对调度程序的调用最终将导致处理器被上下文切换。相反，当所有者线程解锁锁时，失败者线程必须被唤醒；同样，它将被上下文切换回处理器。因此，互斥锁/解锁操作的最小“成本”是在给定机器上执行两次上下文切换所需的时间。（请参阅下一节中的*信息框*。）通过再次查看前面的屏幕截图，我们可以确定一些事情，包括在关键部分中花费的时间（“锁定”代码路径）；即，`t_locked = t3 - t2`。

假设`t_ctxsw`代表上下文切换的时间。正如我们所了解的，互斥锁/解锁操作的最小成本是`2 * t_ctxsw`。现在，假设以下表达式为真：

```
t_locked < 2 * t_ctxsw
```

换句话说，如果在关键部分内花费的时间少于两次上下文切换所需的时间，那么使用互斥锁就是错误的，因为这会带来太多的开销；执行元工作的时间比实际工作的时间更多——这种现象被称为**抖动**。这种精确的用例——非常短的关键部分的存在——在现代操作系统（如 Linux）中经常出现。因此，总的来说，对于短的非阻塞关键部分，使用自旋锁（远远）优于使用互斥锁。

## 在实践中确定使用哪种锁

因此，在“`t_locked < 2 * t_ctxsw`”的“规则”下运行在理论上可能很好，但是等等：你真的期望精确地测量每种情况下关键部分的上下文切换时间和花费的时间吗？当然不是——那是相当不现实和迂腐的。

从实际角度来看，可以这样理解：互斥锁通过在解锁时使失败者线程休眠来工作；自旋锁不会（失败者“自旋”）。让我们回顾一下 Linux 内核的一个黄金规则：内核不能在任何类型的原子上下文中休眠（调用`schedule()`）。因此，我们永远不能在中断上下文中使用互斥锁，或者在任何不安全休眠的上下文中使用；然而，使用自旋锁是可以的。让我们总结一下：

+   **关键部分是在原子（中断）上下文中运行，还是在进程上下文中运行，无法休眠？** 使用自旋锁。

+   **关键部分是在进程上下文中运行，且在关键部分中需要休眠？** 使用互斥锁。

当然，使用自旋锁的开销比使用互斥锁的开销要低；因此，您甚至可以在进程上下文中使用自旋锁（例如我们虚构的驱动程序的读取方法），只要关键部分不会阻塞（休眠）。

**[1]** 上下文切换所需的时间是不同的；这在很大程度上取决于硬件和操作系统的质量。最近（2018 年 9 月）的测量结果显示，在固定的 CPU 上，上下文切换时间在 1.2 到 1.5**us**（**微秒**）左右，在没有固定的情况下大约为 2.2 微秒（[`eli.thegreenplace.net/2018/measuring-context-switching-and-memory-overheads-for-linux-threads/`](https://eli.thegreenplace.net/2018/measuring-context-switching-and-memory-overheads-for-linux-threads/)）。

硬件和 Linux 操作系统都有了巨大的改进，因此平均上下文切换时间也有所改善。一篇旧的（1998 年 12 月）Linux Journal 文章确定，在 x86 类系统上，平均上下文切换时间为 19 微秒（微秒），最坏情况下为 30 微秒。

这带来了一个问题，我们如何知道代码当前是在进程上下文还是中断上下文中运行？很简单：我们的`PRINT_CTX()`宏（在我们的`convenient.h`头文件中）可以显示这一点：

```
if (in_task())
    /* we're in process context (usually safe to sleep / block) */
else
    /* we're in an atomic or interrupt context (cannot sleep / block) */
```

现在您了解了何时使用互斥锁或自旋锁，让我们进入实际用法。我们将从如何使用互斥锁开始！

# 使用互斥锁

如果关键部分可以休眠（阻塞），则互斥锁也称为可休眠或阻塞互斥排他锁。它们必须不在任何类型的原子或中断上下文（顶半部，底半部，如 tasklets 或 softirqs 等），内核定时器，甚至不允许阻塞的进程上下文中使用。

## 初始化互斥锁

互斥锁“对象”在内核中表示为`struct mutex`数据结构。考虑以下代码：

```
#include <linux/mutex.h>
struct mutex mymtx;
```

要使用互斥锁，*必须*将其显式初始化为未锁定状态。可以使用`DEFINE_MUTEX()`宏静态地（声明并初始化对象）进行初始化，也可以通过`mutex_init()`函数动态进行初始化（这实际上是对`__mutex_init()`函数的宏包装）。

例如，要声明并初始化名为`mymtx`的互斥锁对象，我们可以使用`DEFINE_MUTEX(mymtx);`。

我们也可以动态地执行此操作。为什么要动态执行？通常，互斥锁是它所保护的（全局）数据结构的成员（聪明！）。例如，假设我们在驱动程序代码中有以下全局上下文结构（请注意，此代码是虚构的）：

```
struct mydrv_priv {
    <member 1>
    <member 2>
    [...]
    struct mutex mymtx; /* protects access to mydrv_priv */
    [...]
};
```

然后，在您的驱动程序（或 LKM）的`init`方法中，执行以下操作：

```
static int init_mydrv(struct mydrv_priv *drvctx)
{
    [...]
    mutex_init(drvctx-mymtx);
    [...]
}
```

将锁变量作为（父）数据结构的成员保护是 Linux 中常用的（聪明）模式；这种方法还有一个额外的好处，即避免命名空间污染，并且清楚地说明哪个互斥锁保护哪个共享数据项（这可能是一个比起初看起来更大的问题，尤其是在像 Linux 内核这样的庞大项目中！）。

将保护全局或共享数据结构的锁作为该数据结构的成员。

## 正确使用互斥锁

通常，您可以在内核源树中找到非常有见地的注释。这里有一个很好的总结了您必须遵循的规则以正确使用互斥锁的注释；请仔细阅读：

```
// include/linux/mutex.h
/*
 * Simple, straightforward mutexes with strict semantics:
 *
 * - only one task can hold the mutex at a time
 * - only the owner can unlock the mutex
 * - multiple unlocks are not permitted
 * - recursive locking is not permitted
 * - a mutex object must be initialized via the API
 * - a mutex object must not be initialized via memset or copying
 * - task may not exit with mutex held
 * - memory areas where held locks reside must not be freed
 * - held mutexes must not be reinitialized
 * - mutexes may not be used in hardware or software interrupt
 * contexts such as tasklets and timers
 *
 * These semantics are fully enforced when DEBUG_MUTEXES is
 * enabled. Furthermore, besides enforcing the above rules, the mutex
 * [ ... ]
```

作为内核开发人员，您必须了解以下内容：

+   关键部分导致代码路径*被串行化，破坏了并行性*。因此，至关重要的是尽量保持关键部分的时间尽可能短。与此相关的是**锁定数据，而不是代码**。

+   尝试重新获取已经获取（锁定）的互斥锁 - 这实际上是递归锁定 - 是*不*支持的，并且会导致自死锁。

+   **锁定顺序**：这是防止危险死锁情况的一个非常重要的经验法则。在存在多个线程和多个锁的情况下，关键的是*记录锁被获取的顺序，并且所有参与项目开发的开发人员都严格遵循*。实际的锁定顺序本身并不是不可侵犯的，但一旦决定了，就必须遵循。在浏览内核源代码时，您会发现许多地方，内核开发人员确保这样做，并且（通常）为其他开发人员编写注释以便查看和遵循。这是来自 slab 分配器代码（`mm/slub.c`）的一个示例注释：

```
/*
 * Lock order:
 * 1\. slab_mutex (Global Mutex)
 * 2\. node-list_lock
 * 3\. slab_lock(page) (Only on some arches and for debugging)
```

现在我们从概念上理解了互斥锁的工作原理（并且了解了它们的初始化），让我们学习如何使用锁定/解锁 API。

## 互斥锁定和解锁 API 及其用法

互斥锁的实际锁定和解锁 API 如下。以下代码分别显示了如何锁定和解锁互斥锁：

```
void __sched mutex_lock(struct mutex *lock);
void __sched mutex_unlock(struct mutex *lock);
```

（这里忽略`__sched`；这只是一个编译器属性，使得这个函数在`WCHAN`输出中消失，在 procfs 中显示，并且在`ps(1)`的某些选项开关（如`-l`）中显示）。

同样，在`kernel/locking/mutex.c`中的源代码中的注释非常详细和描述性；我鼓励您更详细地查看这个文件。我们在这里只显示了其中的一些代码，这些代码直接来自 5.4 Linux 内核源代码树：

```
// kernel/locking/mutex.c
[ ... ]
/**
 * mutex_lock - acquire the mutex
 * @lock: the mutex to be acquired
 *
 * Lock the mutex exclusively for this task. If the mutex is not
 * available right now, it will sleep until it can get it.
 *
 * The mutex must later on be released by the same task that
 * acquired it. Recursive locking is not allowed. The task
 * may not exit without first unlocking the mutex. Also, kernel
 * memory where the mutex resides must not be freed with
 * the mutex still locked. The mutex must first be initialized
 * (or statically defined) before it can be locked. memset()-ing
 * the mutex to 0 is not allowed.
 *
 * (The CONFIG_DEBUG_MUTEXES .config option turns on debugging
 * checks that will enforce the restrictions and will also do
 * deadlock debugging)
 *
 * This function is similar to (but not equivalent to) down().
 */
void __sched mutex_lock(struct mutex *lock)
{
    might_sleep();

    if (!__mutex_trylock_fast(lock))
        __mutex_lock_slowpath(lock);
}
EXPORT_SYMBOL(mutex_lock);
```

`might_sleep()`是一个具有有趣调试属性的宏；它捕捉到了本应在原子上下文中执行但实际上没有执行的代码！所以，请思考一下：`might_sleep()`是`mutex_lock()`中的第一行代码，这意味着这段代码路径不应该被任何处于原子上下文中的东西执行，因为它可能会睡眠。这意味着只有在安全睡眠时才应该在进程上下文中使用互斥锁！

**一个快速而重要的提醒**：Linux 内核可以配置大量的调试选项；在这种情况下，`CONFIG_DEBUG_MUTEXES`配置选项将帮助您捕捉可能的与互斥锁相关的错误，包括死锁。同样，在 Kernel Hacking 菜单下，您将找到大量与调试相关的内核配置选项。我们在配套指南*Linux Kernel Programming - Chapter 5*，*Writing Your First Kernel Module – LKMs Part 2*中讨论了这一点。关于锁调试，有几个非常有用的内核配置，我们将在下一章中介绍，在*内核中的锁调试*部分。

### 互斥锁 - 通过[不]可中断的睡眠？

和往常一样，互斥锁比我们迄今所见到的更复杂。您已经知道 Linux 进程（或线程）在状态机的各种状态之间循环。在 Linux 上，睡眠有两种离散状态 - 可中断睡眠和不可中断睡眠。处于可中断睡眠状态的进程（或线程）是敏感的，这意味着它将响应用户空间信号，而处于不可中断睡眠状态的任务对用户信号不敏感。

在具有底层驱动程序的人机交互应用程序中，通常的经验法则是，您应该将一个进程放入可中断的睡眠状态（当它在锁上阻塞时），这样就由最终用户决定是否通过按下*Ctrl* + *C*（或某种涉及信号的机制）来中止应用程序。在类 Unix 系统上通常遵循的设计规则是：**提供机制，而不是策略**。话虽如此，在非交互式代码路径上，通常情况下，您必须等待锁来无限期地等待，语义上，已传递给任务的信号不应中止阻塞等待。在 Linux 上，不可中断的情况是最常见的情况。

因此，这里的问题是：`mutex_lock()` API 总是将调用任务置于不可中断的睡眠状态。如果这不是你想要的，使用`mutex_lock_interruptible()` API 将调用任务置于可中断的睡眠状态。在语法上有一个不同之处；后者在成功时返回整数值`0`，在失败时返回`-EINTR`（记住`0`/`-E`返回约定）（由于信号中断）。

一般来说，使用`mutex_lock()`比使用`mutex_lock_interruptible()`更快；当临界区很短时使用它（因此几乎可以保证锁定时间很短，这是一个非常理想的特性）。

5.4.0 内核包含超过 18,500 个`mutex_lock()`和 800 多个`mutex_lock_interruptible()` API 的调用实例；你可以通过内核源树上强大的`cscope(1)`实用程序来检查这一点。

理论上，内核提供了`mutex_destroy()` API。这是`mutex_init()`的相反操作；它的工作是将互斥锁标记为不可用。只有在互斥锁处于未锁定状态时才能调用它，一旦调用，互斥锁就不能再使用。这有点理论性，因为在常规系统上，它只是一个空函数；只有在启用了`CONFIG_DEBUG_MUTEXES`的内核上，它才变成实际的（简单的）代码。因此，当使用互斥锁时，我们应该使用这种模式，如下面的伪代码所示：

```
DEFINE_MUTEX(...);        // init: initialize the mutex object
/* or */ mutex_init();
[ ... ]
    /* critical section: perform the (mutex) locking, unlocking */
    mutex_lock[_interruptible]();
    << ... critical section ... >>
    mutex_unlock();
    mutex_destroy();      // cleanup: destroy the mutex object
```

现在你已经学会了如何使用互斥锁 API，让我们把这些知识付诸实践。在下一节中，我们将在之前的一个（编写不好 - 没有保护！）“misc”驱动程序的基础上，通过使用互斥对象来锁定必要的临界区来构建。

## 互斥锁定 - 一个示例驱动程序

我们在*第一章* - *编写一个简单的 misc 字符设备驱动程序*中创建了一个简单的设备驱动程序示例，即`ch1/miscdrv_rdwr`。在那里，我们编写了一个简单的`misc`类字符设备驱动程序，并使用了一个用户空间实用程序（`ch12/miscdrv_rdwr/rdwr_drv_secret.c`）来从设备驱动程序的内存中读取和写入一个（所谓的）秘密。

然而，在那段代码中，我们明显（egregiously 是正确的词！）未能保护共享（全局）可写数据！这在现实世界中会让我们付出昂贵的代价。我敦促你花些时间考虑一下：两个（或三个或更多）用户模式进程打开该驱动程序的设备文件，然后同时发出各种 I/O 读写是不可行的。在这里，全局共享可写数据（在这种特殊情况下，两个全局整数和驱动程序上下文数据结构）很容易被破坏。

因此，让我们从错误中吸取教训，并通过复制这个驱动程序（我们现在将其称为`ch12/1_miscdrv_rdwr_mutexlock/1_miscdrv_rdwr_mutexlock.c`）并重写其中的一些部分来纠正错误。关键点是我们必须使用互斥锁来保护所有关键部分。而不是在这里显示代码（毕竟，它在这本书的 GitHub 存储库中[`github.com/PacktPublishing/Linux-Kernel-Programming`](https://github.com/PacktPublishing/Linux-Kernel-Programming)，请使用`git clone`！），让我们做一些有趣的事情：让我们看一下旧的未受保护版本和新的受保护代码版本之间的“diff”（`diff(1)`生成的差异 - ）的输出在这里已经被截断：

```
$ pwd
<.../ch12/1_miscdrv_rdwr_mutexlock
$ diff -u ../../ch12/miscdrv_rdwr/miscdrv_rdwr.c miscdrv_rdwr_mutexlock.c>> miscdrv_rdwr.patch
$ cat miscdrv_rdwr.patch
[ ... ]
+#include <linux/mutex.h> // mutex lock, unlock, etc
 #include "../../convenient.h"
[ ... ] 
-#define OURMODNAME "miscdrv_rdwr"
+#define OURMODNAME "miscdrv_rdwr_mutexlock"

+DEFINE_MUTEX(lock1); // this mutex lock is meant to protect the integers ga and gb
[ ... ]
+     struct mutex lock; // this mutex protects this data structure
 };
[ ... ]
```

在这里，我们可以看到在驱动程序的更新的安全版本中，我们声明并初始化了一个名为`lock1`的互斥变量；我们将用它来保护（仅用于演示目的）驱动程序中的两个全局整数`ga`和`gb`。接下来，重要的是，在“驱动程序上下文”数据结构`drv_ctx`中声明了一个名为`lock`的互斥锁；这将用于保护对该数据结构成员的任何访问。它在`init`代码中初始化：

```
+     mutex_init(&ctx->lock);
+
+     /* Initialize the "secret" value :-) */
      strscpy(ctx->oursecret, "initmsg", 8);
-     dev_dbg(ctx->dev, "A sample print via the dev_dbg(): driver initialized\n");
+     /* Why don't we protect the above strscpy() with the mutex lock?
+      * It's working on shared writable data, yes?
+      * Yes, BUT this is the init code; it's guaranteed to run in exactly
+      * one context (typically the insmod(8) process), thus there is
+      * no concurrency possible here. The same goes for the cleanup
+      * code path.
+      */
```

这个详细的注释清楚地解释了为什么我们不需要在`strscpy()`周围进行锁定/解锁。再次强调，这应该是显而易见的，但是局部变量隐式地对每个进程上下文都是私有的（因为它们驻留在该进程或线程的内核模式堆栈中），因此不需要保护（每个线程/进程都有一个变量的单独*实例*，所以没有人会干涉别人的工作！）。在我们忘记之前，*清理*代码路径（通过`rmmod(8)`进程上下文调用）必须销毁互斥锁：

```
-static void __exit miscdrv_rdwr_exit(void)
+static void __exit miscdrv_exit_mutexlock(void)
 {
+     mutex_destroy(&lock1);
+     mutex_destroy(&ctx->lock);
      misc_deregister(&llkd_miscdev);
 }
```

现在，让我们看一下驱动程序的打开方法的差异：

```
+
+     mutex_lock(&lock1);
+     ga++; gb--;
+     mutex_unlock(&lock1);
+
+     dev_info(dev, " filename: \"%s\"\n"
      [ ... ]
```

这是我们操纵全局整数的地方，*使其成为关键部分*；与程序的先前版本不同，在这里，我们使用`lock1`互斥锁*保护这个关键部分*。所以，关键部分就是这里的代码`ga++; gb--;`：在（互斥）锁定和解锁操作之间的代码。

但是（总是有一个但是，不是吗？），一切并不顺利！看一下`mutex_unlock()`代码行后面的`printk`函数（`dev_info()`）：

```
+ dev_info(dev, " filename: \"%s\"\n"
+         " wrt open file: f_flags = 0x%x\n"
+         " ga = %d, gb = %d\n",
+         filp->f_path.dentry->d_iname, filp->f_flags, ga, gb);
```

这对你来说看起来还好吗？不，仔细看：我们正在*读取*全局整数`ga`和`gb`的值。回想一下基本原理：在并发存在的情况下（在这个驱动程序的*打开*方法中肯定是可能的），*即使没有锁定，读取共享可写数据也可能是不安全的*。如果这对你来说没有意义，请想一想：如果一个线程正在读取整数，同时另一个线程正在更新（写入）它们；那么呢？这种情况被称为**脏读**（或**断裂读**）；我们可能会读取过时的数据，必须加以保护。（事实上，这并不是一个真正的脏读的很好的例子，因为在大多数处理器上，读取和写入单个整数项目确实 tend to be an atomic operation。然而，我们不应该假设这样的事情 - 我们只需要做好我们的工作并保护它。）

实际上，还有另一个类似的潜在错误：我们从打开文件结构（`filp`指针）中读取数据而没有进行保护（的确，打开文件结构有一个锁；我们应该使用它！我们以后会这样做）。

诸如*脏读*之类的事情发生的具体语义通常非常依赖于体系结构（机器），然而，我们作为模块或驱动程序的作者的工作是清楚的：我们必须确保保护所有关键部分。这包括对共享可写数据的读取。

目前，我们将把这些标记为潜在的错误（bug）。我们将在*使用原子整数操作符*部分以更加性能友好的方式处理这个问题。查看驱动程序的读取方法的差异会发现一些有趣的东西（忽略这里显示的行号；它们可能会改变）：

![](img/ad26b085-7d4a-4090-96b8-44aef98664ce.png)

图 6.7 - 驱动程序的 read()方法的差异；查看新版本中互斥锁的使用

我们现在使用驱动程序上下文结构的互斥锁来保护关键部分。对于设备驱动程序的*写*和*关闭*（释放）方法也是一样的（生成补丁并查看）。

请注意用户模式应用程序保持不变，这意味着为了测试新的更安全的版本，我们必须继续使用用户模式应用程序`ch12/miscdrv_rdwr/rdwr_drv_secret.c`。在调试内核上运行和测试此驱动程序代码，其中包含各种锁定错误和死锁检测功能，这是至关重要的（我们将在下一章中返回到这些“调试”功能，在*内核中的锁调试*部分）。

在前面的代码中，我们在`copy_to_user()`例程之前获取了互斥锁；这很好。然而，我们只在`dev_info()`之后释放它。为什么不在这个`printk`之前释放它，从而缩短关键部分的时间？

仔细观察`dev_info()`，可以看出为什么它*在*关键部分。我们在这里打印了三个变量的值：`secret_len`读取的字节数，以及`ctx->tx`和`ctx->rx`分别“传输”和“接收”的字节数。`secret_len`是一个局部变量，不需要保护，但另外两个变量在全局驱动程序上下文结构中，因此需要保护，即使是（可能是脏的）读取也需要。

## 互斥锁 - 一些剩余的要点

在本节中，我们将涵盖有关互斥锁的一些其他要点。

### 互斥锁 API 变体

首先，让我们看一下互斥锁 API 的几个变体；除了可中断变体（在*互斥锁 - 通过[不]可中断睡眠？*部分中描述），我们还有*trylock，可杀死*和*io*变体。

#### 互斥 trylock 变体

如果你想实现一个**忙等待**语义；也就是说，测试（互斥）锁的可用性，如果可用（意味着当前未锁定），则获取/锁定它并继续关键部分代码路径？如果不可用（当前处于锁定状态），则不等待锁；而是执行其他工作并重试。实际上，这是一个非阻塞的互斥锁变体，称为 trylock；以下流程图显示了它的工作原理：

![](img/421daaad-97a1-4acc-8cfc-e4d33751eb84.png)

图 6.8 - “忙等待”语义，一个非阻塞的 trylock 操作

这个互斥锁的 trylock 变体的 API 如下：

```
int mutex_trylock(struct mutex *lock);
```

这个 API 的返回值表示了运行时发生了什么：

+   返回值`1`表示成功获取了锁。

+   返回值`0`表示当前争用（已锁定）。

尽管尝试使用`mutex_trylock()` API 来确定互斥锁是处于锁定还是未锁定状态可能听起来很诱人，但*不要*尝试这样做，因为这本质上是“竞争的”。另外，要注意，在高度竞争的锁路径中使用这个 trylock 变体可能会降低你获取锁的机会。trylock 变体传统上用于死锁预防代码，可能需要退出某个锁定顺序序列并通过另一个序列（顺序）重试。

另外，关于 trylock 变体，尽管文献中使用了术语*尝试原子地获取互斥锁*，但它不适用于原子或中断上下文——它*只*适用于进程上下文（与任何类型的互斥锁一样）。通常情况下，锁必须由拥有者上下文调用的`mutex_unlock()`来释放。

我建议你尝试作为练习使用 trylock 互斥锁变体。请参阅本章末尾的*问题*部分进行作业！

#### 互斥可中断和可杀死变体

正如你已经学到的，当驱动程序（或模块）愿意接受任何（用户空间）信号中断时，会使用`mutex_lock_interruptible()` API（并返回`-ERESTARTSYS`告诉内核 VFS 层执行信号处理；用户空间系统调用将以`errno`设置为`EINTR`失败）。一个例子可以在内核中的模块处理代码中找到，在`delete_module(2)`系统调用中（由`rmmod(8)`调用）：

```
// kernel/module.c
[ ... ]
SYSCALL_DEFINE2(delete_module, const char __user *, name_user,
        unsigned int, flags)
{
    struct module *mod;
    [ ... ]
    if (!capable(CAP_SYS_MODULE) || modules_disabled)
        return -EPERM;
    [ ... ]
    if (mutex_lock_interruptible(&module_mutex) != 0)
 return -EINTR;
    mod = find_module(name);
    [ ... ]
out:
    mutex_unlock(&module_mutex);
    return ret;
}
```

注意 API 在失败时返回`-EINTR`。（`SYSCALL_DEFINEn()`宏成为系统调用签名；`n`表示这个特定系统调用接受的参数数量。还要注意权限检查——除非你以 root 身份运行或具有`CAP_SYS_MODULE`权限（或者模块加载完全被禁用），否则系统调用将返回失败（`-EPERM`）。）

然而，如果你的驱动程序只愿意被致命信号（那些*将杀死*用户空间上下文的信号）中断，那么使用`mutex_lock_killable()` API（签名与可中断变体相同）。

#### 互斥 io 变体

`mutex_lock_io()` API 在语法上与`mutex_lock()` API 相同；唯一的区别是内核认为失败线程的等待时间与等待 I/O 相同（`kernel/locking/mutex.c:mutex_lock_io()`中的代码注释清楚地记录了这一点；看一下）。这在会计方面很重要。

您可以在内核中找到相当奇特的 API，比如`mutex_lock[_interruptible]_nested()`，这里重点是`nested`后缀。但是，请注意，Linux 内核不希望开发人员使用嵌套（或递归）锁定（正如我们在*正确使用互斥锁*一节中提到的）。此外，这些 API 只在存在`CONFIG_DEBUG_LOCK_ALLOC`配置选项时才会被编译；实际上，嵌套 API 是为了支持内核锁验证器机制而添加的。它们只应在特殊情况下使用（在同一类型的锁实例之间必须包含嵌套级别的情况下）。

在下一节中，我们将回答一个典型的常见问题：互斥锁和信号量对象有什么区别？Linux 是否有信号量对象？继续阅读以了解更多！

### 信号量和互斥锁

Linux 内核确实提供了一个信号量对象，以及您可以对（二进制）信号量执行的常规操作：

+   通过`down[_interruptible]()`（和变体）API 获取信号量锁

+   通过`up()` API 解锁信号量。

一般来说，信号量是一种较旧的实现，因此建议您使用互斥锁来代替它。

值得一看的常见问题是：*互斥锁和信号量之间有什么区别？*它们在概念上看起来相似，但实际上是非常不同的。

+   信号量是互斥锁的一种更一般化的形式；互斥锁可以被获取（然后释放或解锁）一次，而信号量可以被获取（然后释放）多次。

+   互斥锁用于保护临界区免受同时访问，而信号量应该被用作一种机制，用于向另一个等待任务发出信号，表明已经达到了某个里程碑（通常，生产者任务通过信号量对象发布信号，等待接收的消费者任务可以继续进行进一步的工作）。

+   互斥锁具有锁的所有权概念，只有所有者上下文才能执行解锁；二进制信号量没有所有权。

### 优先级反转和 RT-互斥锁

在使用任何类型的锁定时需要注意的一点是，您应该仔细设计和编码，以防止可能出现的可怕的*死锁*情况（在*锁验证器 lockdep - 及早捕捉锁定问题*一节中将更多地讨论这一点）。

除了死锁之外，使用互斥锁时还会出现另一种风险情况：优先级反转（在本书中我们不会深入讨论细节）。可以说，无界**优先级反转**情况可能是致命的；最终结果是产品的高（最高）优先级线程被长时间挡在 CPU 之外。

正如我在早期的书籍*使用 Linux 进行系统编程*中详细介绍的那样，正是这种优先级反转问题在 1997 年 7 月击中了 NASA 的火星探路者机器人，而且还是在火星表面！请参阅本章的*进一步阅读*部分，了解有关这一问题的有趣资源，这是每个软件开发人员都应该知道的内容！

用户空间 Pthreads 互斥锁实现当然具有**优先级继承**（**PI**）语义。但在 Linux 内核中呢？对此，Ingo Molnar 提供了基于 PI-futex 的 RT 互斥锁（实时互斥锁；实际上是扩展为具有 PI 功能的互斥锁。`futex(2)`是一个提供快速用户空间互斥锁的复杂系统调用）。当启用`CONFIG_RT_MUTEXES`配置选项时，这些就可用了。与“常规”互斥锁语义非常相似，RT 互斥锁 API 用于初始化、（解）锁定和销毁 RT 互斥锁对象。（此代码已从 Ingo Molnar 的`-rt`树合并到主线内核）。就实际使用而言，RT 互斥锁用于在内部实现 PI futex（`futex(2)`系统调用本身在内部实现了用户空间 Pthreads 互斥锁）。除此之外，内核锁定自测代码和 I2C 子系统直接使用 RT 互斥锁。

因此，对于典型的模块（或驱动程序）作者来说，这些 API 并不经常使用。内核确实提供了一些关于 RT 互斥锁内部设计的文档（涵盖了优先级反转、优先级继承等）。

### 内部设计

关于互斥锁在内核结构深处的内部实现的现实：Linux 在可能的情况下尝试实现*快速路径*方法。

**快速路径**是最优化的高性能代码路径；例如，没有锁和阻塞。目的是让代码尽可能地遵循这条快速路径。只有在真的不可能的情况下，内核才会退回到“中间路径”，然后是“慢路径”；它仍然可以工作，但速度较慢。

在没有锁争用的情况下（即，锁最初处于未锁定状态），会采用这条快速路径。因此，锁会立即被锁定，没有麻烦。然而，如果互斥锁已经被锁定，那么内核通常会使用中间路径的乐观自旋实现，使其更像是混合（互斥锁/自旋锁）锁类型。如果甚至这也不可能，就会遵循“慢路径” – 尝试获取锁的进程上下文可能会进入睡眠状态。如果您对其内部实现感兴趣，可以在官方内核文档中找到更多详细信息。

*LDV（Linux 驱动程序验证）项目：*在伴随指南*Linux 内核编程 - 第一章*，*内核工作空间设置*的*LDV – Linux 驱动程序验证 – 项目*部分中，我们提到该项目对 Linux 模块（主要是驱动程序）以及核心内核的各种编程方面有有用的“规则”。

关于我们当前的主题，这里有一个规则：*两次锁定互斥锁或在先前未锁定的情况下解锁*。它提到了您不能使用互斥锁做的事情（我们已经在*正确使用互斥锁*部分中涵盖了这一点）。有趣的是：您可以看到一个实际的 bug 示例 – 一个互斥锁双重获取尝试，导致（自身）死锁 – 在内核驱动程序中（以及随后的修复）。

现在您已经了解了如何使用互斥锁，让我们继续看看内核中另一个非常常见的锁 – 自旋锁。

# 使用自旋锁

在*互斥锁还是自旋锁？何时使用*部分，您学会了何时使用自旋锁而不是互斥锁，反之亦然。为了方便起见，我们在此重复了我们之前提供的关键声明。

+   **关键部分是在原子（中断）上下文中运行还是在不能睡眠的进程上下文中运行？**使用自旋锁。

+   **关键部分是在进程上下文中运行并且在关键部分中睡眠是必要的吗？**使用互斥锁。

在这一部分，我们假设您现在决定使用自旋锁。

## 自旋锁 - 简单用法

对于所有自旋锁 API，您必须包括相关的头文件；即`include <linux/spinlock.h>`。

与互斥锁类似，您*必须*在使用之前声明和初始化自旋锁为未锁定状态。自旋锁是通过`typedef`数据类型`spinlock_t`（在内部，它是在`include/linux/spinlock_types.h`中定义的结构）声明的“对象”。它可以通过`spin_lock_init()`宏动态初始化：

```
spinlock_t lock;
spin_lock_init(&lock);
```

或者，这可以通过`DEFINE_SPINLOCK(lock);`静态执行（声明和初始化）。

与互斥锁一样，在（全局/静态）数据结构中声明自旋锁是为了防止并发访问，并且通常是一个非常好的主意。正如我们之前提到的，这个想法在内核中经常被使用；例如，表示 Linux 内核上打开文件的数据结构被称为`struct file`：

```
// include/linux/fs.h
struct file {
    [...]
    struct path f_path;
    struct inode *f_inode; /* cached value */
    const struct file_operations *f_op;
    /*
     * Protects f_ep_links, f_flags.
     * Must not be taken from IRQ context.
     */
    spinlock_t f_lock;
    [...]
    struct mutex f_pos_lock;
    loff_t f_pos;
    [...]
```

看一下：对于`file`结构，名为`f_lock`的自旋锁变量是保护`file`数据结构的`f_ep_links`和`f_flags`成员的自旋锁（它还有一个互斥锁来保护另一个成员；即文件的当前寻位位置 - `f_pos`）。

你如何实际上锁定和解锁自旋锁？内核向我们模块/驱动程序作者公开了许多 API 的变体；自旋锁 API 的最简单形式如下：

```
void spin_lock(spinlock_t *lock);
<< ... critical section ... >>
void spin_unlock(spinlock_t *lock);
```

请注意，`mutex_destroy()`API 没有自旋锁的等效 API。

现在，让我们看看自旋锁 API 的实际应用！

## 自旋锁 - 一个示例驱动程序

与我们的互斥锁示例驱动程序（*互斥锁 - 一个示例驱动程序*部分）所做的类似，为了说明自旋锁的简单用法，我们将复制我们之前的`ch12/1_miscdrv_rdwr_mutexlock`驱动程序作为起始模板，然后将其放置在一个新的内核驱动程序中；也就是`ch12/2_miscdrv_rdwr_spinlock`。同样，在这里，我们只会显示差异的小部分（`diff(1)`生成的差异，我们不会显示每一行差异，只显示相关部分）。

```
// location: ch12/2_miscdrv_rdwr_spinlock/
+#include <linux/spinlock.h>
[ ... ]
-#define OURMODNAME "miscdrv_rdwr_mutexlock"
+#define OURMODNAME "miscdrv_rdwr_spinlock"
[ ... ]
static int ga, gb = 1;
-DEFINE_MUTEX(lock1); // this mutex lock is meant to protect the integers ga and gb
+DEFINE_SPINLOCK(lock1); // this spinlock protects the global integers ga and gb
[ ... ]
+/* The driver 'context' data structure;
+ * all relevant 'state info' reg the driver is here.
  */
 struct drv_ctx {
    struct device *dev;
@@ -63,10 +66,22 @@
    u64 config3;
 #define MAXBYTES 128
    char oursecret[MAXBYTES];
- struct mutex lock; // this mutex protects this data structure
+ struct mutex mutex; // this mutex protects this data structure
+ spinlock_t spinlock; // ...so does this spinlock
 };
 static struct drv_ctx *ctx;
```

这一次，为了保护我们的`drv_ctx`全局数据结构的成员，我们既有原始的互斥锁，又有一个新的自旋锁。这是相当常见的；互斥锁用于保护关键部分中可能发生阻塞的成员使用，而自旋锁用于保护关键部分中不会发生阻塞（睡眠 - 请记住它可能会睡眠）的成员。

当然，我们必须确保初始化所有锁，使它们处于未锁定状态。我们可以在驱动程序的`init`代码中执行这个操作（继续使用补丁输出）：

```
-   mutex_init(&ctx->lock);
+   mutex_init(&ctx->mutex);
+   spin_lock_init(&ctx->spinlock);
```

在驱动程序的`open`方法中，我们用自旋锁替换互斥锁来保护全局整数的增量和减量：

```
 * open_miscdrv_rdwr()
@@ -82,14 +97,15 @@

    PRINT_CTX(); // displays process (or intr) context info

-   mutex_lock(&lock1);
+   spin_lock(&lock1);
    ga++; gb--;
-   mutex_unlock(&lock1);
+   spin_unlock(&lock1);
```

现在，在驱动程序的`read`方法中，我们使用自旋锁而不是互斥锁来保护一些关键部分：

```
 static ssize_t read_miscdrv_rdwr(struct file *filp, char __user *ubuf, size_t count, loff_t  *off)
 {
-   int ret = count, secret_len;
+   int ret = count, secret_len, err_path = 0;
    struct device *dev = ctx->dev;

-   mutex_lock(&ctx->lock);
+   spin_lock(&ctx->spinlock);
    secret_len = strlen(ctx->oursecret);
-   mutex_unlock(&ctx->lock);
+   spin_unlock(&ctx->spinlock);
```

然而，这还不是全部！继续使用驱动程序的`read`方法，仔细看一下以下代码和注释：

```
[ ... ]
@@ -139,20 +157,28 @@
     * member to userspace.
     */
    ret = -EFAULT;
-   mutex_lock(&ctx->lock);
+   mutex_lock(&ctx->mutex);
+   /* Why don't we just use the spinlock??
+    * Because - VERY IMP! - remember that the spinlock can only be used when
+    * the critical section will not sleep or block in any manner; here,
+    * the critical section invokes the copy_to_user(); it very much can
+    * cause a 'sleep' (a schedule()) to occur.
+    */
    if (copy_to_user(ubuf, ctx->oursecret, secret_len)) {
[ ... ]
```

在保护关键部分可能有阻塞 API 的数据时 - 例如在`copy_to_user()`中 - 我们*必须*只使用互斥锁！（由于空间不足，我们没有在这里显示更多的代码差异；我们希望您阅读自旋锁示例驱动程序代码并自行尝试。）

## 测试 - 在原子上下文中睡眠

你已经学会了我们*不应该在任何类型的原子或中断上下文中睡眠（阻塞）*。让我们来测试一下。一如既往，经验主义方法 - 在测试自己的东西而不是依赖他人的经验时 - 是关键！

我们究竟如何测试这个？很简单：我们将使用一个简单的整数模块参数`buggy`，当设置为`1`（默认值为`0`）时，会执行违反此规则的自旋锁临界区内的代码路径。我们将调用`schedule_timeout()` API（正如您在第五章中学到的，*使用内核定时器、线程和工作队列*，在*理解如何使用*sleep()阻塞 API*部分中）内部调用`schedule()`；这是我们在内核空间中进入睡眠的方式）。以下是相关代码：

```
// ch12/2_miscdrv_rdwr_spinlock/2_miscdrv_rdwr_spinlock.c
[ ... ]
static int buggy;
module_param(buggy, int, 0600);
MODULE_PARM_DESC(buggy,
"If 1, cause an error by issuing a blocking call within a spinlock critical section");
[ ... ]
static ssize_t write_miscdrv_rdwr(struct file *filp, const char __user *ubuf,
                size_t count, loff_t *off)
{
    int ret, err_path = 0;
    [ ... ]
    spin_lock(&ctx->spinlock);
    strscpy(ctx->oursecret, kbuf, (count > MAXBYTES ? MAXBYTES : count));
    [ ... ]
    if (1 == buggy) {
        /* We're still holding the spinlock! */
        set_current_state(TASK_INTERRUPTIBLE);
        schedule_timeout(1*HZ); /* ... and this is a blocking call!
 * Congratulations! you've just engineered a bug */
    }
    spin_unlock(&ctx->spinlock);
    [ ... ]
}
```

现在，有趣的部分：让我们在两个内核中测试这个（错误的）代码路径：首先是在我们的自定义 5.4“调试”内核中（我们在这个内核中启用了几个内核调试配置选项（主要是从`make menuconfig`中的`Kernel Hacking`菜单中），如伴随指南*Linux 内核编程*-*第五章*，*编写您的第一个内核模块-LKMs 第二部分*中所解释的），其次是在一个没有启用任何相关内核调试选项的通用发行版（我们通常在 Ubuntu 上运行）5.4 内核上。

### 在 5.4 调试内核上进行测试

首先确保您已经构建了自定义的 5.4 内核，并且所有必需的内核调试配置选项都已启用（再次回到伴随指南*Linux 内核编程*-*第五章*，*编写您的第一个内核模块-LKMs 第二部分*，*配置调试内核*部分，如果需要的话）。然后，从调试内核启动（这里命名为`5.4.0-llkd-dbg`）。现在，在这个调试内核中构建驱动程序（在`ch12/2_miscdrv_rdwr_spinlock/`中）（在驱动程序目录中通常使用`make`命令即可完成；您可能会发现，在调试内核上，构建速度明显较慢！）：

```
$ lsb_release -a 2>/dev/null | grep "^Description" ; uname -r
Description: Ubuntu 20.04.1 LTS
5.4.0-llkd-dbg $ make
[ ... ]
$ modinfo ./miscdrv_rdwr_spinlock.ko 
filename: /home/llkd/llkd_src/ch12/2_miscdrv_rdwr_spinlock/./miscdrv_rdwr_spinlock.ko
[ ... ]
description: LLKD book:ch12/2_miscdrv_rdwr_spinlock: simple misc char driver rewritten with spinlocks
[ ... ]
parm: buggy:If 1, cause an error by issuing a blocking call within a spinlock critical section (int)
$ sudo virt-what
virtualbox
kvm
$ 
```

如您所见，我们在我们的 x86_64 Ubuntu 20.04 客户 VM 上运行我们的自定义 5.4.0“调试”内核。

您如何知道自己是在**虚拟机**（VM）上运行还是在“裸机”（本机）系统上运行？`virt-what(1)`是一个有用的小脚本，可以显示这一点（您可以在 Ubuntu 上使用`sudo apt install virt-what`进行安装）。

要运行我们的测试用例，将驱动程序插入内核并将`buggy`模块参数设置为`1`。调用驱动程序的`read`方法（通过我们的用户空间应用程序；也就是`ch12/miscdrv_rdwr/rdwr_test_secret`）不是问题，如下所示：

```
$ sudo dmesg -C
$ sudo insmod ./miscdrv_rdwr_spinlock.ko buggy=1
$ ../../ch12/miscdrv_rdwr/rdwr_test_secret 
Usage: ../../ch12/miscdrv_rdwr/rdwr_test_secret opt=read/write device_file ["secret-msg"]
 opt = 'r' => we shall issue the read(2), retrieving the 'secret' form the driver
 opt = 'w' => we shall issue the write(2), writing the secret message <secret-msg>
  (max 128 bytes)
$ 
$ ../../ch12/miscdrv_rdwr/rdwr_test_secret r /dev/llkd_miscdrv_rdwr_spinlock 
Device file /dev/llkd_miscdrv_rdwr_spinlock opened (in read-only mode): fd=3
../../ch12/miscdrv_rdwr/rdwr_test_secret: read 7 bytes from /dev/llkd_miscdrv_rdwr_spinlock
The 'secret' is:
 "initmsg"
$ 
```

接下来，我们通过用户模式应用程序向驱动程序发出`write(2)`；这次，我们的错误代码路径被执行。正如您所看到的，我们在自旋锁的临界区内发出了`schedule_timeout()`（也就是在锁定和解锁之间）。调试内核将此检测为错误，并在内核日志中生成（令人印象深刻的大量）调试诊断（请注意，这样的错误很可能会使您的系统挂起，因此请先在虚拟机上进行测试）：

![](img/3c6f7129-6f1c-4a04-9f5c-df29e28b0420.png)

图 6.9-由我们故意触发的“在原子上下文中调度”错误触发的内核诊断

前面的屏幕截图显示了发生的部分情况（在查看`ch12/2_miscdrv_rdwr_spinlock/2_miscdrv_rdwr_spinlock.c`中的驱动程序代码时，请跟随一起）：

1.  首先，我们有我们的用户模式应用程序的进程上下文（`rdwr_test_secre`；请注意名称被截断为前 16 个字符，包括`NULL`字节），它进入驱动程序的写入方法；也就是`write_miscdrv_rdwr()`。这可以在我们有用的`PRINT_CTX()`宏的输出中看到（我们在这里重现了这一行）：

```
miscdrv_rdwr_spinlock:write_miscdrv_rdwr(): 004) rdwr_test_secre :23578 | ...0 /*  write_miscdrv_rdwr() */
```

1.  它从用户空间写入进程中复制新的“秘密”并将其写入，共 24 个字节。

1.  然后，“获取”自旋锁，进入临界区，并将这些数据复制到我们驱动程序上下文结构的`oursecret`成员中。

1.  之后，`if (1 == buggy) {`评估为 true。

1.  然后，它调用`schedule_timeout()`，这是一个阻塞 API（因为它内部调用`schedule()`），触发了错误，这在红色中得到了很好的突出显示：

```
BUG: scheduling while atomic: rdwr_test_secre/23578/0x00000002
```

1.  内核现在会输出大量的诊断输出。首先要输出的是**调用堆栈**。

进程的内核模式堆栈或堆栈回溯（或“调用跟踪”）- 在这里，它是我们的用户空间应用程序`rdwr_drv_secret`，它正在运行我们（有缺陷的）驱动程序的代码在进程上下文中- 可以在*图 6.9*中清楚地看到。`Call Trace:`标题之后的每一行本质上都是内核堆栈上的一个调用帧。

作为提示，忽略以`?`符号开头的堆栈帧；它们很可能是同一内存区域中以前堆栈使用的“剩余物”。在这里值得进行一次与内存相关的小的偏离：这就是堆栈分配的真正工作原理；堆栈内存不是按照每个调用帧的基础分配和释放的，因为那将是非常昂贵的。只有在堆栈内存页耗尽时，才会自动*故障*新的内存页！（回想一下我们在伴随指南*Linux 内核编程-第九章*，*模块作者的内核内存分配-第二部分*中的讨论，在*内存分配和需求分页的简短说明*部分。）因此，现实情况是，当代码调用和从函数返回时，相同的堆栈内存页往往会不断被重用。

不仅如此，出于性能原因，内存并不是每次都被擦除，这导致以前的帧留下的情况经常出现。（它们可以真正“破坏”图像。然而，幸运的是，现代堆栈调用帧跟踪算法通常能够出色地找出正确的堆栈跟踪。）

从下到上（*总是从下到上阅读*）跟踪堆栈，我们可以看到，如预期的那样，我们的用户空间`write(2)`系统调用（它经常显示为（类似于）`SyS_write`或在 x86 上显示为`__x64_sys_write`，尽管在*图 6.9*中看不到）调用了内核的 VFS 层代码（您可以在这里看到`vfs_write()`，它调用了`__vfs_write()`），进一步调用了我们驱动程序的写方法；也就是`write_miscdrv_rdwr()`！正如我们所知，这段代码调用了有缺陷的代码路径，我们在其中调用了`schedule_timeout()`，这又调用了`schedule()`（和`__schedule()`），导致整个**`BUG：scheduling while atomic`**错误触发。

`scheduling while atomic`代码路径的格式是从以下代码行中检索的，该代码行可以在`kernel/sched/core.c`中找到：

```
printk(KERN_ERR "BUG: scheduling while atomic: %s/%d/0x%08x\n", prev->comm, prev->pid, preempt_count());
```

有趣！在这里，您可以看到它打印了以下字符串：

```
      BUG: scheduling while atomic: rdwr_test_secre/23578/0x00000002
```

在`atomic:`之后，它打印进程名称-PID-，然后调用`preempt_count()`内联函数，该函数打印*抢占深度*；抢占深度是一个计数器，每次获取锁时递增，每次解锁时递减。因此，如果它是正数，这意味着代码在关键或原子部分内；在这里，它显示为值`2`。

请注意，这个错误在这次测试运行中得到了很好的解决，因为`CONFIG_DEBUG_ATOMIC_SLEEP`调试内核配置选项已经打开。这是因为我们正在运行一个自定义的“调试内核”（内核版本 5.4.0）！配置选项的详细信息（您可以在`make menuconfig`中交互地找到并设置此选项，在`Kernel Hacking`菜单下）如下：

```
// lib/Kconfig.debug
[ ... ]
config DEBUG_ATOMIC_SLEEP
    bool "Sleep inside atomic section checking"
    select PREEMPT_COUNT
    depends on DEBUG_KERNEL
    depends on !ARCH_NO_PREEMPT
    help 
      If you say Y here, various routines which may sleep will become very 
 noisy if they are called inside atomic sections: when a spinlock is
 held, inside an rcu read side critical section, inside preempt disabled
 sections, inside an interrupt, etc...
```

### 在 5.4 非调试 distro 内核上进行测试

作为对比测试，我们现在将在我们的 Ubuntu 20.04 LTS VM 上执行完全相同的操作，我们将通过其默认的通用“distro” 5.4 Linux 内核引导，通常*未配置为“调试”内核*（这里，`CONFIG_DEBUG_ATOMIC_SLEEP`内核配置选项尚未设置）。

首先，我们插入我们的（有缺陷的）驱动程序。然后，当我们运行我们的`rdwr_drv_secret`进程以向驱动程序写入新的秘密时，有缺陷的代码路径被执行。然而，这一次，内核*既不崩溃，也不报告任何问题*（查看`dmesg(1)`输出验证了这一点）：

```
$ uname -r
5.4.0-56-generic
$ sudo insmod ./miscdrv_rdwr_spinlock.ko buggy=1
$ ../../ch12/miscdrv_rdwr/rdwr_test_secret w /dev/llkd_miscdrv_rdwr_spinlock "passwdcosts500bucksdude"
Device file /dev/llkd_miscdrv_rdwr_spinlock opened (in write-only mode): fd=3
../../ch12/miscdrv_rdwr/rdwr_test_secret: wrote 24 bytes to /dev/llkd_miscdrv_rdwr_spinlock
$ dmesg 
[ ... ]
[ 65.420017] miscdrv_rdwr_spinlock:miscdrv_init_spinlock(): LLKD misc driver (major # 10) registered, minor# = 56, dev node is /dev/llkd_miscdrv_rdwr
[ 81.665077] miscdrv_rdwr_spinlock:miscdrv_exit_spinlock(): miscdrv_rdwr_spinlock: LLKD misc driver deregistered, bye
[ 86.798720] miscdrv_rdwr_spinlock:miscdrv_init_spinlock(): VERMAGIC_STRING = 5.4.0-56-generic SMP mod_unload 
[ 86.799890] miscdrv_rdwr_spinlock:miscdrv_init_spinlock(): LLKD misc driver (major # 10) registered, minor# = 56, dev node is /dev/llkd_miscdrv_rdwr
[ 130.214238] misc llkd_miscdrv_rdwr_spinlock: filename: "llkd_miscdrv_rdwr_spinlock"
                wrt open file: f_flags = 0x8001
                ga = 1, gb = 0
[ 130.219233] misc llkd_miscdrv_rdwr_spinlock: stats: tx=0, rx=0
[ 130.219680] misc llkd_miscdrv_rdwr_spinlock: rdwr_test_secre wants to write 24 bytes
[ 130.220329] misc llkd_miscdrv_rdwr_spinlock: 24 bytes written, returning... (stats: tx=0, rx=24)
[ 131.249639] misc llkd_miscdrv_rdwr_spinlock: filename: "llkd_miscdrv_rdwr_spinlock"
                ga = 0, gb = 1
[ 131.253511] misc llkd_miscdrv_rdwr_spinlock: stats: tx=0, rx=24
$ 
```

我们知道我们的写入方法有一个致命的错误，但它似乎并没有以任何方式失败！这真的很糟糕；这种事情可能会误导你错误地认为你的代码很好，而实际上一个难以察觉的致命错误悄悄地等待着某一天突然袭击！

为了帮助我们调查底层到底发生了什么，让我们再次运行我们的测试应用程序（`rdwr_drv_secret`进程），但这次通过强大的`trace-cmd（1）`工具（一个非常有用的包装器，覆盖了 Ftrace 内核基础设施；以下是它的截断输出：

Linux 内核的**Ftrace**基础设施是内核的主要跟踪基础设施；它提供了内核空间中几乎每个执行的函数的详细跟踪。在这里，我们通过一个方便的前端利用 Ftrace：`trace-cmd（1）`实用程序。这些确实是非常强大和有用的调试工具；我们在伴随指南* Linux 内核编程 - 第一章* *内核工作空间设置*中提到了其他几个，但不幸的是，这些细节超出了本书的范围。查看手册以了解更多。

```
$ sudo trace-cmd record -p function_graph -F ../../ch12/miscdrv_rdwr/rdwr_test_secret w /dev/llkd_miscdrv_rdwr_spinlock "passwdcosts500bucks"
$ sudo trace-cmd report -I -S -l > report.txt
$ sudo less report.txt
[ ... ]
```

输出可以在以下截图中看到：

![](img/63b163f7-5ce7-45f1-907e-b10a06909ef3.png)

图 6.10 - trace-cmd（1）报告输出的部分截图

正如你所看到的，我们用户模式应用程序的`write（2）`系统调用变成了预期的`vfs_write()`，它本身（经过安全检查后）调用了`__vfs_write()`，然后调用了我们的驱动程序的写入方法 - `write_miscdrv_rdwr()`函数！

在（大量的）Ftrace 输出流中，我们可以看到`schedule_timeout()`函数确实被调用了：

![](img/1cd0401a-b6a2-43e1-998d-10994995cdd6.png)

图 6.11 - trace-cmd（1）报告输出的部分截图，显示了在原子上下文中调用 schedule_timeout()和 schedule()的（错误的！）调用

在`schedule_timeout()`之后的几行输出中，我们可以清楚地看到`schedule()`被调用！所以，我们的驱动程序（当然是故意的）执行了一些错误的操作 - 在原子上下文中调用`schedule()`。但这里的关键点是，在这个 Ubuntu 系统上，我们*没有*运行“调试”内核，这就是为什么我们有以下情况：

```
$ grep DEBUG_ATOMIC_SLEEP /boot/config-5.4.0-56-generic
# CONFIG_DEBUG_ATOMIC_SLEEP is not set
$
```

这就是为什么错误没有被报告的原因！这证明了运行测试用例的有用性 - 事实上，在“调试”内核上进行内核开发 - 一个启用了许多调试功能的内核。（作为练习，如果您还没有这样做，请准备一个“调试”内核并在其上运行此测试用例。）

Linux 驱动程序验证（LDV）项目：在伴随指南* Linux 内核编程 - 第一章* *内核工作空间设置*中，我们提到了这个项目对 Linux 模块（主要是驱动程序）以及核心内核的各种编程方面有用的“规则”。

关于我们当前的主题，这是其中一条规则：*使用自旋锁和解锁函数*（[`linuxtesting.org/ldv/online?action=show_rule&rule_id=0039`](http://linuxtesting.org/ldv/online?action=show_rule&rule_id=0039)）。它提到了关于正确使用自旋锁的关键点；有趣的是，它在这里展示了一个驱动程序中实际的错误实例，其中尝试两次释放自旋锁 - 这是对锁定规则的明显违反，导致系统不稳定。

# 锁定和中断

到目前为止，我们已经学会了如何使用互斥锁，对于自旋锁，基本的`spin_[un]lock()` API。自旋锁还有一些其他 API 变体，我们将在这里检查更常见的一些。

为了确切理解为什么你可能需要其他的自旋锁 API，让我们来看一个情景：作为驱动程序的作者，你发现你正在处理的设备断言了一个硬件中断；因此，你为其编写了中断处理程序。现在，在为你的驱动程序实现`read`方法时，你发现其中有一个非阻塞的临界区。这很容易处理：正如你所学的，你应该使用自旋锁来保护它。太好了！但是，如果在`read`方法的临界区内，设备的硬件中断触发了怎么办？正如你所知，*硬件中断会抢占任何事情*；因此，控制权将转移到中断处理程序代码，抢占了驱动程序的`read`方法。

关键问题在于：这是一个问题吗？答案取决于你的中断处理程序和`read`方法在做什么以及它们是如何实现的。让我们想象一些情景：

+   中断处理程序（理想情况下）仅使用局部变量，因此即使`read`方法处于临界区，它实际上并不重要；中断处理将非常快速地完成，并且控制权将被交还给被中断的内容（同样，这还不止这些；正如你所知，任何现有的底半部，比如任务 let 或软中断，也可能需要执行）。换句话说，在这种情况下实际上没有竞争。

+   中断处理程序正在处理（全局）共享可写数据，但*不是*你的读取方法正在使用的数据项。因此，再次，没有冲突，也没有与读取代码的竞争。当然，你应该意识到，中断代码*确实有一个临界区，它必须受到保护*（也许需要另一个自旋锁）。

+   中断处理程序正在处理与你的`read`方法使用的相同的全局共享可写数据。在这种情况下，我们可以看到存在竞争的潜力，因此我们需要锁！

让我们专注于第三种情况。显然，我们应该使用自旋锁来保护中断处理代码中的临界区（请记住，在任何类型的中断上下文中使用互斥锁是不允许的）。此外，*除非我们在`read`方法和中断处理程序的代码路径中都使用完全相同的自旋锁*，否则它们将根本得不到保护！（在处理锁时要小心；花时间仔细思考你的设计和代码细节。）

让我们尝试更加实际一些（暂时使用伪代码）：假设我们有一个名为`gCtx`的全局（共享）数据结构；我们在驱动程序的`read`方法和中断处理程序（硬中断处理程序）中都在操作它。由于它是共享的，它是一个临界区，因此需要保护；由于我们在一个原子（中断）上下文中运行，我们*不能使用互斥锁*，因此必须使用自旋锁（这里，自旋锁变量称为`slock`）。以下伪代码显示了这种情况的一些时间戳（`t1，t2，...`）：

```
// Driver read method ; WRONG ! driver_read(...)                  << time t0 >>
{
    [ ... ]
    spin_lock(&slock);
    <<--- time t1 : start of critical section >>
... << operating on global data object gCtx >> ...
    spin_unlock(&slock);
    <<--- time t2 : end of critical section >>
    [ ... ]
}                                << time t3 >>
```

以下伪代码是设备驱动程序的中断处理程序：

```
handle_interrupt(...)           << time t4; hardware interrupt fires!     >>
{
    [ ... ]
    spin_lock(&slock);
    <<--- time t5: start of critical section >>
    ... << operating on global data object gCtx >> ...
    spin_unlock(&slock);
    <<--- time t6 : end of critical section >>
    [ ... ]
}                               << time t7 >> 
```

这可以用以下图表总结：

![](img/3d9d5f60-bb83-44a5-a694-2a05f28df4f8.png)

图 6.12 - 时间轴 - 当处理全局数据时，驱动程序的读取方法和硬中断处理程序按顺序运行；这里没有问题

幸运的是，一切都进行得很顺利 - “幸运”是因为硬件中断是在`read`函数的临界区完成之后触发的。当然，我们不能指望幸运成为我们产品的唯一安全标志！硬件中断是异步的；如果它在一个不太合适的时间（对我们来说）触发了 - 比如，在`read`方法的临界区在时间 t1 和 t2 之间运行时怎么办？好吧，自旋锁会执行它的工作并保护我们的数据吗？

此时，中断处理程序的代码将尝试获取相同的自旋锁（`&slock`）。等一下——它无法“获取”它，因为它当前被锁定了！在这种情况下，它“自旋”，实际上是在等待解锁。但它怎么能解锁呢？它不能，这就是我们所面临的一个**(自身)死锁**。

有趣的是，自旋锁在 SMP（多核）系统上更直观，更有意义。让我们假设`read`方法在 CPU 核心 1 上运行；中断可以在另一个 CPU 核心上，比如核心 2 上被传递。中断代码路径将在 CPU 核心 2 上的锁上“自旋”，而`read`方法在核心 1 上完成临界区，然后解锁自旋锁，从而解除中断处理程序的阻塞。但是在**UP**（单处理器，只有一个 CPU 核心）上呢？那么它会怎么工作呢？啊，所以这是解决这个难题的方法：当与中断“竞争”时，*无论是单处理器还是 SMP，都简单地使用*自旋锁 API*的*`_irq` *变体*：

```
#include <linux/spinlock.h>
void spin_lock_irq(spinlock_t *lock);
```

`spin_lock_irq()` API 在处理器核心上禁用中断；也就是说，在本地核心上。因此，通过在我们的`read`方法中使用这个 API，中断将在本地核心上被禁用，从而通过中断使任何可能的“竞争”变得不可能。（如果中断在另一个 CPU 核心上触发，自旋锁技术将像之前讨论的那样正常工作！）

`spin_lock_irq()`的实现是相当嵌套的（就像大多数自旋锁功能一样），但是很快；在下一行，它最终调用了`local_irq_disable()`和`preempt_disable()`宏，在运行它的本地处理器核心上禁用了中断和内核抢占。（禁用硬件中断也会有禁用内核抢占的（理想的）副作用。）

`spin_lock_irq()`与相应的`spin_unlock_irq()` API 配对。因此，对于这种情况（与我们之前看到的情况相反），自旋锁的正确用法如下：

```
// Driver read method ; CORRECT ! driver_read(...)                  << time t0 >>
{
    [ ... ]
    spin_lock_irq(&slock);
    <<--- time t1 : start of critical section >>
*[now all interrupts + preemption on local CPU core are masked (disabled)]*
... << operating on global data object gCtx >> ...
    spin_unlock_irq(&slock);
    <<--- time t2 : end of critical section >>
    [ ... ]
}                                << time t3 >>
```

在我们自满地拍拍自己的背并休息一天之前，让我们考虑另一种情况。这一次，在一个更复杂的产品（或项目）上，有可能在代码库上工作的几个开发人员中，有人故意将中断屏蔽设置为某个值，从而阻止一些中断，同时允许其他中断。为了我们的例子，让我们假设这在某个时间点`t0`之前发生过。现在，正如我们之前描述的，另一个开发人员（你！）过来了，为了保护驱动程序`read`方法中的临界区，使用了`spin_lock_irq()` API。听起来正确，是吗？是的，但是这个 API 有权利*关闭（屏蔽）所有硬件中断*（和内核抢占，我们现在将忽略）。它通过在低级别上操作（非常特定于架构的）硬件中断屏蔽寄存器来做到这一点。假设将与中断对应的位设置为`1`会启用该中断，而清除该位（为`0`）会禁用或屏蔽它。由于这个原因，我们可能会得到以下情况：

+   时间`t0`：中断屏蔽被设置为某个值，比如`0x8e (10001110b)`，启用了一些中断并禁用了一些中断。这对项目很重要（在这里，为了简单起见，我们假设有一个 8 位掩码寄存器）

*[...时间流逝...].*

+   时间`t1`：就在进入驱动程序`read`方法的临界区之前，调用

`spin_lock_irq(&slock);`。这个 API 的内部效果是将中断屏蔽寄存器中的所有位清零，从而禁用所有中断（正如我们*认为*我们所期望的）。

+   时间`t2`：现在，硬件中断无法在这个 CPU 核心上触发，所以我们继续完成临界区。完成后，我们调用`spin_unlock_irq(&slock);`。这个 API 的内部效果是将中断屏蔽寄存器中的所有位设置为`1`，重新启用所有中断。

然而，中断掩码寄存器现在被错误地“恢复”为`0xff (11111111b)`的值，*而不是*原始开发人员想要、需要和假设的`0x8e`的值！这可能会（并且可能会）在项目中出现问题。

解决方案非常简单：不要假设任何东西，**只需保存和恢复中断掩码**。可以通过以下 API 对实现这一点：

```
#include <linux/spinlock.h>>
 unsigned long spin_lock_irqsave(spinlock_t *lock, unsigned long flags);
 void spin_unlock_irqrestore(spinlock_t *lock, unsigned long flags);
```

锁定和解锁函数的第一个参数都是要使用的自旋锁变量。第二个参数`flags` *必须是*`unsigned long`类型的本地变量。这将用于保存和恢复中断掩码：

```
spinlock_t slock;
spin_lock_init(&slock);
[ ... ]
driver_read(...) 
{
    [ ... ]
    spin_lock_irqsave(&slock, flags);
    << ... critical section ... >>
    spin_unlock_irqrestore(&slock, flags);
    [ ... ]
}
```

要严格，`spin_lock_irqsave()`不是一个 API，而是一个宏；我们将其显示为 API 是为了可读性。此宏的返回值虽然不是 void，但这是一个内部细节（这里更新了`flags`参数变量）。

如果一个任务或软中断（底半部中断机制）有一个关键部分与您的进程上下文代码路径“竞争”，在这种情况下，使用`spin_lock_bh()`例程可能是所需的，因为它可以在本地处理器上禁用底半部，然后获取自旋锁，从而保护关键部分（类似于`spin_lock_irq[save]()`在进程上下文中保护关键部分，通过在本地核心上禁用硬件中断）：

```
void spin_lock_bh(spinlock_t *lock);
```

当然，*开销*在高性能敏感的代码路径中很重要（网络堆栈是一个很好的例子）。因此，使用最简单形式的自旋锁将有助于处理更复杂的变体。尽管如此，肯定会有需要使用更强形式的自旋锁 API 的情况。例如，在 Linux 内核 5.4.0 上，这是我们看到的不同形式自旋锁 API 的使用实例数量的近似值：`spin_lock()`:超过 9,400 个使用实例；`spin_lock_irq()`:超过 3,600 个使用实例；`spin_lock_irqsave()`:超过 15,000 个使用实例；和`spin_lock_bh()`:超过 3,700 个使用实例。（我们不从中得出任何重大推论；只是我们希望指出，在 Linux 内核中广泛使用更强形式的自旋锁 API）。

最后，让我们简要介绍一下自旋锁的内部实现：在底层内部实现方面，实现往往是非常特定于体系结构的代码，通常由在微处理器上执行非常快的原子机器语言指令组成。例如，在流行的 x86[_64]体系结构上，自旋锁最终归结为自旋锁结构的成员上的*原子测试和设置*机器指令（通常通过`cmpxchg`机器语言指令实现）。在 ARM 机器上，正如我们之前提到的，实现的核心通常是`wfe`（等待事件，以及**SetEvent**（**SEV**））机器指令。（您将在*进一步阅读*部分找到关于其内部实现的资源）。无论如何，作为内核或驱动程序的作者，您在使用自旋锁时应该只使用公开的 API（和宏）。

## 使用自旋锁-快速总结

让我们快速总结一下自旋锁：

+   **最简单，开销最低**：在保护进程上下文中的关键部分时，请使用非 irq 自旋锁原语`spin_lock()`/`spin_unlock()`（要么没有中断需要处理，要么有中断，但我们根本不与它们竞争；实际上，当中断不发挥作用或不重要时使用这个）。

+   **中等开销**：当中断发挥作用并且很重要时，请使用禁用 irq（以及内核抢占禁用）版本的`spin_lock_irq() / spin_unlock_irq()`（进程和中断上下文可能会“竞争”；也就是说，它们共享全局数据）。

+   **最强（相对）高开销**：这是使用自旋锁的最安全方式。它与中等开销的方式相同，只是通过`spin_lock_irqsave()` / `spin_unlock_irqrestore()`对中断掩码执行保存和恢复，以确保以前的中断掩码设置不会被意外覆盖，这可能会发生在前一种情况下。

正如我们之前所看到的，自旋锁 - 在等待锁时在其运行的处理器上“自旋” - 在 UP 系统上是不可能的（在另一个线程同时在同一 CPU 上运行时，您如何在仅有的一个 CPU 上自旋？）。实际上，在 UP 系统上，自旋锁 API 的唯一真正效果是它可以禁用处理器上的硬件中断和内核抢占！然而，在 SMP（多核）系统上，自旋逻辑实际上会发挥作用，因此锁定语义会按预期工作。但是请注意 - 这不应该让您感到压力，新手内核/驱动程序开发人员；事实上，整个重点是您应该简单地按照描述使用自旋锁 API，您将永远不必担心 UP 与 SMP 之间的区别；做什么和不做什么的细节都被内部实现隐藏起来。

尽管本书基于 5.4 LTS 内核，但从**实时 Linux**（**RTL**，以前称为 PREEMPT_RT）项目中添加了一个新功能到 5.8 内核，值得在这里快速提一下：“**本地锁**”。虽然本地锁的主要用例是用于（硬）实时内核，但它们也对非实时内核有所帮助，主要用于通过静态分析进行锁调试，以及通过 lockdep 进行运行时调试（我们将在下一章中介绍 lockdep）。这是有关该主题的 LWN 文章：[`lwn.net/Articles/828477/`](https://lwn.net/Articles/828477/)。

通过这一部分，我们完成了自旋锁的部分，这是 Linux 内核中几乎所有子系统（包括驱动程序）都使用的一种极为常见和关键的锁。

# 总结

祝贺您完成了本章！

理解并发性及其相关问题对于任何软件专业人员来说都是非常关键的。在本章中，您学习了关于临界区的关键概念，其中需要在其中进行独占执行，以及原子性的含义。然后，您了解了在为 Linux 操作系统编写代码时为什么需要关注并发性。之后，我们详细探讨了实际的锁技术 - 互斥锁和自旋锁。您还学会了在何时使用哪种锁。最后，学习了在硬件中断（以及可能的底半部分）参与时如何处理并发性问题。

但我们还没有完成！我们还需要学习更多概念和技术，这正是我们将在本书的下一章，也是最后一章中要做的。我建议您先浏览本章的内容，以及*进一步阅读*部分和提供的练习，然后再深入研究最后一章！

# 问题

随着我们的结束，这里有一些问题供您测试对本章材料的了解：[`github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions`](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions)。您会在书的 GitHub 存储库中找到一些问题的答案：[`github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn`](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn)。

# 进一步阅读

为了帮助您深入了解这个主题并提供有用的材料，我们在本书的 GitHub 存储库中提供了一个相当详细的在线参考和链接列表（有时甚至包括书籍）。*进一步阅读*文档在这里可用：[`github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md`](https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md)。

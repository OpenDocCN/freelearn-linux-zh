- en: Advanced File I/O
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级文件I/O
- en: In [Appendix A](https://www.packtpub.com/sites/default/files/downloads/File_IO_Essentials.pdf)
    , *File I/O Essentials*, we covered how an application developer can exploit the
    available glibc library APIs as well as the typical system calls for performing
    file I/O (open, read, write, and close). While they work, of course, the reality
    is that performance is not really optimized. In this chapter, we focus on more
    advanced file I/O techniques, and how the developer can exploit newer and better
    APIs, for gaining performance.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[附录A](https://www.packtpub.com/sites/default/files/downloads/File_IO_Essentials.pdf)，*文件I/O基础*中，我们介绍了应用程序开发人员如何利用可用的glibc库API以及执行文件I/O（打开、读取、写入和关闭）的典型系统调用。虽然它们可以工作，但实际上性能并没有得到真正优化。在本章中，我们将重点介绍更高级的文件I/O技术，以及开发人员如何利用更新和更好的API来提高性能。
- en: Often, one gets stressed about the CPU(s) and its/their performance. While important,
    in many (if not most) real-world application workloads, it's really not the CPU(s)
    that drag down performance but the I/O code paths that are the real culprit. This
    is quite understandable; recall, from [Chapter 2](976fc2af-8bb4-4060-96cd-3b921682ed75.xhtml),
    *Virtual Memory*, we showed that disk speed, in contrast with RAM, is orders of
    magnitude slower. The case is similar with network I/O; thus, it stands to reason
    that the real performance bottlenecks occur due to  heavy sustained disk and network
    I/O.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，人们会对CPU及其性能感到紧张。虽然重要，但在许多（如果不是大多数）真实的应用工作负载中，真正拖慢性能的不是CPU，而是I/O代码路径。这是可以理解的；回想一下，从[第2章](976fc2af-8bb4-4060-96cd-3b921682ed75.xhtml)
    *虚拟内存*中我们展示了磁盘速度与RAM相比要慢几个数量级。网络I/O也是类似的情况；因此，可以推断真正的性能瓶颈是由于大量持续的磁盘和网络I/O造成的。
- en: 'In this chapter, the reader will learn several approaches to improve I/O performance;
    broadly speaking, these approaches will include the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，读者将学习几种改进I/O性能的方法；广义上讲，这些方法将包括以下内容：
- en: Taking full advantage of the kernel page cache
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 充分利用内核页面缓存
- en: Giving hints and advice to the kernel on file usage patterns
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向内核提供关于文件使用模式的提示和建议
- en: Using scatter-gather (vectored) I/O
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分散-聚集（向量）I/O
- en: Leveraging memory mapping for file I/O
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用内存映射进行文件I/O
- en: Learning about and using sophisticated DIO and AIO techniques
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习和使用复杂的DIO和AIO技术
- en: Learning about I/O schedulers
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习I/O调度程序
- en: Utilities/tools/APIs/cgroups for monitoring, analysis, and bandwidth control
    on I/O
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于监视、分析和带宽控制I/O的实用程序/工具/API/cgroups
- en: I/O performance recommendations
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: I/O性能建议
- en: The key point when performing I/O is realizing that the underlying storage (disk)
    hardware is much, much slower than RAM. So, devising strategies to minimize going
    to the disk and working more from memory will always help. In fact, both the library
    layer (we have already discussed studio buffering in some detail), and the OS
    (via the page cache and other features within the block I/O layers, and, in fact,
    even within modern hardware) will perform a lot of work to ensure this. For the
    (systems) application developer, a few suggestions to consider are made next.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 进行I/O时的关键是意识到底层存储（磁盘）硬件比RAM慢得多。因此，制定策略以最小化对磁盘的访问并更多地从内存中工作总是有帮助的。事实上，库层（我们已经详细讨论了studio缓冲区），以及操作系统（通过页面缓存和块I/O层中的其他功能，事实上，甚至在现代硬件中）将执行大量工作来确保这一点。对于（系统）应用程序开发人员，下面提出了一些建议。
- en: 'If feasible, use large buffers (to hold the data read or to be written) when
    performing I/O operations upon a file—but how large? A good rule of thumb is to
    use the same size for the local buffer as the I/O block size of the filesystem
    upon which the file resides (in fact, this field is internally documented as block
    size for filesystem I/O). To query it is simple: issue the  `stat(1)` command
    upon the file in which you want to perform I/O. As an example, let''s say that
    on an Ubuntu 18.04 system we want to read in the content of the currently running
    kernel''s configuration file:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可行，执行文件I/O操作时使用大缓冲区（用于保存读取或要写入的数据）——但有多大？一个经验法则是使用与文件系统的I/O块大小相同的本地缓冲区大小（实际上，这个字段在文件系统I/O中内部记录为块大小）。查询很简单：在要执行I/O的文件上发出`stat(1)`命令。例如，假设在Ubuntu
    18.04系统上，我们想要读取当前运行的内核配置文件的内容：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As can be seen from the code, `stat(1)` reveals several file characteristics
    (or attributes) from the file's inode data structure within the kernel, among
    them the I/O block size.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从代码中可以看出，`stat(1)`从文件的inode数据结构中显示了几个文件特性（或属性），其中包括I/O块大小。
- en: Internally, the `stat(1)` utility issues the `stat(2)` system call, which parses
    the inode of the underlying file and supplies all details to user space. So, when
    required programmatically, make use of the `[f]stat(2)` API(s).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，`stat(1)`实用程序发出`stat(2)`系统调用，解析底层文件的inode并向用户空间提供所有细节。因此，当需要以编程方式时，利用`[f]stat(2)`API。
- en: Further, if memory is not a constraint, why not just allocate a moderately-to-really-large
    buffer and perform I/O via it; it will help. Determining how large requires some
    investigation on your target platform; to give you an idea, in the earlier days,
    pipe I/O used to use a kernel buffer of size one page; on the modern Linux kernels,
    the pipe I/O buffer size is increased to a megabyte by default.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果内存不是一个限制，为什么不分配一个中等到非常大的缓冲区并通过它执行I/O；这将有所帮助。确定需要多大需要在目标平台上进行一些调查；为了给你一个概念，在早期，管道I/O通常使用一个页面大小的内核缓冲区；在现代Linux内核上，默认情况下管道I/O缓冲区大小增加到了一兆字节。
- en: The kernel page cache
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内核页面缓存
- en: As we learned from [Appendix A](https://www.packtpub.com/sites/default/files/downloads/File_IO_Essentials.pdf),
    *File I/O Essentials*, when a process (or thread) performs file I/O by, say, using
    the `fread(3)` or `fwrite(3)` library layer APIs, they ultimately are issued to
    the underlying OS via the `read(2)` and `write(2)` system calls. These system
    calls get the kernel to perform the I/O; though it seems intuitive, the reality
    is that the read-and-write system calls are not synchronous; that is, they may
    return before the actual I/O has completed. (Obviously, this will be the case
    for writes to a file; synchronous reads have to return the data read to the user
    space memory buffer; until then, the read blocks. However, using **Asynchronous
    I/O** (**AIO**), even reads can be made asynchronous.)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从[附录A](https://www.packtpub.com/sites/default/files/downloads/File_IO_Essentials.pdf)中我们了解到，当一个进程（或线程）通过使用`fread(3)`或`fwrite(3)`库层API执行文件I/O时，最终会通过`read(2)`和`write(2)`系统调用发出到底层操作系统。这些系统调用让内核执行I/O；尽管这似乎是直观的，但实际情况是读写系统调用并不是同步的；也就是说，它们可能在实际I/O完成之前返回。（显然，对文件的写入会是这种情况；同步读取必须将读取的数据返回到用户空间内存缓冲区；在此之前，读取会被阻塞。然而，即使是读取也可以通过**异步I/O**（**AIO**）变成异步。）
- en: 'The fact is, within the kernel, every single-file I/O operation is cached within
    a global kernel cache called the *page cache*. So, when a process writes data
    to a file, the data buffer is not immediately flushed to the underlying block
    device (disk or flash storage), it''s cached in the page cache. Similarly, when
    a process reads data from the underlying block device, the data buffer is not
    instantly copied to the user space process memory buffer; no, you guessed it,
    it''s stored within the page cache first (and the process will actually receive
    it from there). Refer again to [Appendix A](https://www.packtpub.com/sites/default/files/downloads/File_IO_Essentials.pdf),
    *File I/O Essentials*, *Figure 3: More detail—app to stdio I/O buffer to kernel
    page cache*, to see this.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，在内核中，每个单个文件I/O操作都被缓存在一个称为*页缓存*的全局内核缓存中。因此，当一个进程向文件写入数据时，数据缓冲区并不会立即刷新到底层块设备（磁盘或闪存存储），而是被缓存在页缓存中。同样，当一个进程从底层块设备读取数据时，数据缓冲区也不会立即复制到用户空间进程内存缓冲区；不，你猜对了，它首先存储在页缓存中（进程实际上会从那里接收到它）。再次参考[附录A](https://www.packtpub.com/sites/default/files/downloads/File_IO_Essentials.pdf)，*文件I/O基础*，*图3：更多细节—应用到stdio
    I/O缓冲区到内核页缓存*，来看看这一点。
- en: 'Why is this caching within the kernel''s page cache helpful? Simple: by exploiting
    the key property of a cache, that it, the speed discrepancy between the cached
    memory region (RAM) and the region it is caching (the block device), we gain tremendous
    performance. The page cache is in RAM, thus keeping the contents of all file I/O
    cached (as far as is possible) pretty much guarantees hits on the cache when applications
    perform reads on file data; reading from RAM is far faster than reading from the
    storage device. Similarly, instead of slowly and synchronously writing application
    data buffers directly to the block device, the kernel caches the write data buffers
    within the page cache. Obviously, the work of flushing the written data to the
    underlying block devices and the management of the page cache memory itself is
    well within the Linux kernel''s scope of work (we do not discuss these internal
    details here).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么内核页缓存中的缓存有用呢？简单：通过利用缓存的关键属性，即缓存内存区域（RAM）和正在缓存的区域（块设备）之间的速度差异，我们获得了巨大的性能提升。页缓存位于RAM中，因此保持所有文件I/O的内容被缓存（尽可能）几乎可以保证应用程序在文件数据上执行读取时命中缓存；从RAM读取比从存储设备读取要快得多。同样，内核将写入数据缓冲区缓存到页缓存中，而不是将应用程序数据缓冲区慢慢同步地直接写入块设备。显然，刷新已写入的数据到底层块设备以及管理页缓存内存本身的工作都在Linux内核的工作范围之内（我们在这里不讨论这些内部细节）。
- en: The programmer can always explicitly flush file data to the underlying storage
    device; we have covered the relevant APIs and their usage back in [Appendix A](https://www.packtpub.com/sites/default/files/downloads/File_IO_Essentials.pdf),
    *File I/O Essentials*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 程序员总是可以显式地将文件数据刷新到底层存储设备；我们在[附录A](https://www.packtpub.com/sites/default/files/downloads/File_IO_Essentials.pdf)中已经介绍了相关的API及其用法，*文件I/O基础*。
- en: Giving hints to the kernel on file I/O patterns
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 给内核提供文件I/O模式的提示
- en: 'We now understand that the kernel goes ahead and caches all file I/O within
    its page cache; this is good for performance. It''s useful to think about an example:
    an application sets up and performs streaming reads on a very large video file
    (to display it within some app window to the user; we shall assume the particular
    video file is being accessed for the first time). It''s easy to understand that,
    in general, caching a file as it''s read from the disk helps, but here, in this
    particular case, it would not really help much, as, the first time, we still have
    to first go to the disk and read it in. So, we shrug our shoulders and continue
    coding it in the usual way, sequentially reading in chunks of video data (via
    it''s underlying codec) and passing it along to the render code.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在明白了内核会缓存所有文件I/O在其页缓存中；这对性能有好处。想象一个例子会很有用：一个应用程序设置并对一个非常大的视频文件进行流式读取（在某个应用窗口中向用户显示；我们假设特定的视频文件是第一次被访问）。一般来说，从磁盘读取文件时进行缓存是有帮助的，但在这种特殊情况下，它并不会帮助太多，因为第一次，我们仍然必须首先去磁盘读取。因此，我们耸耸肩，继续以通常的方式编写代码，顺序读取视频数据块（通过其底层编解码器）并将其传递给渲染代码。
- en: Via the posix_fadvise(2) API
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过posix_fadvise(2) API
- en: 'Can we do better? Yes, indeed: Linux provides the  `posix_fadvise(2)` system
    call, allowing an application process to give hints to the kernel on it''s pattern
    of access to file data, via a parameter called `advice`. Relevant to our example,
    we can pass advice as the value `POSIX_FADV_SEQUENTIAL`, `POSIX_FADV_WILLNEED`,
    to inform the kernel that we expect to read file data sequentially and that we
    expect we shall require access to the file''s data in the near future. This advice
    causes the kernel to initiate an aggressive read-ahead of the file''s data in
    sequential order (lower-to-higher file offsets) into the kernel page cache. This
    will greatly help increase performance.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能做得更好吗？是的，Linux提供了`posix_fadvise(2)`系统调用，允许应用程序进程通过一个名为`advice`的参数向内核提供关于其对文件数据访问模式的提示。与我们的示例相关，我们可以将建议作为值`POSIX_FADV_SEQUENTIAL`，`POSIX_FADV_WILLNEED`传递，以通知内核我们期望按顺序读取文件数据，并且我们期望我们将来会需要访问文件的数据。这个建议会导致内核按顺序（从较低到较高的文件偏移）积极地预读文件数据到内核页缓存中。这将极大地帮助提高性能。
- en: 'The signature of the `posix_fadvise(2)` system call is as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`posix_fadvise(2)`系统调用的签名如下：'
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Clearly, the first parameter `fd` represents the file descriptor (we refer the
    reader to [Appendix A](https://www.packtpub.com/sites/default/files/downloads/File_IO_Essentials.pdf),
    *File I/O Essentials*), and the second and third parameters, `offset` and `len`,
    specify a region of the file upon which we pass the hint or advice via the fourth
    parameter, `advice`. (The length is actually rounded up to the page granularity.)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，第一个参数`fd`代表文件描述符（我们参考读者到[附录A](https://www.packtpub.com/sites/default/files/downloads/File_IO_Essentials.pdf)，*文件I/O基础*），第二个和第三个参数`offset`和`len`指定了文件的一个区域，我们通过第四个参数`advice`传递了提示或建议。（长度实际上是按页粒度四舍五入的。）
- en: Not only that, the application, upon finishing processing upon a chunk of video
    data, could even specify to the OS that it will not require that particular piece
    of memory any longer by invoking `posix_fadvise(2)` with advice set to the value
    `POSIX_FADV_DONTNEED`; this will be a hint to the kernel that it can free up the
    page(s) of the page cache holding that data, thereby creating space for incoming
    data of consequence (and for already cached data that may still be useful).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅如此，应用程序在处理完视频数据块后，甚至可以通过调用`posix_fadvise(2)`并将建议设置为值`POSIX_FADV_DONTNEED`来告知操作系统，它将不再需要那个特定的内存块；这将是一个提示给内核，它可以释放持有该数据的页（页）的页缓存，从而为重要数据（以及可能仍然有用的已缓存数据）腾出空间。
- en: There are some caveats to be aware of. First, it's important for the developer
    to realize that this advice is really just a hint, a suggestion, to the OS; it
    may or may not be honored. Next, again, even if the target file's pages are read
    into the page cache, they could be evicted for various reasons, memory pressure
    being a typical one. There's no harm in trying though; the kernel will often take
    the advice into account, and it can really benefit performance. (More advice values
    can be looked up, as usual, within the man page pertaining to this API.)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意一些注意事项。首先，开发人员要意识到这个建议实际上只是对操作系统的一个提示，一个建议；它可能会被采纳，也可能不会。其次，即使目标文件的页被读入页缓存，它们也可能因为各种原因被驱逐，内存压力是一个典型的原因。尽管如此，尝试也没有坏处；内核通常会考虑这个建议，并且它确实可以提高性能。（关于这个API的更多建议值可以像往常一样在man页面中查找。）
- en: 'Interestingly, and now understandably, `cat(1)` uses the `posix_fadvise(2)`
    system call to inform the kernel that it intends to perform sequential reads until
    EOF. Using the powerful `strace(1)` utility on `cat(1)` reveals the following:
    `...fadvise64(3, 0, 0, POSIX_FADV_SEQUENTIAL) = 0`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，现在可以理解，`cat(1)`使用`posix_fadvise(2)`系统调用通知内核，它打算执行顺序读取直到文件结束。在`cat(1)`上使用强大的`strace(1)`工具可以发现以下内容：`...fadvise64(3,
    0, 0, POSIX_FADV_SEQUENTIAL) = 0`
- en: Don't get stressed with the fadvise64; it's just the underlying system call
    implementation on Linux for the `posix_fadvise(2)` system call.  Clearly, `cat(1)`
    has invoked this on the file (descriptor 3), offset 0 and length 0—implying until
    EOF, and with the advice parameter set to `POSIX_FADV_SEQUENTIAL`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 不要被fadvise64搞得紧张，它只是Linux上`posix_fadvise(2)`系统调用的底层实现。显然，`cat(1)`已经在文件（描述符3），偏移量0和长度0上调用了这个系统调用，意味着直到文件结束，并且将`advice`参数设置为`POSIX_FADV_SEQUENTIAL`。
- en: Via the readahead(2) API
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过readahead(2) API
- en: 'The Linux (GNU)-specific `readahead(2)` system call achieves a similar result
    as the `posix_fadvise(2)` we just saw in terms of performing aggressive file read-ahead.
    Its signature is as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Linux（GNU）特定的`readahead(2)`系统调用实现了与我们刚刚看到的`posix_fadvise(2)`类似的结果，以便进行积极的文件预读。它的签名如下：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The read-aheads are performed on the target file specified by `fd`, starting
    from the file `offset` and for a maximum of `count` bytes (rounded up to page
    granularity).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 预读是从指定为`fd`的目标文件开始的，从文件`offset`开始，最多`count`字节（按页粒度四舍五入）。
- en: 'Though not normally required, what if you want to explicitly empty (clean)
    the contents of the Linux kernel''s page cache? If required, do this as the root user:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常不需要，但如果您想要明确地清空（清理）Linux内核页缓存的内容怎么办？如果需要，以root用户身份执行以下操作：
- en: '`# sync && echo 1 > /proc/sys/vm/drop_caches`'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`# sync && echo 1 > /proc/sys/vm/drop_caches`'
- en: Don't miss the `sync(1)` first, or you risk losing data. Again, we stress that
    flushing the kernel page cache should not be done in the normal course, as this
    could actually hurt I/O performance. A collection of useful **command -line interface** (**CLI**)
    wrapper utilities called linux-ftools is available on GitHub here: [https://github.com/david415/linux-ftools](https://github.com/david415/linux-ftools).
    It provides the `fincore(1)` (that's read as f-in-core), `fadvise(1)`, and `fallocate(1)`
    utilities; it's very educational to check out their GitHub README, read their
    man pages, and try them out.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记先使用`sync(1)`，否则会有丢失数据的风险。再次强调，正常情况下不应该刷新内核页缓存，因为这实际上可能会损害I/O性能。在GitHub上有一个有用的**命令行接口**（**CLI**）包装工具集合，称为linux-ftools，可以在这里找到：[https://github.com/david415/linux-ftools](https://github.com/david415/linux-ftools)。它提供了`fincore(1)`（读作f-in-core）、`fadvise(1)`和`fallocate(1)`工具；查看它们的GitHub
    README，阅读它们的man页面，并尝试使用它们，这是非常有教育意义的。
- en: MT app file I/O with the pread, pwrite APIs
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用pread、pwrite API的MT应用程序文件I/O
- en: Recall the `read(2)` and `write(2`) system calls that we saw in [Appendix A](https://www.packtpub.com/sites/default/files/downloads/File_IO_Essentials.pdf),
    *File I/O Essentials*;they form the basis of performing I/O to files. You will
    also recall that, upon using these APIs, the underlying file offset will be implicitly
    updated by the OS. For example, if a process opens a file (via `open(2)`), and
    then performs a `read(2)` of 512 bytes, the file's offset (or the so-called seek position)
    will now be 512\. If it now writes, say, 200 bytes, the write will occur from
    position 512 up to position 712, thereby setting the new seek position or offset
    to this number.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们在[附录A](https://www.packtpub.com/sites/default/files/downloads/File_IO_Essentials.pdf)中看到的`read(2)`和`write(2)`系统调用，它们构成了对文件进行I/O的基础。你还会记得，使用这些API时，操作系统会隐式更新底层文件的偏移量。例如，如果一个进程通过`open(2)`打开一个文件，然后执行512字节的`read(2)`，文件的偏移量（或所谓的寻位位置）现在将是512。如果现在写入，比如说200字节，写入将从位置512到712进行，从而将新的寻位位置或偏移量设置为这个数字。
- en: 'Well, so what? Our point is simply that the file''s offset being set implicitly
    causes issues when a multithreaded application has multiple threads simultaneously
    performing I/O upon the same underlying file. But wait, we have mentioned this
    before: the file is required to be locked and then worked upon. But, locking creates
    major performance bottlenecks. What if you design an MT app whose threads work
    upon different portions of the same file in parallel? That sounds great, except
    that the file''s offset would keep changing and thus ruin our parallelism and
    thus performance (you will also recall from our discussions in [Appendix A](https://www.packtpub.com/sites/default/files/downloads/File_IO_Essentials.pdf),
    *File I/O Essentials*, that simply using  `lseek(2)` to set the file''s seek position
    explicitly can result in dangerous races).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 那又怎样？我们的观点很简单，文件的偏移量被隐式设置会在多线程应用程序中引起问题，当多个线程同时对同一底层文件进行I/O操作时。但是，等等，我们之前提到过：文件需要被锁定然后进行操作。但是，锁定会导致主要的性能瓶颈。如果你设计一个MT应用程序，其线程并行地处理同一文件的不同部分，听起来很棒，除了文件的偏移量会不断变化，从而破坏我们的并行性和性能（你还会记得我们在[附录A](https://www.packtpub.com/sites/default/files/downloads/File_IO_Essentials.pdf)中的讨论，*文件I/O基础*，简单地使用`lseek(2)`来显式设置文件的寻位位置可能导致危险的竞争）。
- en: 'So, what do you do? Linux provides the `pread(2)` and `pwrite(2)` system calls
    (p for positioned I/O) for this very purpose; with these APIs, the file offset
    to perform I/O at can be specified (or positioned) and the actual underlying file
    offset is not changed by the OS. Their signature is as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你该怎么办？Linux提供了`pread(2)`和`pwrite(2)`系统调用（p代表定位I/O）来解决这个问题；使用这些API，可以指定要执行I/O的文件偏移量（或定位），操作系统不会改变实际的底层文件偏移量。它们的签名如下：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The difference between the `pread(2)`/`pwrite(2)` and the usual `read(2)`/`write(2)`
    system calls is that the former APIs take an additional fourth parameter—the file
    offset at which to perform the read or write I/O operation, without modifying
    it. This allows us to achieve what we wanted: having an MT app perform high-performance
    I/O by having multiple threads simultaneously read and write to different portions
    of the file in parallel. (We leave the task of trying this out as an interesting
    exercise to the reader.)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`pread(2)`/`pwrite(2)`与通常的`read(2)`/`write(2)`系统调用的区别在于，前者的API需要额外的第四个参数——文件偏移量，用于执行读取或写入I/O操作，而不会修改它。这使我们能够实现我们想要的：通过多个线程同时并行读取和写入文件的不同部分，从而使MT应用程序执行高性能I/O。（我们将尝试这个任务留给读者作为一个有趣的练习。）'
- en: 'A few caveats to be aware of: first, just as with `read(2)` and `write(2)`,
    `pread(2)`, and `pwrite(2)` too can return without having transferred all requested
    bytes; it is the programmer''s responsibility to check and call the APIs in a
    loop until no bytes remain to transfer (revisit [Appendix A](https://www.packtpub.com/sites/default/files/downloads/File_IO_Essentials.pdf),
    *File I/O Essentials*). Correctly using the read/write APIs, where issues such
    as this are addressed). Secondly, when a file is opened with the `O_APPEND` flag
    specified, Linux''s `pwrite(2)` system call always appends data to the EOF irrespective
    of the current offset value; this violates the POSIX standard, which states that
    the `O_APPEND` flag should have no effect on the start location where the write
    occurs. Thirdly, and quite obviously (but we must state it), the file being operated
    upon must be capable of being seeked upon (that is, the `fseek(3)` or `lseek(2)`
    APIs are supported). Regular files always do support the seek operation, but pipes
    and some types of devices do not).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一些注意事项：首先，就像`read(2)`和`write(2)`一样，`pread(2)`和`pwrite(2)`也可能在没有传输所有请求的字节的情况下返回；程序员有责任检查并调用API，直到没有剩余的字节需要传输（参见[附录A](https://www.packtpub.com/sites/default/files/downloads/File_IO_Essentials.pdf)，*文件I/O基础知识*）。正确使用读/写API，解决这类问题）。其次，当文件以指定了`O_APPEND`标志打开时，Linux的`pwrite(2)`系统调用总是将数据追加到EOF，而不管当前偏移值如何；这违反了POSIX标准，该标准规定`O_APPEND`标志不应对写入发生的起始位置产生影响。第三，显而易见的是（但我们必须声明），正在操作的文件必须能够进行寻址（即支持`fseek(3)`或`lseek(2)`的API）。常规文件总是支持寻址操作，但管道和某些类型的设备不支持）。
- en: Scatter – gather I/O
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分散-聚集I/O
- en: 'To help explain this topic, let''s say that we are commissioned with writing
    data to a file such that three discontiguous data regions A, B, and C are written
    (filled with As, Bs, and Cs, respectively); the following diagram shows this:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助解释这个主题，让我们假设我们被委托向文件写入数据，使得三个不连续的数据区域A、B和C被写入（分别填充为A、B和C）；以下图表显示了这一点：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The discontiguous data file
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 不连续的数据文件
- en: 'Notice how the files have holes—regions that do not contain any data content;
    this is possible to achieve with regular files (files that are largely holes are
    termed sparse files). How do you create the hole? Simple: just perform an `lseek(2)`
    and then `write(2)` data; the length seeked forward determines the size of the
    hole in the file.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意文件中有空洞——不包含任何数据内容的区域；这在常规文件中是可能实现的（大部分是空洞的文件称为稀疏文件）。如何创建空洞呢？简单：只需执行`lseek(2)`，然后`write(2)`数据；向前寻找的长度确定了文件中空洞的大小。
- en: So, how can we achieve this data file layout as shown? We shall show two approaches—one,
    the traditional manner, and two, a far more optimized-for-performance approach.
    Let's get started with the traditional approach.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何实现所示的数据文件布局呢？我们将展示两种方法——一种是传统的方法，另一种是更为优化性能的方法。让我们从传统方法开始。
- en: Discontiguous data file – traditional approach
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不连续的数据文件-传统方法
- en: 'This seems quite simple: first seek to the required start offset and then write
    the data content for the required length; this can be done via the pair of `lseek(2)`
    and `write(2)` system calls. Of course, we will have to invoke this pair of system
    calls three times. So, we write some code to actually perform this task; see the
    (relevant snippets) of the code here (`ch18/sgio_simple.c`):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎很简单：首先寻找所需的起始偏移量，然后为所需的长度写入数据内容；这可以通过一对`lseek(2)`和`write(2)`系统调用来完成。当然，我们将不得不调用这一对系统调用三次。因此，我们编写一些代码来实际执行这个任务；在这里查看代码的（相关片段）(`ch18/sgio_simple.c`)：
- en: For readability, only key parts of the source code are displayed; to view the
    complete source code, build, and run it, the entire tree is available for cloning
    from GitHub here: [https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux](https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可读性，只显示了源代码的关键部分；要查看完整的源代码，构建并运行它，整个树都可以从GitHub克隆到这里：[https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux](https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux)。
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Notice how we have written the code to use an `{lseek, write}` pair of system
    calls three times in succession; let''s try it out:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们已经编写了代码，连续三次使用`{lseek, write}`系统调用；让我们试一试：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: It worked; the file we created, `tmptest` (we have not shown the code to create
    the file, allocate and initialize the buffers, and so on, here; please look it
    up via the book's GitHub repository), is of length 222 bytes, although the actual
    data content (the As, Bs, and Cs) is of length 20+30+42 = 92 bytes. The remaining
    (222 - 92) 130 bytes are the three holes in the file (of length 10+100+20 bytes;
    see the macros that define these in the code). The `hexdump(1)` utility conveniently
    dumps the file's content; 0x41 being A, 0x42 is B, and 0x43 is C. The holes are
    clearly seen as NULL-populated regions of the length we wanted.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 它起作用了；我们创建的文件`tmptest`（我们没有展示创建文件、分配和初始化缓冲区等代码，请通过书的GitHub存储库查找），长度为222字节，尽管实际的数据内容（A、B和C）的长度为20+30+42=92字节。剩下的（222-92）130字节是文件中的三个空洞（长度分别为10+100+20字节；请查看代码中定义这些的宏）。`hexdump(1)`实用程序方便地转储了文件的内容；0x41代表A，0x42代表B，0x43代表C。空洞清楚地显示为我们想要的长度的空值填充区域。
- en: Discontiguous data file – the SG – I/O approach
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不连续的数据文件- SG-I/O方法
- en: 'The traditional approach using the `{lseek, write}` pair of system calls three
    times in succession worked, of course, but at a rather large performance penalty;
    the fact is, issuing system calls is considered very expensive. A far superior
    approach performance-wise is called *scatter-gather I/O* (SG-I/O, or vectored
    I/O). The relevant system calls are `readv(2)` and `writev(2)`; this is their
    signature:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，使用连续三次`{lseek, write}`系统调用的传统方法是有效的，但性能上存在相当大的惩罚；事实上，发出系统调用被认为是非常昂贵的。从性能上讲，一种更优越的方法是称为*分散-聚集I/O*（SG-I/O，或向量I/O）。相关的系统调用是`readv(2)`和`writev(2)`；这是它们的签名：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'These system calls allow you to specify a bunch of segments to read or write
    in one shot; each segment describes a single I/O operation via a structure called `iovec`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这些系统调用允许您一次性指定一堆要读取或写入的段；每个段通过称为`iovec`的结构描述一个单独的I/O操作：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The programmer can pass along an array of segments describing the I/O operations
    to perform; this is precisely the second parameter—a pointer to an array of struct
    iovecs; the third parameter is the number of segments to process. The first parameter
    is obvious—the file descriptor representing the file upon which to perform the
    gathered read or scattered write.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 程序员可以传递一个描述要执行的I/O操作的段数组；这正是第二个参数——指向struct iovecs数组的指针；第三个参数是要处理的段数。第一个参数很明显——表示要执行聚集读或分散写的文件描述符。
- en: 'So, think about it: you can gather together discontiguous reads from a given
    file into buffers (and their sizes) you specify via the I/O vector pointer, and
    you can scatter discontiguous writes to a given file from buffers (and their sizes)
    you specify via the I/O vector pointer; these types of multiple discontiguous
    I/O operations are thus called scatter-gather I/O! Here is the really cool part:
    the system calls are guaranteed to perform these I/O operations in array order
    and atomically; that is, they will return only when all operations are done. Again,
    though, watch out: the return value from `readv(2)` or `writev(2)` is the actual
    number of bytes read or written, and `-1` on failure. It''s always possible that
    an I/O operation performs less than the amount requested; this is not a failure,
    and it''s up to the developer to check.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，请考虑一下：您可以通过I/O向量指针将给定文件的不连续读取聚集到您指定的缓冲区（及其大小）中，也可以通过I/O向量指针将给定文件的不连续写入分散到您指定的缓冲区（及其大小）中；这些类型的多个不连续I/O操作因此称为scatter-gather
    I/O！这里是真正酷的部分：系统调用保证按数组顺序和原子方式执行这些I/O操作；也就是说，它们只有在所有操作完成时才会返回。不过，要注意：`readv(2)`或`writev(2)`的返回值是实际读取或写入的字节数，失败时为-1。始终有可能I/O操作执行的字节数少于请求的数量；这不是一个失败，开发人员需要检查。
- en: 'Now, for our earlier data file example, let''s look at the code that sets up
    and performs the discontiguous scattered ordered-and-atomic writes via `writev(2)`:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于我们之前的数据文件示例，让我们看一下通过`writev(2)`设置和执行不连续的分散有序和原子写入的代码：
- en: '[PRE9]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The end result is identical to that of the traditional approach; we leave it
    to the reader to try it out and see. This is the key point: the traditional approach
    had us issuing a minimum of six system calls (3 x `{lseek, write}` pairs) to perform
    the discontiguous data writes into the file, whereas the SG-I/O code performs
    the very same discontiguous data writes with just one system call. This results
    in significant performance gains, especially for applications under heavy I/O
    workloads.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果与传统方法相同；我们将其留给读者去尝试并查看。这是关键：传统方法要求我们发出至少六个系统调用（3 x `{lseek, write}`对）来执行对文件的不连续数据写入，而SG-I/O代码只需一个系统调用就可以执行相同的不连续数据写入。这将带来显著的性能提升，特别是对于I/O工作负载较重的应用程序。
- en: 'The interested reader, delving into the full source code of the previous example
    program (`ch18/sgio_simple.c`) will notice something that perhaps seems peculiar
    (or even just wrong): the blatant use of the controversial `goto` statement! The
    fact, though, is that the `goto` can be very useful in error handling—performing
    the code cleanup required when exiting a deep-nested path within a function due
    to failure. Please check out the links provided in the *Further reading* section
    on the GitHub repository for more. The Linux kernel community has been quite happily
    using the `goto` for a long while now; we urge developers to look into appropriate
    usage of the same.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前面示例程序（`ch18/sgio_simple.c`）的完整源代码感兴趣的读者可能会注意到一些奇怪的事情（甚至是错误的）：明目张胆地使用了备受争议的`goto`语句！事实上，`goto`在错误处理中非常有用——在由于失败而退出函数内部的深层嵌套路径时执行所需的代码清理。请查看GitHub存储库中*进一步阅读*部分提供的链接以获取更多信息。Linux内核社区已经很长时间以来一直很高兴地使用`goto`；我们敦促开发人员研究其适当的用法。
- en: SG – I/O variations
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SG - I/O变体
- en: 'Recall from the *MT app file I/O with the pread, pwrite APIs *section, we could
    use the `pread(2)` and `pwrite(2)` system calls to effectively perform file I/O
    in parallel via multiple threads (in a multithreaded app). Similarly, Linux provides
    the `preadv(2)` and the `pwritev(2)` system calls; as you can guess, they provide
    the functionality of the `readv(2)` and `writev(2)` with the addition of a fourth
    parameter offset; just as with the `readv(2)` and `writev(2)`, the file offset
    at which SG-IO is to be performed can be specified and it will not be changed
    (again, perhaps useful for an MT application). The signature of the `preadv(2)`
    and `pwritev(2)` is shown here:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下*MT app文件I/O与pread、pwrite APIs*部分，我们可以使用`pread(2)`和`pwrite(2)`系统调用通过多个线程有效地并行执行文件I/O（在多线程应用程序中）。类似地，Linux提供了`preadv(2)`和`pwritev(2)`系统调用；正如你猜到的那样，它们提供了`readv(2)`和`writev(2)`的功能，并增加了第四个参数offset；就像`readv(2)`和`writev(2)`一样，可以指定要执行SG-IO的文件偏移量，并且不会更改（再次，对于MT应用程序可能有用）。`preadv(2)`和`pwritev(2)`的签名如下所示：
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Recent Linux kernels (version 4.6 onward for some) also provide a further variation
    on the APIs: the `preadv2(2)` and the `pwritev2(2)` system calls. The difference
    from the previous APIs is that they take an additional fifth parameter flag allowing
    the developer to have more control over the behavior of the SG-I/O operations
    by being able to specify whether they are synchronous (via the RWF_DSYNC and the RWF_SYNC
    flags), high-priority (via the RWF_HIPRI flag), or non-blocking (via the RWF_NOWAIT
    flag). We refer the reader to the man page on `preadv2(2)`/ `pwritev2(2)` for
    details.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的Linux内核（某些版本从4.6开始）还提供了API的另一个变体：`preadv2(2)`和`pwritev2(2)`系统调用。与以前的API不同之处在于，它们接受一个额外的第五个参数flag，允许开发人员更多地控制SG-I/O操作的行为，可以指定它们是同步的（通过RWF_DSYNC和RWF_SYNC标志）、高优先级的（通过RWF_HIPRI标志）还是非阻塞的（通过RWF_NOWAIT标志）。我们建议读者查看`preadv2(2)`/`pwritev2(2)`的手册页面以获取详细信息。
- en: File I/O via memory mapping
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文件I/O通过内存映射
- en: Both in [Appendix A](https://www.packtpub.com/sites/default/files/downloads/File_IO_Essentials.pdf),
    *File I/O Essentials*, and in this chapter, we have on several occasions mentioned
    how the Linux kernel's page cache helps greatly enhance performance by caching
    the content of files within it (alleviating the need to each time go to the really
    slow storage device and instead just read or write data chunks within RAM). However,
    though we gain performance via the page cache, there remains a hidden problem
    with using both the traditional `read(2)`, `write(2)` APIs or even the faster
    SG-I/O (the `[p][read|write][v][2](2)`) APIs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在[附录A](https://www.packtpub.com/sites/default/files/downloads/File_IO_Essentials.pdf)中，*文件I/O基础*，以及本章中，我们已经多次提到Linux内核的*页面缓存*通过在其中缓存文件内容大大提高了性能（减轻了每次都需要访问真正缓慢的存储设备的需求，而是在RAM中只读取或写入数据块）。然而，尽管我们通过页面缓存获得了性能，但使用传统的`read(2)`、`write(2)`API或者更快的SG-I/O（`[p][read|write][v][2](2)`）API仍然存在一个隐藏的问题。
- en: The Linux I/O code path in brief
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Linux I/O代码路径简介
- en: 'To understand what the issue is, we must first gain a bit of a deeper understanding
    of how the I/O code path actually works; the following diagram encapsulates the
    points of relevance:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解问题所在，我们必须首先更深入地了解I/O代码路径的工作原理；以下图表概括了相关的要点：
- en: '![](img/ed260a42-11ae-4f52-bc88-a2347b755b7a.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ed260a42-11ae-4f52-bc88-a2347b755b7a.png)'
- en: 'Figure 1: Page cache populated with'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：页面缓存填充
- en: disk data
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 磁盘数据
- en: The reader should realize that though this diagram seems quite detailed, we're
    actually seeing a rather simplistic view of the entire Linux I/O code path (or
    I/O stack), only what is relevant to this discussion. For a more detailed overview
    (and diagram), please see the link provided in the *Further reading *section on
    the GitHub repository.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 读者应该意识到，尽管这个图表看起来相当详细，但我们实际上只是看到了整个Linux I/O代码路径（或I/O堆栈）的一个相当简化的视图，只有与本讨论相关的内容。有关更详细的概述（和图表），请参见GitHub存储库中*进一步阅读*部分提供的链接。
- en: 'Let''s say that a **Process P1** intends to read some 12 KB of data from a
    target file that it has open (via the `open(2)` system call); we envision that
    it does so via the usual manner:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 假设**进程P1**打算从它打开的目标文件（通过`open(2)`系统调用）中读取大约12KB的数据；我们设想它是通过通常的方式来做到这一点：
- en: Allocate a heap buffer of 12 KB (3 pages = 12,288 bytes) via the `malloc(3)`
    API.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`malloc(3)`API分配一个12KB的堆缓冲区（3页= 12,288字节）。
- en: Issue the `read(2)` system call to read in the data from the file into the heap
    buffer.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发出`read(2)`系统调用，将数据从文件读入堆缓冲区。
- en: The `read(2)` system call performs the work within the OS; when the read is
    done, it returns (hopefully the value `12,288`; remember, it's the programmer's
    job to check this and not assume anything).
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`read(2)`系统调用在操作系统中执行工作；当读取完成时，它返回（希望值为`12,288`；请记住，检查这一点是程序员的工作，不要假设任何东西）。'
- en: 'This sounds simple, but there''s a lot more that happens under the hood, and
    it is in our interest to dig a little deeper. Here''s a more detailed view of
    what happens (the numerical points **1**, **2**, and **3** are shown in a circle
    in the previous diagram; follow along):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来很简单，但在幕后发生了更多的事情，我们有兴趣深入挖掘一下。以下是更详细的视图（在前面的图表中，数字点**1**、**2**和**3**以圆圈的形式显示；请跟随）：
- en: '**Process P1** allocates a heap buffer of 12 KB via the `malloc(3)` API (len
    = 12 KB = 12,288 bytes).'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**进程P1**通过`malloc(3)`API分配了一个12KB的堆缓冲区（长度= 12KB = 12,288字节）。'
- en: Next, it issues a `read(2)` system call to read data from the file (specified
    by fd) into the heap buffer buf just allocated, for length 12 KB.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，它发出一个`read(2)`系统调用，从文件（由fd指定）中读取数据到刚刚分配的堆缓冲区buf中，长度为12KB。
- en: 'As `read(2)` is a system call, the process (or thread) now switches to kernel
    mode (remember the monolithic design we covered back in [Chapter 1](c17af8c2-a426-4ab6-aabb-aa1374e56cc4.xhtml),
    *Linux System Architecture*); it enters the Linux kernel''s generic filesystem
    layer (called the **Virtual Filesystem Switch** (**VFS**)), from where it will
    be auto-shunted on to its appropriate underlying filesystem driver (perhaps the
    ext4 fs), after which the Linux kernel will first check: are these pages of the
    required file data already cached in our page cache? If yes, the job is done,
    (we short circuit to *step 7*), just copy back the pages to the user space buffer.
    Let''s say we get a cache miss—the required file data pages aren''t in the page
    cache.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于`read(2)`是一个系统调用，进程（或线程）现在切换到内核模式（记得我们在[第1章](c17af8c2-a426-4ab6-aabb-aa1374e56cc4.xhtml)中讨论过的单olithic设计吗？），它进入Linux内核的通用文件系统层（称为**虚拟文件系统开关**（**VFS**）），然后将自动转移到适当的底层文件系统驱动程序（也许是ext4
    fs），之后Linux内核首先检查：所需文件数据的这些页面是否已经缓存在我们的页面缓存中？如果是，工作就完成了（我们直接跳到*步骤7*），只需将页面复制回用户空间缓冲区。假设我们遇到了缓存未命中-所需的文件数据页面不在页面缓存中。
- en: Thus, the kernel first allocates sufficient RAM (page frames) for the page cache
    (in our example, three frames, shown as pink squares within the page cache memory
    region). It then fires off appropriate I/O requests to the underlying layers requesting
    the file data.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，内核首先为页面缓存分配足够的RAM（页面框架）（在我们的示例中，三个框架，显示为页面缓存内存区域中的粉色方块）。然后，它向底层层发出适当的I/O请求，请求文件数据。
- en: The request ultimately ends up at the block (storage) driver; we assume it knows
    its job and reads the required data blocks from the underlying storage device
    controller (a disk or flash controller chip, perhaps). It then (here's the interesting
    thing) is given a destination address to write the file data to; it's the address
    of the page frames allocated (step 4) within the page cache; thus, the block driver
    always writes the file data into the kernel's page cache and never directly back
    to the user mode process buffers.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请求最终到达块（存储）驱动程序；我们假设它知道自己的工作，并从底层存储设备控制器（磁盘或闪存控制器芯片，也许）读取所需的数据块。然后（有趣的是）给出一个目标地址来写入文件数据；这是页面缓存内分配的页面框架的地址；因此，块驱动程序总是将文件数据写入内核的页面缓存，而不是直接写回用户模式进程缓冲区。
- en: The block driver has successfully copied the data blocks from the storage device
    (or whatever) into the previously allocated frames within the kernel page cache.
    (In reality, these data transfers are highly optimized via an advanced memory
    transfer technique called **Direct Memory Access** (**DMA**), wherein, essentially,
    the driver exploits the hardware to directly transfer data to and from the device
    and system memory without the CPU's intervention. Obviously, these topics are
    well beyond the scope of this book.)
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 块驱动程序已成功地将数据块从存储设备（或其他设备）复制到内核页缓存中先前分配的帧中。（实际上，这些数据传输是通过一种称为**直接内存访问**（**DMA**）的高级内存传输技术进行高度优化的，在这种技术中，驱动程序利用硬件直接在设备和系统内存之间传输数据，而无需CPU的干预。显然，这些话题远远超出了本书的范围。）
- en: The just-populated kernel page cache frames are now copied into the user space
    heap buffer by the kernel.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 刚刚填充的内核页缓存帧现在由内核复制到用户空间堆缓冲区。
- en: The (blocking) `read(2)` system call now terminates, returning the value 12,288
    indicating that all three pages of file data have indeed been transferred (again,
    you, the app developer, are supposed to check this return value and not assume
    anything).
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （阻塞的）`read(2)`系统调用现在终止，返回值为12,288，表示文件数据的三个页面确实已经被传输（再次强调，您作为应用程序开发人员应该检查此返回值，而不是假设任何内容）。
- en: 'It''s all looking great, yes? Well, not really; think carefully on this: though
    the `read(2)` (or `pread[v][2](2)`) API did succeed, this success came at a considerable
    price: the kernel had to allocate RAM (page frames) in order to hold the file
    data within its page cache (step 4) and, once data transfer was done (step 6)
    then copied that content into the user space heap memory (step 7). Thus, we have
    used twice the amount of RAM that we should have by keeping an extra copy of the
    data. This is highly wasteful, and, obviously, the multiple copying around of
    the data buffers between the block driver to the kernel page cache and then the
    kernel page cache to the user space heap buffer, reduces performance as well (not
    to mention that the CPU caches get unnecessarily caught up with all this trashing
    their content). With the previous pattern of code, the issue of not waiting for
    the slow storage device is taken care of (via the page cache efficiencies), but
    everything else is really poor—we have actually doubled the required memory usage
    and the CPU caches are overwritten with (unnecessary) file data while copying
    takes place.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来一切都很好，是吗？实际上并不是；仔细考虑一下：尽管`read(2)`（或`pread[v][2](2)`）API确实成功了，但这种成功是以相当大的代价为代价的：内核必须分配RAM（页面帧）以在其页缓存中保存文件数据（步骤4），一旦数据传输完成（步骤6），然后将该内容复制到用户空间堆内存（步骤7）。因此，我们使用了应该使用的两倍RAM来保留数据的额外副本。这是非常浪费的，显然，数据缓冲区在块驱动程序和内核页缓存之间以及内核页缓存和用户空间堆缓冲区之间的多次复制也会降低性能（更不用说CPU缓存不必要地被这些内容占用）。通过以前的代码模式，解决了不等待慢存储设备的问题（通过页缓存的效率），但其他方面都非常糟糕——我们实际上将所需的内存使用量加倍了，而且在复制过程中CPU缓存被（不必要的）文件数据覆盖。
- en: Memory mapping a file for I/O
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为I/O映射文件
- en: Here is a solution to these issues: memory mapping via the  `mmap(2)` system
    call. Linux provides the very powerful `mmap(2)` system call; it enables the developer
    to map any content directly into the process  **virtual address space** (**VAS**).
    This content includes file data, hardware device (adapter) memory regions, or
    just generic memory regions. In this chapter, we shall only focus on using  `mmap(2)`
    to map in a regular file's content into the process VAS. Before getting into how the
    `mmap(2)` becomes a solution to the memory wastage issue we just discussed, we
    first need to understand more about using the `mmap(2)` system call itself.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是解决这些问题的方法：通过`mmap(2)`系统调用进行内存映射。Linux提供了非常强大的`mmap(2)`系统调用；它使开发人员能够将任何内容直接映射到进程的虚拟地址空间（VAS）。这些内容包括文件数据、硬件设备（适配器）内存区域或通用内存区域。在本章中，我们将只关注使用`mmap(2)`将常规文件的内容映射到进程的VAS。在深入讨论`mmap(2)`如何成为我们刚刚讨论的内存浪费问题的解决方案之前，我们首先需要更多地了解如何使用`mmap(2)`系统调用本身。
- en: 'The signature of the `mmap(2)` system call is shown here:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`mmap(2)`系统调用的签名如下所示：'
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We want to map a given region of a file, from a given `offset` and for `length` bytes
    into our process VAS; a simplistic view of what we want to achieve is depicted
    in this diagram:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将文件的给定区域，从给定的`offset`开始，映射到我们的进程VAS中的`length`字节；我们希望实现的简单视图如下图所示：
- en: '![](img/61b1e79e-6de8-4622-a1f7-3ee3b24b376e.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61b1e79e-6de8-4622-a1f7-3ee3b24b376e.png)'
- en: 'Figure 2: Memory mapping a file region into process VAS'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：将文件区域映射到进程虚拟地址空间
- en: 'To achieve this file mapping to process VAS, we use the `mmap(2)` system call.
    Glancing at its signature, it''s quite obvious what we need to do first: open
    the file to be mapped via the `open(2)` (in the appropriate mode: read-only or
    read-write, depending on what you want to do), thereby obtaining a file descriptor;
    pass this descriptor as the fifth parameter to   `mmap(2)`. The file region to
    be mapped into the process VAS can be specified via the sixth and second parameters
    respectively—the file `offset` at which the mapping should begin and the `length`
    (in bytes).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将文件映射到进程VAS，我们使用`mmap(2)`系统调用。从其签名可以看出，我们首先需要做的是：通过`open(2)`打开要映射的文件（以适当的模式：只读或读写，取决于您想要做什么），从而获得文件描述符；将此描述符作为第五个参数传递给`mmap(2)`。要映射到进程VAS的文件区域可以分别通过第六个和第二个参数指定——映射应该从哪个文件`offset`开始以及`length`（以字节为单位）。
- en: The first parameter, `addr`, is a hint to the kernel as to where in the process
    VAS the mapping should be created; the recommendation is to pass `0` (NULL) here,
    allowing the OS to decide the location of the new mapping. This is the correct
    portable way to use the `mmap(2)`; however, some applications (and, yes, some
    malicious security hacks too!) use this parameter to try to predict where the
    mapping will occur. In any case, the actual (virtual) address where the mapping
    is created within the process VAS is the return value from the  `mmap(2)`; a NULL
    return indicates failure and must be checked for.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数`addr`是对内核的提示，指示在进程VAS中应该创建映射的位置；建议在这里传递`0`（NULL），允许操作系统决定新映射的位置。这是使用`mmap(2)`的正确可移植方式；然而，一些应用程序（是的，一些恶意的安全黑客也是如此！）使用此参数来尝试预测映射将发生的位置。无论如何，映射在进程VAS中创建的实际（虚拟）地址是`mmap(2)`的返回值；NULL返回表示失败，必须进行检查。
- en: 'Here is an interesting technique to fix the location of the mapping: first
    perform a  `malloc(3)` of the required mapping size and pass the return value
    from this `malloc(3)` to the `mmap(2)`''s first parameter (also set the flags parameter
    to include the MAP_FIXED bit)! This will probably work if the length is above
    MMAP_THRESHOLD (128 KB by default) and the size is a multiple of the system page
    size. Note, again, this technique is not portable and may or may not work.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有趣的技术，用于修复映射的位置：首先执行所需映射大小的`malloc(3)`，并将此`malloc(3)`的返回值传递给`mmap(2)`的第一个参数（还要设置flags参数以包括MAP_FIXED位）！如果长度超过MMAP_THRESHOLD（默认为128
    KB）并且大小是系统页面大小的倍数，则这可能有效。再次注意，这种技术不具有可移植性，可能有效也可能无效。
- en: Another point to note is that most mappings—and always file mappings—are performed
    to page granularity, that is, in multiples of the page size; thus, the return
    address is usually page-aligned.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个要注意的是，大多数映射（始终是文件映射）都是以页面大小的倍数进行的；因此，返回地址通常是页面对齐的。
- en: 'The third parameter to `mmap(2)` is an integer bitmask `prot`—the memory protections
    of the given region (recall we have already come across memory protections in [Chapter
    4](0b4868f7-a8d0-4ced-831f-20af9929de9f.xhtml), *Dynamic Memory Allocation*, in
    the *Memory protection* section). The `prot` parameter is a bitmask and can either
    be just the `PROT_NONE` bit (implying no permissions) or the bitwise OR of the
    remainder; this table enumerates the bits and their meaning:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`mmap(2)`的第三个参数是一个整数位掩码`prot`——给定区域的内存保护（回想一下我们已经在[第4章](0b4868f7-a8d0-4ced-831f-20af9929de9f.xhtml)的*Dynamic
    Memory Allocation*中的*Memory protection*部分遇到的内存保护）。`prot`参数是一个位掩码，可以是只有`PROT_NONE`位（意味着没有权限）或其余位的按位或；这个表列举了位及其含义：'
- en: '| **Protection bit** | **Meaning** |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| **保护位** | **含义** |'
- en: '| `PROT_NONE` | No access allowed on the page(s) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| `PROT_NONE` | 页面上不允许访问 |'
- en: '| `PROT_READ` | Reads allowed on the page(s) |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '`PROT_READ` 读取页面允许'
- en: '| `PROT_WRITE` | Writes allowed on the page(s) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| `PROT_WRITE` | 页面上允许写入 |'
- en: '| `PROT_EXEC` | Execute access allowed on the page(s) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| `PROT_EXEC` | 页面上允许执行访问 |'
- en: mmap(2) protection bits
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: mmap(2)保护位
- en: 'The page protections must match those of the file''s `open(2)`, of course.
    Also note that, on older x86 systems, writable memory used to imply readable memory
    (that is, `PROT_WRITE => PROT_READ`). This is no longer the case; you must explicitly specify
    whether the mapped pages are readable or not (the same holds true for executable
    pages too: it must be specified, the text segment being the canonical example).
    Why would you use PROT_NONE? A guard page is one realistic example (recall the *Stack
    guards *section from [Chapter 14](586f3099-3953-4816-8688-490c9cf2bfd7.xhtml),
    *Multithreading with Pthreads Part I - Essentials*).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 页面的保护必须与文件的`open(2)`相匹配。还要注意，在旧的x86系统上，可写内存意味着可读内存（即`PROT_WRITE => PROT_READ`）。这不再适用；必须明确指定映射的页面是否可读（可执行页面也是如此：必须指定，文本段是典型示例）。为什么要使用`PROT_NONE`？一个现实的例子是*guard
    page*（回想一下[第14章](586f3099-3953-4816-8688-490c9cf2bfd7.xhtml)的*Stack guards*部分，*使用Pthreads的多线程Part
    I - Essentials*）。
- en: File and anonymous mappings
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文件和匿名映射
- en: The next point to understand is that there are broadly two types of mappings;
    a file-mapped region or an anonymous region. A file-mapped region quite obviously
    maps the (full, or partial) content of a file (as shown in the previous figure).
    We think of the region as being backed by a file; that is, if the OS runs short
    of memory and decides to reclaim some of the file-mapped pages, it need not write
    them to the swap partition—they're already available within the file that was
    mapped. On the other hand, an anonymous mapping is a mapping whose content is
    dynamic; the data segments (initialized data, BSS, heap), the data sections of
    library mappings, and the process (or thread) stack(s) are excellent examples
    of anonymous mappings. Think of them as not being file-backed; thus, if memory
    runs short, their pages may indeed be written to swap by the OS. Also, recall
    what we learned back in [Chapter 4](0b4868f7-a8d0-4ced-831f-20af9929de9f.xhtml), *Dynamic
    Memory Allocation*, regarding the `malloc(3)`; the fact is that the glibc `malloc(3)`
    engine uses the heap segment to service the allocation only when it's for a small
    amount—less than MMAP_THRESHOLD (defaults to 128 KB). Any `malloc(3)` above that
    will result in `mmap(2)` being internally invoked to set up an anonymous memory
    region—a mapping!—of the required size. These mappings (or segments) will live
    in the available virtual address space between the top of the heap and the stack
    of main.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个要理解的要点是，有广泛两种类型的映射；文件映射区域或匿名区域。文件映射区域很明显地映射了文件的（全部或部分）内容（如前面的图所示）。我们认为该区域由文件支持；也就是说，如果操作系统内存不足并决定回收一些文件映射的页面，它不需要将它们写入交换分区——它们已经在映射的文件中可用。另一方面，匿名映射是内容动态的映射；初始化数据段、BSS、堆的数据段，库映射的数据部分以及进程（或线程）的堆栈都是匿名映射的绝佳例子。将它们视为没有文件支持；因此，如果内存不足，它们的页面可能确实被操作系统写入交换分区。还要记得我们在[第4章](0b4868f7-a8d0-4ced-831f-20af9929de9f.xhtml)中学到的关于`malloc(3)`的内容；事实上，glibc的`malloc(3)`引擎仅在分配小额时（默认为128
    KB以下）才使用堆段来提供分配。超过这个值的任何`malloc(3)`都将导致内部调用`mmap(2)`来设置所需大小的匿名内存区域——映射！这些映射（或段）将存在于堆的顶部和主栈之间的可用虚拟地址空间中。
- en: 'Back to the `mmap(2)`: the fourth parameter is a bitmask called `flags`; there
    are several flags, and they affect many attributes of the mapping. Among them,
    two flags determine the privacy of the mapping and are mutually exclusive (you
    can only use any one of them at a time):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 回到`mmap(2)`：第四个参数是一个称为`flags`的位掩码；有几个标志，它们影响映射的许多属性。其中，两个标志确定了映射的私密性，并且彼此互斥（一次只能使用其中任何一个）：
- en: '**MAP_SHARED**: The mapping is a shared one; other processes might work on
    the same mapping simultaneously (this, in fact, is the generic manner in which
    a common IPC mechanism—shared memory —can be implemented). In the case of a file
    mapping, if the memory region is written to, the underlying file is updated! (You
    can use the `msync(2)` to control the flushing of in-memory writes to the underlying
    file.)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAP_SHARED：映射是共享的；其他进程可能同时在同一映射上工作（实际上，这是实现常见IPC机制——共享内存的通用方式）。在文件映射的情况下，如果对内存区域进行写入，底层文件将被更新！（您可以使用`msync(2)`来控制将内存中的写入刷新到底层文件。）
- en: '**MAP_PRIVATE**: This sets up a private mapping; if it''s writable, it implies
    COW  semantics (leading to optimal memory usage, as explained in [Chapter 10](607ad988-406d-4736-90a4-3a318672ab6e.xhtml),
    *Process Creation*). A file-mapped region that is private will not carry through
    writes to the underlying file. Actually, a private file-mapping is very common
    on Linux: this is precisely how, at the time of starting to execute a process,
    the loader (see the information box) brings in the text and data of both the binary
    executable as well as the text and data of all shared libraries that the process
    uses.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MAP_PRIVATE：这设置了一个私有映射；如果可写，它意味着COW语义（导致最佳内存使用，如[第10章](607ad988-406d-4736-90a4-3a318672ab6e.xhtml)中所解释的，*进程创建*）。私有的文件映射区域不会将写入传递到底层文件。实际上，在Linux上私有文件映射是非常常见的：这正是在开始执行进程时，加载器（见信息框）如何将二进制可执行文件的文本和数据以及进程使用的所有共享库的文本和数据带入的方式。
- en: 'The reality is that when a process runs, control first goes to a program embedded
    into your `a.out` binary executable—the loader (`ld.so` or `ld-linux[-*].so`).
    It performs the key work of setting up the C runtime environment: it memory maps (via
    the `mmap(2)`) the text (code) and initialized data segments from the binary executable
    file into the process, thereby creating the segments in the VAS that we have been
    talking about since [Chapter 2](976fc2af-8bb4-4060-96cd-3b921682ed75.xhtml), *Virtual
    Memory*. Further, it sets up the initialized data segment, the BSS, the heap,
    and the stack (of `main()`), and then it looks for and memory maps all shared
    libraries into the process VAS.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，当一个进程运行时，控制首先转到嵌入到您的`a.out`二进制可执行文件中的程序——加载器（`ld.so`或`ld-linux[-*].so`）。它执行设置C运行时环境的关键工作：它通过`mmap(2)`将二进制可执行文件中的文本（代码）和初始化数据段映射到进程中，从而在VAS中创建我们自从[第2章](976fc2af-8bb4-4060-96cd-3b921682ed75.xhtml)以来一直在谈论的段。此外，它设置了初始化数据段、BSS、堆和`main()`的栈，然后寻找并将所有共享库内存映射到进程VAS中。
- en: 'Try performing a `strace(1)` on a program; you will see (early in the execution)
    all the `mmap(2)` system calls setting up the process VAS! The `mmap(2)` is critical
    to Linux: in effect, the entire setup of the process VAS, the segments or mappings—both
    at process startup as well as later—are all done via the `mmap(2)` system call.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试对程序执行`strace(1)`；您将看到（在执行早期）所有`mmap(2)`系统调用设置进程VAS！`mmap(2)`对Linux至关重要：实际上，进程VAS的整个设置，包括进程启动时和以后的段或映射，都是通过`mmap(2)`系统调用完成的。
- en: 'To help get these important facts clear, we show some (truncated) output of
    running  `strace(1)` upon `ls(1)`; (for example) see how the `open(2)` is done
    upon glibc, file descriptor 3 is returned, and that in turn is used by the `mmap(2)`
    to create a private file-mapped read-only mapping of glibc''s code (we can tell
    by seeing that the offset in the first `mmap` is `0`) in the process VAS! (A detail:
    the `open(2)` becomes the `openat(2)` function within the kernel; ignore that,
    just as quite often on Linux, the `mmap(2)` becomes `mmap2(2)`.) The `strace(1)`
    (truncated) output follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助读者清楚这些重要事实，我们展示了运行`strace(1)`对`ls(1)`的（截断的）输出；（例如）看看`open(2)`是如何在glibc上执行的，返回文件描述符3，然后`mmap(2)`使用它创建glibc代码的私有文件映射只读映射（我们可以通过看到第一个`mmap`中的偏移量为`0`来判断）。`strace(1)`（截断的）输出如下：
- en: '[PRE12]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The kernel maintains a data structure called the **virtual memory area** (**VMA**)
    for each such mapping per process; the proc filesystem reveals all mappings to
    us in user space via `/proc/PID/maps`. Do take a look; you will literally see
    the virtual memory map of the process user space. (Try `sudo cat /proc/self/maps` to
    see the map of the cat process itself.) The man page on `proc(5)` explains in
    detail how to interpret this map; please take a look.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 内核为每个进程的每个这样的映射维护一个称为**虚拟内存区域**（**VMA**）的数据结构；proc文件系统通过`/proc/PID/maps`向我们展示所有映射。请看一下；您将在进程用户空间中实际看到虚拟内存映射。（尝试`sudo cat
    /proc/self/maps`查看cat进程本身的映射。）`proc(5)`手册详细解释了如何解释这个映射；请查看。
- en: The mmap advantage
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: mmap优势
- en: 'Now that we understand how to use the `mmap(2)` system call, we revisit our
    earlier discussion: recall, using the `read(2)`/`write(2)` or even the SG-I/O
    type APIs (the `[p]readv|writev[2](2)`) resulted in a double-copy; memory wastage
    (plus the fact that CPU caches get trashed as well).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了如何使用`mmap(2)`系统调用，我们重新讨论了之前的讨论：回想一下，使用`read(2)`/`write(2)`甚至SG-I/O类型的API（`[p]readv|writev[2](2)`）会导致双重拷贝；内存浪费（还有CPU缓存也会被清空）。
- en: 'The key to realizing why the `mmap(2)` so effectively solves this serious issue
    is this: the `mmap(2)` sets up a file mapping by internally mapping the kernel
    page caches pages that contain the file data (that was read in from the storage
    device) directly into the process virtual address space. This diagram (*Figure
    3*) puts this into perspective (and makes it self-explanatory):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 理解`mmap(2)`如此有效地解决这个严重问题的关键在于：`mmap(2)`通过内部映射包含文件数据（从存储设备中读取的数据）的内核页缓存页面，直接映射到进程虚拟地址空间。这个图表（*图3*）将这一点放入了透视图中（并使其不言自明）：
- en: '![](img/17d428b0-e611-49dc-9c50-908fa2bc0d98.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/17d428b0-e611-49dc-9c50-908fa2bc0d98.png)'
- en: 'Figure 3: Page cache populated with'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：页面缓存填充
- en: disk data
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 磁盘数据
- en: A mapping is not a copy; thus `mmap(2)`-based file I/O is called a zero-copy
    technique: a way of performing work on an I/O buffer of which exactly one copy
    is maintained by the kernel in it's page cache; no more copies are required.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 映射不是复制；因此，基于`mmap(2)`的文件I/O被称为零拷贝技术：一种在内核的页面缓存中维护的I/O缓冲区上执行工作的方式；不需要更多的拷贝。
- en: The fact is that the device driver authors look to optimize their data path
    using zero-copy techniques, of which the `mmap(2)` is certainly a candidate. See
    more on this interesting advanced topic within links provided in the *Further
    reading* section on the GitHub repository.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，设备驱动程序作者寻求使用零拷贝技术优化其数据路径，其中`mmap(2)`当然是一个候选者。在GitHub存储库的*进一步阅读*部分提供了有关这个有趣的高级主题的更多信息。
- en: 'The `mmap(2)` does incur significant overhead in setting up the mapping (the
    first time), but, once done, I/O is very quick, as it is essentially performed
    in memory. Think about it: to seek to a location within the file and perform I/O
    there, just use your regular ''C'' code to move to a given location from the `mmap(2)`
    return value (it''s just a pointer offset) and do the I/O work in memory itself
    (via the `memcpy(3)`, `s[n]printf(3)`, or whatever you prefer); no `lseek(2)`,
    no `read(2)`/`write(2)`, or SG-I/O system call overheads at all. Using the `mmap(2)`
    for very small amounts of I/O work may not be optimal; it''s usage is recommended
    when large and continuous I/O workloads are indicated.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`mmap(2)`在设置映射时确实会产生显著的开销（第一次），但一旦完成，I/O速度非常快，因为它基本上是在内存中执行的。想想看：要在文件中寻找位置并在那里执行I/O，只需使用常规的''C''代码从`mmap(2)`的返回值（它只是一个指针偏移量）移动到给定位置，并在内存中进行I/O工作（通过`memcpy(3)`、`s[n]printf(3)`或您喜欢的其他方法）；完全没有`lseek(2)`、`read(2)`/`write(2)`或SG-I/O系统调用开销。对于非常小的I/O工作量，使用`mmap(2)`可能不是最佳选择；建议在指示大量和连续的I/O工作负载时使用它。'
- en: Code example
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码示例
- en: To aid the reader in working with the `mmap(2)` for the purpose of file I/O,
    we have provided the code of a simple application; it memory maps a given file
    (the file's pathname, start offset, and length are provided as parameters) via
    the `mmap(2)` and hexdumps (using a, slightly enhanced, open source `hexdump`
    function) the memory region specified on to `stdout`. We urge the reader to look
    up the code, build, and try it out.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助读者使用`mmap(2)`进行文件I/O，我们提供了一个简单应用程序的代码；它通过`mmap(2)`内存映射给定文件（文件路径名、起始偏移量和长度作为参数提供），并将指定的内存区域的十六进制转储（使用略微增强的开源`hexdump`函数）到`stdout`。我们敦促读者查看代码，构建并尝试运行它。
- en: 'The complete source code for this book is available for cloning from GitHub
    here: [https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux](https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux).
    The aforementioned program is here within the source tree: `ch18/mmap_file_simple.c`.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的完整源代码可以从GitHub克隆到这里：[https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux](https://github.com/PacktPublishing/Hands-on-System-Programming-with-Linux)。前述程序在源代码树中的位置是：`ch18/mmap_file_simple.c`。
- en: Memory mapping – additional points
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存映射-额外要点
- en: 'A quick summation of a few additional points to wrap up the memory mapping
    discussion follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些额外要点的快速总结，以结束内存映射的讨论：
- en: The fourth parameter to `mmap(2)`, `flags`, can take on several other (quite
    interesting) values; we refer the reader to the man page on `mmap(2)` to browse
    through them: [http://man7.org/linux/man-pages/man2/mmap.2.html](http://man7.org/linux/man-pages/man2/mmap.2.html).
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mmap(2)`的第四个参数`flags`可以采用其他几个（非常有趣的）值；我们建议读者查阅`mmap(2)`的man页面以浏览这些值：[http://man7.org/linux/man-pages/man2/mmap.2.html](http://man7.org/linux/man-pages/man2/mmap.2.html)。'
- en: Directly analogous to how we can give hints or advice to the kernel regarding
    kernel page cache pages with the `posix_fadvise(2)` API, you can provide similar
    hints or advice to the kernel regarding memory usage patterns for a given memory
    range (start address, length provided) via the `posix_madvise(3)` library API.
    The advice values include being able to say that we expect random access to data
    (thereby reducing read-ahead, via the `POSIX_MADV_RANDOM` bit), or that we expect
    to access data in the specified range soon (via the `POSIX_MADV_WILLNEED` bit,
    resulting in more read-ahead and mapping). This routine invokes the underlying
    system call `madvise(2)` on Linux.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与我们可以通过`posix_fadvise(2)`API向内核提供关于内核页缓存页面的提示或建议类似，您可以通过`posix_madvise(3)`库API向内核提供关于给定内存范围的内存使用模式的类似提示或建议（提供起始地址、长度）。建议值包括能够说我们期望对数据进行随机访问（从而通过`POSIX_MADV_RANDOM`位减少预读取），或者我们期望很快访问指定范围内的数据（通过`POSIX_MADV_WILLNEED`位，导致更多的预读取和映射）。此例程在Linux上调用底层系统调用`madvise(2)`。
- en: Let's say we have mapped a region of a file into our process address space;
    how do we know which pages of the mapping are currently residing in the kernel
    page (or buffer) cache? Precisely this can be determined via the `mincore(2)`
    system call (read as "m-in-core").
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设我们已经将文件的某个区域映射到我们的进程地址空间中；我们如何知道映射的哪些页面当前驻留在内核页（或缓冲）缓存中？可以通过`mincore(2)`系统调用（读作“m-in-core”）精确确定这一点。
- en: The programmer has explicit (and fine-tuned) control over synchronizing (flushing)
    file-mapped regions (back to the file) via the `msync(2)` system call.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 程序员可以通过`msync(2)`系统调用显式（和精细调整的）控制同步（刷新）文件映射区域（返回到文件）。
- en: Once complete, the memory mapping should be unmapped via the `munmap(2)` system
    call; the parameters are the base address of the mapping (the return value from
    `mmap(2)`) and the length. If the process terminates, the mapping is implicitly
    unmapped.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完成后，应通过`munmap(2)`系统调用取消内存映射；参数是映射的基地址（从`mmap(2)`返回的值）和长度。如果进程终止，映射将被隐式取消。
- en: On `fork(2)`, a memory mapping is inherited by the child process.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`fork(2)`中，内存映射被子进程继承。
- en: What if an enormous file is memory mapped, and at runtime when allocating page
    frames to hold the mapping in the process VAS (recall our discussion on demand-paging from [Chapter
    4](0b4868f7-a8d0-4ced-831f-20af9929de9f.xhtml), *Dynamic Memory Allocation*),
    the system runs out of memory (drastic, but it could occur); in cases such as
    these, the process will receive the `SIGSEGV` signal (and thus it's up to the
    app's signal-handling ability to gracefully terminate).
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果映射了一个巨大的文件，并且在运行时分配页面帧以在进程VAS中保存映射时，系统耗尽了内存（这是极端的，但可能发生）；在这种情况下，进程将收到`SIGSEGV`信号（因此，这取决于应用程序的信号处理能力是否能够优雅地终止）。
- en: DIO and AIO
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DIO和AIO
- en: 'A significant downside of using both the blocking `[p]read[v](2)` / `[p]write[v](2)`
    APIs as well as the `mmap(2)` (actually much more so with the `mmap`) is this:
    they depend on the kernel page cache always being populated with the file''s pages
    (that it''s working upon or mapping). If this is not the case—which can happen
    when the data store is much larger than RAM size (that it, files can be enormous)—it
    will result in a lot of meta-work by the kernel **memory management** (**mm**)
    code to bring in pages from disk to page cache, allocating frames, stitching up
    page table entries for them, and so on. Thus, the `mmap` technique works best
    when the ratio of RAM to storage is as close to 1:1 as possible. When the storage
    size is much larger than the RAM (often the case with enterprise-scale software
    such as databases, cloud virtualization at scale, and so on), it can suffer from
    latencies caused by all the meta work, plus the fact that significant amounts
    of memory will be used for paging metadata.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用阻塞的`[p]read[v](2)` / `[p]write[v](2)` API以及`mmap(2)`（实际上更多地是使用`mmap`）的一个重要缺点是：它们依赖于内核页缓存始终填充有文件的页面（它正在处理或映射）。如果不是这种情况——当数据存储远大于RAM大小时（也就是说，文件可能非常庞大）——它将导致内核**内存管理**（**mm**）代码进行大量的元工作，从磁盘中带入页面到页缓存，分配帧，为它们编制页表条目等等。因此，当RAM与存储的比率尽可能接近1:1时，`mmap`技术效果最好。当存储大小远大于RAM时（通常是数据库、云虚拟化等大规模软件的情况），它可能会因为所有元工作而导致延迟，再加上大量的内存将用于分页元数据。
- en: Two I/O technologies—DIO and AIO—alleviate these issues (at the cost of complexity);
    we provide a brief note on them next. (Due to space constraints, we focus on the
    conceptual side of these topics; learning to use the relevant APIs is then a relatively
    easy task. Do refer to the *Further reading* section on the GitHub repository.)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 两种I/O技术——DIO和AIO——缓解了这些问题（以复杂性为代价）；我们接下来简要介绍它们。（由于空间限制，我们将重点放在这些主题的概念方面；学习使用相关API实际上是一个相对容易的任务。请参考GitHub存储库上的*进一步阅读*部分。）
- en: Direct I/O (DIO)
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直接I/O（DIO）
- en: An interesting I/O technology is **Direct I/O** (**DIO**); to use it, specify
    the `O_DIRECT` flag when opening the file via the `open(2)` system call.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的I/O技术是**直接I/O**（**DIO**）；要使用它，在通过`open(2)`系统调用打开文件时指定`O_DIRECT`标志。
- en: With DIO, the kernel page cache is completely bypassed, thereby immediately
    giving the benefit that all the issues that can be faced with the `mmap` technique
    now disappear. On the other hand, this does imply that the entire cache management
    is to be completely handled by the user space app (large projects such as databases
    would certainly require caching!). For regular small apps with no special I/O
    requirements, using DIO will likely degrade performance; be careful, test your
    workload under stress, and determine whether to use DIO or skip it.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DIO，内核页缓存完全被绕过，因此立即获得了使用`mmap`技术可能面临的所有问题的好处。另一方面，这意味着整个缓存管理完全由用户空间应用程序处理（像数据库这样的大型项目肯定需要缓存！）。对于没有特殊I/O要求的常规小型应用程序，使用DIO可能会降低性能；要小心，对工作负载进行压力测试，并确定是否使用DIO或跳过它。
- en: 'Traditionally, the kernel handles which pieces of I/O (the I/O requests) are
    serviced when—in other words, I/O scheduling (it''s not directly related, but
    also see the section on *I/O schedulers*). With DIO (and with AIO, seen next),
    the application developer can essentially take over I/O scheduling by determining
    when to perform I/O. This can be both a blessing and a curse: it provides the
    flexibility to the (sophisticated) app developer to design and implement I/O scheduling,
    but this is not a trivial thing to perform well; as usual, it''s a trade-off.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，内核处理哪些I/O请求在何时服务——换句话说，I/O调度（虽然与此不直接相关，但也请参阅*I/O调度器*部分）。使用DIO（以及接下来要介绍的AIO），应用程序开发人员可以基本上接管I/O调度，决定何时执行I/O。这既是一种福音，也是一种诅咒：它为（复杂的）应用程序开发人员提供了灵活性，可以设计和实现I/O调度，但这并不是一件容易做好的事情；像往常一样，这是一种权衡。
- en: Also, you should realize that though we call the I/O path direct, it does not
    guarantee that writes are immediately flushed to the underlying storage medium;
    that's a separate feature, one that can be requested by specifying the `O_SYNC`
    flag to the `open(2)` or of course explicitly flushing (via the `[f]sync(2)` system
    calls).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你应该意识到，尽管我们称I/O路径是直接的，但这并不保证写入会立即刷新到底层存储介质；这是一个单独的特性，可以通过在`open(2)`中指定`O_SYNC`标志或显式刷新（通过`[f]sync(2)`系统调用）来请求。
- en: Asynchronous I/O (AIO)
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异步I/O（AIO）
- en: '**Asynchronous I/O** (**AIO**) is a modern high-performance asynchronous non-blocking
    I/O technology that Linux implements. Think about it: non-blocking and asynchronous
    implies that an application thread can issue a read (for file or network data);
    the usermode API returns immediately; the I/O is queued up within the kernel;
    the application thread can continue working on CPU-bound stuff; once the I/O request
    completes, the kernel notifies the thread that the read is ready; the thread then
    actually performs the read. This is high-performance—the app does not remain blocked
    on I/O and can instead perform useful work while the I/O request is processed;
    not only that, it is asynchronously notified when the I/O work is done. (On the
    other hand, the multiplexing APIs such as `select(2)`, `poll(2)`, and `epoll(7)` are
    asynchronous—you can issue the system call and return immediately—but they actually
    are still blocking in nature because the thread must check for I/O completion—for
    example, by using the `poll(2)` in tandem with a `read(2)` system call when it
    returns—which is still a blocking operation.)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 异步I/O（AIO）是Linux实现的一种现代高性能的异步非阻塞I/O技术。想象一下：非阻塞和异步意味着应用程序线程可以发出读取（文件或网络数据）的请求；用户模式API立即返回；I/O在内核中排队；应用程序线程可以继续在CPU密集型任务上工作；一旦I/O请求完成，内核通知线程读取已准备就绪；然后线程实际执行读取操作。这是高性能的——应用程序不会在I/O上保持阻塞，而是可以在I/O请求处理时执行有用的工作；不仅如此，当I/O工作完成时，它还会异步通知应用程序。
- en: With AIO, a thread can initiate multiple I/O transfers concurrently; each transfer
    will require a context—called the *[a]iocb*—the [async] I/O control block data
    structure (Linux calls the structure an iocb, the POSIX AIO framework (a wrapper
    library) calls it aiocb). The [a]iocb structure contains the file descriptor,
    the data buffer, the async event notification structure `sigevent`, and so on.
    The alert reader will recall that we have already made use of this powerful `sigevent` structure
    in [Chapter 13](1f621f72-e067-42db-b2eb-b82e20161dec.xhtml), *Timers*, within
    the *Creating and using a POSIX (interval) timer *section. It's really via this `sigevent` structure
    that the asynchronous notification mechanism is implemented (we had used it in [Chapter
    13](1f621f72-e067-42db-b2eb-b82e20161dec.xhtml), *Timers*, to be asynchronously 
    informed that our timer expired; this was done by setting `sigevent.sigev_notify` to
    the value `SIGEV_SIGNAL`, thereby receiving a signal upon timer expiry). Linux
    exposes five system calls for the app developer to exploit AIO; they are as follows: ​`io_setup(2)`, `io_submit(2)`, `io_cancel(2)`, `io_getevents(2)`,
    and `io_destroy(2)`.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AIO，一个线程可以同时启动多个I/O传输；每个传输都需要一个上下文——称为*[a]iocb*——即异步I/O控制块数据结构（Linux将该结构称为iocb，POSIX
    AIO框架（一个包装库）将其称为aiocb）。[a]iocb结构包含文件描述符、数据缓冲区、异步事件通知结构`sigevent`等。细心的读者会记得，我们已经在[第13章](1f621f72-e067-42db-b2eb-b82e20161dec.xhtml)的*定时器*部分中使用了这个强大的`sigevent`结构，在*创建和使用POSIX（间隔）定时器*部分。实际上，正是通过这个`sigevent`结构实现了异步通知机制（我们在[第13章](1f621f72-e067-42db-b2eb-b82e20161dec.xhtml)的*定时器*中使用它，以异步通知我们的定时器已过期；这是通过将`sigevent.sigev_notify`设置为值`SIGEV_SIGNAL`来实现的，从而在定时器到期时接收信号）。Linux为应用程序开发人员暴露了五个系统调用来利用AIO；它们分别是：`io_setup(2)`、`io_submit(2)`、`io_cancel(2)`、`io_getevents(2)`和`io_destroy(2)`。
- en: AIO wrapper APIs are provided by two libraries—libaio and librt (which is released
    along with glibc); you can use their wrappers which will ultimately invoke the
    system calls of course. There are also the POSIX AIO wrappers; see the man page
    on `aio(7)` for an overview on using it, as well as example code. (Also see the
    articles in the *Further reading* section on the GitHub repository for more details
    and example code.)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: AIO包装器API由两个库提供-libaio和librt（与glibc一起发布）；您可以使用它们的包装器，最终会调用系统调用。还有POSIX AIO包装器；请参阅`aio(7)`的手册页，了解如何使用它以及示例代码。（还可以在GitHub存储库的*进一步阅读*部分中查看更多详细信息和示例代码的文章。）
- en: I/O technologies – a quick comparison
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: I/O技术-快速比较
- en: 'The following table provides a quick comparison to some of the more salient
    comparison points between the four to five Linux I/O technologies we have seen,
    namely: the blocking `read(2)`/`write(2)` (and the SG-I/O/positioned `[p]read[v](2)`/`[p]write[v](2)`),
    memory mapping, non-blocking (mostly synchronous) DIO, and non-blocking asynchronous
    AIO:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格提供了我们所见过的四到五种Linux I/O技术之间一些更显著的比较要点的快速比较，即：阻塞`read(2)`/`write(2)`（以及SG-I/O/定位`[p]read[v](2)`/`[p]write[v](2)`），内存映射，非阻塞（大部分同步）DIO和非阻塞异步AIO：
- en: '| **I/O Type** | **APIs** | **Pros** | **Cons** |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|**I/O类型**|**API**|**优点**|**缺点**|'
- en: '| Blocking (regular and SG-IO / positioned) | `[p]read[v](2)` /`[p]write[v](2)`
    | Easy to use | Slow; double-copy of data buffers  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|阻塞（常规和SG-IO / 定位）|`[p]read[v](2)`/`[p]write[v](2)`|易于使用|慢；数据缓冲区的双重拷贝|'
- en: '| Memory Mapping | `mmap(2)` | (Relatively) easy to use; fast (in memory I/O);
    single copy of data (a zero-copy technique); works best when RAM:Storage :: ~
    1:1 | MMU-intensive (high page table overhead, meta-work) when RAM: Storage ratio
    is 1:N (N>>1) |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|内存映射| `mmap(2)` |（相对）易于使用；快速（内存I/O）；数据的单次拷贝（零拷贝技术）；当RAM:Storage :: ~ 1:1时效果最佳|
    当RAM: Storage比例为1:N（N>>1）时，需要大量MMU（高页表开销，元工作）|'
- en: '| DIO (non-blocking, mostly synchronous) | `open(2)` with `O_DIRECT` flag |
    Zero-copy technique; no impact on page cache; control over caching; some control
    over I/O scheduling | Moderately complex to set up and use: app must perform its
    own caching |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|DIO（非阻塞，大部分同步）|带有`O_DIRECT`标志的`open(2)`|零拷贝技术；对页面缓存没有影响；对缓存有控制；对I/O调度有一定控制|设置和使用相对复杂：应用程序必须执行自己的缓存|'
- en: '| AIO  (non-blocking, asynchronous) | <Various: see aio(7) - POSIX AIO, Linux
    `io_*(2)`, and so on> | Truly async and non-blocking—required for high-performance
    apps; zero-copy technique; no impact on page cache; full control over caching,
    I/O and thread scheduling | Complex to set up and use |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| AIO（非阻塞，异步）| <各种：参见aio(7)-POSIX AIO，Linux `io_*(2)`等> | 真正的异步和非阻塞-适用于高性能应用程序；零拷贝技术；对页面缓存没有影响；完全控制缓存、I/O和线程调度|
    设置和使用复杂|'
- en: Linux I/O technologies—a quick comparison
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Linux I/O技术-快速比较
- en: In the *Further reading* section on the GitHub repository, we provide links
    to two blog articles (from two real-world products: Scylla, a modern high-performance
    distributed No SQL data store, and NGINX, a modern high-performance web server), that
    discuss in depth how these alternative powerful I/O technologies (AIO, thread
    pools) are used in (their respective) real-world products; do take a look.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub存储库的*进一步阅读*部分，我们提供了两篇博客文章的链接（来自两个真实世界的产品：Scylla，一个现代高性能的分布式No SQL数据存储，以及NGINX，一个现代高性能的Web服务器），深入讨论了这些替代强大的I/O技术（AIO，线程池）在（各自的）真实世界产品中的使用方式；一定要看一看。
- en: Multiplexing or async blocking I/O – a quick note
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多路复用或异步阻塞I/O-简要说明
- en: You often hear about powerful multiplexing I/O APIs—the `select(2)`, `poll(2)`,
    and, more recently, Linux's powerful `epoll(7)` framework. These APIs, `select(2)`,
    `poll(2)`, and/or `epoll(7)`, provide what is known as asynchronous blocking I/O.
    They work well upon descriptors that remain blocked on I/O; examples are sockets,
    both Unix and internet domain, as well as pipes—both unnamed and named pipes (FIFOs).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 您经常听说强大的多路复用I/O API-`select(2)`，`poll(2)`，以及最近的Linux强大的`epoll(7)`框架。这些API，`select(2)`，`poll(2)`，和/或`epoll(7)`，提供了所谓的异步阻塞I/O。它们在保持I/O阻塞的描述符上工作良好；例如套接字，Unix和Internet域，以及管道-无名管道和命名管道（FIFO）。
- en: These I/O technologies are asynchronous (you can issue the system call and return
    immediately) but they actually are still blocking in nature because the thread
    must check for I/O completion, for example, by using the `poll(2)` in tandem with
    a `read(2)` system call, which is still a blocking operation.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这些I/O技术是异步的（您可以发出系统调用并立即返回），但实际上它们仍然是阻塞的，因为线程必须检查I/O完成，例如通过使用`poll(2)`与`read(2)`系统调用配合使用，这仍然是一个阻塞操作。
- en: 'These APIs are really very useful for network I/O operations, the canonical
    example being a busy (web)server monitoring hundreds (and perhaps thousands) of
    connections. First, each connection being represented by a socket descriptor makes
    using the `select(2)` or `poll(2)` system calls appealing. However, the fact is
    that `select(2)` is old and limited (to a maximum of 1,024 descriptors; not enough);
    secondly, both `select(2)` and `poll(2)`''s internal implementations have an algorithmic
    time complexity of O(n), which makes them non-scalable. The `epoll(7)` implementation
    has no (theoretical) descriptor limit and uses an O(1) algorithm and what''s known
    as edge-triggered notifications. This table summarizes these points:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这些API对于网络I/O操作非常有用，典型的例子是繁忙的（Web）服务器监视数百（甚至数千）个连接。首先，每个连接由套接字描述符表示，使用`select(2)`或`poll(2)`系统调用非常吸引人。然而，事实是`select(2)`已经过时且受限（最多1,024个描述符；不够）；其次，`select(2)`和`poll(2)`的内部实现具有O(n)的算法时间复杂度，这使它们不可扩展。`epoll(7)`的实现没有（理论上的）描述符限制，并使用O(1)算法以及所谓的边缘触发通知。这张表总结了这些要点。
- en: '| **API** | **Algorithmic Time-Complexity** | **Max number of clients** |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|**API**|**算法时间复杂度**|**最大客户端数**|'
- en: '| `select(2)` | O(n) | FD_SETSIZE (1024) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|`select(2)`| O(n)| FD_SETSIZE（1024）|'
- en: '| `poll(2)` | O(n) | (theoretically) unlimited |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|`poll(2)`| O(n)|（理论上）无限|'
- en: '| `epoll(7)` APIs | O(1) | (theoretically) unlimited |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|`epoll(7)` API| O(1)|（理论上）无限|'
- en: Linux asynchronous blocking APIs
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Linux异步阻塞API
- en: These features have thus made the `epoll(7)` set of APIs (`epoll_create(2)`,
    `epoll_ctl(2)`, `epoll_wait(2)`, and `epoll_pwait(2)`) a favorite for implementing
    non-blocking I/O on  network applications that require very high scalability.
    (See a link to a blog article providing more details on using multiplexed I/O,
    including the epoll, on Linux in the *Further reading* section on the GitHub repository.)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特性使得`epoll(7)`一组API（`epoll_create(2)`、`epoll_ctl(2)`、`epoll_wait(2)`和`epoll_pwait(2)`）成为实现网络应用程序上非阻塞I/O的首选，这些应用程序需要非常高的可扩展性。（在GitHub存储库的*进一步阅读*部分中，有一篇博客文章提供了有关在Linux上使用多路复用I/O，包括epoll的更多详细信息的链接。）
- en: I/O – miscellaneous
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: I/O – 其他
- en: A few miscellaneous remaining topics to round off this chapter follow.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本章的一些其他杂项主题。
- en: Linux's inotify framework
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Linux的inotify框架
- en: While brilliant for network I/O, these multiplexing APIs, though they can in
    theory be used for monitoring regular file descriptors, will simply report them
    as always being ready (for reading, writing, or an error condition has arisen),
    thereby diminishing their usefulness (when used upon regular files).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对于网络I/O非常出色，这些多路复用API在理论上可以用于监视常规文件描述符，但它们将简单地报告这些描述符始终准备就绪（用于读取、写入或发生错误条件），从而降低了它们的实用性（当用于常规文件时）。
- en: Perhaps Linux's inotify framework, a means to monitor filesystem events including
    events on individual files, might be what you are looking for. The inotify framework
    provides the following system calls to help developers monitor files: `inotify_init(2)`, `inotify_add_watch(2)`
    (which can be subsequently `read(2)`), and then `inotify_rm_watch(2)`. Check out
    the man page on `inotify(7)` for more details: [http://man7.org/linux/man-pages/man7/inotify.7.html](http://man7.org/linux/man-pages/man7/inotify.7.html).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 也许Linux的inotify框架，一种监视文件系统事件（包括单个文件上的事件）的方法，可能是你正在寻找的。inotify框架提供以下系统调用来帮助开发人员监视文件：`inotify_init(2)`、`inotify_add_watch(2)`（随后可以`read(2)`），然后`inotify_rm_watch(2)`。查看`inotify(7)`的手册页面以获取更多详细信息：[http://man7.org/linux/man-pages/man7/inotify.7.html](http://man7.org/linux/man-pages/man7/inotify.7.html)。
- en: I/O schedulers
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: I/O调度程序
- en: 'An important feature within the Linux I/O stack is a part of the kernel block
    layer called the I/O scheduler. The issue being addressed here is basically this:
    I/O requests are being more or less continually issued by the kernel (due to apps
    wanting to perform various file data/code reads and writes); this results in a
    continuous stream of I/O requests being ultimately received and processed by the
    block driver(s). The kernel folks know that one of the primary reasons that I/O
    sucks out performance is that the physical seek of a typical SCSI disk is really
    slow (compared to silicon speeds; yes, of course, SSDs (solid state devices) are
    making this a lot more palatable nowadays).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Linux I/O堆栈中的一个重要特性是内核块层的一部分，称为I/O调度程序。这里要解决的问题基本上是：内核不断地发出I/O请求（因为应用程序希望执行各种文件数据/代码读取和写入）；这导致块驱动程序最终接收和处理连续的I/O请求流。内核人员知道I/O影响性能的主要原因之一是典型SCSI磁盘的物理搜索速度非常慢（与硅速度相比；是的，当然，SSD（固态设备）现在使这变得更加可接受）。
- en: 'So, if we could use some intelligence to sort the block I/O requests in a way
    that makes the most sense in terms of the underlying physical medium, it would
    help performance. Think of an elevator in a building: it uses a sort algorithm,
    optimally taking people on and dropping them off as it traverses various floors.
    This is what the OS I/O schedulers essentially try to do; in fact, the first implementation
    was called Linus''s elevator.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们可以使用一些智能来对块I/O请求进行排序，以使其在底层物理介质方面最有意义，这将有助于性能。想象一下建筑物中的电梯：它使用一种排序算法，以最佳方式在穿越各个楼层时搭载和卸载人员。这基本上是操作系统I/O调度程序试图做的事情；事实上，第一个实现被称为Linus的电梯。
- en: 'Various I/O scheduler algorithms exist (deadline, **completely fair queuing**
    (**cfq**), noop, anticipatory scheduler: these are now considered legacy; the
    newest as of the time of writing seem to be the mq-deadline and **budget fair 
    queuing** (**bfq**) I/O schedulers, with bfq looking very promising for heavy
    or light I/O workloads (bfq is a recent addition, kernel version 4.16). The I/O
    schedulers present within your Linux OS are a kernel feature; you can check which
    they are and which is being used; see it being done here on my Ubuntu 18.04 x86_64
    box:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 存在各种I/O调度程序算法（截止时间、完全公平队列（cfq）、noop、预期调度程序：这些现在被认为是传统的；截至撰写本文时，最新的似乎是mq-deadline和预算公平队列（bfq）I/O调度程序，bfq对于重型或轻型I/O工作负载看起来非常有前途（bfq是最近的添加，内核版本为4.16）。您的Linux操作系统中存在的I/O调度程序是一个内核特性；您可以检查它们是哪些以及正在使用哪个；在我的Ubuntu
    18.04 x86_64系统上进行了演示：
- en: '[PRE13]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here, `bfq` is the I/O scheduler being used on my Fedora 28 system (with a
    more recent kernel):'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的Fedora 28系统上正在使用的I/O调度程序是`bfq`（使用了更近期的内核）：
- en: '[PRE14]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The default I/O scheduler here is `bfq`. Here''s the interesting bit: the user
    can actually select between I/O schedulers, run their I/O stress workloads and/or
    benchmarks, and see which one yields the maximum benefit! How? To select the I/O
    scheduler at boot time, pass along a kernel parameter (via the bootloader, typically
    GRUB on an x86-based laptop, desktop or server system, U-Boot on an embedded Linux);
    the parameter in question is passed as `elevator=<iosched-name>`; for example,
    to set the I/O scheduler to noop (useful for systems with SSDs perhaps), pass
    the parameter to the kernel as `elevator=noop`.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的默认I/O调度程序是`bfq`。有趣的是：用户实际上可以在I/O调度程序之间进行选择，运行他们的I/O压力工作负载和/或基准测试，并查看哪个产生了最大的好处！如何？要在引导时选择I/O调度程序，请通过内核参数传递（通常是GRUB在基于x86的笔记本电脑、台式机或服务器系统上，嵌入式Linux上是U-Boot）；所涉及的参数作为`elevator=<iosched-name>`传递；例如，要将I/O调度程序设置为noop（对于可能使用SSD的系统有用），将参数传递给内核为`elevator=noop`。
- en: 'There''s an easier way to change the I/O scheduler immediately at runtime;
    just `echo(1)` the one you want into the pseudo-file; for example, to change the
    I/O scheduler to `mq-deadline`,do the following:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个更简单的方法可以立即在运行时更改I/O调度程序；只需将所需的调度程序写入伪文件中；例如，要将I/O调度程序更改为`mq-deadline`，请执行以下操作：
- en: '[PRE15]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now, you can (stress) test your I/O workloads on different I/O schedulers, thus
    deciding upon which yields the optimal performance for your workload.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以对不同的I/O调度程序进行（压力）测试，从而决定哪种对您的工作负载产生最佳性能。
- en: Ensuring sufficient disk space
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确保有足够的磁盘空间
- en: 'Linux provides the `posix_fallocate(3)` API; its job is to guarantee that sufficient
    disk space is available for a given range specific to a given file. What that
    actually means is that whenever the app writes to that file within that range,
    the write is guaranteed not to fail due to lack of disk space (if it does fail,
    `errno` will be set to ENOSPC; that won''t happen). It''s signature is as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Linux提供了`posix_fallocate(3)` API；它的作用是保证给定文件的特定范围内有足够的磁盘空间。这实际上意味着每当应用程序在该范围内写入该文件时，由于磁盘空间不足而导致写入失败是被保证不会发生的（如果失败，`errno`将被设置为ENOSPC；这不会发生）。它的签名如下：
- en: '[PRE16]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here are some quick points to note regarding this API:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于此API的一些要点：
- en: The file is the one referred to by the descriptor `fd`.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文件是由描述符`fd`引用的文件。
- en: The range is from `offset` for `len` bytes; in effect, this is the disk space
    that will be reserved for the file.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 范围是从`offset`开始，长度为`len`字节；实际上，这是将为文件保留的磁盘空间。
- en: If the current file size is less than what the range requests (that is, `offset`+`len`),
    then the file is grown to this size; otherwise, the file's size remains unaltered.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果当前文件大小小于范围请求（即`offset`+`len`），则文件将增长到这个大小；否则，文件的大小保持不变。
- en: '`posix_fallocate(3)` is a portable wrapper over the underlying system call
    `fallocate(2)`.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`posix_fallocate(3)`是对底层系统调用`fallocate(2)`的可移植包装。'
- en: For this API to succeed, the underlying filesystem must support the `fallocate`;
    if not, it's emulated (but with a lot of caveats and issues; refer to the man
    page for more).
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了使此API成功，底层文件系统必须支持`fallocate`；如果不支持，则会进行模拟（但有很多警告和问题；请参阅手册页以了解更多）。
- en: Also, a CLI utility called `fallocate(1)` exists to perform the same task from,
    say, a shell script.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，还存在一个名为`fallocate(1)`的CLI实用程序，可以从shell脚本中执行相同的任务。
- en: These APIs and tools may come in very useful for software such as backup, cloud
    provisioning, digitization, and so on, guaranteeing sufficient disk space is available
    before a long I/O operation begins.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这些API和工具可能对诸如备份、云提供、数字化等软件非常有用，确保在长时间I/O操作开始之前有足够的磁盘空间可用。
- en: Utilities for I/O monitoring, analysis, and bandwidth control
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于I/O监控、分析和带宽控制的实用程序
- en: This table summarizes various utilities, APIs, tools, and even a cgroup blkio
    controller; these tools/features will prove very useful in monitoring, analyzing
    (to pinpoint I/O bottlenecks), and allocating I/O bandwidth (via the `ioprio_set(2)`
    and the powerful cgroups blkio controller.)
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这张表总结了各种实用程序、API、工具，甚至包括cgroup blkio控制器；这些工具/功能在监视、分析（以确定I/O瓶颈）和分配I/O带宽（通过`ioprio_set(2)`和强大的cgroups
    blkio控制器）方面将非常有用。
- en: '| **Utility name** | **What it does** |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| **实用程序名称** | **功能** |'
- en: '| `iostat(1)`  | Monitors I/O and displays I/O statistics about devices and
    storage device partitions. From the man page on `iostat(1)`: The `iostat` command
    is used for monitoring system input/output device loading by observing the time
    the devices are active in relation to their average transfer rates. The `iostat`
    command generates reports that can be used to change system configuration to better
    balance the input/output load between physical disks. |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| `iostat(1)` | 监控I/O并显示有关设备和存储设备分区的I/O统计信息。从`iostat(1)`的手册页上：`iostat`命令用于通过观察设备活动时间与其平均传输速率的关系来监视系统输入/输出设备的负载。`iostat`命令生成的报告可用于更好地平衡物理磁盘之间的输入/输出负载，从而改变系统配置。'
- en: '| `iotop(1)` | In the style of `top(1)` (for CPU), iotop continually displays
    threads sorted by their I/O usage. Must run as root. |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| `iotop(1)` | 类似于`top(1)`（用于CPU），iotop不断显示按其I/O使用情况排序的线程。必须以root身份运行。'
- en: '| `ioprio_[get&#124;set](2)` | System calls to query and set I/O scheduling
    class and priority of a given thread; see the man pages for details: [http://man7.org/linux/man-pages/man2/ioprio_set.2.html](http://man7.org/linux/man-pages/man2/ioprio_set.2.html);
    see its wrapper utility `ionice(1)` as well. |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| `ioprio_[get&#124;set](2)` | 用于查询和设置给定线程的I/O调度类和优先级的系统调用；有关详细信息，请参阅手册页面：[http://man7.org/linux/man-pages/man2/ioprio_set.2.html](http://man7.org/linux/man-pages/man2/ioprio_set.2.html)；也可以查看其包装实用程序`ionice(1)`。'
- en: '| perf-tools | Among these tools (from B Gregg) is `iosnoop-perf(1)` and `iolatecy-perf(1)`
    to snoop I/O transactions and observe I/O latencies respectively. Install these
    tools from their GitHub repository here: [https://github.com/brendangregg/perf-tools](https://github.com/brendangregg/perf-tools).
    |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| perf-tools | 在这些工具（来自B Gregg）中有`iosnoop-perf(1)`和`iolatecy-perf(1)`，分别用于窥探I/O事务和观察I/O延迟。从这里的GitHub存储库安装这些工具：[https://github.com/brendangregg/perf-tools](https://github.com/brendangregg/perf-tools)。'
- en: '| cgroup blkio controller | Use the powerful Linux cgroup''s blkio controller
    to limit I/O bandwidth for a process or group of processes in any required fashion
    (heavily used in cloud environments, including Docker); see the relevant link
    in the *Further reading* section on the GitHub repository. |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| cgroup blkio控制器 | 使用强大的Linux cgroup的blkio控制器以任何所需的方式限制进程或一组进程的I/O带宽（在云环境中广泛使用，包括Docker）；请在GitHub存储库的*进一步阅读*部分中查看相关链接。'
- en: Tools/utilities/APIs/cgroups for I/O monitoring, analysis, and bandwidth control
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 用于I/O监控、分析和带宽控制的工具/实用程序/API/cgroups
- en: 'Note: the preceding mentioned utilities may not be installed on the Linux system
    by default; (obviously) install them to try them out.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：前面提到的实用程序可能不会默认安装在Linux系统上；（显然）安装它们以尝试它们。
- en: Do also check out Brendan Gregg's superb Linux Performance blog pages and tools
    (which include perf-tools, iosnoop, and iosnoop latency heat maps); please find
    the relevant links in the *Further reading* section on the GitHub repository.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 还要查看Brendan Gregg的出色的Linux性能博客页面和工具（其中包括perf-tools、iosnoop和iosnoop延迟热图）；请在GitHub存储库的*进一步阅读*部分中找到相关链接。
- en: Summary
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we learned powerful approaches to a critical aspect of working
    with files: ensuring that I/O performance is kept as high as is possible, as I/O
    is really the performance-draining bottleneck in many real-world workloads. These
    techniques ranged from file access pattern advice passing to the OS, SG-I/O techniques
    and APIs, memory mapping for file I/O, DIO, AIO, and so on.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了处理文件时确保 I/O 性能尽可能高的强大方法，因为在许多真实世界的工作负载中，I/O 确实是性能瓶颈。这些技术包括向操作系统传递文件访问模式建议、SG-I/O
    技术和 API、文件 I/O 的内存映射、DIO、AIO 等等。
- en: The next chapter in the book is a brief look at daemon processes; what they
    are and how to set them up. Kindly take a look at this chapter here: [https://www.packtpub.com/sites/default/files/downloads/Daemon_Processes.pdf](https://www.packtpub.com/sites/default/files/downloads/Daemon_Processes.pdf).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 书中的下一章简要介绍了守护进程；它们是什么以及如何设置。请查看这一章节：[https://www.packtpub.com/sites/default/files/downloads/Daemon_Processes.pdf](https://www.packtpub.com/sites/default/files/downloads/Daemon_Processes.pdf)。

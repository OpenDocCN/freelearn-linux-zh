- en: Chapter 5. Tangled Web? Not At All!
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。纠缠的网络？一点也不！
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: Downloading from a web page
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从网页下载
- en: Downloading a web page as formatted plain text
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将网页下载为格式化的纯文本
- en: A primer on cURL
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: cURL入门
- en: Accessing unread Gmail mails from the command line
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从命令行访问未读的Gmail邮件
- en: Parsing data from a website
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从网站解析数据
- en: Creating an image crawler and downloader
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建图像爬虫和下载器
- en: Creating a web photo album generator
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建网页相册生成器
- en: Building a Twitter command-line client
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建Twitter命令行客户端
- en: Define utility with Web backend
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义具有Web后端的实用程序
- en: Finding broken links in a website
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在网站中查找损坏的链接
- en: Tracking changes to a website
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪网站的更改
- en: Posting to a web page and reading response
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布到网页并读取响应
- en: Introduction
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: The Web is becoming the face of technology. It is the central access point for
    data processing. Though shell scripting cannot do everything that languages like
    PHP can do on the Web, there are still many tasks to which shell scripts are ideally
    suited. In this chapter we will explore some recipes that can be used to parse
    website content, download and obtain data, send data to forms, and automate website
    usage tasks and similar activities. We can automate many activities that we perform
    interactively through a browser with a few lines of scripting. Access to the functionalities
    provided by the HTTP protocol with command-line utilities enables us to write
    scripts that are suitable to solve most of the web-automation utilities. Have
    fun while going through the recipes of this chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 网络正在成为技术的面孔。这是数据处理的中心访问点。虽然shell脚本不能像PHP等语言在Web上做的那样，但仍然有许多任务适合使用shell脚本。在本章中，我们将探讨一些可以用于解析网站内容、下载和获取数据、发送数据到表单以及自动执行网站使用任务和类似活动的方法。我们可以用几行脚本自动执行许多我们通过浏览器交互执行的活动。通过命令行实用程序访问HTTP协议提供的功能，我们可以编写适合解决大多数Web自动化实用程序的脚本。在阅读本章的食谱时玩得开心。
- en: Downloading from a web page
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从网页下载
- en: Downloading a file or a web page from a given URL is simple. A few command-line
    download utilities are available to perform this task.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从给定URL下载文件或网页很简单。有几个命令行下载实用程序可用于执行此任务。
- en: Getting ready
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: '`wget` is a file download command-line utility. It is very flexible and can
    be configured with many options.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`wget`是一个文件下载命令行实用程序。它非常灵活，可以配置许多选项。'
- en: How to do it...
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'A web page or a remote file can be downloaded using `wget` as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`wget`下载网页或远程文件，如下所示：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For example:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'It is also possible to specify multiple download URLs as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以指定多个下载URL，如下所示：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'A file can be downloaded using `wget` using the URL as:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用URL下载文件，如下所示：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Usually, files are downloaded with the same filename as in the URL and the download
    log information or progress is written to `stdout`.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，文件以与URL相同的文件名下载，并且下载日志信息或进度被写入`stdout`。
- en: You can specify the output file name with the `-O` option. If the file with
    the specified filename already exists, it will be truncated first and the downloaded
    file will be written to the specified file.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`-O`选项指定输出文件名。如果指定的文件名已经存在，它将首先被截断，然后下载的文件将被写入指定的文件。
- en: 'You can also specify a different logfile path rather than printing logs to
    `stdout` by using the `-o` option as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用`-o`选项指定不同的日志文件路径，而不是将日志打印到`stdout`，如下所示：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: By using the above command, nothing will be printed on screen. The log or progress
    will be written to `log` and the output file will be `dloaded_file.img`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用上述命令，屏幕上将不会打印任何内容。日志或进度将被写入`log`，输出文件将是`dloaded_file.img`。
- en: There is a chance that downloads might break due to unstable Internet connections.
    Then we can use the number of tries as an argument so that once interrupted, the
    utility will retry the download that many times before giving up.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于不稳定的互联网连接可能会导致下载中断，因此我们可以使用尝试次数作为参数，以便一旦中断，实用程序将在放弃之前重试下载那么多次。
- en: 'In order to specify the number of tries, use the `-t` flag as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了指定尝试次数，请使用`-t`标志，如下所示：
- en: '[PRE5]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: There's more...
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The `wget` utility has several additional options that can be used under different
    problem domains. Let's go through a few of them.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`wget`实用程序有几个额外的选项，可以在不同的问题领域下使用。让我们来看看其中的一些。'
- en: Restricted with speed downloads
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 限速下载
- en: When we have a limited Internet downlink bandwidth and many applications sharing
    the internet connection, if a large file is given for download, it will suck all
    the bandwidth and may cause other process to starve for bandwidth. The `wget`
    command comes with a built-in option to specify the maximum bandwidth limit the
    download job can possess. Hence all the applications can simultaneously run smoothly.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有限的互联网下行带宽和许多应用程序共享互联网连接时，如果给定一个大文件进行下载，它将吸取所有带宽，可能导致其他进程因带宽不足而挨饿。`wget`命令带有一个内置选项，可以指定下载作业可以拥有的最大带宽限制。因此，所有应用程序可以同时平稳运行。
- en: 'We can restrict the speed of `wget` by using the `--limit-rate` argument as
    follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`--limit-rate`参数限制`wget`的速度，如下所示：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In this command `k` (kilobyte) and `m` (megabyte) specify the speed limit.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个命令中，`k`（千字节）和`m`（兆字节）指定了速度限制。
- en: We can also specify the maximum quota for the download. It will stop when the
    quota is exceeded. It is useful when downloading multiple files limited by the
    total download size. This is useful to prevent the download from accidently using
    too much disk space.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以指定下载的最大配额。当配额超过时，它将停止。在下载多个受总下载大小限制的文件时很有用。这对于防止下载意外使用太多磁盘空间很有用。
- en: 'Use `--quota` or `–Q` as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`--quota`或`-Q`如下：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Resume downloading and continue
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 恢复下载并继续
- en: 'If a download using `wget` gets interrupted before it is completed, we can
    resume the download where we left off by using the `-c` option as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用`wget`下载在完成之前中断，我们可以使用`-c`选项恢复下载，从我们离开的地方继续下载，如下所示：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Using cURL for download
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用cURL进行下载
- en: cURL is another advanced command-line utility. It is much more powerful than
    `wget`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: cURL是另一个高级命令行实用程序。它比wget更强大。
- en: 'cURL can be used to download as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: cURL可以用于下载如下：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Unlike `wget`, `curl` writes the downloaded data into standard output (`stdout`)
    rather than to a file. Therefore, we have to redirect the data from `stdout` to
    the file using a redirection operator.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 与wget不同，curl将下载的数据写入标准输出（stdout）而不是文件。因此，我们必须使用重定向运算符将数据从stdout重定向到文件。
- en: Copying a complete website (mirroring)
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 复制完整的网站（镜像）
- en: '`wget` has an option to download the complete website by recursively collecting
    all the URL links in the web pages and downloading all of them like a crawler.
    Hence we can completely download all the pages of a website.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: wget有一个选项，可以通过递归收集网页中的所有URL链接并像爬虫一样下载所有网页，从而下载完整的网站。因此，我们可以完全下载网站的所有页面。
- en: 'In order to download the pages, use the `--mirror` option as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了下载页面，请使用--mirror选项如下：
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Or use:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 或使用：
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`-l` specifies the `DEPTH` of web pages as levels. That means it will traverse
    only that much number of levels. It is used along with `–r` (recursive). The `-N`
    argument is used to enable time stamping for the file. `URL` is the base URL for
    a website for which the download needs to be initiated.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: -l指定网页的深度为级别。这意味着它只会遍历那么多级别。它与-r（递归）一起使用。-N参数用于为文件启用时间戳。URL是需要启动下载的网站的基本URL。
- en: Accessing pages with HTTP or FTP authentication
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用HTTP或FTP身份验证访问页面
- en: 'Some web pages require authentication for HTTP or FTP URLs. This can be provided
    by using the `--user` and `--password` arguments:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有些网页需要对HTTP或FTP URL进行身份验证。这可以通过使用--user和--password参数来提供：
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: It is also possible to ask for a password without specifying the password inline.
    In order to do that use `--ask-password` instead of the `--password` argument.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以在不指定内联密码的情况下请求密码。为此，请使用--ask-password而不是--password参数。
- en: Downloading a web page as formatted plain text
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将网页作为格式化纯文本下载
- en: Web pages are HTML pages containing a collection of HTML tags along with other
    elements, such as JavaScript, CSS, and so on. But the HTML tags define the base
    of a web page. We may need to parse the data in a web page while looking for specific
    content, and this is something Bash scripting can help us with. When we download
    a web page, we receive an HTML file. In order to view formatted data, it should
    be viewed in a web browser. However, in most of the circumstances, parsing a formatted
    text document will be easier than parsing HTML data. Therefore, if we can get
    a text file with formatted text similar to the web page seen on the web browser,
    it is more useful and it saves a lot of effort required to strip off HTML tags.
    Lynx is an interesting command-line web browser. We can actually get the web page
    as plain text formatted output from Lynx. Let's see how to do it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 网页是包含一系列HTML标记以及其他元素（如JavaScript、CSS等）的HTML页面。但是HTML标记定义了网页的基础。在查找特定内容时，我们可能需要解析网页中的数据，这是Bash脚本可以帮助我们的地方。当我们下载一个网页时，我们会收到一个HTML文件。为了查看格式化的数据，它应该在Web浏览器中查看。然而，在大多数情况下，解析格式化的文本文档将比解析HTML数据更容易。因此，如果我们可以获得一个与在Web浏览器上看到的网页类似的格式化文本的文本文件，那将更有用，并且可以节省大量去除HTML标记所需的工作。Lynx是一个有趣的命令行Web浏览器。我们实际上可以从Lynx获取网页作为纯文本格式化输出。让我们看看如何做到这一点。
- en: How to do it...
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Let''s download the webpage view, in ASCII character representation, in a text
    file using the `–dump` flag with the `lynx` command:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用lynx命令的--dump标志将网页视图以ASCII字符表示形式下载到文本文件中：
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This command will also list all the hyper-links (`<a href="link">`) separately
    under a heading **References** as the footer of the text output. This would help
    us avoid parsing of links separately using regular expressions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令还将所有超链接（<a href="link">）单独列在文本输出的页脚下的**References**标题下。这将帮助我们避免使用正则表达式单独解析链接。
- en: 'For example:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You can see the plain text version of text by using the `cat` command as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用cat命令查看文本的纯文本版本如下：
- en: '[PRE15]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: A primer on cURL
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: cURL入门
- en: cURL is a powerful utility that supports many protocols including HTTP, HTTPS,
    FTP, and much more. It supports many features including POST, cookie, authentication,
    downloading partial files from a specified offset, referers, user agent strings,
    extra headers, limit speed, maximum file size, progress bars, and so on. cURL
    is useful for when we want to play around with automating a web page usage sequence
    and to retrieve data. This recipe is a list of the most important features of
    cURL.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: cURL是一个强大的实用程序，支持包括HTTP、HTTPS、FTP等在内的许多协议。它支持许多功能，包括POST、cookie、身份验证、从指定偏移量下载部分文件、引用、用户代理字符串、额外标头、限制速度、最大文件大小、进度条等。cURL对于我们想要玩转自动化网页使用序列并检索数据时非常有用。这个配方是cURL最重要的功能列表。
- en: Getting ready
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: cURL doesn't come with any of the main Linux distros by default, so you may
    have to install it using the package manager. By default, most distributions ship
    with `wget`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: cURL不会默认随主要Linux发行版一起提供，因此您可能需要使用软件包管理器安装它。默认情况下，大多数发行版都附带wget。
- en: cURL usually dumps downloaded files to `stdout` and progress information to
    `stderr`. To avoid progress information from being shown, we always use the`--silent`
    option.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: cURL通常将下载的文件转储到stdout，并将进度信息转储到stderr。为了避免显示进度信息，我们总是使用--silent选项。
- en: How to do it…
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: The `curl` command can be used to perform different activities such as downloading,
    sending different HTTP requests, specifying HTTP headers, and so on. Let's see
    how to perform different tasks with cURL.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: curl命令可用于执行不同的活动，如下载、发送不同的HTTP请求、指定HTTP标头等。让我们看看如何使用cURL执行不同的任务。
- en: '[PRE16]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The above command dumps the downloaded file into the terminal (the downloaded
    data is written to `stdout`).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将下载的文件转储到终端（下载的数据写入stdout）。
- en: The `--silent` option is used to prevent the `curl` command from displaying
    progress information. If progress information is required, remove `--silent`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`--silent`选项用于防止`curl`命令显示进度信息。如果需要进度信息，请删除`--silent`。'
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `-O` option is used to write the downloaded data into a file with the filename
    parsed from the URL rather than writing into the standard output.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`-O`选项将下载的数据写入文件，文件名从URL中解析而不是写入标准输出。
- en: 'For example:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`index.html` will be created.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 将创建`index.html`。
- en: It writes a web page or file to the filename as in the URL instead of writing
    to `stdout`. If filenames are not there in the URL, it will produce an error.
    Hence, make sure that the URL is a URL to a remote file. `curl http://slynux.org
    -O --silent` will display an error since the filename cannot be parsed from the
    URL.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 它将网页或文件写入与URL中的文件名相同的文件，而不是写入`stdout`。如果URL中没有文件名，将产生错误。因此，请确保URL是指向远程文件的URL。`curl
    http://slynux.org -O --silent`将显示错误，因为无法从URL中解析文件名。
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `-o` option is used to download a file and write to a file with a specified
    file name.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`-o`选项用于下载文件并写入指定的文件名。'
- en: In order to show the `#` progress bar while downloading, use `–-progress` instead
    of `–-silent`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在下载时显示`#`进度条，使用`--progress`而不是`--silent`。
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: There's more...
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: In the previous sections we have learned how to download files and dump HTML
    pages to the terminal. There several advanced options that come along with cURL.
    Let's explore more on cURL.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，我们已经学习了如何下载文件并将HTML页面转储到终端。cURL还有一些高级选项。让我们更深入地了解cURL。
- en: Continue/Resume downloading
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 继续/恢复下载
- en: cURL has advanced resume download features to continue at a given offset unlike
    `wget`. It helps to download portions of files by specifying an offset.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: cURL具有高级的恢复下载功能，可以在给定的偏移量继续下载，而`wget`不具备这个功能。它可以通过指定偏移量来下载文件的部分。
- en: '[PRE21]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The offset is an integer value in bytes.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 偏移是以字节为单位的整数值。
- en: 'cURL doesn''t require us to know the exact byte offset if we want to resume
    downloading a file. If you want cURL to figure out the correct resume point, use
    the `-C -` option, like this:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要恢复下载文件，cURL不需要我们知道确切的字节偏移量。如果要cURL找出正确的恢复点，请使用`-C -`选项，就像这样：
- en: '[PRE22]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: cURL will automatically figure out where to restart the download of the specified
    file.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: cURL将自动找出重新启动指定文件的下载位置。
- en: Set referer string with cURL
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用cURL设置引用字符串
- en: Referer is a string in the HTTP header used to identify the page from which
    the user reaches the current web page. When a user clicks on a link from web page
    A and it reaches web page B, the referer header string in the page B will contain
    a URL of page A.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 引用者是HTTP头中的一个字符串，用于标识用户到达当前网页的页面。当用户从网页A点击链接到达网页B时，页面B中的引用头字符串将包含页面A的URL。
- en: Some dynamic pages check the referer string before returning HTML data. For
    example, a web page shows a Google logo attached page when a user navigates to
    a website by searching on Google, and shows a different page when they navigate
    to the web page by manually typing the URL.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一些动态页面在返回HTML数据之前会检查引用字符串。例如，当用户通过在Google上搜索导航到网站时，网页会显示一个附加了Google标志的页面，当他们通过手动输入URL导航到网页时，会显示不同的页面。
- en: The web page can write a condition to return a Google page if the referer is
    [www.google.com](http://www.google.com) or else return a different page.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 网页可以编写一个条件，如果引用者是[www.google.com](http://www.google.com)，则返回一个Google页面，否则返回一个不同的页面。
- en: 'You can use `--referer` with the `curl` command to specify the referer string
    as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`curl`命令的`--referer`选项指定引用字符串如下：
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'For example:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '[PRE24]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Cookies with cURL
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用cURL的cookies
- en: Using `curl` we can specify as well as store cookies encountered during HTTP
    operations.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`curl`我们可以指定并存储在HTTP操作期间遇到的cookies。
- en: In order to specify cookies, use the `--cookie "COOKIES"` option.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了指定cookies，使用`--cookie "COOKIES"`选项。
- en: 'Cookies should be provided as `name=value`. Multiple cookies should be delimited
    by a semicolon ";". For example:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Cookies应该提供为`name=value`。多个cookies应该用分号“;”分隔。例如：
- en: '[PRE25]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'In order to specify a file to which cookies encountered are to be stored, use
    the `--cookie-jar` option. For example:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 为了指定存储遇到的cookies的文件，使用`--cookie-jar`选项。例如：
- en: '[PRE26]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Setting a user agent string with cURL
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用cURL设置用户代理字符串
- en: Some web pages that check the user-agent won't work if there is no user-agent
    specified. You may have noticed that certain websites work well only in Internet
    Explorer (IE). If a different browser is used, the website will show a message
    that it will work only on IE. This is because the website checks for a user agent.
    You can set the user agent as IE with `curl` and see that it returns a different
    web page in this case.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一些检查用户代理的网页如果没有指定用户代理就无法工作。您可能已经注意到，某些网站只在Internet Explorer（IE）中运行良好。如果使用不同的浏览器，网站将显示一个消息，表示只能在IE上运行。这是因为网站检查用户代理。您可以使用`curl`将用户代理设置为IE，并查看在这种情况下返回不同的网页。
- en: 'Using cURL it can be set using `--user-agent` or `–A` as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用cURL可以使用`--user-agent`或`-A`来设置如下：
- en: '[PRE27]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Additional headers can be passed with cURL. Use `–H "Header"` to pass multiple
    additional headers. For example:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用cURL传递附加的标头。使用`-H "Header"`传递多个附加标头。例如：
- en: '[PRE28]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Specifying bandwidth limit on cURL
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在cURL上指定带宽限制
- en: 'When the available bandwidth is limited and multiple users are sharing the
    Internet, in order to perform the sharing of bandwidth smoothly, we can limit
    the download rate to a specified limit from `curl` by using the `--limit-rate`
    option as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当可用带宽有限且多个用户共享互联网时，为了平稳地共享带宽，我们可以通过使用`--limit-rate`选项从`curl`限制下载速率到指定的限制。
- en: '[PRE29]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In this command `k` (kilobyte) and `m` (megabyte) specify the download rate
    limit.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个命令中，`k`（千字节）和`m`（兆字节）指定了下载速率限制。
- en: Specifying the maximum download size
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指定最大下载大小
- en: 'The maximum download file size for cURL can be specified using the `--max-filesize`
    option as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`--max-filesize`选项指定cURL的最大下载文件大小如下：
- en: '[PRE30]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: It will return a non-zero exit code if the file size exceeds. It will return
    zero if it succeeds.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果文件大小超过，则返回非零退出代码。如果成功，则返回零。
- en: Authenticating with cURL
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用cURL进行身份验证
- en: HTTP authentication or FTP authentication can be done using cURL with the `-u`
    argument.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用cURL和`-u`参数进行HTTP身份验证或FTP身份验证。
- en: The username and password can be specified using `-u username:password`. It
    is possible to not provide a password such that it will prompt for password while
    executing.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`-u username:password`指定用户名和密码。也可以不提供密码，这样在执行时会提示输入密码。
- en: 'If you prefer to be prompted for the password, you can do that by using only
    `-u username`. For example:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望提示输入密码，可以仅使用`-u username`。例如：
- en: '[PRE31]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In order to be prompted for the password use:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提示输入密码，请使用：
- en: '[PRE32]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Printing response headers excluding data
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 打印响应标头，不包括数据
- en: It is useful to print only response headers to apply many checks or statistics.
    For example, to check whether a page is reachable or not, we don't need to download
    the entire page contents. Just reading the HTTP response header can be used to
    identify if a page is available or not.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 仅打印响应标头非常有用，可以应用许多检查或统计。例如，要检查页面是否可访问，我们不需要下载整个页面内容。只需读取HTTP响应标头即可用于识别页面是否可用。
- en: An example usage case for checking the HTTP header is to check the file size
    before downloading. We can check the `Content-Length` parameter in the HTTP header
    to find out the length of a file before downloading. Also, several useful parameters
    can be retrieved from the header. The `Last-Modified` parameter enables to know
    the last modification time for the remote file.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 检查HTTP标头的一个示例用例是在下载之前检查文件大小。我们可以检查HTTP标头中的`Content-Length`参数以找出文件的长度。还可以从标头中检索到几个有用的参数。`Last-Modified`参数使我们能够知道远程文件的最后修改时间。
- en: 'Use the `–I` or `–head` option with `curl` to dump only HTTP headers without
    downloading the remote file. For example:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`curl`的`–I`或`–head`选项仅转储HTTP标头而不下载远程文件。例如：
- en: '[PRE33]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: See also
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '*Posting to a web page and reading response*'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*发布到网页并读取响应*'
- en: Accessing Gmail from the command line
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从命令行访问Gmail
- en: 'Gmail is a widely-used free e-mail service from Google[: http://mail.google.com/](http://:
    http://mail.google.com/). Gmail allows you to read your mail via authenticated
    RSS feeds. We can parse the RSS feeds with the sender''s name and an e-mail with
    subject. It will help to have a look at unread mails in the inbox without opening
    the web browser.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 'Gmail是谷歌提供的广泛使用的免费电子邮件服务[: http://mail.google.com/](http://: http://mail.google.com/)。
    Gmail允许您通过经过身份验证的RSS订阅来阅读邮件。我们可以解析RSS订阅，其中包括发件人的姓名和主题为电子邮件。这将有助于在不打开网络浏览器的情况下查看收件箱中的未读邮件。'
- en: How to do it...
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s go through the shell script to parse the RSS feeds for Gmail to display
    the unread mails:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过shell脚本来解析Gmail的RSS订阅以显示未读邮件：
- en: '[PRE34]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output will be as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE35]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: How it works...
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The script uses cURL to download the RSS feed by using user authentication.
    User authentication is provided by the `-u username:password` argument. You can
    use `-u user` without providing the password. Then while executing cURL it will
    interactively ask for the password.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本使用cURL通过用户身份验证下载RSS订阅。用户身份验证由`-u username:password`参数提供。您可以使用`-u user`而不提供密码。然后在执行cURL时，它将交互式地要求输入密码。
- en: Here we can split the piped commands into different blocks to illustrate how
    they work.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以将管道命令拆分为不同的块，以说明它们的工作原理。
- en: '`tr -d ''\n''` removes the newline character so that we restructure each mail
    entry with `\n` as the delimiter. `sed ''s:</entry>:\n:g''` replaces every `</entry>`
    with a newline so that each mail entry is delimited by a newline and hence mails
    can be parsed one by one. Have a look at the source of [https://mail.google.com/mail/feed/atom](https://mail.google.com/mail/feed/atom)
    for XML tags used in the RSS feeds. `<entry> TAGS </entry>` corresponds to a single
    mail entry.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`tr -d ''\n''`删除换行符，以便我们使用`\n`作为分隔符重构每个邮件条目。`sed ''s:</entry>:\n:g''`将每个`</entry>`替换为换行符，以便每个邮件条目都由换行符分隔，因此可以逐个解析邮件。查看[https://mail.google.com/mail/feed/atom](https://mail.google.com/mail/feed/atom)的源代码，了解RSS订阅中使用的XML标记。`<entry>
    TAGS </entry>`对应于单个邮件条目。'
- en: 'The next block of script is as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个脚本块如下：
- en: '[PRE36]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This script matches the substring title using `<title>\(.*\)<\/title`, the
    sender name using `<author><name>\([^<]*\)<\/name>`, and e-mail using `<email>\([^<]*\)`.
    Then back referencing is used as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本使用`<title>\(.*\)<\/title`匹配子字符串标题，使用`<author><name>\([^<]*\)<\/name>`匹配发件人姓名，使用`<email>\([^<]*\)`匹配电子邮件。然后使用反向引用如下：
- en: '`Author: \2 [\3] \nSubject: \1\n` is used to replace an entry for a mail with
    the matched items in an easy-to-read format. `\1` corresponds to the first substring
    match, `\2` for the second substring match, and so on.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Author: \2 [\3] \nSubject: \1\n`用于以易于阅读的格式替换邮件的条目。`\1`对应于第一个子字符串匹配，`\2`对应于第二个子字符串匹配，依此类推。'
- en: The `SHOW_COUNT=5` variable is used to take the number of unread mail entries
    to be printed on terminal.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SHOW_COUNT=5`变量用于在终端上打印未读邮件条目的数量。'
- en: '`head` is used to display only `SHOW_COUNT*3` lines from the first line. `SHOW_COUNT`
    is used three times in order to show three lines of the output.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`head`用于仅显示来自第一行的`SHOW_COUNT*3`行。 `SHOW_COUNT`被使用三次，以便显示输出的三行。'
- en: See also
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '*A primer on cURL*, explains the curl command'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*cURL入门*，解释了curl命令'
- en: '*Basic sed primer* of[Chapter 4](ch04.html "Chapter 4. Texting and Driving"),
    explains the sed command'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基本的sed入门*[第4章](ch04.html "第4章。发短信和开车")，解释了sed命令'
- en: Parsing data from a website
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从网站解析数据
- en: It is often useful to parse data from web pages by eliminating unnecessary details.
    `sed` and `awk` are the main tools that we will use for this task. You might have
    come across a list of access rankings in a grep recipe in the previous chapter
    *Texting and driving*; it was generated by parsing the website page [http://www.johntorres.net/BoxOfficefemaleList.html](http://www.johntorres.net/BoxOfficefemaleList.html).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 通过消除不必要的细节，从网页中解析数据通常很有用。`sed`和`awk`是我们将用于此任务的主要工具。您可能已经在上一章的grep示例中看到了一个访问排名列表；它是通过解析网站页面[http://www.johntorres.net/BoxOfficefemaleList.html](http://www.johntorres.net/BoxOfficefemaleList.html)生成的。
- en: Let's see how to parse the same data using text-processing tools.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用文本处理工具解析相同的数据。
- en: How to do it...
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s go through the command sequence used to parse details of actresses from
    the website:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过用于解析女演员详情的命令序列：
- en: '[PRE37]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output will be as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE38]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: How it works...
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Lynx is a command-line web browser; it can dump the text version of the website
    as we would see in a web browser rather than showing us the raw code. Hence it
    avoids the job of removing the HTML tags. We parse the lines starting with Rank,
    using `sed` as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Lynx是一个命令行网页浏览器；它可以转储网站的文本版本，就像我们在网页浏览器中看到的那样，而不是显示原始代码。因此，它避免了删除HTML标记的工作。我们使用`sed`解析以Rank开头的行，如下所示：
- en: '[PRE39]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: These lines could be then sorted according to the ranks. `awk` is used here
    to keep the spacing between rank and the name uniform by specifying the width.
    `%-4s` specifies a four-character width. All the fields except the first field
    are concatenated to form a single string as `$2`.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以根据排名对这些行进行排序。这里使用`awk`来保持排名和名称之间的间距，通过指定宽度来使其统一。`%-4s`指定四个字符的宽度。除了第一个字段之外的所有字段都被连接在一起形成一个单个字符串`$2`。
- en: See also
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '*Basic sed primer* of[Chapter 4](ch04.html "Chapter 4. Texting and Driving"),
    explains the sed command'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第4章的基本sed入门*，解释了sed命令'
- en: '*Basic awk primer* of[Chapter 4](ch04.html "Chapter 4. Texting and Driving"),
    explains the awk command'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第4章的基本awk入门*，解释了awk命令'
- en: '*Downloading a web page as formatted plain text*, explains the lynx command'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*以格式化纯文本形式下载网页*，解释了lynx命令'
- en: Image crawler and downloader
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像爬虫和下载器
- en: Image crawlers are very useful when we need to download all the images that
    appear in a web page. Instead of going through the HTML sources and picking all
    the images, we can use a script to parse the image files and download them automatically.
    Let's see how to do it.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们需要下载出现在网页中的所有图像时，图像爬虫非常有用。我们可以使用脚本来解析图像文件并自动下载，而不是查看HTML源并选择所有图像。让我们看看如何做到这一点。
- en: How to do it...
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s write a Bash script to crawl and download the images from a web page
    as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个Bash脚本来爬取并从网页下载图像，如下所示：
- en: '[PRE40]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'An example usage is as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例用法如下：
- en: '[PRE41]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: How it works...
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The above image downloader script parses an HTML page, strips out all tags except
    `<img>`, then parses `src="img/URL"` from the `<img>` tag and downloads them to
    the specified directory. This script accepts a web page URL and the destination
    directory path as command-line arguments. The first part of the script is a tricky
    way to parse command-line arguments. The `[ $# -ne 3 ]` statement checks whether
    the total number of arguments to the script is three, else it exits and returns
    a usage example.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图像下载器脚本解析HTML页面，除了`<img>`之外剥离所有标记，然后从`<img>`标记中解析`src="img/URL"`并将其下载到指定目录。此脚本接受网页URL和目标目录路径作为命令行参数。脚本的第一部分是解析命令行参数的一种巧妙方法。`[
    $# -ne 3 ]`语句检查脚本的参数总数是否为三，否则退出并返回一个使用示例。
- en: 'If it is 3 arguments, then parse the URL and the destination directory. In
    order to do that a tricky hack is used:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有3个参数，那么解析URL和目标目录。为了做到这一点，使用了一个巧妙的技巧：
- en: '[PRE42]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: A `for` loop is iterated four times (there is no significance to the number
    four, it is just to iterate a couple of times to run the case statement).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`for`循环迭代了四次（数字四没有特殊意义，只是为了运行`case`语句几次）。'
- en: 'The `case` statement will evaluate the first argument (`$1`), and matches `-d`
    or any other string arguments that are checked. We can place the `-d` argument
    anywhere in the format as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`case`语句将评估第一个参数（`$1`），并匹配`-d`或任何其他检查的字符串参数。我们可以在格式中的任何位置放置`-d`参数，如下所示：'
- en: '[PRE43]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Or:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 或：
- en: '[PRE44]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '`shift` is used to shift arguments such that when `shift` is called `$1` will
    be assigned with `$2`, when again called `$1=$3` and so on as it shifts `$1` to
    the next arguments. Hence we can evaluate all arguments through `$1` itself.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`shift`用于移动参数，这样当调用`shift`时，`$1`将被赋值为`$2`，再次调用时，`$1=$3`，依此类推，因为它将`$1`移动到下一个参数。因此，我们可以通过`$1`本身评估所有参数。'
- en: When `-d` is matched ( `-d)` ), it is obvious that the next argument is the
    value for the destination directory. `*)` corresponds to default match. It will
    match anything other than `-d`. Hence while iteration `$1=""` or `$1=URL` in the
    default match, we need to take `$1=URL` avoiding `""` to overwrite. Hence we use
    the `url=${url:-$1}` trick. It will return a URL value if already not `""` else
    it will assign `$1`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 当匹配`-d`（`-d)`）时，很明显下一个参数是目标目录的值。`*）`对应默认匹配。它将匹配除`-d`之外的任何内容。因此，在迭代时，`$1=""`或`$1=URL`在默认匹配中，我们需要取`$1=URL`避免`""`覆盖。因此我们使用`url=${url:-$1}`技巧。如果已经不是`""`，它将返回一个URL值，否则它将分配`$1`。
- en: '`egrep -o "<img src=[^>]*>"` will print only the matching strings, which are
    the `<img>` tags including their attributes. `[^>]*` used to match all characters
    except the closing `>`, that is, `<img src="img/image.jpg" …. >`.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`egrep -o "<img src=[^>]*>"`将仅打印匹配的字符串，即包括其属性的`<img>`标记。`[^>]*`用于匹配除了结束`>`之外的所有字符，即`<img
    src="img/image.jpg" …. >`。'
- en: '`sed ''s/<img src=\"\([^"]*\).*/\1/g''` parses `src="img/url"` so that all
    image URLs can be parsed from the `<img>` tags already parsed.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`sed ''s/<img src=\"\([^"]*\).*/\1/g''`解析`src="img/url"`，以便可以从已解析的`<img>`标记中解析所有图像URL。'
- en: 'There are two types of image source paths: relative and absolute. Absolute
    paths contain full URLs that start with `http://` or `https://`. Relative URLs
    starts with `/` or `image_name` itself.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的图像源路径：相对和绝对。绝对路径包含以`http://`或`https://`开头的完整URL。相对URL以`/`或`image_name`本身开头。
- en: 'An example of an absolute URL is: [http://example.com/image.jpg](http://example.com/image.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对URL的示例是：[http://example.com/image.jpg](http://example.com/image.jpg)
- en: 'An example of a relative URL is: `/image.jpg`'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 相对URL的示例是：`/image.jpg`
- en: For relative URLs the starting `/` should be replaced with the base URL to transform
    it to [http://example.com/image.jpg](http://example.com/image.jpg).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 对于相对URL，起始的`/`应该被替换为基本URL，以将其转换为[http://example.com/image.jpg](http://example.com/image.jpg)。
- en: For that transformation, we initially find out `baseurl sed` by parsing.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行转换，我们首先通过解析找出`baseurl sed`。
- en: Then replace every occurrence of the starting `/` with `baseurl sed` as `sed
    -i "s|^/|$baseurl/|" /tmp/$$.list`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 然后用`sed -i "s|^/|$baseurl/|" /tmp/$$.list`将起始的`/`替换为`baseurl sed`。
- en: Then a `while` loop is used to iterate the list line by line and download the
    URL using `curl`. The `--silent` argument is used with `curl` to avoid other progress
    messages from being printed on the screen.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用`while`循环逐行迭代列表，并使用`curl`下载URL。使用`--silent`参数与`curl`一起，以避免在屏幕上打印其他进度消息。
- en: See also
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '*A primer on cURL*, explains the curl command'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*cURL入门*，解释了curl命令'
- en: '*Basic sed primer* of[Chapter 4](ch04.html "Chapter 4. Texting and Driving"),
    explains the sed command'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基本的sed入门* [第4章](ch04.html "第4章。发短信和开车") ，解释sed命令'
- en: '*Searching and mining "text" inside a file with grep* of[Chapter 4](ch04.html
    "Chapter 4. Texting and Driving"), explains the grep command'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用grep在文件中搜索和挖掘“文本”* [第4章](ch04.html "第4章。发短信和开车") ，解释grep命令'
- en: Web photo album generator
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Web相册生成器
- en: Web developers commonly design photo album pages for websites that consist of
    a number of image thumbnails on the page. When thumbnails are clicked, a large
    version of the picture will be displayed. But when many images are required, copying
    the `<img>` tag every time, resizing the image to create a thumbnail, placing
    them in the thumbs directory, testing the links, and so on are real hurdles. It
    takes a lot of time and repeats the same task. It can be automated easily by writing
    a simple Bash script. By writing a script, we can create thumbnails, place them
    in exact directories, and generate the code fragment for `<img>` tags automatically
    in few seconds. This recipe will teach you how to do it.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Web开发人员通常为网站设计照片相册页面，该页面包含页面上的许多图像缩略图。单击缩略图时，将显示图片的大版本。但是，当需要许多图像时，每次复制`<img>`标签，调整图像以创建缩略图，将它们放在thumbs目录中，测试链接等都是真正的障碍。这需要很多时间并且重复相同的任务。通过编写一个简单的Bash脚本，可以轻松自动化。通过编写脚本，我们可以在几秒钟内自动创建缩略图，将它们放在确切的目录中，并自动生成`<img>`标签的代码片段。这个配方将教你如何做到这一点。
- en: Getting ready
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We can perform this task with a `for` loop that iterates every image in the
    current directory. The usual Bash utilities such as `cat` and `convert` (image
    magick) are used. These will generate an HTML album, using all the images, to
    `index.html`. In order to use `convert`, make sure you have Imagemagick installed.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`for`循环执行此任务，该循环遍历当前目录中的每个图像。通常使用Bash实用程序，如`cat`和`convert`（image magick）。这些将生成一个HTML相册，使用所有图像，放在`index.html`中。为了使用`convert`，请确保已安装Imagemagick。
- en: How to do it...
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s write a Bash script to generate a HTML album page:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个Bash脚本来生成HTML相册页面：
- en: '[PRE45]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Run the script as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 按以下方式运行脚本：
- en: '[PRE46]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: How it works...
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The initial part of the script is to write the header part of the HTML page.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本的初始部分是编写HTML页面的标题部分。
- en: 'The following script redirects all the contents up to EOF (excluding) to the
    `index.html`:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本将所有内容重定向到EOF（不包括）到`index.html`：
- en: '[PRE47]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The header includes the HTML and stylesheets.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 标题包括HTML和样式表。
- en: '`for img in *.jpg;` will iterate through names of each file and will perform
    actions.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`for img in *.jpg;`将遍历每个文件的名称并执行操作。'
- en: '`convert "$img" -resize "100x" "thumbs/$img"` will create images of 100px width
    as thumbnails.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`convert "$img" -resize "100x" "thumbs/$img"`将创建宽度为100px的图像作为缩略图。'
- en: 'The following statement will generate the required `<img>` tag and appends
    it to the `index.html`:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以下语句将生成所需的`<img>`标签并将其附加到`index.html`：
- en: '[PRE48]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Finally, the footer HTML tags are appended with `cat` again.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用`cat`附加页脚HTML标记。
- en: See also
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '*Playing with file descriptors and redirection* of[Chapter 1](ch01.html "Chapter 1. Shell
    Something Out"), explains EOF and stdin redirection.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*玩转文件描述符和重定向* [第1章](ch01.html "第1章。外壳的某些东西") ，解释EOF和stdin重定向。'
- en: Twitter command-line client
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Twitter命令行客户端
- en: Twitter is the hottest micro blogging platform as well as the latest buzz of
    online social media. Tweeting and reading tweets is fun. What if we can do both
    from command line? It is pretty simple to write a command-line Twitter client.
    Twitter has RSS feeds and hence we can make use of them. Let's see how to do it.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter是最热门的微博平台，也是在线社交媒体的最新热点。发推文和阅读推文很有趣。如果我们可以从命令行做这两件事呢？编写命令行Twitter客户端非常简单。Twitter有RSS
    feeds，因此我们可以利用它们。让我们看看如何做到这一点。
- en: Getting ready
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We can use cURL to authenticate and send twitter updates as well as download
    the RSS feed pages to parse the tweets. Just four lines of code can do it. Let's
    do it.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用cURL进行身份验证并发送twitter更新，以及下载RSS feed页面以解析tweets。只需四行代码就可以做到。让我们来做吧。
- en: How to do it...
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s write a Bash script using the `curl` command to manipulate twitter APIs:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个Bash脚本，使用`curl`命令来操作twitter API：
- en: '[PRE49]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Run the script as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下脚本：
- en: '[PRE50]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: How it works...
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'Let''s see the working of above script by splitting it into two parts. The
    first part is about reading tweets. To read tweets the script downloads the RSS
    information from [http://twitter.com/statuses/friends_timeline.rss](http://twitter.com/statuses/friends_timeline.rss)
    and parses the lines containing the `<title>` tag. Then it strips off the `<title>`
    and `</title>` tags using `sed` to form the required tweet text. Then a `COUNT`
    variable is used to remove all other text except the number of recent tweets by
    using the `head` command. `tail –n +2` is used to remove an unnecessary header
    text "Twitter: Timeline of friends".'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们通过将上述脚本分成两部分来看看它的工作。第一部分是关于阅读推文的。要阅读推文，脚本会从[http://twitter.com/statuses/friends_timeline.rss](http://twitter.com/statuses/friends_timeline.rss)下载RSS信息，并解析包含`<title>`标签的行。然后，它使用`sed`剥离`<title>`和`</title>`标签，以形成所需的推文文本。然后使用`COUNT`变量来使用`head`命令除了最近推文的数量之外的所有其他文本。使用`tail
    -n +2`来删除不必要的标题文本“Twitter: Timeline of friends”。'
- en: 'In the sending tweet part, the `-d` status argument of `curl` is used to post
    data to Twitter using their API: [http://twitter.com/statuses/update.xml](http://twitter.com/statuses/update.xml).'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在发送推文部分，`curl`的`-d`状态参数用于使用Twitter的API发布数据到Twitter：[http://twitter.com/statuses/update.xml](http://twitter.com/statuses/update.xml)。
- en: '`$1` of the script will be the tweet in the case of sending a tweet. Then to
    obtain the status we take `$@` (list of all arguments of the script) and remove
    the word "tweet" from it.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在发送推文的情况下，脚本的`$1`将是推文。然后，为了获取状态，我们使用`$@`（脚本的所有参数的列表）并从中删除单词“tweet”。
- en: See also
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '*A primer on cURL*, explains the curl command'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*cURL入门*，解释了curl命令'
- en: '*head and tail - printing the last or first 10 lines* of[Chapter 3](ch03.html
    "Chapter 3. File In, File Out"), explains the commands head and tail'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*头和尾-打印最后或前10行* [第3章](ch03.html "第3章。文件输入，文件输出")，解释了头和尾命令'
- en: define utility with Web backend
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 具有Web后端的定义实用程序
- en: Google provides Web definitions for any word by using the search query `define:WORD`.
    We need a GUI web browser to fetch the definitions. However, we can automate it
    and parse the required definitions by using a script. Let's see how to do it.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌通过使用搜索查询`define:WORD`为任何单词提供Web定义。我们需要一个GUI网页浏览器来获取定义。但是，我们可以通过使用脚本来自动化并解析所需的定义。让我们看看如何做到这一点。
- en: Getting ready
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We can use `lynx`, `sed`, `awk`, and `grep` to write the define utility.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`lynx`，`sed`，`awk`和`grep`来编写定义实用程序。
- en: How to do it...
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s go through the code for the define utility script to fetch definitions
    from Google search:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看从Google搜索中获取定义的定义实用程序脚本的核心部分：
- en: '[PRE51]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Run the script as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 按以下方式运行脚本：
- en: '[PRE52]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: How it works...
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We will look into the core part of the definition parser. Lynx is used to obtain
    the plain text version of the web page. [http://www.google.co.in/search?q=define:$word](http://www.google.co.in/search?q=define:$word)
    is the URL for the web definition web page. Then we reduce the text between "Definitions
    on web" and "Find definitions". All the definitions are occurring in between these
    lines of text (`awk '/Defini/,/Find defini/'`).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究定义解析器的核心部分。Lynx用于获取网页的纯文本版本。[http://www.google.co.in/search?q=define:$word](http://www.google.co.in/search?q=define:$word)是网页定义网页的URL。然后我们缩小“网页上的定义”和“查找定义”之间的文本。所有的定义都出现在这些文本行之间（`awk
    '/Defini/,/Find defini/'`）。
- en: '`''s:*:\n*:''` is used to replace * with * and newline in order to insert a
    newline in between each definition, and `s:^[ ]*::` is used to remove extra spaces
    in the start of lines. Hyperlinks are marked as [number] in lynx output. Those
    lines are removed by `grep -v`, the invert match lines option. Then `awk` is used
    to replace the * occurring at start of the line with a number so that each definition
    can assign a serial number. If we have read a `-n` count in the script, it has
    to output only a few definitions as per count. So `awk` is used to print the definitions
    with number 1 to count (this makes it easier since we replaced * with the serial
    number).'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`''s:*:\n*:''`用于将*替换为*和换行符，以便在每个定义之间插入换行符，`s:^[ ]*::`用于删除行首的额外空格。在lynx输出中，超链接标记为[数字]。这些行通过`grep
    -v`（反向匹配行选项）被移除。然后使用`awk`将出现在行首的*替换为数字，以便为每个定义分配一个序号。如果我们在脚本中读取了一个`-n`计数，它必须根据计数输出一些定义。因此，使用`awk`打印序号1到计数的定义（这样做更容易，因为我们用序号替换了*）。'
- en: See also
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '*Basic sed primer* of[Chapter 4](ch04.html "Chapter 4. Texting and Driving"),
    explains the sed command'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基本的sed入门* [第4章](ch04.html "第4章。发短信和开车")，解释了sed命令'
- en: '*Basic awk primer* of[Chapter 4](ch04.html "Chapter 4. Texting and Driving"),
    explains the awk command'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基本的awk入门* [第4章](ch04.html "第4章。发短信和开车")，解释了awk命令'
- en: '*Searching and mining "text" inside a file with grep* of[Chapter 4](ch04.html
    "Chapter 4. Texting and Driving"), explains the grep command'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用grep在文件中搜索和挖掘“文本”* [第4章](ch04.html "第4章。发短信和开车")，解释了grep命令'
- en: '*Downloading a web page as formatted plain text*, explains the lynx command'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将网页下载为格式化的纯文本*，解释了lynx命令'
- en: Finding broken links in a website
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在网站中查找损坏的链接
- en: I have seen people manually checking each and every page on a site to search
    for broken links. It is possible only for websites having very few pages. When
    the number of pages become large, it will become impossible. It becomes really
    easy if we can automate finding broken links. We can find the broken links by
    using HTTP manipulation tools. Let's see how to do it.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我看到人们手动检查网站上的每个页面以查找损坏的链接。这仅适用于页面非常少的网站。当页面数量变得很多时，这将变得不可能。如果我们可以自动查找损坏的链接，那将变得非常容易。我们可以使用HTTP操作工具来查找损坏的链接。让我们看看如何做到这一点。
- en: Getting ready
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In order to identify the links and find the broken ones from the links, we can
    use `lynx` and `curl`. It has an option `-traversal`, which will recursively visit
    pages in the website and build the list of all hyperlinks in the website. We can
    use cURL to verify whether each of the links are broken or not.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 为了识别链接并从链接中找到损坏的链接，我们可以使用`lynx`和`curl`。它有一个`-traversal`选项，它将递归访问网站中的页面，并构建网站中所有超链接的列表。我们可以使用cURL来验证每个链接是否损坏。
- en: How to do it...
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s write a Bash script with the help of the `curl` command to find out
    the broken links on a web page:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过`curl`命令编写一个Bash脚本来查找网页上的损坏链接：
- en: '[PRE53]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: How it works...
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: '`lynx -traversal URL` will produce a number of files in the working directory.
    It includes a file `reject.dat` which will contain all the links in the website.
    `sort -u` is used to build a list by avoiding duplicates. Then we iterate through
    each link and check the header response by using `curl -I`. If the header contains
    first line `HTTP/1.0 200 OK` as the response, it means that the target is not
    broken. All other responses correspond to broken links and are printed out to
    `stdout`.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '`lynx -traversal URL`将在工作目录中生成多个文件。其中包括一个名为`reject.dat`的文件，其中包含网站中的所有链接。使用`sort
    -u`来避免重复构建列表。然后我们遍历每个链接，并使用`curl -I`检查标题响应。如果标题包含第一行`HTTP/1.0 200 OK`作为响应，这意味着目标不是损坏的。所有其他响应对应于损坏的链接，并打印到`stdout`。'
- en: See also
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '*Downloading a web page as formatted plain text*, explains the lynx command'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*以格式化纯文本形式下载网页*，解释了lynx命令'
- en: '*A primer on cURL*, explains the curl command'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*cURL入门*，解释了curl命令'
- en: Tracking changes to a website
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跟踪网站的变化
- en: Tracking changes to a website is helpful to web developers and users. Checking
    a website manually in intervals is really hard and impractical. Hence we can write
    a change tracker running at repeated intervals. When a change occurs, it can play
    a sound or send a notification. Let's see how to write a basic tracker for the
    website changes.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪网站的变化对于网页开发人员和用户非常有帮助。在间隔时间内手动检查网站非常困难和不切实际。因此，我们可以编写一个在重复间隔时间内运行的变化跟踪器。当发生变化时，它可以播放声音或发送通知。让我们看看如何编写一个基本的网站变化跟踪器。
- en: Getting ready
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Tracking changes in terms of Bash scripting means fetching websites at different
    times and taking the difference using the `diff` command. We can use `curl` and
    `diff` to do this.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在Bash脚本中跟踪网页变化意味着在不同时间获取网站并使用`diff`命令进行差异。我们可以使用`curl`和`diff`来做到这一点。
- en: How to do it...
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s write a Bash script by combining different commands to track changes
    in a web page:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过组合不同的命令来编写一个Bash脚本来跟踪网页中的变化：
- en: '[PRE54]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let''s look at the output of the `track_changes.sh` script when changes are
    made to the web page and when the changes are not made to the page:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`track_changes.sh`脚本在网页发生变化和网页未发生变化时的输出：
- en: 'First run:'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首次运行：
- en: '[PRE55]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Second Run:'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二次运行：
- en: '[PRE56]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Third run after making changes to the web page:'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对网页进行更改后的第三次运行：
- en: '[PRE57]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: How it works...
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The script checks whether the script is running for the first time using `[
    ! -e "last.html" ];`. If `last.html` doesn't exist, that means it is the first
    time and hence the webpage must be downloaded and copied as `last.html`.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本通过`[！-e“last.html”]`检查脚本是否是第一次运行。如果`last.html`不存在，这意味着这是第一次，因此必须下载网页并将其复制为`last.html`。
- en: If it is not the first time, it should download the new copy (`recent.html`)
    and check the difference using the `diff` utility. If changes are there, it should
    print the changes and finally it should copy `recent.html` to `last.html`.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不是第一次，它应该下载新副本（`recent.html`）并使用`diff`实用程序检查差异。如果有变化，它应该打印出变化，最后应该将`recent.html`复制到`last.html`。
- en: See also
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '*A primer on cURL*, explains the curl command'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*cURL入门*，解释了curl命令'
- en: Posting to a web page and reading response
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向网页提交并读取响应
- en: POST and GET are two types of requests in HTTP to send information to or retrieve
    information from a website. In a GET request, we send parameters (name-value pairs)
    through the web page URL itself. In the case of POST, it won't be attached with
    the URL. POST is used when a form needs to be submitted. For example, a username,
    the password to be submitted, and the login page to be retrieved.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: POST和GET是HTTP中用于向网站发送信息或检索信息的两种请求类型。在GET请求中，我们通过网页URL本身发送参数（名称-值对）。在POST的情况下，它不会附加在URL上。当需要提交表单时使用POST。例如，需要提交用户名、密码和检索登录页面。
- en: POSTing to pages comes as frequent use while writing scripts based on web page
    retrievals. Let's see how to work with POST. Automating the HTTP GET and POST
    request by sending POST data and retrieving output is a very important task that
    we practice while writing shell scripts that parse data from websites.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写基于网页检索的脚本时，POST到页面的使用频率很高。让我们看看如何使用POST。通过发送POST数据和检索输出来自动执行HTTP GET和POST请求是我们在编写从网站解析数据的shell脚本时练习的非常重要的任务。
- en: Getting ready
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Both cURL and `wget` can handle POST requests by arguments. They are to be passed
    as name-value pairs.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: cURL和`wget`都可以通过参数处理POST请求。它们作为名称-值对传递。
- en: How to do it...
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s see how to POST and read HTML response from a real website using `curl`:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用`curl`从真实网站进行POST和读取HTML响应：
- en: '[PRE58]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'We have a website ([http://book.sarathlakshman.com/lsc/mlogs/](http://book.sarathlakshman.com/lsc/mlogs/))
    and it is used to submit the current user information such as hostname and username.
    Assume that, in the home page of the website there are two fields HOSTNAME and
    USER, and a SUBMIT button. When the user enters a hostname, a user name, and clicks
    on the SUBMIT button, the details will be stored in the website. This process
    can be automated using a single line of `curl` command by automating the POST
    request. If you look at the website source (use the view source option from the
    web browser), you can see an HTML form defined similar to the following code:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个网站（[http://book.sarathlakshman.com/lsc/mlogs/](http://book.sarathlakshman.com/lsc/mlogs/)），用于提交当前用户信息，如主机名和用户名。假设在网站的主页上有两个字段HOSTNAME和USER，以及一个SUBMIT按钮。当用户输入主机名、用户名并单击SUBMIT按钮时，详细信息将存储在网站中。可以使用一行`curl`命令自动化此过程，通过自动化POST请求。如果查看网站源代码（使用Web浏览器的查看源代码选项），可以看到类似于以下代码的HTML表单定义：
- en: '[PRE59]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Here, [http://book.sarathlakshman.com/lsc/mlogs/submit.php](http://book.sarathlakshman.com/lsc/mlogs/submit.php)
    is the target URL. When the user enters the details and clicks on the Submit button.
    The host and user inputs are sent to `submit.php` as a POST request and the response
    page is returned on the browser.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，[http://book.sarathlakshman.com/lsc/mlogs/submit.php](http://book.sarathlakshman.com/lsc/mlogs/submit.php)是目标URL。当用户输入详细信息并单击提交按钮时，主机和用户输入将作为POST请求发送到`submit.php`，并且响应页面将返回到浏览器。
- en: 'We can automate the POST request as follows:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按照以下方式自动化POST请求：
- en: '[PRE60]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Now `curl` returns the response page.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 现在`curl`返回响应页面。
- en: '`-d` is the argument used for posting. The string argument for `-d` is similar
    to the GET request semantics. `var=value` pairs are to be delimited by `&`.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '`-d`是用于发布的参数。 `-d`的字符串参数类似于GET请求语义。 `var=value`对应关系应该由`&`分隔。'
- en: Note
  id: totrans-315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `-d` argument should always be given in quotes. If quotes are not used,
    `&` is interpreted by the shell to indicate this should be a background process.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`-d`参数应该总是用引号括起来。如果不使用引号，`&`会被shell解释为表示这应该是一个后台进程。'
- en: There's more
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多
- en: Let's see how to perform POST using cURL and `wget`.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用cURL和`wget`执行POST。
- en: POST in curl
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在curl中进行POST
- en: 'You can POST data in `curl` by using `-d` or `–data` as follows:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`-d`或`–data`在`curl`中发送POST数据，如下所示：
- en: '[PRE61]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'If multiple variables are to be sent, delimit them with `&`. Note that when
    `&` is used the name-value pairs should be enclosed in quotes, else the shell
    will consider `&` as a special character for background process. For example:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要发送多个变量，请用`&`分隔它们。请注意，当使用`&`时，名称-值对应该用引号括起来，否则shell将把`&`视为后台进程的特殊字符。例如：
- en: '[PRE62]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: POST data using wget
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用wget发送POST数据
- en: 'You can POST data using `wget` by using `-–post-data "string"`. For example:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`wget`通过使用`-–post-data "string"`来发送POST数据。例如：
- en: '[PRE63]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Use the same format as cURL for name-value pairs.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与cURL相同的格式进行名称-值对。
- en: See also
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: '*A primer on cURL*, explains the curl command'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*关于cURL的入门*，解释了curl命令'
- en: '*Downloading from a web page* explains the wget command'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*从网页下载*解释了wget命令'

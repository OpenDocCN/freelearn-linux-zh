["```\nstatic int ga, gb = 1;\n[...]\nga++; gb--;\n```", "```\nspin_lock(&lock1);\nga++; gb--;\nspin_unlock(&lock1);\n```", "```\n// include/linux/refcount.h\n/**\n * refcount_set - set a refcount's value\n * @r: the refcount\n * @n: value to which the refcount will be set\n */\nstatic inline void refcount_set(refcount_t *r, unsigned int n)\n{\n    atomic_set(&r->refs, n); \n}\n```", "```\n// include/linux/sched/task.h\nstatic inline struct task_struct *get_task_struct(struct task_struct *t) \n{\n    refcount_inc(&t->usage);\n    return t;\n}\n```", "```\nstatic inline void put_task_struct(struct task_struct *t) \n{\n    if (refcount_dec_and_test(&t->usage))\n        __put_task_struct(t);\n}\n```", "```\n#define REG_BASE        0x5a00\n#define STATUS_REG      (REG_BASE+0x0)\n#define CTRL_REG        (REG_BASE+0x1)\n```", "```\nturn_on_dev()\n{\n    u8 tmp;\n\n    tmp = ioread8(CTRL_REG);  /* read: current register value into tmp */\n    tmp |= 0x80;              /* modify: set bit 7 (MSB) */\n    iowrite8(tmp, CTRL_REG);  /* write: new tmp value into register */\n}\n```", "```\n// Documentation/atomic_t.txt\n[ ... ]\nNon-RMW ops:\n  atomic_read(), atomic_set()\n  atomic_read_acquire(), atomic_set_release()\n\nRMW atomic operations:\n\nArithmetic:\n  atomic_{add,sub,inc,dec}()\n  atomic_{add,sub,inc,dec}_return{,_relaxed,_acquire,_release}()\n  atomic_fetch_{add,sub,inc,dec}{,_relaxed,_acquire,_release}()\n\nBitwise:\n  atomic_{and,or,xor,andnot}()\n  atomic_fetch_{and,or,xor,andnot}{,_relaxed,_acquire,_release}()\n\nSwap:\n  atomic_xchg{,_relaxed,_acquire,_release}()\n  atomic_cmpxchg{,_relaxed,_acquire,_release}()\n  atomic_try_cmpxchg{,_relaxed,_acquire,_release}()\n\nReference count (but please see refcount_t):\n  atomic_add_unless(), atomic_inc_not_zero()\n  atomic_sub_and_test(), atomic_dec_and_test()\n\nMisc:\n  atomic_inc_and_test(), atomic_add_negative()\n  atomic_dec_unless_positive(), atomic_inc_unless_negative()\n[ ... ]\n```", "```\nvoid set_bit(unsigned int nr, volatile unsigned long *p);\n```", "```\nset_bit(7, CTRL_REG);\n```", "```\n// ch13/1_rmw_atomic_bitops/rmw_atomic_bitops.c\n[ ... ]\n#include <linux/spinlock.h>\n#include <linux/atomic.h>\n#include <linux/bitops.h>\n#include \"../../convenient.h\"\n[ ... ]\nstatic unsigned long mem;\nstatic u64 t1, t2; \nstatic int MSB = BITS_PER_BYTE - 1;\nDEFINE_SPINLOCK(slock);\n```", "```\n[ ... ]\n#define SHOW(n, p, msg) do {                                   \\\n    pr_info(\"%2d:%27s: mem : %3ld = 0x%02lx\\n\", n, msg, p, p); \\\n} while (0)\n[ ... ]\nstatic int __init atomic_rmw_bitops_init(void)\n{\n    int i = 1, ret;\n\n    pr_info(\"%s: inserted\\n\", OURMODNAME);\n    SHOW(i++, mem, \"at init\");\n\n    setmsb_optimal(i++);\n    setmsb_suboptimal(i++);\n\n    clear_bit(MSB, &mem);\n    SHOW(i++, mem, \"clear_bit(7,&mem)\");\n\n    change_bit(MSB, &mem);\n    SHOW(i++, mem, \"change_bit(7,&mem)\");\n\n    ret = test_and_set_bit(0, &mem);\n    SHOW(i++, mem, \"test_and_set_bit(0,&mem)\");\n    pr_info(\" ret = %d\\n\", ret);\n\n    ret = test_and_clear_bit(0, &mem);\n    SHOW(i++, mem, \"test_and_clear_bit(0,&mem)\");\n    pr_info(\" ret (prev value of bit 0) = %d\\n\", ret);\n\n    ret = test_and_change_bit(1, &mem);\n    SHOW(i++, mem, \"test_and_change_bit(1,&mem)\");\n    pr_info(\" ret (prev value of bit 1) = %d\\n\", ret);\n\n    pr_info(\"%2d: test_bit(%d-0,&mem):\\n\", i, MSB);\n    for (i = MSB; i >= 0; i--)\n        pr_info(\" bit %d (0x%02lx) : %s\\n\", i, BIT(i), test_bit(i, &mem)?\"set\":\"cleared\");\n\n    return 0; /* success */\n}\n```", "```\n/* Set the MSB; optimally, with the set_bit() RMW atomic API */\nstatic inline void setmsb_optimal(int i)\n{\n    t1 = ktime_get_real_ns();\n    set_bit(MSB, &mem);\n    t2 = ktime_get_real_ns();\n    SHOW(i, mem, \"set_bit(7,&mem)\");\n    SHOW_DELTA(t2, t1);\n}\n/* Set the MSB; the traditional way, using a spinlock to protect the RMW\n * critical section */\nstatic inline void setmsb_suboptimal(int i)\n{\n    u8 tmp;\n\n    t1 = ktime_get_real_ns();\n    spin_lock(&slock);\n /* critical section: RMW : read, modify, write */\n    tmp = mem;\n    tmp |= 0x80; // 0x80 = 1000 0000 binary\n    mem = tmp;\n    spin_unlock(&slock);\n    t2 = ktime_get_real_ns();\n\n    SHOW(i, mem, \"set msb suboptimal: 7,&mem\");\n    SHOW_DELTA(t2, t1);\n}\n```", "```\nspin_lock(mylist_lock);\nfor (p = &listhead; (p = next_node(p)) != &listhead; ) {\n    << ... search for something ... \n         found? break out ... >>\n}\nspin_unlock(mylist_lock);\n```", "```\n#include <linux/rwlock.h>\nrwlock_t mylist_lock;\n```", "```\nvoid read_lock(rwlock_t *lock);\nvoid write_lock(rwlock_t *lock);\n```", "```\n// drivers/tty/tty_io.c\nvoid __do_SAK(struct tty_struct *tty)\n{\n    [...]\n    read_lock(&tasklist_lock);\n    /* Kill the entire session */\n    do_each_pid_task(session, PIDTYPE_SID, p) {\n        tty_notice(tty, \"SAK: killed process %d (%s): by session\\n\", task_pid_nr(p), p->comm);\n        group_send_sig_info(SIGKILL, SEND_SIG_PRIV, p, PIDTYPE_SID);\n    } while_each_pid_task(session, PIDTYPE_SID, p);\n    [...]\n    /* Now kill any processes that happen to have the tty open */\n    do_each_thread(g, p) {\n        [...]\n    } while_each_thread(g, p);\n    read_unlock(&tasklist_lock);\n```", "```\n#include <linux/percpu.h>\nDEFINE_PER_CPU(int, pcpa);      // signature: DEFINE_PER_CPU(type, name)\n```", "```\nalloc_percpu[_gfp](type [,gfp]);\n```", "```\nDEFINE_PER_CPU(int, pcpa);\nint val;\n[ ... ]\nval = get_cpu_var(pcpa);\npr_info(\"cpu0: pcpa = %+d\\n\", val);\nput_cpu_var(pcpa);\n```", "```\nvoid *p;\nval = get_cpu_var(pcpa);\np = vmalloc(20000);\npr_info(\"cpu1: pcpa = %+d\\n\", val);\nput_cpu_var(pcpa);\nvfree(p);\n[ ... ]\n\n$ sudo insmod <whatever>.ko\n$ dmesg\n[ ... ]\nBUG: sleeping function called from invalid context at mm/slab.h:421\n[67641.443225] in_atomic(): 1, irqs_disabled(): 0, pid: 12085, name:\nthrd_1/1\n[ ... ]\n$\n```", "```\nget_cpu_var(pcpa) ++;\nput_cpu_var(pcpa);\n```", "```\nper_cpu(var, cpu);\n```", "```\nfor_each_online_cpu(i) {\n val = per_cpu(pcpa, i);\n    pr_info(\" cpu %2d: pcpa = %+d\\n\", i, val);\n}\n```", "```\n// ch13/2_percpu/percpu_var.c\n[ ... ]\n/*--- The per-cpu variables, an integer 'pcpa' and a data structure --- */\n/* This per-cpu integer 'pcpa' is statically allocated and initialized to 0 */\nDEFINE_PER_CPU(int, pcpa);\n\n/* This per-cpu structure will be dynamically allocated via alloc_percpu() */\nstatic struct drv_ctx {\n    int tx, rx; /* here, as a demo, we just use these two members,\n                   ignoring the rest */\n    [ ... ]\n} *pcp_ctx;\n[ ... ]\n\nstatic int __init init_percpu_var(void)\n{\n    [ ... ]\n    /* Dynamically allocate the per-cpu structures */\n    ret = -ENOMEM;\n pcp_ctx = (struct drv_ctx __percpu *) alloc_percpu(struct drv_ctx);\n    if (!pcp_ctx) {\n        [ ... ]\n}\n```", "```\n/* Our kernel thread worker routine */\nstatic int thrd_work(void *arg)\n{\n    int i, val;\n    long thrd = (long)arg;\n    struct drv_ctx *ctx;\n    [ ... ]\n\n    /* Set CPU affinity mask to 'thrd', which is either 0 or 1 */\n    if (set_cpuaffinity(thrd) < 0) {\n        [ ... ]\n    SHOW_CPU_CTX();\n\n    if (thrd == 0) { /* our kthread #0 runs on CPU 0 */\n        for (i=0; i<THRD0_ITERS; i++) {\n            /* Operate on our perpcu integer */\n val = ++ get_cpu_var(pcpa);\n            pr_info(\" thrd_0/cpu0: pcpa = %+d\\n\", val);\n            put_cpu_var(pcpa);\n\n            /* Operate on our perpcu structure */\n ctx = get_cpu_ptr(pcp_ctx);\n            ctx->tx += 100;\n            pr_info(\" thrd_0/cpu0: pcp ctx: tx = %5d, rx = %5d\\n\",\n                ctx->tx, ctx->rx);\n            put_cpu_ptr(pcp_ctx);\n        }\n    } else if (thrd == 1) { /* our kthread #1 runs on CPU 1 */\n        for (i=0; i<THRD1_ITERS; i++) {\n            /* Operate on our perpcu integer */\n val = -- get_cpu_var(pcpa);\n            pr_info(\" thrd_1/cpu1: pcpa = %+d\\n\", val);\n           put_cpu_var(pcpa);\n\n            /* Operate on our perpcu structure */\n            ctx = get_cpu_ptr(pcp_ctx); ctx->rx += 200;\n            pr_info(\" thrd_1/cpu1: pcp ctx: tx = %5d, rx = %5d\\n\",\n                ctx->tx, ctx->rx); put_cpu_ptr(pcp_ctx);        }}\n    disp_vars();\n    pr_info(\"Our kernel thread #%ld exiting now...\\n\", thrd);\n    return 0;\n}\n```", "```\n// arch/x86/include/asm/current.h\n[ ... ]\nDECLARE_PER_CPU(struct task_struct *, current_task);\nstatic __always_inline struct task_struct *get_current(void)\n{\n    return this_cpu_read_stable(current_task);\n}\n#define current get_current()\n```", "```\n__visible __notrace_funcgraph struct task_struct *\n__switch_to(struct task_struct *prev_p, struct task_struct *next_p)\n{\n    [ ... ]\n this_cpu_write(current_task, next_p);\n    [ ... ]\n}\n```", "```\n// https://www.kernel.org/doc/html/v5.4/process/submit-checklist.html\n[...]\n12\\. Has been tested with CONFIG_PREEMPT, CONFIG_DEBUG_PREEMPT, CONFIG_DEBUG_SLAB, CONFIG_DEBUG_PAGEALLOC, CONFIG_DEBUG_MUTEXES, CONFIG_DEBUG_SPINLOCK, CONFIG_DEBUG_ATOMIC_SLEEP, CONFIG_PROVE_RCU and CONFIG_DEBUG_OBJECTS_RCU_HEAD all simultaneously enabled. \n13\\. Has been build- and runtime tested with and without CONFIG_SMP and CONFIG_PREEMPT.\n\n16\\. All codepaths have been exercised with all lockdep features enabled.\n[ ... ]\n```", "```\n$ uname -r\n5.4.0-llkd-dbg\n$ grep PROVE_LOCKING /boot/config-5.4.0-llkd-dbg\nCONFIG_PROVE_LOCKING=y\n$\n```", "```\n// ch6/foreach/thrd_showall/thrd_showall.c\nstatic int showthrds(void)\n{\n    struct task_struct *g = NULL, *t = NULL; /* 'g' : process ptr; 't': thread ptr */\n    [ ... ]\n    do_each_thread(g, t) { /* 'g' : process ptr; 't': thread ptr */\n        task_lock(t);\n        [ ... ]\n        if (!g->mm) {    // kernel thread\n            snprintf(tmp, TMPMAX-1, \" [%16s]\", t->comm);\n        } else {\n            snprintf(tmp, TMPMAX-1, \" %16s \", t->comm);\n        }\n        snprintf(buf, BUFMAX-1, \"%s%s\", buf, tmp);\n        [ ... ]\n```", "```\n// ch13/3_lockdep/buggy_lockdep/thrd_showall_buggy.c\nstatic int showthrds_buggy(void)\n{\n    struct task_struct *g, *t; /* 'g' : process ptr; 't': thread ptr */\n    [ ... ]\n    char buf[BUFMAX], tmp[TMPMAX], tasknm[TASK_COMM_LEN];\n    [ ... ]\n    do_each_thread(g, t) { /* 'g' : process ptr; 't': thread ptr */\n        task_lock(t);\n        [ ... ]\n        get_task_comm(tasknm, t);\n        if (!g->mm) // kernel thread\n            snprintf(tmp, sizeof(tasknm)+3, \" [%16s]\", tasknm);\n        else\n            snprintf(tmp, sizeof(tasknm)+3, \" %16s \", tasknm);\n        [ ... ]\n```", "```\n// fs/exec.c\nchar *__get_task_comm(char *buf, size_t buf_size, struct task_struct *tsk)\n{\n    task_lock(tsk);\n    strncpy(buf, tsk->comm, buf_size);\n    task_unlock(tsk);\n    return buf; \n}\nEXPORT_SYMBOL_GPL(__get_task_comm);\n```", "```\ndo_each_thread(g, t) {   /* 'g' : process ptr; 't': thread ptr */\n    task_lock(t);\n    [ ... ]\n    get_task_comm(tasknm, t);\n```", "```\n// include/linux/sched/task.h */\nstatic inline void task_lock(struct task_struct *p)\n{\n    spin_lock(&p->alloc_lock);\n}\n```", "```\n[ 1021.449384] insmod/2367 is trying to acquire lock:\n[ 1021.451361] ffff88805de73f08 (&(&p->alloc_lock)->rlock){+.+.}, at: __get_task_comm+0x28/0x50\n[ 1021.453676]\n               but task is already holding lock:\n[ 1021.457365] ffff88805de73f08 (&(&p->alloc_lock)->rlock){+.+.}, at: showthrds_buggy+0x13e/0x6d1 [thrd_showall_buggy]\n```", "```\nkthread 0 on CPU #0                kthread 1 on CPU #1\n  Take lockA                           Take lockB\n     <perform work>                       <perform work>\n                                          (Try and) take lockA\n                                          < ... spins forever :\n                                                DEADLOCK ... >\n(Try and) take lockB\n< ... spins forever : \n      DEADLOCK ... >\n```", "```\n// ch13/3_lockdep/deadlock_eg_AB-BA/deadlock_eg_AB-BA.c\n[ ... ]\n/* Our kernel thread worker routine */\nstatic int thrd_work(void *arg)\n{\n    [ ... ]\n   if (thrd == 0) { /* our kthread #0 runs on CPU 0 */\n        pr_info(\" Thread #%ld: locking: we do:\"\n            \" lockA --> lockB\\n\", thrd);\n        for (i = 0; i < THRD0_ITERS; i ++) {\n            /* In this thread, perform the locking per the lock ordering 'rule';\n * first take lockA, then lockB */\n            pr_info(\" iteration #%d on cpu #%ld\\n\", i, thrd);\n            spin_lock(&lockA);\n            DELAY_LOOP('A', 3); \n            spin_lock(&lockB);\n            DELAY_LOOP('B', 2); \n            spin_unlock(&lockB);\n            spin_unlock(&lockA);\n        }\n```", "```\n   [ ... ]\n   } else if (thrd == 1) { /* our kthread #1 runs on CPU 1 */\n        for (i = 0; i < THRD1_ITERS; i ++) {\n            /* In this thread, if the parameter lock_ooo is 1, *violate* the\n * lock ordering 'rule'; first (attempt to) take lockB, then lockA */\n            pr_info(\" iteration #%d on cpu #%ld\\n\", i, thrd);\n            if (lock_ooo == 1) {        // violate the rule, naughty boy!\n                pr_info(\" Thread #%ld: locking: we do: lockB --> lockA\\n\",thrd);\n                spin_lock(&lockB);\n                DELAY_LOOP('B', 2);\n                spin_lock(&lockA);\n                DELAY_LOOP('A', 3);\n                spin_unlock(&lockA);\n                spin_unlock(&lockB);\n            } else if (lock_ooo == 0) { // follow the rule, good boy!\n                pr_info(\" Thread #%ld: locking: we do: lockA --> lockB\\n\",thrd);\n                spin_lock(&lockA);\n                DELAY_LOOP('B', 2);\n                spin_lock(&lockB);\n                DELAY_LOOP('A', 3);\n                spin_unlock(&lockB);\n                spin_unlock(&lockA);\n            }\n    [ ... ]\n```", "```\n$ sudo insmod ./deadlock_eg_AB-BA.ko\n$ dmesg\n[10234.023746] deadlock_eg_AB-BA: inserted (param: lock_ooo=0)\n[10234.026753] thrd_work():115: *** thread PID 6666 on cpu 0 now ***\n[10234.028299] Thread #0: locking: we do: lockA --> lockB\n[10234.029606] iteration #0 on cpu #0\n[10234.030765] A\n[10234.030766] A\n[10234.030847] thrd_work():115: *** thread PID 6667 on cpu 1 now ***\n[10234.031861] A\n[10234.031916] B\n[10234.032850] iteration #0 on cpu #1\n[10234.032853] Thread #1: locking: we do: lockA --> lockB\n[10234.038831] B\n[10234.038836] Our kernel thread #0 exiting now...\n[10234.038869] B\n[10234.038870] B\n[10234.042347] A\n[10234.043363] A\n[10234.044490] A\n[10234.045551] Our kernel thread #1 exiting now...\n$ \n```", "```\n======================================================\nWARNING: possible circular locking dependency detected\n5.4.0-llkd-dbg #2 Tainted: G OE\n------------------------------------------------------\nthrd_0/0/6734 is trying to acquire lock:\nffffffffc0fb2518 (lockB){+.+.}, at: thrd_work.cold+0x188/0x24c [deadlock_eg_AB_BA]\n\nbut task is already holding lock:\nffffffffc0fb2598 (lockA){+.+.}, at: thrd_work.cold+0x149/0x24c [deadlock_eg_AB_BA]\n\nwhich lock already depends on the new lock.\n[ ... ]\nother info that might help us debug this:\n\n Possible unsafe locking scenario:\n\n       CPU0                    CPU1\n       ----                    ----\n  lock(lockA);\n                               lock(lockB);\n                               lock(lockA);\n  lock(lockB);\n\n *** DEADLOCK ***\n\n[ ... lots more output follows ... ]\n```", "```\nMessage from syslogd@seawolf-VirtualBox at Dec 24 11:01:51 ...\nkernel:[10939.279524] watchdog: BUG: soft lockup - CPU#0 stuck for 22s! [thrd_0/0:6734]\nMessage from syslogd@seawolf-VirtualBox at Dec 24 11:01:51 ...\nkernel:[10939.287525] watchdog: BUG: soft lockup - CPU#1 stuck for 23s! [thrd_1/1:6735]\n```", "```\n$ sudo cat /proc/lockdep_chains\n[ ... ]\nirq_context: 0\n[000000005c6094ba] lockA\n[000000009746aa1e] lockB\n[ ... ]\nirq_context: 0\n[000000009746aa1e] lockB\n[000000005c6094ba] lockA\n```", "```\n// include/linux/lockdep.h\n#define lockdep_assert_held(l) do { \\\n        WARN_ON(debug_locks && !lockdep_is_held(l)); \\\n    } while (0)\n```", "```\negrep \"alloc_lock|task|mm\" /proc/lock_stat                                                                        \n```", "```\n$ sudo cat /proc/lock_stat\nlock_stat version 0.4\n*WARNING* lock debugging disabled!! - possibly due to a lockdep warning\n```", "```\n//\u200b drivers/net/ethernet/realtek/8139cp.c\nstruct cp_desc {\n    __le32 opts1;\n    __le32 opts2;\n    __le64 addr;\n};\n```", "```\ndesc->word0 = address;\nwmb();\ndesc->word1 = DESC_VALID;\n```", "```\n// drivers/net/ethernet/realtek/8139cp.c\n[ ... ]\nstatic netdev_tx_t cp_start_xmit([...])\n{\n    [ ... ]\n    len = skb->len;\n    mapping = dma_map_single(&cp->pdev->dev, skb->data, len, PCI_DMA_TODEVICE);\n    [ ... ]\n    struct cp_desc *txd;\n    [ ... ]\n    txd->opts2 = opts2;\n    txd->addr = cpu_to_le64(mapping);\n    wmb();\n    opts1 |= eor | len | FirstFrag | LastFrag;\n    txd->opts1 = cpu_to_le32(opts1);\n    wmb();\n    [...]\n```"]
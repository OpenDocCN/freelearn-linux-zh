- en: '*Chapter 10*: Load Balancer Services for Linux'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第10章*：Linux负载均衡器服务'
- en: In this chapter, we'll be discussing the load balancer services that are available
    for Linux, specifically HAProxy. Load balancers allow client workloads to be spread
    across multiple backend servers. This allows a single IP to scale larger than
    a single server may allow, and also allows for redundancy in the case of a server
    outage or maintenance window.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论适用于Linux的负载均衡器服务，具体来说是HAProxy。负载均衡器允许客户端工作负载分布到多个后端服务器。这允许单个IP扩展到比单个服务器更大，并且在服务器故障或维护窗口的情况下也允许冗余。
- en: Once you've completed these examples, you should have the skills to deploy Linux-based
    load balancer services in your own environment via several different methods.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些示例后，您应该具备通过几种不同的方法在自己的环境中部署基于Linux的负载均衡器服务的技能。
- en: 'In particular, we''ll cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们将涵盖以下主题：
- en: Introduction to load balancing
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡简介
- en: Load balancing algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡算法
- en: Server and service health checks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器和服务健康检查
- en: Datacenter load balancer design considerations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中心负载均衡器设计考虑
- en: Building a HAProxy NAT/proxy load balancer
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建HAProxy NAT/代理负载均衡器
- en: A final note on load balancer security
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于负载均衡器安全性的最后说明
- en: Because of the complexity of setting up the infrastructure for this section,
    there are a few choices you can make with respect to the example configurations.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 由于设置此部分的基础设施的复杂性，您可以在示例配置方面做出一些选择。
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we''ll be exploring load balancer functions. As we work through
    the examples later in this book, you can follow along and implement our example
    configurations in your current Ubuntu host or virtual machine. However, to see
    our load balancing example in action, you''ll need a number of things:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨负载均衡器功能。当我们在本书的后面示例中工作时，您可以跟着进行，并在当前Ubuntu主机或虚拟机中实施我们的示例配置。但是，要看到我们的负载均衡示例的实际效果，您需要一些东西：
- en: At least two target hosts to balance a load across
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少有两个目标主机来平衡负载
- en: Another network adapter in the current Linux host
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前Linux主机中的另一个网络适配器
- en: Another subnet to host the target hosts and this new network adapter
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个子网来托管目标主机和这个新的网络适配器
- en: This configuration has a matching diagram, *Figure 10.2*, which will be shown
    later in this chapter that illustrates how all this will bolt together when we're
    done.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置有一个匹配的图表，*图10.2*，将在本章后面显示，说明了当我们完成时所有这些将如何连接在一起。
- en: This adds a whole level of complexity to the configuration of our lab environment.
    When we get to the lab section, we'll offer some alternatives (downloading a pre-built
    virtual machine is one of them), but you may just choose to read along. If that's
    the case, I think you'll still get a good introduction to the topic, along with
    a solid background of the design, implementation, and security implications of
    various load balancer configurations in a modern datacenter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们的实验室环境的配置增加了一整个层次的复杂性。当我们到达实验室部分时，我们将提供一些替代方案（下载预构建的虚拟机是其中之一），但您也可以选择跟着阅读。如果是这种情况，我认为您仍然会对这个主题有一个很好的介绍，以及对现代数据中心中各种负载均衡器配置的设计、实施和安全影响有一个扎实的背景。
- en: Introduction to load balancing
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡简介
- en: In its simplest form, load balancing is all about spreading client load across
    multiple servers. These servers can be in one or several locations, and the method
    of distributing that load can vary quite a bit. In fact, how successful you are
    in spreading that load evenly can vary quite a bit as well (mostly depending on
    the method chosen). Let's explore some of the more common methods of load balancing.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最简单的形式中，负载均衡就是将客户端负载分布到多个服务器上。这些服务器可以在一个或多个位置，以及分配负载的方法可以有很大的不同。事实上，您在均匀分配负载方面的成功程度也会有很大的不同（主要取决于所选择的方法）。让我们探讨一些更常见的负载均衡方法。
- en: Round Robin DNS (RRDNS)
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环DNS（RRDNS）
- en: 'You can do simple load balancing just with a DNS server, in what''s called
    `a.example.com` hostname, the DNS server will return the IP of Server 1; then,
    when the next client requests it, it will return the IP for Server 2, and so on.
    This is the simplest load balancing method and works equally well for both co-located
    servers and servers in different locations. It can also be implemented with no
    changes at all to the infrastructure – no new components and no configuration
    changes:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以只使用DNS服务器进行简单的负载均衡，即所谓的`a.example.com`主机名，DNS服务器将返回服务器1的IP；然后，当下一个客户端请求时，它将返回服务器2的IP，依此类推。这是最简单的负载均衡方法，对于共同放置的服务器和不同位置的服务器同样有效。它也可以在基础设施上不做任何更改-没有新组件，也没有配置更改：
- en: '![Figure 10.1 – Simple load balancing with Round Robin DNS'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.1-循环DNS的简单负载均衡'
- en: '](img/B16336_10_001.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16336_10_001.jpg)'
- en: Figure 10.1 – Simple load balancing with Round Robin DNS
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1-循环DNS的简单负载均衡
- en: 'Configuring RRDNS is simple – in BIND, simply configure multiple `A` records
    for the target hostname with multiple IPs. Successive DNS requests will return
    each `A` record in sequence. It''s a good idea to shorten the domain''s `A` records
    in sequence), random, or fixed (always return matching records in the same order).
    The syntax for changing the return order is as follows (`cyclic`, the default
    setting, is shown here):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 配置RRDNS很简单-在BIND中，只需为目标主机名配置多个`A`记录，其中包含多个IP。连续的DNS请求将按顺序返回每个`A`记录。将域的`A`记录缩短是个好主意，以便按顺序（顺序返回匹配的记录）、随机或固定（始终以相同顺序返回匹配的记录）。更改返回顺序的语法如下（`cyclic`，默认设置，如下所示）：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'There are a few issues with this configuration:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种配置存在一些问题：
- en: There is no good way to incorporate any kind of health check in this model –
    are all the servers operating correctly? Are the services up? Are the hosts even
    up?
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这种模型中，没有好的方法来整合任何类型的健康检查-所有服务器是否正常运行？服务是否正常？主机是否正常？
- en: There is no way of seeing if any DNS request is then actually followed up with
    a connection to the service. There are various reasons why DNS requests might
    be made, and the interaction might end there, with no subsequent connection.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有办法看到任何DNS请求是否实际上会跟随连接到服务。有各种原因可能会发出DNS请求，并且交互可能就此结束，没有后续连接。
- en: There is also no way to monitor when sessions end, which means there's no way
    to send the next request to the least used server – it's just a steady, even rotation
    between all servers. So, at the beginning of any business day, this may seem like
    a good model, but as the day progresses, there will always be longer-lived sessions
    and extremely short ones (or sessions that didn't occur at all), so it's common
    to see the server loads become "lop-sided" as the day progresses. This can become
    even more pronounced if there is no clear beginning or end of the day to effectively
    "zero things out."
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 也没有办法监视会话何时结束，这意味着没有办法将下一个请求发送到最少使用的服务器 - 它只是在所有服务器之间稳定地轮换。因此，在任何工作日的开始，这可能看起来像一个好模型，但随着一天的进展，总会有持续时间更长的会话和极短的会话（或根本没有发生的会话），因此很常见看到服务器负载在一天进展过程中变得“不平衡”。如果没有明确的一天开始或结束来有效地“清零”，这种情况可能会变得更加明显。
- en: For the same reason, if one server in the cluster is brought down for maintenance
    or an unplanned outage, there is no good way to bring it back to parity (as far
    as the session count goes).
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出于同样的原因，如果集群中的一个服务器因维护或非计划中断而下线，没有好的方法将其恢复到与会话计数相同的状态。
- en: With a bit of DNS reconnaissance, an attacker can collect the real IPs of all
    cluster members, then assess them or attack them separately. If any of them is
    particularly vulnerable or has an additional DNS entry identifying it as a backup
    host, this makes the attacker's job even easier.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过一些DNS侦察，攻击者可以收集所有集群成员的真实IP，然后分别评估它们或对它们进行攻击。如果其中任何一个特别脆弱或具有额外的DNS条目标识它为备用主机，这将使攻击者的工作变得更加容易。
- en: Taking any of the target servers offline can be a problem – the DNS server will
    continue to serve that address up in the order it was requested. Even if the record
    is edited, any downstream clients and DNS servers will cache their resolved IPs
    and continue to try to connect to the down host.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将任何目标服务器下线可能会成为一个问题 - DNS服务器将继续按请求的顺序提供该地址。即使记录被编辑，任何下游客户端和DNS服务器都将缓存其解析的IP，并继续尝试连接到已关闭的主机。
- en: Downstream DNS servers (that is, those on the internet) will cache whatever
    record they get for the zone's TTL period. So, all the clients of any of those
    DNS servers will get sent to the same target server.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下游DNS服务器（即互联网上的服务器）将在区域的TTL周期内缓存它们获取的任何记录。因此，任何这些DNS服务器的客户端都将被发送到同一个目标服务器。
- en: For these reasons, RRDNS will do the job in a simple way "in a pinch," but this
    should not normally implemented as a long-term, production solution. That said,
    the **Global Server Load Balancer** (**GSLB**) products are actually based on
    this approach, with different load balancing options and health checks. The disconnect
    between the load balancer and the target server remains in GSLB, so many of these
    same disadvantages carry through to this solution.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，RRDNS可以在紧急情况下简单地完成工作，但通常不应将其实施为长期的生产解决方案。也就是说，**全局服务器负载均衡器**（**GSLB**）产品实际上是基于这种方法的，具有不同的负载均衡选项和健康检查。负载均衡器与目标服务器之间的断开在GSLB中仍然存在，因此许多相同的缺点也适用于这种解决方案。
- en: What we see more often in datacenters is either proxy-based (Layer 7) or NAT-based
    (Layer 4) load balancing. Let's explore these two options.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据中心中，我们更经常看到基于代理（第7层）或基于NAT（第4层）的负载均衡。让我们探讨这两个选项。
- en: Inbound proxy – Layer 7 load balancing
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 入站代理 - 第7层负载均衡
- en: In this architecture, the client's sessions are terminated at the proxy server,
    and a new session is started between the inside interface of the proxy and the
    real server IP.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种架构中，客户端的会话在代理服务器上终止，并在代理的内部接口和真实服务器IP之间启动新的会话。
- en: 'This also brings up several architectural terms that are common to many of
    the load balancing solutions. In the following diagram, can we see the concept
    of the **FRONTEND**, which faces the clients, and the **BACKEND**, which faces
    the servers. We should also discuss IP addressing at this point. The frontend
    presents a **Virtual IP** (**VIP**) that is shared by all of target servers, and
    the servers'' **Real IPs** (**RIPs**) are not seen by the clients at all:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这也提出了许多在许多负载均衡解决方案中常见的架构术语。在下图中，我们可以看到**前端**的概念，面向客户端，以及**后端**，面向服务器。我们还应该在这一点上讨论IP地址。前端呈现了所有目标服务器共享的**虚拟IP**（**VIP**），客户端根本看不到服务器的**真实IP**（**RIPs**）：
- en: '![Figure 10.2 – Load balancing using a reverse proxy'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.2 - 使用反向代理进行负载均衡'
- en: '](img/B16336_10_002.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16336_10_002.jpg)'
- en: Figure 10.2 – Load balancing using a reverse proxy
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 - 使用反向代理进行负载均衡
- en: 'This approach has a few disadvantages:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有一些缺点：
- en: It has the highest CPU load on the load balancer out of all the methods we will
    discuss in this chapter and can, in extreme cases, translate into a performance
    impact on the client's end.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章讨论的所有方法中，它对负载均衡器的CPU负载最高，并且在极端情况下可能会对客户端产生性能影响。
- en: In addition, because the client traffic on the target server all comes from
    the proxy server (or servers), without some special handling, the client IP seen
    on the target/application server will always be the backend IP of the load balancer.
    This makes logging direct client interaction in the application problematic. To
    parse out traffic from one session and relate it to a client's actual address,
    we must match the client session from the load balancer (which sees the client
    IP address but not the user identity) with the application/web server logs (which
    sees the user identity but not the client IP address). Matching the session between
    these logs can be a real issue; the common elements between them are the timestamp
    and the source port at the load balancer, and the source ports are often not on
    the web server.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，由于目标服务器上的客户端流量都来自代理服务器（或服务器），如果没有一些特殊处理，那么在目标/应用程序服务器上看到的客户端IP将始终是负载均衡器的后端IP。这使得在应用程序中记录直接客户端交互成为问题。要从一个会话中解析出流量并将其与客户端的实际地址相关联，我们必须将负载均衡器的客户端会话（它看到客户端IP地址但看不到用户身份）与应用程序/网络服务器日志（它看到用户身份但看不到客户端IP地址）进行匹配。在这些日志之间匹配会话可能是一个真正的问题；它们之间的共同元素是负载均衡器上的时间戳和源端口，而源端口通常不在Web服务器上。
- en: This can be mitigated by having some application awareness. For instance, it's
    common to see a TLS frontend for a Citrix ICA Server or Microsoft RDP server backend.
    In those cases, the proxy server has some excellent "hooks" into the protocol,
    allowing the client IP address to be carried all the way through to the server,
    as well as the identity being detected by the load balancer.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这可以通过具有一些应用程序意识来减轻。例如，常见的是为Citrix ICA服务器或Microsoft RDP服务器后端设置TLS前端。在这些情况下，代理服务器对协议有一些出色的“钩子”，允许客户端IP地址一直传递到服务器，并且负载均衡器检测到的身份也被检测到。
- en: On the plus side, though, using a proxy architecture allows us to fully inspect
    the traffic for attacks, if the tooling is in place. In fact, because of the proxy
    architecture, that final "hop" between the load balancer and the target servers
    is an entirely new session – this means that invalid protocol attacks are mostly
    filtered out, without any special configuration being required at all.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用代理架构允许我们完全检查流量是否受到攻击，如果工具设置好的话。实际上，由于代理架构，负载均衡器和目标服务器之间的最后一个“跳跃”是一个全新的会话
    - 这意味着无效的协议攻击大部分被过滤掉，而无需进行任何特殊配置。
- en: We can mitigate some of the complexity of this proxy approach by running the
    load balancer as an inbound **Network Address Translation** (**NAT**) configuration.
    When decryption is not required, the NAT approach is commonly seen, built into
    most environments.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将负载均衡器作为入站**网络地址转换**（**NAT**）配置来减轻代理方法的一些复杂性。当不需要解密时，NAT方法通常是常见的，并内置于大多数环境中。
- en: Inbound NAT – Layer 4 load balancing
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 入站NAT - 第4层负载平衡
- en: 'This is the most common solution and is the one we''ll start with in our example.
    In a lot of ways, the architecture looks similar to the proxy solution, but with
    a few key differences. Note that in the following diagram, the TCP sessions on
    the frontend and backend now match – this is because the load balancer is no longer
    a proxy; it has been configured for an inbound NAT service. All the clients still
    attach to the single VIP and are redirected to the various server RIPs by the
    load balancer:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最常见的解决方案，也是我们在示例中将要开始使用的解决方案。在许多方面，这种架构看起来与代理解决方案相似，但有一些关键的区别。请注意，在下图中，前端和后端的TCP会话现在匹配
    - 这是因为负载均衡器不再是代理；它已被配置为入站NAT服务。所有客户端仍然连接到单个VIP，并由负载均衡器重定向到各种服务器RIP：
- en: '![Figure 10.3 – Load balancing using inbound NAT'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.3 - 使用入站NAT进行负载平衡'
- en: '](img/B16336_10_003.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16336_10_003.jpg)'
- en: Figure 10.3 – Load balancing using inbound NAT
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 - 使用入站NAT进行负载平衡
- en: 'There are several reasons why this is a preferred architecture in many cases:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，这是首选架构的几个原因：
- en: The servers see the real IPs of the client, and the server logs correctly reflect
    that.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器看到客户端的真实IP，并且服务器日志正确地反映了这一点。
- en: The load balancer maintains the NAT table in memory, and the load balancer logs
    reflect the various NAT operations but can't "see" the session. For instance,
    if the servers are running an HTTPS session, if this is a simple Layer 4 NAT,
    then the load balancer can see the TCP session but can't decrypt the traffic.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡器在内存中维护NAT表，负载均衡器日志反映了各种NAT操作，但无法“看到”会话。例如，如果服务器正在运行HTTPS会话，如果这是一个简单的第4层NAT，则负载均衡器可以看到TCP会话，但无法解密流量。
- en: We have the option of terminating the HTTPS session on the frontend, then either
    running encrypted or clear text on the backend in this architecture. However,
    since we're maintaining two sessions (the frontend and backend), this starts to
    look more like a proxy configuration.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以选择在前端终止HTTPS会话，然后在此架构中在后端运行加密或明文。然而，由于我们维护了两个会话（前端和后端），这开始看起来更像是代理配置。
- en: Because the entire TCP session (up to Layer 4) is seen by the load balancer,
    several load balancing algorithms are now available (see the next section on load
    balancing algorithms for more information).
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于负载均衡器看到整个TCP会话（直到第4层），现在可以使用多种负载平衡算法（有关更多信息，请参见负载平衡算法的下一节）。
- en: This architecture allows us to place **Web Application Firewall** (**WAF**)
    functions on the load balancer, which can mask some vulnerabilities on target
    server web applications. For example, a WAF is a common defense against cross-site
    scripting or buffer overflow attacks, or any other attack that might rely on a
    lapse in input validation. For those types of attacks, the WAF identifies what
    is an acceptable input for any given field or URI, and then drops any inputs that
    don't match. However, WAFs are not limited to those attacks. Think of a WAF function
    as a web-specific IPS (see [*Chapter 14*](B16336_14_Final_NM_ePub.xhtml#_idTextAnchor252),
    *Honeypot Services on Linux*).
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种架构允许我们在负载均衡器上放置**Web应用程序防火墙**（**WAF**）功能，这可以掩盖目标服务器Web应用程序上的一些漏洞。例如，WAF是对跨站脚本或缓冲区溢出攻击的常见防御，或者任何可能依赖输入验证中断的攻击。对于这些类型的攻击，WAF识别任何给定字段或URI的可接受输入，然后丢弃任何不匹配的输入。但是，WAF并不局限于这些攻击。将WAF功能视为Web特定的IPS（见[*第14章*](B16336_14_Final_NM_ePub.xhtml#_idTextAnchor252)，*Linux上的蜜罐服务*）。
- en: This architecture is well-suited to making sessions persistent or "sticky" –
    by that, we mean that once a client session is "attached" to a server, subsequent
    requests will be directed to that same server. This is well-suited for pages that
    have a backend database, for instance, where if you didn't keep the same backend
    server, your activity (for instance, a shopping cart on an ecommerce site) might
    be lost. Dynamic or parametric websites – where the pages are generated in real
    time as you navigate (for instance, most sites that have a product catalog or
    inventory) – also usually need session persistence.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种架构非常适合使会话持久或“粘性” - 我们的意思是一旦客户端会话“附加”到服务器，随后的请求将被定向到同一台服务器。这非常适合具有后端数据库的页面，例如，如果您不保持相同的后端服务器，您的活动（例如，电子商务网站上的购物车）可能会丢失。动态或参数化网站
    - 在这些网站上，页面在您导航时实时生成（例如，大多数具有产品目录或库存的网站） - 通常也需要会话持久性。
- en: You can also load balance each successive request independently, so, for instance,
    as a client navigates a site, their session might be terminated by a different
    server for each page. This type of approach is well-suited to static websites.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您还可以独立地负载均衡每个连续的请求，因此，例如，当客户端浏览网站时，他们的会话可能会在每个页面由不同的服务器终止。这种方法非常适合静态网站。
- en: You can layer other functions on top of this architecture. For instance, these
    are often deployed in parallel with a firewall or even with a native interface
    on the public internet. Because of this, you'll often see load balancer vendors
    with VPN clients to go with their load balancers.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在这种架构的基础上叠加其他功能。例如，这些通常与防火墙并行部署，甚至与公共互联网上的本机接口并行部署。因此，您经常会看到负载均衡器供应商配备VPN客户端以配合其负载均衡器。
- en: As shown in the preceding diagram, an inbound NAT and proxy load balancer have
    a very similar topology – the connections all look very similar. This is carried
    through to the implementation, where it's possible to see some things proxied
    and some things running through the NAT process on the same load balancer.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前图所示，入站NAT和代理负载均衡器具有非常相似的拓扑结构 - 连接看起来都非常相似。这一点延续到了实现中，可以看到一些东西通过代理和一些东西通过同一负载均衡器上的NAT过程。
- en: However, even though the CPU impact of this configuration is much lower than
    the proxy solution, every workload packet must route through the load balancer,
    in both directions. We can reduce this impact dramatically using the **Direct
    Server Return** (**DSR**) architecture.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使这种配置的CPU影响远低于代理解决方案，每个工作负载数据包仍必须通过负载均衡器在两个方向上进行路由。我们可以使用**直接服务器返回**（**DSR**）架构大大减少这种影响。
- en: DSR load balancing
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DSR负载平衡
- en: In DSR, all the incoming traffic is still load balanced from the VIP on the
    load balancer to the various server RIPs. However, the return traffic goes directly
    from the server back to the client, bypassing the load balancer.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在DSR中，所有传入的流量仍然从负载均衡器上的VIP负载均衡到各个服务器的RIP。然而，返回流量直接从服务器返回到客户端，绕过负载均衡器。
- en: 'How can this work? Here''s how:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这怎么可能？这是怎么回事：
- en: On the way in, the load balancer rewrites the MAC address of each packet, load
    balancing them across the MAC addresses of the target servers.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在进入时，负载均衡器会重写每个数据包的MAC地址，将它们负载均衡到目标服务器的MAC地址上。
- en: Each server has a loopback address, a configured address that matches the VIP
    address. This is the interface that returns all the traffic (because the client
    is expecting return traffic from the VIP address). However, it must be configured
    to not reply to ARP requests (otherwise, the load balancer will be bypassed on
    the inbound path).
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每台服务器都有一个环回地址，这是一个配置的地址，与VIP地址匹配。这是返回所有流量的接口（因为客户端期望从VIP地址返回流量）。但是，它必须配置为不回复ARP请求（否则，负载均衡器将在入站路径上被绕过）。
- en: 'This may seem convoluted, but the following diagram should make things a bit
    clearer. Note that there is only one target host in this diagram, to make the
    traffic flow a bit easier to see:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来很复杂，但以下图表应该使事情变得更清晰一些。请注意，此图表中只有一个目标主机，以使流量流动更容易看到：
- en: '![Figure 10.4 – DSR load balancing'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.4 - DSR负载平衡'
- en: '](img/B16336_10_004.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16336_10_004.jpg)'
- en: Figure 10.4 – DSR load balancing
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 - DSR负载平衡
- en: 'There are some pretty strict requirements for building this out:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 构建这个的要求非常严格：
- en: The load balancer and all the target servers need to be on the same subnet.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡器和所有目标服务器都需要在同一个子网上。
- en: 'This mechanism requires some games on the default gateway, since, on the way
    in, it must direct all the client traffic to the VIP on the load balancer, but
    it also has to accept replies from multiple target servers with the same address
    but different MAC addresses. The Layer 3 default gateway for this must have an
    ARP entry for each of the target servers, all with the same IP address. In many
    architectures, this is done via multiple static ARP entries. For instance, on
    a Cisco router, we would do the following:'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种机制需要在默认网关上进行一些设置，因为在进入时，它必须将所有客户端流量定向到负载均衡器上的VIP，但它还必须接受来自具有相同地址但不同MAC地址的多个目标服务器的回复。这个必须有一个ARP条目，对于每个目标服务器，都有相同的IP地址。在许多架构中，这是通过多个静态ARP条目来完成的。例如，在Cisco路由器上，我们会这样做：
- en: '[PRE1]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note in this example that `192.168.124.21` and `22` are the target hosts being
    load balanced. Also, the MAC addresses have an OUI, indicating that they're both
    VMware virtual hosts, which is typical in most datacenters.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这个例子中，`192.168.124.21`和`22`是被负载均衡的目标主机。此外，MAC地址具有OUI，表明它们都是VMware虚拟主机，在大多数数据中心都是典型的。
- en: Why would you go through all this bother and unusual network configuration?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要经历所有这些麻烦和不寻常的网络配置？
- en: A DSR configuration has the advantage of minimizing the traffic through the
    load balancer by a wide margin. In web applications, for instance, it's common
    to see the return traffic outweigh the incoming traffic by a factor of 10 or more.
    This means that for that traffic model, a DSR implementation will see 90% or less
    of the traffic that a NAT or proxy load balancer will see.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DSR配置的优势在于大大减少了通过负载均衡器的流量。例如，在Web应用程序中，通常会看到返回流量超过传入流量的10倍以上。这意味着对于这种流量模型，DSR实现将看到负载均衡器将看到的流量的90%或更少。
- en: No "backend" subnet is required; the load balancer and the target servers are
    all in the same subnet – in fact, that's a requirement. This has some disadvantages
    too, as we've already discussed. We'll cover this in more detail in the *Specific
    Server Settings for DSR* section.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不需要“后端”子网；负载均衡器和目标服务器都在同一个子网中 - 实际上，这是一个要求。这也有一些缺点，正如我们已经讨论过的那样。我们将在*DSR的特定服务器设置*部分详细介绍这一点。
- en: 'However, there are some downsides:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也有一些缺点：
- en: The relative load across the cluster, or the individual load on any one server,
    is, at best, inferred by the load balancer. If a session ends gracefully, the
    load balancer will catch enough of the "end of session" handshake to figure out
    that a session has ended, but if a session doesn't end gracefully, it depends
    entirely on timeouts to end a session.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群中的相对负载，或者任何一个服务器上的个体负载，最多只能由负载均衡器推断出来。如果一个会话正常结束，负载均衡器将捕捉到足够的“会话结束”握手来判断会话已经结束，但如果一个会话没有正常结束，它完全依赖超时来结束会话。
- en: All the hosts must be configured with the same IP (the original target) so that
    the return traffic doesn't come from an unexpected address. This is normally done
    with a loopback interface, and usually requires some additional configuration
    on the host.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有主机必须配置相同的IP（原始目标），以便返回流量不会来自意外的地址。这通常是通过环回接口完成的，并且通常需要对主机进行一些额外的配置。
- en: The upstream router (or Layer 3 switch, if that's the gateway for the subnet)
    needs to be configured to allow all the possible MAC addresses for the target
    IP address. This is a manual process, and if it's possible to see MAC addresses
    change unexpectedly, this can be a problem.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上游路由器（或者如果它是子网的网关，则是第3层交换机）需要配置为允许目标IP地址的所有可能的MAC地址。这是一个手动过程，如果可能看到MAC地址意外更改，这可能是一个问题。
- en: If any function that needs proxying or full visibility of the session (as in
    the NAT implementation) can't work, the load balancer only sees half of the session.
    This means that any HTTP header parsing, cookie manipulation (for instance, for
    session persistence), or SYN cookies can't be implemented.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果任何需要代理或完全可见会话的功能（如NAT实现中）无法工作，负载均衡器只能看到会话的一半。这意味着任何HTTP头解析、cookie操作（例如会话持久性）或SYN
    cookie都无法实现。
- en: In addition, because (as far as the router is concerned), all the target hosts
    have different MAC addresses but the same IP address, and the target hosts cannot
    reply to any ARP requests (otherwise, they'd bypass the load balancer), there's
    a fair amount of work that needs to be done on the target hosts.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，因为（就路由器而言）所有目标主机具有不同的MAC地址但相同的IP地址，而目标主机不能回复任何ARP请求（否则，它们将绕过负载均衡器），因此需要在目标主机上进行大量的工作。
- en: Specific server settings for DSR
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DSR的特定服务器设置
- en: For a Linux client, ARP suppression for the "VIP" addressed interface (whether
    it's a loopback or a logical Ethernet) must be done. This can be completed with
    `sudo ip link set <interface name> arp off` or (using the older `ifconfig` syntax)
    `sudo ifconfig <interface name> -arp`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Linux客户端，必须对“VIP”寻址接口（无论是环回还是逻辑以太网）进行ARP抑制。可以使用`sudo ip link set <interface
    name> arp off`或（使用较旧的`ifconfig`语法）`sudo ifconfig <interface name> -arp`来完成。
- en: You'll also need to implement the `strong host` and `weak host` settings on
    the target servers. A server interface is configured as a `strong host` if it's
    not a router and cannot send or receive packets from an interface, unless the
    source or destination IP in the packet matches the interface IP. If an interface
    has been configured as a `weak host`, this restriction doesn't apply – it can
    receive or send packets on behalf of other interfaces.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要在目标服务器上实现“强主机”和“弱主机”设置。如果服务器接口不是路由器，并且不能发送或接收来自接口的数据包，除非数据包中的源或目的IP与接口IP匹配，则将其配置为“强主机”。如果接口已配置为“弱主机”，则不适用此限制-它可以代表其他接口接收或发送数据包。
- en: 'Linux and BSD Unix have `weak host` enabled by default on all interfaces (`sysctl
    net.ip.ip.check_interface = 0`). Windows 2003 and older also have this enabled.
    However, Windows Server 2008 and newer have a `strong host` model for all interfaces.
    To change this for DSR in newer Windows versions, execute the following code:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Linux和BSD Unix默认在所有接口上启用了`weak host`（`sysctl net.ip.ip.check_interface = 0`）。Windows
    2003及更早版本也启用了这个功能。但是，Windows Server 2008及更高版本为所有接口采用了`strong host`模型。要更改新版本Windows中的DSR，执行以下代码：
- en: '[PRE2]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You'll also need to disable any IP checksum offload and TCP checksum offload
    functions on the target servers. On a Windows host, these two settings are in
    the `Network Adapter/Advanced` settings. On a Linux host, the `ethtool` command
    can manipulate these settings, but these hardware-based offload features are disabled
    by default in Linux, so normally, you won't need to adjust them.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要在目标服务器上禁用任何IP校验和TCP校验和卸载功能。在Windows主机上，这两个设置位于`网络适配器/高级`设置中。在Linux主机上，`ethtool`命令可以操作这些设置，但这些基于硬件的卸载功能在Linux中默认情况下是禁用的，因此通常不需要调整它们。
- en: With the various architectures described, we still need to work out how exactly
    we want to distribute the client load across our group of target servers.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述的各种架构中，我们仍然需要确定如何精确地分配客户端负载到我们的目标服务器组。
- en: Load balancing algorithms
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载平衡算法
- en: 'So far, we''ve touched on a few load balancing algorithms, so let''s explore
    the more common approaches in a bit more detail (note that this list is not exhaustive;
    just the most commonly seen methods have been provided here):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涉及了一些负载平衡算法，让我们更详细地探讨一下更常见的方法（请注意，这个列表不是详尽无遗的；这里只提供了最常见的方法）：
- en: '![](img/B16336_10_Table_01.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16336_10_Table_01.jpg)'
- en: '**Least Connections**, as you may expect, is the most often assigned algorithm.
    We''ll use this method in the configuration examples later in this chapter.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**最少连接**，正如你可能期望的那样，是最常分配的算法。我们将在本章后面的配置示例中使用这种方法。'
- en: Now that we've seen some options for how to balance a workload, how can we make
    sure that those backend servers are working correctly?
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经看到了如何平衡工作负载的一些选项，那么我们如何确保那些后端服务器正常工作呢？
- en: Server and service health checks
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务器和服务健康检查
- en: 'One of the issues we discussed in the section on DNS load balancing was health
    checks. Once you start load balancing, you usually want some method of knowing
    which servers (and services) are operating correctly. Methods for checking the
    *health* of any connection include the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在DNS负载平衡部分讨论的问题之一是健康检查。一旦开始负载平衡，通常希望有一种方法来知道哪些服务器（和服务）正在正确运行。检查任何连接的*健康*的方法包括以下内容：
- en: Use ICMP to effectively "ping" the target servers periodically. If no pings
    return with an ICMP echo reply, then they are considered down, and they don't
    receive any new clients. Existing clients will be spread across the other servers.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定期使用ICMP有效地“ping”目标服务器。如果没有ICMP回显回复，则认为它们宕机，并且不会接收任何新的客户端。现有客户端将分布在其他服务器上。
- en: Use the TCP handshake and check for an open port (for instance `80/tcp` and
    `443/tcp` for a web service). Again, if the handshake doesn't complete, then the
    host is considered down.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TCP握手并检查开放端口（例如`80/tcp`和`443/tcp`用于Web服务）。同样，如果握手未完成，则主机被视为宕机。
- en: In UDP, you would typically make an application request. For instance, if you
    are load balancing DNS servers, the load balancer would make a simple DNS query
    – if a DNS response is received, then the sever is considered up.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在UDP中，您通常会发出应用程序请求。例如，如果您正在负载均衡DNS服务器，负载均衡器将进行简单的DNS查询-如果收到DNS响应，则认为服务器正常运行。
- en: Finally, when balancing a web application, you may make an actual web request.
    Often, you'll request the index page (or any known page) and look for known text
    on that page. If that text doesn't appear, then that host and service combination
    is considered down. In a more complex environment, the test page you check may
    make a known call to a backend database to verify it.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在平衡Web应用程序时，您可以进行实际的Web请求。通常，您会请求索引页面（或任何已知页面）并查找该页面上的已知文本。如果该文本不出现，则该主机和服务组合被视为宕机。在更复杂的环境中，您检查的测试页面可能会对后端数据库进行已知调用以进行验证。
- en: Testing the actual application (as in the preceding two points) is, of course,
    the most reliable way to verify that the application is working.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 测试实际应用程序（如前两点所述）当然是验证应用程序是否正常工作的最可靠方法。
- en: We'll show a few of these health checks in our example configuration. Before
    we get to that, though, let's dive into how you might see load balancers implemented
    in a typical datacenter – both in a "legacy" configuration and in a more modern
    implementation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在示例配置中展示一些这些健康检查。在我们开始之前，让我们深入了解一下您可能会在典型数据中心中看到负载均衡器的实现方式-无论是在“传统”配置中还是在更现代的实现中。
- en: Datacenter load balancer design considerations
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据中心负载均衡器设计考虑
- en: Load balancing has been part of larger architectures for decades, which means
    that we've gone through several common designs.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 负载平衡已经成为较大架构的一部分几十年了，这意味着我们经历了几种常见的设计。
- en: The "legacy" design that we still frequently see is a single pair (or cluster)
    of physical load balancers that service all the load balanced workloads in the
    datacenter. Often, the same load balancer cluster is used for internal and external
    workloads, but sometimes, you'll see one internal pair of load balancers on the
    internal network, and one pair that only services DMZ workloads (that is, for
    external clients).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常看到的“传统”设计是一个单一对（或集群）物理负载均衡器，为数据中心中的所有负载平衡工作负载提供服务。通常，相同的负载均衡器集群用于内部和外部工作负载，但有时，您会看到一个内部负载均衡器对内部网络进行服务，另一个对只服务DMZ工作负载（即对外部客户端）的负载均衡器对外部工作负载进行服务。
- en: This model was a good approach in the days when we had physical servers, and
    load balancers were expensive pieces of hardware.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模型在我们拥有物理服务器且负载均衡器是昂贵的硬件的时代是一个很好的方法。
- en: 'In a virtualized environment, though, the workload VMs are tied to the physical
    load balancers, which complicates the network configuration, limits disaster recovery
    options, and can often result in traffic making multiple "loops" between the (physical)
    load balancers and the virtual environment:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在虚拟化环境中，工作负载VM绑定到物理负载均衡器，这使得网络配置复杂化，限制了灾难恢复选项，并且通常导致流量在（物理）负载均衡器和虚拟环境之间进行多次“循环”：
- en: '![Figure 10.5 – Legacy load balancing architecture'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.5 - 传统负载均衡架构'
- en: '](img/B16336_10_005.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16336_10_005.jpg)'
- en: Figure 10.5 – Legacy load balancing architecture
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 - 传统负载均衡架构
- en: 'This has all changed with the advent of virtualization. The use of physical
    load balancers now makes very little sense – you are far better off sticking to
    a dedicated, small VM for each workload, as shown here:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 随着虚拟化的出现，一切都发生了变化。现在使用物理负载均衡器几乎没有意义 - 最好是为每个工作负载使用专用的小型虚拟机，如下所示：
- en: '![Figure 10.6 – Modern load balancing architecture'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.6 - 现代负载均衡架构'
- en: '](img/B16336_10_006.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16336_10_006.jpg)'
- en: Figure 10.6 – Modern load balancing architecture
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6 - 现代负载均衡架构
- en: 'This approach has several advantages:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有几个优点：
- en: '**Cost** is one advantage as these small virtual load balancers are much cheaper
    if licensed, or free if you use a solution such as HAProxy (or any other free/open
    source solution). This is probably the advantage that should have the least impact
    but is unsurprisingly usually the one factor that changes opinions.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本**是一个优势，因为这些小型虚拟负载均衡器如果获得许可要便宜得多，或者如果使用诸如HAProxy（或任何其他免费/开源解决方案）的解决方案则是免费的。这可能是影响最小的优势，但毫不奇怪通常是改变意见的因素。'
- en: '**Configurations are much simpler** and easier to maintain as each load balancer
    only services one workload. If a change is made and possibly needs subsequent
    debugging, it''s much simpler to "pick out" something from a smaller configuration.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**配置要简单得多**，更容易维护，因为每个负载均衡器只服务一个工作负载。如果进行了更改并且可能需要后续调试，从较小的配置中“挑出”某些东西要简单得多。'
- en: In the event of a failure or, more likely, a configuration error, the **splatter
    zone** or **blast radius** is much smaller. If you tie each load balancer to a
    single workload, any errors or failures are more likely to affect only that workload.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在发生故障或更可能的是配置错误时，**散射区**或**影响范围**要小得多。如果将每个负载均衡器绑定到单个工作负载，任何错误或故障更可能只影响该工作负载。
- en: Also, from an operational standpoint, **using an orchestration platform or API
    used to scale the workload is much easier** (adding or removing backend servers
    to the cluster as demand rises and falls). This approach makes it much simpler
    to build those playbooks – mainly because of the simpler configuration and smaller
    blast radius in the event of a playbook error.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，从运营的角度来看，**使用编排平台或API来扩展工作负载要简单得多**（根据需求增加或删除后端服务器到集群）。这种方法使得构建这些playbook要简单得多
    - 主要是因为配置更简单，在playbook出错时影响范围更小。
- en: '**More rapid deployments for developers**. Since you are keeping this configuration
    simple, in the development environment, you can provide developers with exactly
    this configuration as they are writing or modifying the applications. This means
    that the applications are written with the load balancer in mind. Also, most of
    the testing is done during the development cycle, rather than the configuration
    being shoe-horned and tested in a single change window at the end of development.
    Even if the load balancers are licensed products, most vendors have a free (low
    bandwidth) tier of licensing for exactly this scenario.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发人员更快的部署**。由于您保持了这种简单的配置，在开发环境中，您可以在开发或修改应用程序时向开发人员提供这种配置。这意味着应用程序是针对负载均衡器编写的。此外，大部分测试是在开发周期内完成的，而不是在开发结束时在单个更改窗口中进行配置和测试。即使负载均衡器是有许可的产品，大多数供应商也为这种情况提供了免费（低带宽）许可的产品。'
- en: Providing **securely configured templates** to developers or for deployments
    is much easier with a smaller configuration.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向开发人员或部署提供**安全配置的模板**要简单得多。
- en: '**Security testing during the development or DevOps cycle** includes the load
    balancer, not just the application and the hosting server.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在开发或DevOps周期中进行安全测试**包括负载均衡器，而不仅仅是应用程序和托管服务器。'
- en: '**Training and testing is much simpler**. Since the load balancing products
    are free, setting up a training or test environment is quick and easy.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**培训和测试要简单得多**。由于负载均衡产品是免费的，设置培训或测试环境是快速简单的。'
- en: '**Workload optimization** is a significant advantage since, in a virtualized
    environment, you can usually "tie" groups of servers together. In a VMware vSphere
    environment, for instance, this is called a **vApp**. This construct allows you
    to keep all vApp members together if, for example, you vMotion them to another
    hypervisor server. You may need to do this for maintenance, or this may happen
    automatically using **Dynamic Resource Scheduling** (**DRS**), which balances
    CPU or memory load across multiple servers. Alternatively, the migration might
    be part of a disaster recovery workflow, where you migrate the vApp to another
    datacenter, either using vMotion or simply by activating a replica set of VMs.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作负载优化**是一个重要的优势，因为在虚拟化环境中，通常可以将一组服务器“绑定”在一起。例如，在VMware vSphere环境中，这被称为**vApp**。这个结构允许您将所有vApp成员一起保持在一起，例如，如果您将它们vMotion到另一个hypervisor服务器。您可能需要进行这样的操作进行维护，或者这可能会自动发生，使用**动态资源调度**（**DRS**），它可以在多个服务器之间平衡CPU或内存负载。或者，迁移可能是灾难恢复工作流的一部分，您可以将vApp迁移到另一个数据中心，使用vMotion或者简单地激活一组VM的副本。'
- en: '**Cloud deployments lend themselves even more strongly to this distributed
    model**. This is taken to the extreme in the larger cloud service providers, where
    load balancing is simply a service that you subscribe to, rather than a discrete
    instance or virtual machine. Examples of this include the AWS Elastic Load Balancing
    Service, Azure Load Balancer, and Google''s Cloud Load Balancing service.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**云部署更适合这种分布式模型**。在较大的云服务提供商中，这一点被推到了极致，负载平衡只是一个您订阅的服务，而不是一个离散的实例或虚拟机。其中包括AWS弹性负载均衡服务、Azure负载均衡器和Google的云负载均衡服务。'
- en: Load balancing brings with it several management challenges, though, most of
    which stem from one issue – if all the target hosts have a default gateway for
    the load balancer, how can we monitor and otherwise manage those hosts?
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 负载平衡带来了一些管理挑战，其中大部分源于一个问题 - 如果所有目标主机都有负载平衡器的默认网关，我们如何监视和管理这些主机？
- en: Datacenter network and management considerations
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据中心网络和管理考虑
- en: If a workload is load balanced using the NAT approach, routing becomes a concern.
    The route to the potential application clients must point to the load balancer.
    If those targets are internet-based, this makes administering the individual servers
    a problem – you don't want your server administrative traffic to be load balanced.
    You also don't want to have unnecessary traffic (such as backups or bulk file
    copies) route through the load balancer – you want it to route application traffic,
    not all the traffic!
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用NAT方法对工作负载进行负载平衡，路由就成了一个问题。潜在应用程序客户端的路由必须指向负载平衡器。如果这些目标是基于互联网的，这将使管理单个服务器成为一个问题
    - 您不希望服务器管理流量被负载平衡。您也不希望不必要的流量（例如备份或大容量文件复制）通过负载平衡器路由 - 您希望它路由应用程序流量，而不是所有流量！
- en: This is commonly dealt with by adding static routes and possibly a management
    VLAN.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常通过添加静态路由和可能的管理VLAN来处理。
- en: This is a good time to bring up that the management VLAN should have been there
    from the start – my "win the point" phrase on management VLANs is "does your accounting
    group (or receptionist or manufacturing group) need access to your SAN or hypervisor
    login?" If you can get an answer that leads you toward protecting sensitive interfaces
    from internal attacks, then a management VLAN is an easy thing to implement.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是一个好时机提出，管理VLAN应该从一开始就存在 - 我对管理VLAN的“赢得一点”的短语是“您的会计组（或接待员或制造组）需要访问您的SAN或超级管理员登录吗？”如果您可以得到一个答案，让您朝着保护内部攻击的敏感接口的方向，那么管理VLAN就很容易实现。
- en: 'In any case, in this model, the default gateway remains pointed toward the
    load balancer (to service internet clients), but specific routes are added to
    the servers to point toward internal or service resources. In most cases, this
    list of resources remains small, so even if internal clients plan to use the same
    load balanced application, this can still work:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，在这种模型中，默认网关仍然指向负载平衡器（为了服务互联网客户端），但是特定路由被添加到服务器以指向内部或服务资源。在大多数情况下，这些资源的列表仍然很小，因此即使内部客户端计划使用相同的负载平衡应用程序，这仍然可以工作：
- en: '![Figure 10.7 – Routing non-application traffic (high level)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.7 - 路由非应用流量（高级）'
- en: '](img/B16336_10_007.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16336_10_007.jpg)'
- en: Figure 10.7 – Routing non-application traffic (high level)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7 - 路由非应用流量（高级）
- en: If this model can't work for one reason or another, then you might want to consider
    adding **policy-based routing** (**PBR**).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果出于某种原因这种模型无法工作，那么您可能需要考虑添加**基于策略的路由**（**PBR**）。
- en: 'In this situation, say, for example, that your servers are load balancing HTTP
    and HTTPS – `80/tcp` and `443/tcp`, respectively. Your policy might look like
    this:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，例如，您的服务器正在负载平衡HTTP和HTTPS - 分别是`80/tcp`和`443/tcp`。您的策略可能如下所示：
- en: Route all traffic `80/tcp` and `443/tcp` to the load balancer (in other words,
    reply traffic from the application).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有流量`80/tcp`和`443/tcp`路由到负载平衡器（换句话说，从应用程序的回复流量）。
- en: Route all other traffic through the subnet's router.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有其他流量通过子网路由器路由。
- en: 'This policy route could be put on the server subnet''s router, as shown here:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个策略路由可以放在服务器子网的路由器上，如下所示：
- en: '![Figure 10.8 – Routing non-application traffic – policy routing on an upstream
    router'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.8 - 路由非应用流量 - 在上游路由器上的策略路由'
- en: '](img/B16336_10_008.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16336_10_008.jpg)'
- en: Figure 10.8 – Routing non-application traffic – policy routing on an upstream
    router
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8 - 路由非应用流量 - 在上游路由器上的策略路由
- en: 'In the preceding diagram, the servers all have a default gateway based on the
    router''s interface (`10.10.10.1`, in this example):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，服务器都有基于路由器接口的默认网关（在本例中为`10.10.10.1`）：
- en: '[PRE3]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This has the benefit of simplicity, but this subnet default gateway device must
    have sufficient horsepower to service the demand of all that reply traffic, without
    affecting the performance of any of its other workloads. Luckily, many modern
    10G switches do have that horsepower. However, this also has the disadvantage
    that your reply traffic now leaves the hypervisor, hits that default gateway router,
    then likely goes back into the virtual infrastructure to reach the load balancer.
    In some environments, this can still work performance-wise, but if not, consider
    moving the policy route to the servers themselves.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的好处是简单，但是这个子网默认网关设备必须有足够的性能来满足所有回复流量的需求，而不会影响其其他工作负载的性能。幸运的是，许多现代的10G交换机确实有这样的性能。然而，这也有一个缺点，即您的回复流量现在离开了超级管理员，到达了默认网关路由器，然后很可能再次进入虚拟基础设施以到达负载平衡器。在某些环境中，这在性能上仍然可以工作，但如果不行，考虑将策略路由移动到服务器本身。
- en: 'To implement this same policy route on a Linux host, follow these steps:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Linux主机上实现相同的策略路由，按照以下步骤进行：
- en: 'First, add the route to `table 5`:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，将路由添加到`表5`：
- en: '[PRE4]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Define the traffic that matches the load balancer (source `10.10.10.0/24`,
    source port `443`):'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义与负载平衡器匹配的流量（源`10.10.10.0/24`，源端口`443`）：
- en: '[PRE5]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Add the lookup, as follows:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加查找，如下所示：
- en: '[PRE6]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This method adds more complexity and CPU overhead than most people want. Also,
    for "network routing issues," support staff are more likely to start any future
    troubleshooting on routers and switches than looking at the host configurations.
    For these reasons, putting the policy route on a router or Layer 3 switch is what
    we often see being implemented.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法比大多数人想要的更复杂，CPU开销也更大。另外，对于“网络路由问题”，支持人员更有可能在未来的故障排除中首先查看路由器和交换机，而不是主机配置。出于这些原因，我们经常看到将策略路由放在路由器或三层交换机上被实施。
- en: 'Using a management interface solves this problem much more elegantly. Also,
    if management interfaces are not already widely in use in the organization, this
    approach nicely introduces them into the environment. In this approach, we keep
    the target hosts configured with their default gateway pointed to the load balancer.
    We then add a management VLAN interface to each host, likely with some management
    services directly in that VLAN. In addition, we can still add specific routes
    to things such as SNMP servers, logging servers, or other internal or internet
    destinations as needed:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 使用管理接口更加优雅地解决了这个问题。另外，如果管理接口在组织中尚未广泛使用，这种方法可以很好地将其引入环境中。在这种方法中，我们保持目标主机配置为默认网关指向负载均衡器。然后，我们为每个主机添加一个管理VLAN接口，可能直接在该VLAN中提供一些管理服务。此外，根据需要，我们仍然可以添加到诸如SNMP服务器、日志服务器或其他内部或互联网目的地的特定路由：
- en: '![Figure 10.9 – Adding a management VLAN'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.9 – 添加管理VLAN'
- en: '](img/B16336_10_009.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16336_10_009.jpg)'
- en: Figure 10.9 – Adding a management VLAN
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9 – 添加管理VLAN
- en: Needless to say, this is what is commonly implemented. Not only is it the simplest
    approach, but it also adds a much-needed management VLAN to the architecture.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 不用说，这是常见的实施方式。这不仅是最简单的方法，而且还为架构添加了一个非常需要的管理VLAN。
- en: With most of the theory covered, let's get on with building a few different
    load balancing scenarios.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在大部分理论已经涵盖的情况下，让我们开始构建几种不同的负载均衡场景。
- en: Building a HAProxy NAT/proxy load balancer
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建HAProxy NAT/代理负载均衡器
- en: First, we likely don't want to use our example host for this, so we must add
    a new network adapter to demonstrate a NAT/proxy (L4/L7) load balancer.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可能不想使用我们的示例主机，因此我们必须添加一个新的网络适配器来演示NAT/代理（L4/L7）负载均衡器。
- en: If your example host is a virtual machine, building a new one should be quick.
    Or, better yet, clone your existing VM and use that. Alternatively, you can download
    an `haproxy –v`.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的示例主机是虚拟机，构建一个新的应该很快。或者更好的是，克隆您现有的虚拟机并使用它。或者，您可以下载一个`haproxy –v`。
- en: Or, if you choose not to "build along" with our example configuration, by all
    means you can still "follow along." While building the plumbing for a load balancer
    can take a bit of work, the actual configuration is pretty simple, and introducing
    you to that configuration is our goal here. You can certainly attain that goal
    without having to build out the supporting virtual or physical infrastructure.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果您选择不使用我们的示例配置进行“构建”，您仍然可以“跟随”。虽然为负载均衡器构建管道可能需要一些工作，但实际配置非常简单，我们的目标是介绍您到该配置。您完全可以在不构建支持虚拟或物理基础设施的情况下实现这一目标。
- en: 'If you are installing this on a fresh Linux host, ensure that you have two
    network adapters (one facing the clients and one facing the servers). As always,
    we''ll start by installing the target application:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在新的Linux主机上安装此软件，请确保您有两个网络适配器（一个面向客户端，一个面向服务器）。与往常一样，我们将从安装目标应用程序开始：
- en: '[PRE7]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*<Start here if you are using the OVA-based installation:>*'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '*<如果您正在使用基于OVA的安装，请从这里开始：>*'
- en: 'You can verify that the installation worked by checking the version number
    using the `haproxy` application itself:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用`haproxy`应用程序本身来检查版本号来验证安装是否成功：
- en: '[PRE8]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that any version newer that the one shown here should work fine.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，任何新版本都应该可以正常工作。
- en: With the package installed, let's look at our example network build.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了软件包后，让我们来看看我们的示例网络构建。
- en: Before you start configuring – NICs, addressing, and routing
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在开始配置之前 – 网卡、寻址和路由
- en: You are welcome to use any IP addressing you choose, but in our example, the
    frontend `192.168.122.21/24` (note that this is different than the interface IP
    of the host), while the backend address of the load balancer will be `192.168.124.1/24`
    – this will be the default gateway of the target hosts. Our target web servers
    will have `192.168.124.10` and `192.168.124.20`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用任何您选择的IP地址，但在我们的示例中，前端为`192.168.122.21/24`（请注意，这与主机的接口IP不同），而负载均衡器的后端地址将为`192.168.124.1/24`
    – 这将是目标主机的默认网关。我们的目标Web服务器将为`192.168.124.10`和`192.168.124.20`。
- en: 'Our final build will look like this:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最终构建将如下所示：
- en: '![Figure 10.10 – Load balancer example build'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.10 – 负载均衡器示例构建'
- en: '](img/B16336_10_010.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16336_10_010.jpg)'
- en: Figure 10.10 – Load balancer example build
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10 – 负载均衡器示例构建
- en: Before we start building our load balancer, this is the perfect time to adjust
    some settings in Linux (some of which will require a system reload).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始构建负载均衡器之前，现在是调整Linux中一些设置的最佳时机（其中一些需要重新加载系统）。
- en: Before you start configuring – performance tuning
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在开始配置之前 – 性能调优
- en: A basic "out of the box" Linux installation must make several assumptions for
    various settings, though many of them result in compromises from a performance
    or security perspective. For a load balancer, there are several Linux settings
    that need to be addressed. Luckily, the HAProxy installation does a lot of this
    work for us (if we installed the licensed version). Once the installation is complete,
    edit the `/etc/sysctl.d/30-hapee-2.2.conf` file and uncomment the lines in the
    following code (in our case, we're installing the Community Edition, so create
    this file and uncomment the lines). As with all basic system settings, test these
    as you go, making these changes one at a time or in logical groups. Also, as expected,
    this may be an iterative process where you may go back and forth from one setting
    to another. As noted in the file comments, not all these values are recommended
    in all or even most cases.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一个基本的“开箱即用”的Linux安装必须对各种设置做出一些假设，尽管其中许多会导致性能或安全方面的妥协。对于负载均衡器，有几个Linux设置需要解决。幸运的是，HAProxy安装为我们做了很多这方面的工作（如果我们安装了许可版本）。安装完成后，编辑`/etc/sysctl.d/30-hapee-2.2.conf`文件，并取消注释以下代码中的行（在我们的情况下，我们正在安装社区版，因此创建此文件并取消注释这些行）。与所有基本系统设置一样，测试这些设置时，逐个或逻辑分组进行更改。此外，正如预期的那样，这可能是一个迭代过程，您可能需要在一个设置和另一个设置之间来回。正如文件注释中所指出的，并非所有这些值在所有情况下甚至在大多数情况下都是推荐的。
- en: These settings and their descriptions can all be found at [https://www.haproxy.com/documentation/hapee/2-2r1/getting-started/system-tuning/](https://www.haproxy.com/documentation/hapee/2-2r1/getting-started/system-tuning/).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设置及其描述都可以在[https://www.haproxy.com/documentation/hapee/2-2r1/getting-started/system-tuning/](https://www.haproxy.com/documentation/hapee/2-2r1/getting-started/system-tuning/)找到。
- en: 'Limit the per-socket default receive/send buffers to limit memory usage when
    running with a lot of concurrent connections. The values are in bytes and represent
    the minimum, default, and maximum. The defaults are `4096`, `87380`, and `4194304`:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 限制每个套接字的默认接收/发送缓冲区，以限制在大量并发连接时的内存使用。这些值以字节表示，分别代表最小值、默认值和最大值。默认值是`4096`、`87380`和`4194304`：
- en: '[PRE9]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Allow early reuse of the same source port for outgoing connections. This is
    required if you have a few hundred connections per second. The default is as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 允许对传出连接早期重用相同的源端口。如果每秒有几百个连接，这是必需的。默认值如下：
- en: '[PRE10]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Extend the source port range for outgoing TCP connections. This limits early
    port reuse and makes use of `64000` source ports. The defaults are `32768` and
    `61000`:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展传出TCP连接的源端口范围。这限制了早期端口重用，并使用了`64000`个源端口。默认值为`32768`和`61000`：
- en: '[PRE11]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Increase the TCP SYN backlog size. This is generally required to support very
    high connection rates, as well as to resist SYN flood attacks. Setting it too
    high will delay SYN cookie usage though. The default is `1024`:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 增加TCP SYN积压大小。这通常需要支持非常高的连接速率，以及抵抗SYN洪水攻击。然而，设置得太高会延迟SYN cookie的使用。默认值是`1024`：
- en: '[PRE12]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Set the timeout in seconds for the `tcp_fin_wait` state. Lowering it speeds
    up the release of dead connections, though it will cause issues below 25-30 seconds.
    It is preferable not to change it if possible. The default is `60`:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 设置`tcp_fin_wait`状态的超时时间（以秒为单位）。降低它可以加快释放死连接，尽管它会在25-30秒以下引起问题。如果可能的话最好不要更改它。默认值为`60`：
- en: '[PRE13]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Limit the number of outgoing SYN-ACK retries. This value is a direct amplification
    factor of SYN floods, so it is important to keep it reasonably low. However, making
    it too low will prevent clients on lossy networks from connecting.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 限制传出SYN-ACK重试次数。这个值是SYN洪水的直接放大因子，所以保持它相当低是很重要的。然而，将它设置得太低会阻止丢包网络上的客户端连接。
- en: 'Using `3` as a default value provides good results (4 SYN-ACK total), while
    lowering it to `1` under a SYN flood attack can save a lot of bandwidth. The default
    is `5`:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`3`作为默认值可以得到良好的结果（总共4个SYN-ACK），而在SYN洪水攻击下将其降低到`1`可以节省大量带宽。默认值为`5`：
- en: '[PRE14]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Set this to `1` to allow local processes to bind to an IP that is not present
    on the system. This is typically what happens with a shared VRRP address, where
    you want both the primary and backup to be started, even though the IP is not
    present. Always leave it as `1`. The default is `0`:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 将其设置为`1`以允许本地进程绑定到系统上不存在的IP。这通常发生在共享VRRP地址的情况下，您希望主备两者都启动，即使IP不存在。始终将其保留为`1`。默认值为`0`：
- en: '[PRE15]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following serves as a higher bound for all the system''s SYN backlogs.
    Put it at least as high as `tcp_max_syn_backlog`; otherwise, clients may experience
    difficulties connecting at high rates or under SYN attacks. The default is `128`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 以下作为系统所有SYN积压的上限。将它至少设置为`tcp_max_syn_backlog`一样高；否则，客户端可能在高速率或SYN攻击下连接时遇到困难。默认值是`128`：
- en: '[PRE16]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Again, note that if you make any of these changes, you may end up coming back
    to this file later to back out or adjust your settings. With all this complete
    (for now, at least), let's configure our load balancer so that it works with our
    two target web servers.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 再次注意，如果您进行了任何这些更改，您可能会在以后回到这个文件来撤消或调整您的设置。完成所有这些（至少现在是这样），让我们配置我们的负载均衡器，使其与我们的两个目标Web服务器配合工作。
- en: Load balancing TCP services – web services
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 负载均衡TCP服务 - Web服务
- en: The configuration for load balancing services is extremely simple. Let's start
    by load balancing between two web server hosts.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡服务的配置非常简单。让我们从在两个Web服务器主机之间进行负载均衡开始。
- en: 'Let''s edit the `/etc/haproxy/haproxy.cfg` file. We''ll create a `frontend`
    section that defines the service that faces clients and a `backend` section that
    defines the two downstream web servers:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们编辑`/etc/haproxy/haproxy.cfg`文件。我们将创建一个`frontend`部分，定义面向客户端的服务，以及一个`backend`部分，定义两个下游Web服务器： '
- en: '[PRE17]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Take note of the following:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意以下内容：
- en: The frontend section has a `default backend` line in it, which tells it which
    services to tie to that frontend.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前端部分中有一行`default backend`，告诉它将哪些服务绑定到该前端。
- en: The frontend has a `bind` statement that allows the load to be balanced across
    all the IPs on that interface. So, in this case, if we're load balancing with
    just one VIP, we can do this on the physical IP of the load balancer.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前端有一个`bind`语句，允许负载在该接口上的所有IP之间平衡。因此，在这种情况下，如果我们只使用一个VIP进行负载平衡，我们可以在负载均衡器的物理IP上执行此操作。
- en: The backend has `roundrobin` as the load balancing algorithm. This means that
    as users connect, they'll get directed to server1, then server2, then server1,
    and so on.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后端使用`roundrobin`作为负载平衡算法。这意味着当用户连接时，他们将被引导到server1，然后是server2，然后是server1，依此类推。
- en: The `check` parameter tells the service to check the target server to ensure
    that it's up. This is much simpler when load balancing TCP services as a simple
    TCP "connect" does the trick, at least to verify that the host and service are
    running.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`check`参数告诉服务检查目标服务器以确保其正常运行。当负载平衡TCP服务时，这要简单得多，因为简单的TCP“连接”就可以解决问题，至少可以验证主机和服务是否正在运行。'
- en: '`fall 3` marks the service as offline after three consecutive failed checks,
    while `rise 2` marks it as online after two successful checks. These rise/fall
    keywords can be used regardless of what check type is being used.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fall 3`在连续三次失败的检查后将服务标记为离线，而`rise 2`在两次成功的检查后将其标记为在线。这些rise/fall关键字可以在使用任何检查类型时使用。'
- en: 'We also want a global section in this file so that we can set some server parameters
    and defaults:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望在此文件中有一个全局部分，以便我们可以设置一些服务器参数和默认值：
- en: '[PRE18]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note that we define the user and group in this section. Going all the way back
    to [*Chapter 3*](B16336_03_Final_NM_ePub.xhtml#_idTextAnchor053), *Using Linux
    and Linux Tools for Network Diagnostics*, we mentioned that you need to have root
    privileges to start a listening port if that port number is less than `1024`.
    What this means for HAProxy is that it needs root rights to start the service.
    The user and group directives in the global section allow the service to "downgrade"
    its rights. This is important because if the service is ever compromised, having
    lower rights gives the attackers far fewer options, likely increases the time
    required for their attack, and hopefully increases their likelihood of being caught.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在此部分定义了用户和组。回溯到[*第3章*](B16336_03_Final_NM_ePub.xhtml#_idTextAnchor053)，*使用Linux和Linux工具进行网络诊断*，我们提到，如果端口号小于`1024`，您需要具有root权限才能启动侦听端口。对于HAProxy来说，这意味着它需要root权限来启动服务。全局部分中的用户和组指令允许服务“降级”其权限。这很重要，因为如果服务被攻击，拥有较低权限会给攻击者提供更少的选项，可能增加攻击所需的时间，并希望增加他们被抓获的可能性。
- en: The `log` line is very straightforward – it tells `haproxy` where to send its
    logs. If you have any problems you need to solve with load balancing, this is
    a good place to start, followed by the target service logs.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`log`行非常直接 - 它告诉`haproxy`将日志发送到哪里。如果您有任何需要解决的负载平衡问题，这是一个很好的起点，接着是目标服务的日志。'
- en: The `stats` directive tells `haproxy` where to store its various performance
    statistics.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '`stats`指令告诉`haproxy`存储其各种性能统计信息的位置。'
- en: The `nbproc` and `nbpthread` directives tell the HAProxy service how many processors
    and threads it has available for use. These numbers should be at least one process
    less than what's available so that in the event of a denial-of-service attack,
    the entire load balancer platform is not incapacitated.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '`nbproc`和`nbpthread`指令告诉HAProxy服务可用于使用的处理器和线程数量。这些数字应该至少比可用的进程少一个，以便在拒绝服务攻击发生时，整个负载平衡器平台不会瘫痪。'
- en: 'The various timeout parameters are there to prevent protocol-level denial-of-service
    attacks. In these situations, the attacker sends the initial requests, but then
    never continues the session – they just continually send requests, "eating up"
    load balancer resources until the memory is entirely consumed. These timeouts
    put a limit on how long the load balancer will keep any one session alive. The
    following table outlines a short description of each of the keep-alive parameters
    we''re discussing here:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 各种超时参数用于防止协议级拒绝服务攻击。在这些情况下，攻击者发送初始请求，但随后从不继续会话 - 他们只是不断发送请求，“耗尽”负载均衡器资源，直到内存完全耗尽。这些超时限制了负载均衡器将保持任何一个会话活动的时间。下表概述了我们在这里讨论的每个保持活动参数的简要描述：
- en: '![](img/B16336_10_Table_02.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16336_10_Table_02.jpg)'
- en: 'Also, the SSL directives are pretty self-explanatory:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，SSL指令相当容易理解：
- en: '`ssl-default-bind-ciphers` lists the ciphers that are allowed in any TLS sessions,
    if the load balancer is terminating or starting a session (that is, if your session
    is in proxy or Layer 7 mode).'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ssl-default-bind-ciphers`列出了在任何TLS会话中允许的密码，如果负载均衡器正在终止或启动会话（即，如果您的会话处于代理或第7层模式）。'
- en: '`ssl-default-bind-options` is there to set a lower bound on TLS versions that
    are supported. At the time of writing, all SSL versions, as well as TLS version
    1.0, are no longer recommended. SSL in particular is susceptible to a number of
    attacks. With all modern browsers capable of negotiating up to TLS version 3,
    most environments elect to support TLS version 1.2 or higher (as shown in the
    example).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ssl-default-bind-options`用于设置支持的TLS版本的下限。在撰写本文时，所有SSL版本以及TLS版本1.0都不再推荐使用。特别是SSL容易受到多种攻击。由于所有现代浏览器都能够协商TLS版本3，大多数环境选择支持TLS版本1.2或更高版本（如示例中所示）。'
- en: Now, from a client machine, you can browse to the HAProxy host, and you'll see
    that you'll connect to one of the backends. If you try again from a different
    browser, you should connect to the second.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，从客户端机器，您可以浏览到HAProxy主机，您会看到您将连接到其中一个后端。如果您尝试从不同的浏览器再次连接，您应该连接到第二个。
- en: 'Let''s expand on this and add support for HTTPS (on `443/tcp`). We''ll add
    an IP to the frontend interface and bind to that. We''ll change the balancing
    algorithm to least connections. Finally, we''ll change the name of the frontend
    and the backend so that they include the port number. This allows us to add the
    additional configuration sections for `443/tcp`. This traffic is load balanced
    nicely if we just monitor at the Layer 4 TCP sessions; no decryption is required:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们扩展一下，为HTTPS（在`443/tcp`上）添加支持。我们将在前端接口上添加一个IP并绑定到该IP。我们将把平衡算法更改为最少连接。最后，我们将更改前端和后端的名称，以包括端口号。这使我们能够为`443/tcp`添加额外的配置部分。如果我们只监视第4层TCP会话，这些流量将得到很好的负载平衡；不需要解密：
- en: '[PRE19]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that we're still just checking that the TCP port is open for a "server
    health" check. This is often referred to as a Layer 3 health check. We put ports
    `80` and `443` into two sections – these can be combined into one section for
    the frontend stanza, but it's normally a best practice to keep these separate
    so that they can be tracked separately. The side effect of this is that the counts
    for the two backend sections aren't aware of each other, but this normally isn't
    an issue since these days, the entire HTTP site is usually just a redirect to
    the HTTPS site.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们仍然只是检查TCP端口是否对“服务器健康”检查打开。这通常被称为第3层健康检查。我们将端口`80`和`443`放入两个部分 - 这些可以合并到前端段的一个部分中，但通常最好将它们分开以便可以分别跟踪它们。这样做的副作用是两个后端部分的计数不会相互影响，但通常这不是一个问题，因为如今整个HTTP站点通常只是重定向到HTTPS站点。
- en: 'Another way to phrase this would be on a `listen` stanza, rather than on the
    frontend and backend stanzas. This approach combines the frontend and backend
    sections into a single stanza and adds a "health check":'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种表达方式是在“listen”段上，而不是在前端和后端段上。这种方法将前端和后端部分合并到一个段中，并添加一个“健康检查”：
- en: '[PRE20]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This default HTTP health check simply opens the default page and ensures that
    something comes back by checking the header for the phrase `HTTP/1.0`. If this
    is not seen in the returned page, it counts as a failed check. You can expand
    this by checking any URI on the site and looking for an arbitrary text string
    on that page. This is often referred to as a "Layer 7" health check since it is
    checking the application. Ensure that you keep your checks simple, though – if
    the application changes even slightly, the text that's returned on a page may
    change enough to have your health check fail and accidentally mark the whole cluster
    as offline!
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这个默认的HTTP健康检查只是打开默认页面，并通过检查标题中的短语“HTTP/1.0”来确保有内容返回。如果在返回的页面中没有看到这个短语，就算作是一次失败的检查。您可以通过检查站点上的任何URI并查找该页面上的任意文本字符串来扩展此功能。这通常被称为“第7层”健康检查，因为它正在检查应用程序。但是请确保您的检查简单
    - 如果应用程序即使稍微更改，页面返回的文本可能会发生足够的变化，导致您的健康检查失败，并意外地标记整个集群为离线！
- en: Setting up persistent (sticky) connections
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建立持久（粘性）连接
- en: 'Let''s inject a cookie into the HTTP sessions by using a variant of the server''s
    name. Let''s also do a basic check of the HTTP service rather than just the open
    port. We will go back to our "frontend/backend" configuration file approach for
    this:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用服务器名称的变体将cookie注入到HTTP会话中。我们还将对HTTP服务进行基本检查，而不仅仅是开放端口。我们将回到我们的“前端/后端”配置文件方法：
- en: '[PRE21]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Make sure that you don't use the IP address or real name of the server as your
    cookie value. If the real server's name is used, attackers may be able to get
    access to that server by looking for that server name in DNS, or in sites that
    have databases of historical DNS entries (`dnsdumpster.com`, for instance). Server
    names can also be used to gain intelligence about the target from certificate
    transparency logs (as we discussed in [*Chapter 8*](B16336_08_Final_NM_ePub.xhtml#_idTextAnchor133),
    *Certificate Services on Linux*). Finally, if the server IP address is used in
    the cookie value, that information gives the attacker some intelligence on your
    internal network architecture, and if the disclosed network is publicly routable,
    it may give them their next target!
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您不要使用服务器的IP地址或真实名称作为cookie值。如果使用真实服务器名称，攻击者可能会通过在DNS中查找该服务器名称或在具有历史DNS条目数据库的站点（例如`dnsdumpster.com`）来访问该服务器。服务器名称也可以用来从证书透明日志中获取有关目标的信息（正如我们在[*第8章*]（B16336_08_Final_NM_ePub.xhtml#_idTextAnchor133）中讨论的那样，*Linux上的证书服务*）。最后，如果服务器IP地址用于cookie值，该信息将使攻击者对您的内部网络架构有所了解，如果披露的网络是公共可路由的，可能会成为他们的下一个目标！
- en: Implementation note
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施说明
- en: Now that we have covered a basic configuration, a very common step is to have
    a "placeholder" website on each server, each identified to match the server. Using
    "1-2-3," "a-b-c," or "red-green-blue" are all common approaches, just enough to
    tell each server session from the next. Now, with different browsers or different
    workstations, browse to the shared address multiple times to ensure that you are
    directed to the correct backend server, as defined by your rule set.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了基本配置，一个非常常见的步骤是在每台服务器上都有一个“占位符”网站，每个网站都被标识为与服务器匹配。使用“1-2-3”，“a-b-c”或“red-green-blue”都是常见的方法，足以区分每个服务器会话。现在，使用不同的浏览器或不同的工作站，多次浏览共享地址，以确保您被定向到正确的后端服务器，如您的规则集所定义的那样。
- en: This is, of course, a great way to ensure things are working as you progressively
    build your configuration, but it's also a great troubleshooting mechanism to help
    you decide on simple things such as "is this still working after the updates?"
    or "I know what the helpdesk ticket says, but is there even a problem to solve?"
    months or even years later. Test pages like this are a great thing to keep up
    long-term for future testing or troubleshooting.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这是一个逐步构建配置的好方法，以确保事情正常运行，但它也是一个很好的故障排除机制，可以帮助您决定一些简单的事情，比如“更新后这还有效吗？”或“我知道帮助台票说了什么，但是真的有问题要解决吗？”甚至几个月甚至几年后。像这样的测试页面是一个很好的长期保留的东西，用于未来的测试或故障排除。
- en: HTTPS frontending
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HTTPS前端
- en: 'In days gone by, server architects were happy to set up load balancers to offload
    HTTPS processing, moving that encrypt/decrypt processing from the server to the
    load balancer. This saved on server CPU, and it also moved the responsibility
    of implementing and maintaining the certificates to whoever was managing the load
    balancer. These reasons are mostly no longer valid though, for several reasons:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 过去，服务器架构师乐意设置负载均衡器来卸载HTTPS处理，将加密/解密处理从服务器转移到负载均衡器。这样可以节省服务器CPU，并且将实施和维护证书的责任转移到管理负载均衡器的人。然而，出于几个原因，这些原因现在大多数已经不再有效：
- en: If the servers and load balancer are all virtual (as is recommended in most
    cases), this just moves the processing around between different VMs on the same
    hardware – there's no net gain.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果服务器和负载均衡器都是虚拟的（在大多数情况下是推荐的），这只是在不同虚拟机之间移动处理 - 没有净增益。
- en: Modern processors are much more efficient in performing encryption and decryption
    – the algorithms are written with CPU performance in mind. In fact, depending
    on the algorithm, the encrypt/decrypt operations may be native to the CPU, which
    is a huge performance gain.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代处理器在执行加密和解密方面效率更高 - 算法是针对CPU性能编写的。事实上，根据算法的不同，加密/解密操作可能是CPU的本地操作，这是一个巨大的性能提升。
- en: The use of wildcard certificates makes the whole "certificate management" piece
    much simpler.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用通配符证书可以使整个“证书管理”过程变得更简单。
- en: However, we still do HTTPS frontending with load balancers, usually to get reliable
    session persistence using cookies – you can't add a cookie to an HTTPS response
    (or read one on the next request) unless you can read and write to the data stream,
    which implies that, at some point, it's been decrypted.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们仍然使用负载均衡器进行HTTPS前端处理，通常是为了使用cookie实现可靠的会话持久性 - 除非你能读取和写入数据流，否则无法在HTTPS响应中添加cookie（或在下一个请求中读取），这意味着在某个时刻它已经被解密。
- en: Remember from our previous discussion that in this configuration, each TLS session
    will be terminated on the frontend side, using a valid certificate. Since this
    is now a proxy setup (Layer 7 load balancing), the backend session is a separate
    HTTP or HTTPS session. In days past, the backend would often be HTTP (mostly to
    save CPU resources), but in modern times, this will be rightly seen as a security
    exposure, especially if you are in the finance, healthcare, or government sectors
    (or any sector that hosts sensitive information). For this reason, in a modern
    build, the backend will almost always be HTTPS as well, often with the same certificate
    on the target web server.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，根据我们之前的讨论，在这个配置中，每个TLS会话将在前端终止，使用有效的证书。由于现在这是一个代理设置（第7层负载平衡），后端会话是一个单独的HTTP或HTTPS会话。在过去，后端通常会是HTTP（主要是为了节省CPU资源），但在现代，这将被视为安全风险，特别是如果你在金融、医疗保健或政府部门（或任何承载敏感信息的部门）。因此，在现代构建中，后端几乎总是会是HTTPS，通常使用目标Web服务器上相同的证书。
- en: Again, the disadvantage of this setup is that since the actual client of the
    target web server is the load balancer, the `X-Forwarded-*` HTTPS header will
    be lost, and the IP address of the actual client will not be available to the
    web server (or its logs).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调这种设置的缺点是，由于目标Web服务器的实际客户端是负载均衡器，`X-Forwarded-*` HTTPS头将丢失，并且实际客户端的IP地址将不可用于Web服务器（或其日志）。
- en: 'How do we go about configuring this setup? First, we must obtain the site certificate
    and private key, whether that''s a "named certificate" or a wildcard. Now, combine
    these into one file (not as a `pfx` file, but as a chain) by simply concatenating
    them together using the `cat` command:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何配置这个设置？首先，我们必须获取站点证书和私钥，无论是“命名证书”还是通配符。现在，将它们合并成一个文件（不是作为`pfx`文件，而是作为一个链），只需使用`cat`命令简单地将它们连接在一起：
- en: '[PRE22]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note that we're using `sudo` in the second half of the command, to give the
    command rights to the `/etc/ssl/sitename.com` directory. Also, note the `tee`
    command, which echoes the command's output to the screen. It also directs the
    output to the desired location.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在命令的后半部分我们使用了`sudo`，以赋予命令对`/etc/ssl/sitename.com`目录的权限。还要注意`tee`命令，它会将命令的输出显示在屏幕上，并将输出定向到所需的位置。
- en: 'Now, we can bind the certificate to the address in the frontend file stanza:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将证书绑定到前端文件段中的地址：
- en: '[PRE23]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Take note of the following in this configuration:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配置中，请注意以下内容：
- en: We can now use cookies for session persistence (in the backend section), which
    is generally the primary objective in this configuration.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现在我们可以在后端部分使用cookie来实现会话持久性，这通常是这个配置中的主要目标。
- en: We use the `redirect scheme` line in the frontend to instruct the proxy to use
    SSL/TLS on the backend.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在前端使用`redirect scheme`行指示代理在后端使用SSL/TLS。
- en: The `forwardfor` keyword adds the actual client IP to the `X-Forwarded-For`
    HTTP header field on the backend request. Note that it's up to the web server
    to parse this out and log it appropriately so that you can use it later.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forwardfor`关键字将实际客户端IP添加到后端请求的`X-Forwarded-For` HTTP头字段中。请注意，需要由Web服务器解析这些内容并适当地记录，以便以后使用。'
- en: 'Depending on the application and browsers, you can also add the client IP to
    the backend HTTP request in the `X-Client-IP` header field with a clause:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 根据应用程序和浏览器的不同，你还可以在`X-Client-IP`头字段中添加客户端IP到后端HTTP请求中：
- en: '[PRE24]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This approach sees mixed results.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法效果参差不齐。
- en: Note, however, that whatever you add or change in the HTTP header, the actual
    client IP that the target server "sees" remains the backend address of the load
    balancer – these changed or added header values are simply fields in the HTTPS
    request. If you intend to use these header values for logging, troubleshooting,
    or monitoring, it's up to the web server to parse them out and log them appropriately.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，无论你在HTTP头中添加或更改什么，目标服务器“看到”的实际客户端IP地址仍然是负载均衡器的后端地址 - 这些更改或添加的头值只是HTTPS请求中的字段。如果你打算使用这些头值进行日志记录、故障排除或监控，就需要Web服务器来解析它们并适当地记录。
- en: That covers our example configurations – we've covered NAT-based and proxy-based
    load balancing, as well as session persistence for both HTTP and HTTPS traffic.
    After all the theory, actually configuring the load balancer is simple – the work
    is all in the design and in setting up the supporting network infrastructure.
    Let's discuss security briefly before we close out this chapter.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这涵盖了我们的示例配置 - 我们涵盖了基于NAT和基于代理的负载平衡，以及HTTP和HTTPS流量的会话持久性。在所有理论之后，实际配置负载均衡器很简单
    - 工作都在设计和设置支持网络基础设施中。在结束本章之前，让我们简要讨论一下安全性。
- en: A final note on load balancer security
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于负载均衡器安全性的最后一点说明
- en: So far, we've discussed how an attacker might be able to gain insight or access
    to the internal network if they can get server names or IP addresses. We discussed
    how a malicious actor can get that information using information disclosed by
    the cookies used in a local balancer configuration for persistent settings. How
    else can an attacker gain information about our target servers (which are behind
    the load balancer and should be hidden)?
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了攻击者如何能够获得有关内部网络的见解或访问权限，如果他们可以获得服务器名称或IP地址。我们讨论了恶意行为者如何使用本地负载均衡器配置中披露的信息来获取这些信息以进行持久设置。攻击者还可以以其他方式获取有关我们的目标服务器（这些服务器位于负载均衡器后面并且应该被隐藏）的信息吗？
- en: Certificate transparency information is another favorite method for getting
    current or old server names, as we discussed in [*Chapter 8*](B16336_08_Final_NM_ePub.xhtml#_idTextAnchor133),
    *Certificate Services on Linux*. Even if the old server names are no longer in
    use, the records of their past certificates are immortal.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 证书透明信息是获取当前或旧服务器名称的另一种常用方法，正如我们在[*第8章*](B16336_08_Final_NM_ePub.xhtml#_idTextAnchor133)中讨论的那样，*Linux上的证书服务*。即使旧的服务器名称不再使用，其过去证书的记录是永恒的。
- en: The Internet Archive site at [https://archive.org](https://archive.org) takes
    "snapshots" of websites periodically, and allows them to be searched and viewed,
    allowing people to go "back in time" and view older versions of your infrastructure.
    If older servers are disclosed in your old DNS or in the older code of your web
    servers, they're likely available on this site.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网档案馆网站[https://archive.org](https://archive.org)定期对网站进行“快照”，并允许搜索和查看它们，使人们可以“回到过去”并查看您基础设施的旧版本。如果旧服务器在您的旧DNS或Web服务器的旧代码中披露，它们很可能可以在此网站上找到。
- en: DNS archive sites such as `dnsdumpster` collect DNS information using passive
    methods such as packet analysis and present it via a web or API interface. This
    allows an attacker to find both older IP addresses and older (or current) hostnames
    that an organization might have used, which sometimes allows them to still access
    those services by IP if the DNS entries are removed. Alternatively, they can access
    them individually by hostname, even if they are behind a load balancer.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: DNS存档网站，如`dnsdumpster`，使用被动方法（如数据包分析）收集DNS信息，并通过Web或API界面呈现。这使攻击者可以找到旧的IP地址和旧（或当前）主机名，组织有时可以通过IP仍然访问这些服务，即使DNS条目被删除。或者，他们可以通过主机名单独访问它们，即使它们在负载均衡器后面。
- en: '*Google Dorks* are another method of obtaining such information – these are
    terms for finding specific information that can be used in search engines (not
    just Google). Often, something as simple as a search term such as `inurl:targetdomain.com`
    will find hostnames that the target organization would rather keep hidden. Some
    Google Dorks that are specific to `haproxy` include the following:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '*Google Dorks*是获得此类信息的另一种方法 - 这些术语用于在搜索引擎（不仅仅是Google）中查找特定信息。通常，像`inurl:targetdomain.com`这样的搜索词将找到目标组织宁愿保持隐藏的主机名。一些特定于`haproxy`的Google
    Dorks包括以下内容：'
- en: '[PRE25]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note that where we say `site:`, you can also specify `inurl:`. In that case,
    you can also shorten the search term to just the domain rather than the full site
    name.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在我们说`site:`时，您也可以指定`inurl:`。在这种情况下，您还可以将搜索词缩短为域而不是完整的站点名称。
- en: Sites such as `shodan.io` will also index historic versions of your servers,
    focusing on server IP addresses, hostnames, open ports, and the services running
    on those ports. Shodan is unique in how well it identifies the service running
    on an open port. While they are not, of course, 100% successful in that (think
    of it as someone else's NMAP results), when they do identify a service, the "proof"
    is posted with it, so if you are using Shodan for reconnaissance, you can use
    that to verify how accurate that determination might be. Shodan has both a web
    interface and a comprehensive API. With this service, you can often find improperly
    secured load balancers by organization or by geographic area.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如`shodan.io`之类的网站还将索引您服务器的历史版本，重点关注服务器IP地址，主机名，开放端口以及在这些端口上运行的服务。 Shodan在识别开放端口上运行的服务方面非常独特。当然，他们在这方面并不百分之百成功（将其视为他人的NMAP结果），但是当他们识别服务时，会附上“证据”，因此，如果您使用Shodan进行侦察，您可以使用它来验证该确定可能有多准确。Shodan既有Web界面又有全面的API。通过这项服务，您通常可以按组织或地理区域找到未经适当保护的负载均衡器。
- en: 'A final comment on search engines: if Google (or any search engine) can reach
    your real servers directly, then that content will be indexed, making it easily
    searchable. If a site may have an authentication bypass issue, the "protected
    by authentication" content will also be indexed and available for anyone who uses
    that engine.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 最后对搜索引擎的评论：如果Google（或任何搜索引擎）可以直接访问您的真实服务器，那么该内容将被索引，使其易于搜索。如果网站可能存在身份验证绕过问题，则“受身份验证保护”的内容也将被索引，并可供使用该引擎的任何人使用。
- en: That said, it's always a good idea to use tools like the ones we've just discussed
    to periodically look for issues on your perimeter infrastructure.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，始终使用我们刚刚讨论过的工具定期查找外围基础设施上的问题是一个好主意。
- en: Another important security issue to consider is management access. It's important
    to restrict access to the management interface of the load balancers (in other
    words, SSH), restricting it to permitted hosts and subnets on all interfaces.
    Remember that if your load balancer is parallel to your firewall, the whole internet
    has access to it, and even if not, everyone on your internal network has access
    to it. You'll want to whittle that access down to just trusted management hosts
    and subnets. If you need a reference for that, remember that we covered this in
    [*Chapter 4*](B16336_04_Final_NM_ePub.xhtml#_idTextAnchor071), *The Linux Firewall*,
    and [*Chapter 5*](B16336_05_Final_NM_ePub.xhtml#_idTextAnchor085), *Linux Security
    Standards with Real-Life Examples*.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的安全问题是管理访问。重要的是要限制对负载均衡器的管理界面（即SSH）的访问，将其限制在所有接口上的允许主机和子网。请记住，如果您的负载均衡器与防火墙平行，整个互联网都可以访问它，即使不是这样，您内部网络上的每个人也可以访问它。您需要将访问权限缩减到可信任的管理主机和子网。如果您需要参考，记住我们在[*第4章*](B16336_04_Final_NM_ePub.xhtml#_idTextAnchor071)中涵盖了这一点，*Linux防火墙*，以及[*第5章*](B16336_05_Final_NM_ePub.xhtml#_idTextAnchor085)，*具有实际示例的Linux安全标准*。
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Hopefully, this chapter has served as a good introduction to load balancers,
    how to deploy them, and the reasons you might choose to make various design and
    implementation decisions around them.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 希望本章对负载均衡器的介绍、部署以及您可能选择围绕它们做出各种设计和实施决策的原因有所帮助。
- en: If you used new VMs to follow along with the examples in this chapter, we won't
    need them in subsequent chapters, but you might wish to keep the HAProxy VMs in
    particular if you need an example to reference later. If you followed the examples
    in this chapter just by reading them, then the examples in this chapter remain
    available to you. Either way, as you read this chapter, I hope you were mentally
    working out how load balancers might fit into your organization's internal or
    perimeter architecture.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在本章节中使用新的虚拟机来跟随示例，那么在接下来的章节中我们将不再需要它们，但是如果您需要以后参考示例，您可能希望保留HAProxy虚拟机。如果您只是通过阅读本章中的示例来跟随，那么本章中的示例仍然对您可用。无论哪种方式，当您阅读本章时，我希望您能够在脑海中思考负载均衡器如何适应您组织的内部或边界架构。
- en: With this chapter completed, you should have the skills needed to build a load
    balancer in any organization. These skills were discussed in the context of the
    (free) version of HAProxy, but the design and implementation considerations are
    almost all directly usable in any vendor's platform, the only changes being the
    wording and syntax in the configuration options or menus. In the next chapter,
    we'll look at enterprise routing implementations based on Linux platforms.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，您应该具备在任何组织中构建负载均衡器所需的技能。这些技能是在（免费）版本的HAProxy的背景下讨论的，但设计和实施考虑几乎都可以直接在任何供应商的平台上使用，唯一的变化是配置选项或菜单中的措辞和语法。在下一章中，我们将看一下基于Linux平台的企业路由实现。
- en: Questions
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'As we conclude, here is a list of questions for you to test your knowledge
    regarding this chapter''s material. You will find the answers in the *Assessments*
    section of the *Appendix*:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这里有一些问题供您测试对本章材料的了解。您将在*附录*的*评估*部分中找到答案：
- en: When would you choose to use a **Direct Server Return** (**DSR**) load balancer?
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 何时您会选择使用**直接服务器返回**（**DSR**）负载均衡器？
- en: Why would you choose to use a proxy-based load balancer as opposed to one that
    is a pure NAT-based solution?
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么您会选择使用基于代理的负载均衡器，而不是纯NAT-based解决方案的负载均衡器？
- en: Further reading
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Take a look at the following links to learn more about the topics that were
    covered in this chapter:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下链接，了解本章涵盖的主题的更多信息：
- en: 'HAProxy documentation: [http://www.haproxy.org/#docs](http://www.haproxy.org/#docs)'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HAProxy文档：[http://www.haproxy.org/#docs](http://www.haproxy.org/#docs)
- en: 'HAProxy documentation (Commercial version): [https://www.haproxy.com/documentation/hapee/2-2r1/getting-started/](https://www.haproxy.com/documentation/hapee/2-2r1/getting-started/)'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HAProxy文档（商业版本）：[https://www.haproxy.com/documentation/hapee/2-2r1/getting-started/](https://www.haproxy.com/documentation/hapee/2-2r1/getting-started/)
- en: 'HAProxy GitHub: [https://github.com/haproxytech](https://github.com/haproxytech)'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HAProxy GitHub：[https://github.com/haproxytech](https://github.com/haproxytech)
- en: 'HAProxy GitHub, OVA VM download: [https://github.com/haproxytech/vmware-haproxy#download](https://github.com/haproxytech/vmware-haproxy#download)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HAProxy GitHub，OVA虚拟机下载：[https://github.com/haproxytech/vmware-haproxy#download](https://github.com/haproxytech/vmware-haproxy#download)
- en: 'HAProxy Community versus Enterprise Differences: [https://www.haproxy.com/products/community-vs-enterprise-edition/](https://www.haproxy.com/products/community-vs-enterprise-edition/)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HAProxy社区与企业版本的区别：[https://www.haproxy.com/products/community-vs-enterprise-edition/](https://www.haproxy.com/products/community-vs-enterprise-edition/)
- en: 'More on Load Balancing Algorithms: [http://cbonte.github.io/haproxy-dconv/2.4/intro.html#3.3.5](http://cbonte.github.io/haproxy-dconv/2.4/intro.html#3.3.5)'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关负载均衡算法的更多信息：[http://cbonte.github.io/haproxy-dconv/2.4/intro.html#3.3.5](http://cbonte.github.io/haproxy-dconv/2.4/intro.html#3.3.5)

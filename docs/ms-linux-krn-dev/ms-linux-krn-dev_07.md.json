["```\nstruct mm_struct {\n               struct vm_area_struct *mmap; /* list of VMAs */\n               struct rb_root mm_rb;\n               u32 vmacache_seqnum; /* per-thread vmacache */\n#ifdef CONFIG_MMU\n             unsigned long (*get_unmapped_area) (struct file *filp, unsigned long addr, unsigned long len,\n                                                                                                    unsigned long pgoff, unsigned long flags);\n #endif\n            unsigned long mmap_base;               /* base of mmap area */\n            unsigned long mmap_legacy_base;  /* base of mmap area in bottom-up allocations */\n            unsigned long task_size;                   /* size of task vm space */\n            unsigned long highest_vm_end;      /* highest vma end address */\n            pgd_t * pgd;  \n            atomic_t mm_users;           /* How many users with user space? */\n            atomic_t mm_count;           /* How many references to \"struct mm_struct\" (users count as 1) */\n            atomic_long_t nr_ptes;      /* PTE page table pages */\n #if CONFIG_PGTABLE_LEVELS > 2\n           atomic_long_t nr_pmds;      /* PMD page table pages */\n #endif\n           int map_count;                           /* number of VMAs */\n         spinlock_t page_table_lock;      /* Protects page tables and some counters */\n         struct rw_semaphore mmap_sem;\n\n       struct list_head mmlist;      /* List of maybe swapped mm's. These are globally strung\n                                                         * together off init_mm.mmlist, and are protected\n                                                         * by mmlist_lock\n                                                         */\n        unsigned long hiwater_rss;     /* High-watermark of RSS usage */\n         unsigned long hiwater_vm;     /* High-water virtual memory usage */\n        unsigned long total_vm;          /* Total pages mapped */\n         unsigned long locked_vm;       /* Pages that have PG_mlocked set */\n         unsigned long pinned_vm;      /* Refcount permanently increased */\n         unsigned long data_vm;          /* VM_WRITE & ~VM_SHARED & ~VM_STACK */\n        unsigned long exec_vm;          /* VM_EXEC & ~VM_WRITE & ~VM_STACK */\n         unsigned long stack_vm;         /* VM_STACK */\n         unsigned long def_flags;\n         unsigned long start_code, end_code, start_data, end_data;\n         unsigned long start_brk, brk, start_stack;\n         unsigned long arg_start, arg_end, env_start, env_end;\n        unsigned long saved_auxv[AT_VECTOR_SIZE];               /* for /proc/PID/auxv */\n/*\n * Special counters, in some configurations protected by the\n * page_table_lock, in other configurations by being atomic.\n */\n        struct mm_rss_stat rss_stat;\n      struct linux_binfmt *binfmt;\n      cpumask_var_t cpu_vm_mask_var;\n /* Architecture-specific MM context */\n        mm_context_t context;\n      unsigned long flags;                   /* Must use atomic bitops to access the bits */\n      struct core_state *core_state;   /* core dumping support */\n       ...\n      ...\n      ...\n };\n```", "```\n/*\n  * This struct defines a memory VMM memory area. There is one of these\n  * per VM-area/task. A VM area is any part of the process virtual memory\n  * space that has a special rule for the page-fault handlers (ie a shared\n  * library, the executable area etc).\n  */\n struct vm_area_struct {\n               /* The first cache line has the info for VMA tree walking. */\n              unsigned long vm_start; /* Our start address within vm_mm. */\n               unsigned long vm_end; /* The first byte after our end address within vm_mm. */\n              /* linked list of VM areas per task, sorted by address */\n               struct vm_area_struct *vm_next, *vm_prev;\n               struct rb_node vm_rb;\n               /*\n                 * Largest free memory gap in bytes to the left of this VMA.\n                 * Either between this VMA and vma->vm_prev, or between one of the\n                 * VMAs below us in the VMA rbtree and its ->vm_prev. This helps\n                 * get_unmapped_area find a free area of the right size.\n                */\n                 unsigned long rb_subtree_gap;\n              /* Second cache line starts here. */\n               struct mm_struct   *vm_mm; /* The address space we belong to. */\n                pgprot_t  vm_page_prot;       /* Access permissions of this VMA. */\n                unsigned long vm_flags;        /* Flags, see mm.h. */\n              /*\n                 * For areas with an address space and backing store,\n                 * linkage into the address_space->i_mmap interval tree.\n                 */\n                struct {\n                              struct rb_node rb;\n                              unsigned long rb_subtree_last;\n                           } shared;\n         /*\n                 * A file's MAP_PRIVATE vma can be in both i_mmap tree and anon_vma\n                 * list, after a COW of one of the file pages. A MAP_SHARED vma\n                 * can only be in the i_mmap tree. An anonymous MAP_PRIVATE, stack\n                 * or brk vma (with NULL file) can only be in an anon_vma list.\n          */\n            struct list_head anon_vma_chain; /* Serialized by mmap_sem & page_table_lock */\n           struct anon_vma *anon_vma;        /* Serialized by page_table_lock */\n            /* Function pointers to deal with this struct. */\n            const struct vm_operations_struct *vm_ops;\n            /* Information about our backing store: */\n            unsigned long vm_pgoff; /* Offset (within vm_file) in PAGE_SIZE units */\n            struct file * vm_file; /* File we map to (can be NULL). */\n            void * vm_private_data; /* was vm_pte (shared mem) */\n#ifndef CONFIG_MMU\n          struct vm_region *vm_region; /* NOMMU mapping region */\n #endif\n #ifdef CONFIG_NUMA\n         struct mempolicy *vm_policy; /* NUMA policy for the VMA */\n #endif\n        struct vm_userfaultfd_ctx vm_userfaultfd_ctx;\n };\n```", "```\n/*\n  * These are the virtual MM functions - opening of an area, closing and\n  * unmapping it (needed to keep files on disk up-to-date etc), pointer\n  * to the functions called when a no-page or a wp-page exception occurs.\n  */\n struct vm_operations_struct {\n         void (*open)(struct vm_area_struct * area);\n         void (*close)(struct vm_area_struct * area);\n         int (*mremap)(struct vm_area_struct * area);\n         int (*fault)(struct vm_area_struct *vma, struct vm_fault *vmf);\n         int (*pmd_fault)(struct vm_area_struct *, unsigned long address,\n                                                 pmd_t *, unsigned int flags);\n         void (*map_pages)(struct fault_env *fe,\n                         pgoff_t start_pgoff, pgoff_t end_pgoff);\n         /* notification that a previously read-only page is about to become\n          * writable, if an error is returned it will cause a SIGBUS */\n         int (*page_mkwrite)(struct vm_area_struct *vma, struct vm_fault *vmf);\n    /* same as page_mkwrite when using VM_PFNMAP|VM_MIXEDMAP */\n         int (*pfn_mkwrite)(struct vm_area_struct *vma, struct vm_fault *vmf);\n/* called by access_process_vm when get_user_pages() fails, typically\n          * for use by special VMAs that can switch between memory and hardware\n          */\n         int (*access)(struct vm_area_struct *vma, unsigned long addr,\n                       void *buf, int len, int write);\n/* Called by the /proc/PID/maps code to ask the vma whether it\n          * has a special name. Returning non-NULL will also cause this\n          * vma to be dumped unconditionally. */\n         const char *(*name)(struct vm_area_struct *vma);\n   ...\n   ...\n```", "```\n/* Look up the first VMA which satisfies addr < vm_end, NULL if none. */\nstruct vm_area_struct *find_vma(struct mm_struct *mm, unsigned long addr)\n{\n        struct rb_node *rb_node;\n        struct vm_area_struct *vma;\n\n        /* Check the cache first. */\n        vma = vmacache_find(mm, addr);\n        if (likely(vma))\n               return vma;\n\n       rb_node = mm->mm_rb.rb_node;\n       while (rb_node) {\n               struct vm_area_struct *tmp;\n               tmp = rb_entry(rb_node, struct vm_area_struct, vm_rb);\n               if (tmp->vm_end > addr) {\n                        vma = tmp;\n                        if (tmp->vm_start <= addr)\n                                 break;\n                        rb_node = rb_node->rb_left;\n               } else\n                        rb_node = rb_node->rb_right;\n        }\n        if (vma)\n               vmacache_update(addr, vma);\n        return vma;\n}\n```", "```\nstruct vm_area_struct *vma_merge(struct mm_struct *mm,\n                        struct vm_area_struct *prev, unsigned long addr,\n                        unsigned long end, unsigned long vm_flags,\n                        struct anon_vma *anon_vma, struct file *file,\n                        pgoff_t pgoff, struct mempolicy *policy,\n                        struct vm_userfaultfd_ctx vm_userfaultfd_ctx)\n{\n         pgoff_t pglen = (end - addr) >> PAGE_SHIFT;\n         struct vm_area_struct *area, *next;\n         int err;  \n         ...\n         ...\n\n```", "```\n        ...  \n        ...\n        /*\n         * Can it merge with the predecessor?\n         */\n        if (prev && prev->vm_end == addr &&\n                        mpol_equal(vma_policy(prev), policy) &&\n                        can_vma_merge_after(prev, vm_flags,\n                                            anon_vma, file, pgoff,\n                                            vm_userfaultfd_ctx)) {\n        ...\n        ...\n```", "```\n                ...                \n                ...               \n                /*\n                 * OK, it can. Can we now merge in the successor as well?\n                 */\n                if (next && end == next->vm_start &&\n                                mpol_equal(policy, vma_policy(next)) &&\n                                can_vma_merge_before(next, vm_flags,\n                                                     anon_vma, file,\n                                                     pgoff+pglen,\n                                                     vm_userfaultfd_ctx) &&\n                                is_mergeable_anon_vma(prev->anon_vma,\n                                                      next->anon_vma, NULL)) {\n                                                        /* cases 1, 6 */\n                        err = __vma_adjust(prev, prev->vm_start,\n                                         next->vm_end, prev->vm_pgoff, NULL,\n                                         prev);\n                } else /* cases 2, 5, 7 */\n                        err = __vma_adjust(prev, prev->vm_start,\n end, prev->vm_pgoff, NULL, prev);\n\n           ...\n           ...\n}\n```", "```\nstruct address_space {\n        struct inode *host; /* owner: inode, block_device */\n        struct radix_tree_root page_tree; /* radix tree of all pages */\n        spinlock_t tree_lock; /* and lock protecting it */\n        atomic_t i_mmap_writable;/* count VM_SHARED mappings */\n        struct rb_root i_mmap; /* tree of private and shared mappings */\n        struct rw_semaphore i_mmap_rwsem; /* protect tree, count, list */\n        /* Protected by tree_lock together with the radix tree */\n        unsigned long nrpages; /* number of total pages */\n        /* number of shadow or DAX exceptional entries */\n        unsigned long nrexceptional;\n        pgoff_t writeback_index;/* writeback starts here */\n        const struct address_space_operations *a_ops; /* methods */\n        unsigned long flags; /* error bits */\n        spinlock_t private_lock; /* for use by the address_space */\n        gfp_t gfp_mask; /* implicit gfp mask for allocations */\n        struct list_head private_list; /* ditto */\n        void *private_data; /* ditto */\n} __attribute__((aligned(sizeof(long))));\n```", "```\nstruct radix_tree_root {\n        gfp_t gfp_mask;\n        struct radix_tree_node __rcu *rnode;\n};\n```", "```\nstruct radix_tree_node {\n        unsigned char shift; /* Bits remaining in each slot */\n        unsigned char offset; /* Slot offset in parent */\n        unsigned int count;\n        union {\n                struct {\n                        /* Used when ascending tree */\n                        struct radix_tree_node *parent;\n                        /* For tree user */\n                        void *private_data;\n                };\n                /* Used when freeing node */\n                struct rcu_head rcu_head;\n        };\n        /* For tree user */\n        struct list_head private_list;\n        void __rcu *slots[RADIX_TREE_MAP_SIZE];\n        unsigned long tags[RADIX_TREE_MAX_TAGS][RADIX_TREE_TAG_LONGS];\n};\n```", "```\n/*\n * Radix-tree tags, for tagging dirty and writeback pages within \n * pagecache radix trees                 \n */\n#define PAGECACHE_TAG_DIRTY 0\n#define PAGECACHE_TAG_WRITEBACK 1\n#define PAGECACHE_TAG_TOWRITE 2\n```", "```\nvoid *radix_tree_tag_set(struct radix_tree_root *root,\n                                     unsigned long index, unsigned int tag);\nvoid *radix_tree_tag_clear(struct radix_tree_root *root,\n                                     unsigned long index, unsigned int tag);\nint radix_tree_tag_get(struct radix_tree_root *root,\n                                     unsigned long index, unsigned int tag);\n```"]
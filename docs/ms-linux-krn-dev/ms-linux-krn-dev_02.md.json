["```\nstruct sched_class {\n    const struct sched_class *next;\n\n     void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags);\n   void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags);\n   void (*yield_task) (struct rq *rq);\n       bool (*yield_to_task) (struct rq *rq, struct task_struct *p, bool preempt);\n\n void (*check_preempt_curr) (struct rq *rq, struct task_struct *p, int flags);\n\n       /*\n         * It is the responsibility of the pick_next_task() method that will\n       * return the next task to call put_prev_task() on the @prev task or\n  * something equivalent.\n   *\n         * May return RETRY_TASK when it finds a higher prio class has runnable\n    * tasks.\n  */\n       struct task_struct * (*pick_next_task) (struct rq *rq,\n                                            struct task_struct *prev,\n                                         struct rq_flags *rf);\n     void (*put_prev_task) (struct rq *rq, struct task_struct *p);\n\n#ifdef CONFIG_SMP\n        int  (*select_task_rq)(struct task_struct *p, int task_cpu, int sd_flag, int flags);\n      void (*migrate_task_rq)(struct task_struct *p);\n\n     void (*task_woken) (struct rq *this_rq, struct task_struct *task);\n\n  void (*set_cpus_allowed)(struct task_struct *p,\n                            const struct cpumask *newmask);\n\n    void (*rq_online)(struct rq *rq);\n void (*rq_offline)(struct rq *rq);\n#endif\n\n      void (*set_curr_task) (struct rq *rq);\n    void (*task_tick) (struct rq *rq, struct task_struct *p, int queued);\n     void (*task_fork) (struct task_struct *p);\n        void (*task_dead) (struct task_struct *p);\n\n  /*\n         * The switched_from() call is allowed to drop rq->lock, therefore we\n   * cannot assume the switched_from/switched_to pair is serialized by\n        * rq->lock. They are however serialized by p->pi_lock.\n      */\n       void (*switched_from) (struct rq *this_rq, struct task_struct *task);\n     void (*switched_to) (struct rq *this_rq, struct task_struct *task);\n       void (*prio_changed) (struct rq *this_rq, struct task_struct *task,\n                            int oldprio);\n\n  unsigned int (*get_rr_interval) (struct rq *rq,\n                                    struct task_struct *task);\n\n void (*update_curr) (struct rq *rq);\n\n#define TASK_SET_GROUP  0\n#define TASK_MOVE_GROUP  1\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n       void (*task_change_group) (struct task_struct *p, int type);\n#endif\n};\n```", "```\nconst struct sched_class fair_sched_class = {\n         .next                   = &idle_sched_class,\n         .enqueue_task           = enqueue_task_fair,\n         .dequeue_task           = dequeue_task_fair,\n         .yield_task             = yield_task_fair,\n         .yield_to_task          = yield_to_task_fair,\n\n         .check_preempt_curr     = check_preempt_wakeup,\n\n         .pick_next_task         = pick_next_task_fair,\n         .put_prev_task          = put_prev_task_fair,\n....\n}\n```", "```\nconst struct sched_class rt_sched_class = {\n         .next                   = &fair_sched_class,\n         .enqueue_task           = enqueue_task_rt,\n         .dequeue_task           = dequeue_task_rt,\n         .yield_task             = yield_task_rt,\n\n         .check_preempt_curr     = check_preempt_curr_rt,\n\n         .pick_next_task         = pick_next_task_rt,\n         .put_prev_task          = put_prev_task_rt,\n....\n}\n```", "```\nconst struct sched_class dl_sched_class = {\n         .next                   = &rt_sched_class,\n         .enqueue_task           = enqueue_task_dl,\n         .dequeue_task           = dequeue_task_dl,\n         .yield_task             = yield_task_dl,\n\n         .check_preempt_curr     = check_preempt_curr_dl,\n\n         .pick_next_task         = pick_next_task_dl,\n         .put_prev_task          = put_prev_task_dl,\n....\n}\n```", "```\nstruct rq (runqueue) will help us comprehend the concept (elements related to SMP have been omitted from the structure to keep our focus on what's relevant):\n```", "```\n struct rq {\n        /* runqueue lock: */\n        raw_spinlock_t lock;\n   /*\n    * nr_running and cpu_load should be in the same cacheline because\n    * remote CPUs use both these fields when doing load calculation.\n    */\n         unsigned int nr_running;\n    #ifdef CONFIG_NUMA_BALANCING\n         unsigned int nr_numa_running;\n         unsigned int nr_preferred_running;\n    #endif\n         #define CPU_LOAD_IDX_MAX 5\n         unsigned long cpu_load[CPU_LOAD_IDX_MAX];\n #ifdef CONFIG_NO_HZ_COMMON\n #ifdef CONFIG_SMP\n         unsigned long last_load_update_tick;\n #endif /* CONFIG_SMP */\n         unsigned long nohz_flags;\n #endif /* CONFIG_NO_HZ_COMMON */\n #ifdef CONFIG_NO_HZ_FULL\n         unsigned long last_sched_tick;\n #endif\n         /* capture load from *all* tasks on this cpu: */\n         struct load_weight load;\n         unsigned long nr_load_updates;\n         u64 nr_switches;\n\n         struct cfs_rq cfs;\n         struct rt_rq rt;\n         struct dl_rq dl;\n\n #ifdef CONFIG_FAIR_GROUP_SCHED\n         /* list of leaf cfs_rq on this cpu: */\n         struct list_head leaf_cfs_rq_list;\n         struct list_head *tmp_alone_branch;\n #endif /* CONFIG_FAIR_GROUP_SCHED */\n\n          unsigned long nr_uninterruptible;\n\n         struct task_struct *curr, *idle, *stop;\n         unsigned long next_balance;\n         struct mm_struct *prev_mm;\n\n         unsigned int clock_skip_update;\n         u64 clock;\n         u64 clock_task;\n\n         atomic_t nr_iowait;\n\n #ifdef CONFIG_IRQ_TIME_ACCOUNTING\n         u64 prev_irq_time;\n #endif\n #ifdef CONFIG_PARAVIRT\n         u64 prev_steal_time;\n #endif\n #ifdef CONFIG_PARAVIRT_TIME_ACCOUNTING\n         u64 prev_steal_time_rq;\n #endif\n\n         /* calc_load related fields */\n         unsigned long calc_load_update;\n         long calc_load_active;\n\n #ifdef CONFIG_SCHED_HRTICK\n #ifdef CONFIG_SMP\n         int hrtick_csd_pending;\n         struct call_single_data hrtick_csd;\n #endif\n         struct hrtimer hrtick_timer;\n #endif\n ...\n #ifdef CONFIG_CPU_IDLE\n         /* Must be inspected within a rcu lock section */\n         struct cpuidle_state *idle_state;\n #endif\n};\n```", "```\n/*\n * Pick up the highest-prio task:\n */\nstatic inline struct task_struct *\npick_next_task(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)\n{\n   const struct sched_class *class;\n  struct task_struct *p;\n\n      /*\n         * Optimization: we know that if all tasks are in the fair class we can\n    * call that function directly, but only if the @prev task wasn't of a\n        * higher scheduling class, because otherwise those loose the\n      * opportunity to pull in more work from other CPUs.\n       */\n       if (likely((prev->sched_class == &idle_sched_class ||\n                  prev->sched_class == &fair_sched_class) &&\n                rq->nr_running == rq->cfs.h_nr_running)) {\n\n         p = fair_sched_class.pick_next_task(rq, prev, rf);\n                if (unlikely(p == RETRY_TASK))\n                    goto again;\n\n         /* Assumes fair_sched_class->next == idle_sched_class */\n               if (unlikely(!p))\n                 p = idle_sched_class.pick_next_task(rq, prev, rf);\n\n          return p;\n }\n\nagain:\n       for_each_class(class) {\n           p = class->pick_next_task(rq, prev, rf);\n               if (p) {\n                  if (unlikely(p == RETRY_TASK))\n                            goto again;\n                       return p;\n         }\n }\n\n   /* The idle class should always have a runnable task: */\n  BUG();\n}\n```", "```\nstatic const int prio_to_weight[40] = {\n  /* -20 */     88761,     71755,     56483,     46273,     36291,\n  /* -15 */     29154,     23254,     18705,     14949,     11916,\n  /* -10 */      9548,      7620,      6100,      4904,      3906,\n  /*  -5 */      3121,      2501,      1991,      1586,      1277,\n  /*   0 */      1024,       820,       655,       526,       423,\n  /*   5 */       335,       272,       215,       172,       137,\n  /*  10 */       110,        87,        70,        56,        45,\n  /*  15 */        36,        29,        23,        18,        15,\n};\n```", "```\nstruct sched_entity {\n        struct load_weight      load;   /* for load-balancing */\n        struct rb_node          run_node;\n        struct list_head        group_node;\n        unsigned int            on_rq;\n\n        u64                     exec_start;\n        u64                     sum_exec_runtime;\n        u64                     vruntime;\n        u64                     prev_sum_exec_runtime;\n\n        u64                     nr_migrations;\n\n #ifdef CONFIG_SCHEDSTATS\n        struct sched_statistics statistics;\n#endif\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n        int depth;\n        struct sched_entity *parent;\n         /* rq on which this entity is (to be) queued: */\n        struct cfs_rq           *cfs_rq;\n        /* rq \"owned\" by this entity/group: */\n        struct cfs_rq           *my_q;\n#endif\n\n....\n};\n```", "```\n/* task group related information */\nstruct task_group {\n       struct cgroup_subsys_state css;\n\n#ifdef CONFIG_FAIR_GROUP_SCHED\n /* schedulable entities of this group on each cpu */\n      struct sched_entity **se;\n /* runqueue \"owned\" by this group on each cpu */\n  struct cfs_rq **cfs_rq;\n   unsigned long shares;\n\n#ifdef CONFIG_SMP\n        /*\n         * load_avg can be heavily contended at clock tick time, so put\n    * it in its own cacheline separated from the fields above which\n   * will also be accessed at each tick.\n     */\n       atomic_long_t load_avg ____cacheline_aligned;\n#endif\n#endif\n\n#ifdef CONFIG_RT_GROUP_SCHED\n     struct sched_rt_entity **rt_se;\n   struct rt_rq **rt_rq;\n\n       struct rt_bandwidth rt_bandwidth;\n#endif\n\n       struct rcu_head rcu;\n      struct list_head list;\n\n      struct task_group *parent;\n        struct list_head siblings;\n        struct list_head children;\n\n#ifdef CONFIG_SCHED_AUTOGROUP\n       struct autogroup *autogroup;\n#endif\n\n    struct cfs_bandwidth cfs_bandwidth;\n};\n```", "```\nstruct sched_rt_entity {\n struct list_head                run_list;\n unsigned long                   timeout;\n  unsigned long                   watchdog_stamp;\n   unsigned int                    time_slice;\n       unsigned short                  on_rq;\n    unsigned short                  on_list;\n\n    struct sched_rt_entity          *back;\n#ifdef CONFIG_RT_GROUP_SCHED\n  struct sched_rt_entity          *parent;\n  /* rq on which this entity is (to be) queued: */\n  struct rt_rq                    *rt_rq;\n   /* rq \"owned\" by this entity/group: */\n    struct rt_rq                    *my_q;\n#endif\n};\n```", "```\nnice(int inc)\n```", "```\ngetpriority(int which, id_t who)\n```", "```\nsetpriority(int which, id_t who, int prio)\n```", "```\nsched_setscheduler(pid_t pid, int policy, const struct sched_param *param)\n```", "```\nsched_getscheduler(pid_t pid)\n```", "```\nsched_setparam(pid_t pid, const struct sched_param *param)\n```", "```\nsched_getparam(pid_t pid, struct sched_param *param)\n```", "```\nsched_setattr(pid_t pid, struct sched_attr *attr, unsigned int flags)\n```", "```\nsched_getattr(pid_t pid, struct sched_attr *attr, unsigned int size, unsigned int flags)\n```", "```\nsched_get_priority_max(int policy) \nsched_get_priority_min(int policy)\n```", "```\nsched_rr_get_interval(pid_t pid, struct timespec *tp)\n```", "```\nsched_yield(void)\n```", "```\nsched_setaffinity(pid_t pid, size_t cpusetsize, const cpu_set_t *mask)\n```", "```\nsched_getaffinity(pid_t pid, size_t cpusetsize, cpu_set_t *mask)\n```"]
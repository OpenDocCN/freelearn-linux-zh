["```\ntypedef struct {\n        int counter;\n} atomic_t;\n\n#ifdef CONFIG_64BIT\ntypedef struct {\n        long counter;\n} atomic64_t;\n#endif\n```", "```\nDEFINE_SPINLOCK(s_lock);\nspin_lock(&s_lock);\n/* critical region ... */\nspin_unlock(&s_lock);\n```", "```\nstatic __always_inline void spin_lock(spinlock_t *lock)\n{\n        raw_spin_lock(&lock->rlock);\n}\n\n...\n...\n\nstatic __always_inline void spin_unlock(spinlock_t *lock)\n{\n        raw_spin_unlock(&lock->rlock);\n}\n```", "```\n/*\n * Pull the _spin_*()/_read_*()/_write_*() functions/declarations:\n */\n#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)\n# include <linux/spinlock_api_smp.h>\n#else\n# include <linux/spinlock_api_up.h>\n#endif\n```", "```\n/*\n * We build the __lock_function inlines here. They are too large for\n * inlining all over the place, but here is only one user per function\n * which embeds them into the calling _lock_function below.\n *\n * This could be a long-held lock. We both prepare to spin for a long\n * time (making _this_ CPU preemptable if possible), and we also signal\n * towards that other CPU that it should break the lock ASAP.\n */\n\n#define BUILD_LOCK_OPS(op, locktype)                                    \\\nvoid __lockfunc __raw_##op##_lock(locktype##_t *lock)                   \\\n{                                                                       \\\n        for (;;) {                                                      \\\n                preempt_disable();                                      \\\n                if (likely(do_raw_##op##_trylock(lock)))                \\\n                        break;                                          \\\n                preempt_enable();                                       \\\n                                                                        \\\n                if (!(lock)->break_lock)                                \\\n                        (lock)->break_lock = 1;                         \\\n                while (!raw_##op##_can_lock(lock) && (lock)->break_lock)\\\n                        arch_##op##_relax(&lock->raw_lock);             \\\n        }                                                               \\\n        (lock)->break_lock = 0;                                         \\\n} \n```", "```\n/*\n * In the UP-nondebug case there's no real locking going on, so the\n * only thing we have to do is to keep the preempt counts and irq\n * flags straight, to suppress compiler warnings of unused lock\n * variables, and to add the proper checker annotations:\n */\n#define ___LOCK(lock) \\\n  do { __acquire(lock); (void)(lock); } while (0)\n\n#define __LOCK(lock) \\\n  do { preempt_disable(); ___LOCK(lock); } while (0)\n\n#define _raw_spin_lock(lock) __LOCK(lock)\n```", "```\nunsigned long __lockfunc __raw_##op##_lock_irqsave(locktype##_t *lock)  \\\n{                                                                       \\\n        unsigned long flags;                                            \\\n                                                                        \\\n        for (;;) {                                                      \\\n                preempt_disable();                                      \\\n                local_irq_save(flags);                                  \\\n                if (likely(do_raw_##op##_trylock(lock)))                \\\n                        break;                                          \\\n                local_irq_restore(flags);                               \\\n                preempt_enable();                                       \\\n                                                                        \\\n                if (!(lock)->break_lock)                                \\\n                        (lock)->break_lock = 1;                         \\\n                while (!raw_##op##_can_lock(lock) && (lock)->break_lock)\\\n                        arch_##op##_relax(&lock->raw_lock);             \\\n        }                                                               \\\n        (lock)->break_lock = 0;                                         \\\n        return flags;                                                   \\\n} \n```", "```\nvoid __lockfunc __raw_##op##_lock_bh(locktype##_t *lock)                \\\n{                                                                       \\\n        unsigned long flags;                                            \\\n                                                                        \\\n        /* */                                                           \\\n        /* Careful: we must exclude softirqs too, hence the */          \\\n        /* irq-disabling. We use the generic preemption-aware */        \\\n        /* function: */                                                 \\\n        /**/                                                            \\\n        flags = _raw_##op##_lock_irqsave(lock);                         \\\n        local_bh_disable();                                             \\\n        local_irq_restore(flags);                                       \\\n} \n```", "```\ntypedef struct {\n        arch_rwlock_t raw_lock;\n#ifdef CONFIG_GENERIC_LOCKBREAK\n        unsigned int break_lock;\n#endif\n#ifdef CONFIG_DEBUG_SPINLOCK\n        unsigned int magic, owner_cpu;\n        void *owner;\n#endif\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n        struct lockdep_map dep_map;\n#endif\n} rwlock_t;\n```", "```\nread_lock(&v_rwlock);\n/* critical section with read only access to shared data */\nread_unlock(&v_rwlock);\n```", "```\nwrite_lock(&v_rwlock);\n/* critical section for both read and write */\nwrite_unlock(&v_lock);\n```", "```\n struct mutex {\n          atomic_long_t owner;\n          spinlock_t wait_lock;\n #ifdef CONFIG_MUTEX_SPIN_ON_OWNER\n          struct optimistic_spin_queue osq; /* Spinner MCS lock */\n #endif\n          struct list_head wait_list;\n #ifdef CONFIG_DEBUG_MUTEXES\n          void *magic;\n #endif\n #ifdef CONFIG_DEBUG_LOCK_ALLOC\n          struct lockdep_map dep_map;\n #endif\n }; \n```", "```\n/**\n * mutex_lock - acquire the mutex\n * @lock: the mutex to be acquired\n *\n * Lock the mutex exclusively for this task. If the mutex is not\n * available right now, Put caller into Uninterruptible sleep until mutex \n * is available.\n */\n    void mutex_lock(struct mutex *lock);\n\n/**\n * mutex_lock_interruptible - acquire the mutex, interruptible\n * @lock: the mutex to be acquired\n *\n * Lock the mutex like mutex_lock(), and return 0 if the mutex has\n * been acquired else put caller into interruptible sleep until the mutex  \n * until mutex is available. Return -EINTR if a signal arrives while sleeping\n * for the lock.                               \n */\n int __must_check mutex_lock_interruptible(struct mutex *lock); /**\n * mutex_lock_Killable - acquire the mutex, interruptible\n * @lock: the mutex to be acquired\n *\n * Similar to mutex_lock_interruptible(),with a difference that the call\n * returns -EINTR only when fatal KILL signal arrives while sleeping for the     \n * lock.                              \n */\n int __must_check mutex_lock_killable(struct mutex *lock); /**\n * mutex_trylock - try to acquire the mutex, without waiting\n * @lock: the mutex to be acquired\n *\n * Try to acquire the mutex atomically. Returns 1 if the mutex\n * has been acquired successfully, and 0 on contention.\n *\n */\n    int mutex_trylock(struct mutex *lock); /**\n * atomic_dec_and_mutex_lock - return holding mutex if we dec to 0,\n * @cnt: the atomic which we are to dec\n * @lock: the mutex to return holding if we dec to 0\n *\n * return true and hold lock if we dec to 0, return false otherwise. Please \n * note that this function is interruptible.\n */\n    int atomic_dec_and_mutex_lock(atomic_t *cnt, struct mutex *lock); \n/**\n * mutex_is_locked - is the mutex locked\n * @lock: the mutex to be queried\n *\n * Returns 1 if the mutex is locked, 0 if unlocked.\n */\n/**\n * mutex_unlock - release the mutex\n * @lock: the mutex to be released\n *\n * Unlock the mutex owned by caller task.\n *\n */\n void mutex_unlock(struct mutex *lock);\n```", "```\nvoid __sched mutex_lock(struct mutex *lock)\n{\n  might_sleep();\n\n  if (!__mutex_trylock_fast(lock))\n    __mutex_lock_slowpath(lock);\n}\n```", "```\nstatic __always_inline bool __mutex_trylock_fast(struct mutex *lock)\n{\n  unsigned long curr = (unsigned long)current;\n\n  if (!atomic_long_cmpxchg_acquire(&lock->owner, 0UL, curr))\n    return true;\n\n  return false;\n}\n```", "```\nstatic int __sched\n__mutex_lock(struct mutex *lock, long state, unsigned int subclass,\n       struct lockdep_map *nest_lock, unsigned long ip)\n{\n  return __mutex_lock_common(lock, state, subclass, nest_lock, ip, NULL,     false);\n}\n\n...\n...\n...\n\nstatic noinline void __sched __mutex_lock_slowpath(struct mutex *lock) \n{\n        __mutex_lock(lock, TASK_UNINTERRUPTIBLE, 0, NULL, _RET_IP_); \n}\n\nstatic noinline int __sched\n__mutex_lock_killable_slowpath(struct mutex *lock)\n{\n  return __mutex_lock(lock, TASK_KILLABLE, 0, NULL, _RET_IP_);\n}\n\nstatic noinline int __sched\n__mutex_lock_interruptible_slowpath(struct mutex *lock)\n{\n  return __mutex_lock(lock, TASK_INTERRUPTIBLE, 0, NULL, _RET_IP_);\n}\n\n```", "```\nThread T1       Thread T2\n===========    ==========\nlock(bufA);     lock(bufB);\nlock(bufB);     lock(bufA);\n ....            ....\n ....            ....\nunlock(bufB);   unlock(bufA);\nunlock(bufA);   unlock(bufB);\n```", "```\nstruct ww_mutex {\n       struct mutex base;\n       struct ww_acquire_ctx *ctx;\n# ifdef CONFIG_DEBUG_MUTEXES\n       struct ww_class *ww_class;\n#endif\n};\n```", "```\nstatic DEFINE_WW_CLASS(bufclass);\n```", "```\nstruct ww_class {\n       atomic_long_t stamp;\n       struct lock_class_key acquire_key;\n       struct lock_class_key mutex_key;\n       const char *acquire_name;\n       const char *mutex_name;\n};\n```", "```\n/**\n * ww_acquire_init - initialize a w/w acquire context\n * @ctx: w/w acquire context to initialize\n * @ww_class: w/w class of the context\n *\n * Initializes a context to acquire multiple mutexes of the given w/w class.\n *\n * Context-based w/w mutex acquiring can be done in any order whatsoever \n * within a given lock class. Deadlocks will be detected and handled with the\n * wait/wound logic.\n *\n * Mixing of context-based w/w mutex acquiring and single w/w mutex locking \n * can result in undetected deadlocks and is so forbidden. Mixing different\n * contexts for the same w/w class when acquiring mutexes can also result in \n * undetected deadlocks, and is hence also forbidden. Both types of abuse will \n * will be caught by enabling CONFIG_PROVE_LOCKING.\n *\n */\n   void ww_acquire_init(struct ww_acquire_ctx *ctx, struct ww_clas *ww_class);\n```", "```\n/**\n * ww_mutex_lock - acquire the w/w mutex\n * @lock: the mutex to be acquired\n * @ctx: w/w acquire context, or NULL to acquire only a single lock.\n *\n * Lock the w/w mutex exclusively for this task.\n *\n * Deadlocks within a given w/w class of locks are detected and handled with \n * wait/wound algorithm. If the lock isn't immediately available this function\n * will either sleep until it is(wait case) or it selects the current context\n * for backing off by returning -EDEADLK (wound case).Trying to acquire the\n * same lock with the same context twice is also detected and signalled by\n * returning -EALREADY. Returns 0 if the mutex was successfully acquired.\n *\n * In the wound case the caller must release all currently held w/w mutexes  \n * for the given context and then wait for this contending lock to be \n * available by calling ww_mutex_lock_slow. \n *\n * The mutex must later on be released by the same task that\n * acquired it. The task may not exit without first unlocking the mutex.Also,\n * kernel memory where the mutex resides must not be freed with the mutex \n * still locked. The mutex must first be initialized (or statically defined) b\n * before it can be locked. memset()-ing the mutex to 0 is not allowed. The\n * mutex must be of the same w/w lock class as was used to initialize the \n * acquired context.\n * A mutex acquired with this function must be released with ww_mutex_unlock.\n */\n    int ww_mutex_lock(struct ww_mutex *lock, struct ww_acquire_ctx *ctx);\n\n/**\n * ww_mutex_lock_interruptible - acquire the w/w mutex, interruptible\n * @lock: the mutex to be acquired\n * @ctx: w/w acquire context\n *\n */\n   int  ww_mutex_lock_interruptible(struct ww_mutex *lock, \n                                             struct  ww_acquire_ctx *ctx);\n```", "```\n/**\n * ww_acquire_done - marks the end of the acquire phase\n * @ctx: the acquire context\n *\n * Marks the end of the acquire phase, any further w/w mutex lock calls using\n * this context are forbidden.\n *\n * Calling this function is optional, it is just useful to document w/w mutex\n * code and clearly designated the acquire phase from actually using the \n * locked data structures.\n */\n void ww_acquire_done(struct ww_acquire_ctx *ctx);\n```", "```\n/**\n * ww_acquire_fini - releases a w/w acquire context\n * @ctx: the acquire context to free\n *\n * Releases a w/w acquire context. This must be called _after_ all acquired \n * w/w mutexes have been released with ww_mutex_unlock.\n */\n    void ww_acquire_fini(struct ww_acquire_ctx *ctx);\n```", "```\nstruct semaphore {\n        raw_spinlock_t     lock;\n        unsigned int       count;\n        struct list_head   wait_list;\n};\n```", "```\nvoid sema_init(struct semaphore *sem, int val)\n```", "```\n/**\n * down_interruptible - acquire the semaphore unless interrupted\n * @sem: the semaphore to be acquired\n *\n * Attempts to acquire the semaphore.  If no more tasks are allowed to\n * acquire the semaphore, calling this function will put the task to sleep.\n * If the sleep is interrupted by a signal, this function will return -EINTR.\n * If the semaphore is successfully acquired, this function returns 0.\n */\n int down_interruptible(struct semaphore *sem); /**\n * down_killable - acquire the semaphore unless killed\n * @sem: the semaphore to be acquired\n *\n * Attempts to acquire the semaphore.  If no more tasks are allowed to\n * acquire the semaphore, calling this function will put the task to sleep.\n * If the sleep is interrupted by a fatal signal, this function will return\n * -EINTR.  If the semaphore is successfully acquired, this function returns\n * 0.\n */\n int down_killable(struct semaphore *sem); /**\n * down_trylock - try to acquire the semaphore, without waiting\n * @sem: the semaphore to be acquired\n *\n * Try to acquire the semaphore atomically.  Returns 0 if the semaphore has\n * been acquired successfully or 1 if it it cannot be acquired.\n *\n */\n int down_trylock(struct semaphore *sem); /**\n * down_timeout - acquire the semaphore within a specified time\n * @sem: the semaphore to be acquired\n * @timeout: how long to wait before failing\n *\n * Attempts to acquire the semaphore.  If no more tasks are allowed to\n * acquire the semaphore, calling this function will put the task to sleep.\n * If the semaphore is not released within the specified number of jiffies,\n * this function returns -ETIME.  It returns 0 if the semaphore was acquired.\n */\n int down_timeout(struct semaphore *sem, long timeout); /**\n * up - release the semaphore\n * @sem: the semaphore to release\n *\n * Release the semaphore.  Unlike mutexes, up() may be called from any\n * context and even by tasks which have never called down().\n */\n void up(struct semaphore *sem);\n```", "```\nstruct rw_semaphore {\n        atomic_long_t count;\n        struct list_head wait_list;\n        raw_spinlock_t wait_lock;\n#ifdef CONFIG_RWSEM_SPIN_ON_OWNER\n       struct optimistic_spin_queue osq; /* spinner MCS lock */\n       /*\n       * Write owner. Used as a speculative check to see\n       * if the owner is running on the cpu.\n       */\n      struct task_struct *owner;\n#endif\n#ifdef CONFIG_DEBUG_LOCK_ALLOC\n     struct lockdep_map dep_map;\n#endif\n};\n```", "```\n/* reader interfaces */\n   void down_read(struct rw_semaphore *sem);\n   void up_read(struct rw_semaphore *sem);\n/* trylock for reading -- returns 1 if successful, 0 if contention */\n   int down_read_trylock(struct rw_semaphore *sem);\n   void up_read(struct rw_semaphore *sem);\n\n/* writer Interfaces */\n   void down_write(struct rw_semaphore *sem);\n   int __must_check down_write_killable(struct rw_semaphore *sem);\n\n/* trylock for writing -- returns 1 if successful, 0 if contention */\n   int down_write_trylock(struct rw_semaphore *sem); \n   void up_write(struct rw_semaphore *sem);\n/* downgrade write lock to read lock */\n   void downgrade_write(struct rw_semaphore *sem); \n\n/* check if rw-sem is currently locked */  \n   int rwsem_is_locked(struct rw_semaphore *sem);\n\n```", "```\ntypedef struct {\n        struct seqcount seqcount;\n        spinlock_t lock;\n} seqlock_t;\n```", "```\n#define DEFINE_SEQLOCK(x) \\\n               seqlock_t x = __SEQLOCK_UNLOCKED(x)\n```", "```\n#define __SEQLOCK_UNLOCKED(lockname)                 \\\n       {                                               \\\n               .seqcount = SEQCNT_ZERO(lockname),     \\\n               .lock = __SPIN_LOCK_UNLOCKED(lockname)   \\\n       }\n```", "```\n  #define seqlock_init(x)                                     \\\n       do {                                                   \\\n               seqcount_init(&(x)->seqcount);                 \\\n               spin_lock_init(&(x)->lock);                    \\\n       } while (0)\n```", "```\nstatic inline void write_seqlock(seqlock_t *sl)\n{\n        spin_lock(&sl->lock);\n        write_seqcount_begin(&sl->seqcount);\n}\n\nstatic inline void write_sequnlock(seqlock_t *sl)\n{\n        write_seqcount_end(&sl->seqcount);\n        spin_unlock(&sl->lock);\n}\n\nstatic inline void write_seqlock_bh(seqlock_t *sl)\n{\n        spin_lock_bh(&sl->lock);\n        write_seqcount_begin(&sl->seqcount);\n}\n\nstatic inline void write_sequnlock_bh(seqlock_t *sl)\n{\n        write_seqcount_end(&sl->seqcount);\n        spin_unlock_bh(&sl->lock);\n}\n\nstatic inline void write_seqlock_irq(seqlock_t *sl)\n{\n        spin_lock_irq(&sl->lock);\n        write_seqcount_begin(&sl->seqcount);\n}\n\nstatic inline void write_sequnlock_irq(seqlock_t *sl)\n{\n        write_seqcount_end(&sl->seqcount);\n        spin_unlock_irq(&sl->lock);\n}\n\nstatic inline unsigned long __write_seqlock_irqsave(seqlock_t *sl)\n{\n        unsigned long flags;\n\n        spin_lock_irqsave(&sl->lock, flags);\n        write_seqcount_begin(&sl->seqcount);\n        return flags;\n}\n```", "```\nstatic inline unsigned read_seqbegin(const seqlock_t *sl)\n{\n        return read_seqcount_begin(&sl->seqcount);\n}\n\nstatic inline unsigned read_seqretry(const seqlock_t *sl, unsigned start)\n{\n        return read_seqcount_retry(&sl->seqcount, start);\n}\n```", "```\nstruct completion {\n        unsigned int done;\n        wait_queue_head_t wait;\n};\n```", "```\n#define DECLARE_COMPLETION(work) \\\n       struct completion work = COMPLETION_INITIALIZER(work)\n```", "```\nstatic inline void init_completion(struct completion *x)\n{\n        x->done = 0;\n        init_waitqueue_head(&x->wait);\n}\n```", "```\nstatic inline void reinit_completion(struct completion *x)\n{\n        x->done = 0;\n}\n```", "```\nextern void wait_for_completion_io(struct completion *);\nextern int wait_for_completion_interruptible(struct completion *x);\nextern int wait_for_completion_killable(struct completion *x);\nextern unsigned long wait_for_completion_timeout(struct completion *x,\n                                                   unsigned long timeout);\nextern unsigned long wait_for_completion_io_timeout(struct completion *x,\n                                                    unsigned long timeout);\nextern long wait_for_completion_interruptible_timeout(\n        struct completion *x, unsigned long timeout);\nextern long wait_for_completion_killable_timeout(\n        struct completion *x, unsigned long timeout);\nextern bool try_wait_for_completion(struct completion *x);\nextern bool completion_done(struct completion *x);\n\nextern void complete(struct completion *);\nextern void complete_all(struct completion *);\n```", "```\nvoid complete(struct completion *x)\n{\n        unsigned long flags;\n\n        spin_lock_irqsave(&x->wait.lock, flags);\n        if (x->done != UINT_MAX)\n                x->done++;\n        __wake_up_locked(&x->wait, TASK_NORMAL, 1);\n        spin_unlock_irqrestore(&x->wait.lock, flags);\n}\nEXPORT_SYMBOL(complete);\nvoid complete_all(struct completion *x)\n{\n        unsigned long flags;\n\n        spin_lock_irqsave(&x->wait.lock, flags);\n        x->done = UINT_MAX;\n        __wake_up_locked(&x->wait, TASK_NORMAL, 0);\n        spin_unlock_irqrestore(&x->wait.lock, flags);\n}\nEXPORT_SYMBOL(complete_all);\n```"]
- en: Analyzing Performance in a Gluster System
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析Gluster系统的性能
- en: In the [Chapter 4](51a50d0f-7932-431e-ba32-c5d62d29b71e.xhtml), *Using GlusterFS
    on the Cloud Infrastructure*, we have completed a working implementation of GlusterFS,
    we can focus on the testing aspect of the solution. We will look at a high-level
    overview of what was deployed and explain the reasoning behind the chosen components.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](51a50d0f-7932-431e-ba32-c5d62d29b71e.xhtml)中，*在云基础设施上使用GlusterFS*，我们已经完成了GlusterFS的工作实施，现在可以专注于解决方案的测试方面。我们将概述部署的情况，并解释选择组件背后的原因。
- en: Once the configuration is defined, we can go through testing the performance
    to verify that we are achieving the expected results. We can then conduct availability
    testing by deliberately bringing down nodes while performing I/O.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦配置被定义，我们可以通过测试性能来验证我们是否达到了预期的结果。然后，我们可以在执行I/O时故意关闭节点来进行可用性测试。
- en: Finally, we will see how we can scale the solution both vertically and horizontally.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将看到如何在垂直和水平两个方向上扩展解决方案。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: A high-level overview of the implementation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施的高级概述
- en: Going through Performance testing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行性能测试
- en: Performance availability testing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能可用性测试
- en: Scaling the solution vertically and horizontally
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在垂直和水平两个方向上扩展解决方案
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Here''s the list of technical resources for this chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的技术资源列表如下：
- en: Zpool iostat—used for performance monitoring on ZFS: [https://docs.oracle.com/cd/E19253-01/819-5461/gammt/index.html](https://docs.oracle.com/cd/E19253-01/819-5461/gammt/index.html)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zpool iostat-用于在ZFS上进行性能监控：[https://docs.oracle.com/cd/E19253-01/819-5461/gammt/index.html](https://docs.oracle.com/cd/E19253-01/819-5461/gammt/index.html)
- en: Sysstat—used for live block performance statistics: [https://github.com/sysstat/sysstat](https://github.com/sysstat/sysstat)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sysstat-用于实时块性能统计：[https://github.com/sysstat/sysstat](https://github.com/sysstat/sysstat)
- en: The iostat man page containing the different options for the command: [http://sebastien.godard.pagesperso-orange.fr/man_iostat.html](http://sebastien.godard.pagesperso-orange.fr/man_iostat.html)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: iostat man页面包含命令的不同选项：[http://sebastien.godard.pagesperso-orange.fr/man_iostat.html](http://sebastien.godard.pagesperso-orange.fr/man_iostat.html)
- en: FIO documentation to provide configuration parameters and usage: [https://media.readthedocs.org/pdf/fio/latest/fio.pdf](https://media.readthedocs.org/pdf/fio/latest/fio.pdf)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FIO文档提供配置参数和用法：[https://media.readthedocs.org/pdf/fio/latest/fio.pdf](https://media.readthedocs.org/pdf/fio/latest/fio.pdf)
- en: GlusterFS monitoring workload documentation on how to view statistics: [https://gluster.readthedocs.io/en/latest/Administrator%20Guide/Monitoring%20Workload/](https://gluster.readthedocs.io/en/latest/Administrator%20Guide/Monitoring%20Workload/)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GlusterFS监控工作负载文档，说明如何查看统计信息：[https://gluster.readthedocs.io/en/latest/Administrator%20Guide/Monitoring%20Workload/](https://gluster.readthedocs.io/en/latest/Administrator%20Guide/Monitoring%20Workload/)
- en: An overview of the implementation
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施概述
- en: After having the solution deployed and configured in [Chapter 4](51a50d0f-7932-431e-ba32-c5d62d29b71e.xhtml),
    *Using GlusterFS on the Cloud Infrastructure*, we can validate the performance
    of the implementation. The primary goal is to understand how this can be done
    and the tools that are available.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](51a50d0f-7932-431e-ba32-c5d62d29b71e.xhtml)中部署和配置了解决方案后，*在云基础设施上使用GlusterFS*，我们可以验证实施的性能。主要目标是了解如何实现以及可用的工具。
- en: Let's first take a step back and see what we implemented.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先退一步，看看我们实施了什么。
- en: An overview of the cluster
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群的概述
- en: 'In [Chapter 4](51a50d0f-7932-431e-ba32-c5d62d29b71e.xhtml), *Using GlusterFS
    on the Cloud Infrastructure*, we deployed GlusterFS version 4.1 on an Azure **virtual
    machine** (**VM**). We used ZFS as the storage backend for the bricks by using
    four disks per node on a three-node setup. The following diagram offers a high-level
    overview of how this is distributed:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](51a50d0f-7932-431e-ba32-c5d62d29b71e.xhtml)中，*在云基础设施上使用GlusterFS*，我们在Azure的**虚拟机**（**VM**）上部署了GlusterFS
    4.1版本。我们使用ZFS作为砖块的存储后端，通过在三节点设置中每个节点使用四个磁盘。以下图表提供了这个分布的高级概述：
- en: '![](img/3ac944da-0715-4ecc-a291-82eb52a1d9bb.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3ac944da-0715-4ecc-a291-82eb52a1d9bb.png)'
- en: This setup gives 1 TB of usable space. The volume can tolerate an entire node
    going down while still serving data to the clients.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设置提供了1TB的可用空间。卷可以容忍整个节点宕机，同时仍然向客户端提供数据。
- en: This setup should be able to deliver approximately 375 **megabytes per second**
    (**MB/s**), handle several hundred clients at once, and should be reasonably straightforward
    to scale both horizontally and vertically.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设置应该能够提供大约375 **兆字节每秒**（**MB/s**），同时处理数百个客户端，并且应该相当容易在垂直和水平两个方向上扩展。
- en: Performance testing
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能测试
- en: We now need to validate that the theoretical performance can be achieved through
    actual implementation. Let's break this down into several parts.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要验证理论性能是否可以通过实际实施来实现。让我们将这个分解成几个部分。
- en: Performance theory
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能理论
- en: Let's figure out how much performance we should be getting based on the specifications
    of the setup. Consider that each of the nodes should provide a maximum of 125
    MB/s. The disk subsystem is more than capable of delivering performance since
    each disk yields 60 MB/s.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们根据设置的规格来确定应该获得多少性能。考虑到每个节点应该提供最大125MB/s。磁盘子系统完全能够提供性能，因为每个磁盘产生60MB/s。
- en: The total achievable performance should be around 375 MB/s, assuming that the
    client or clients can keep up by sending or requesting enough data to the volume.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 总体可实现的性能应该在375MB/s左右，假设客户端可以通过发送或请求足够的数据到卷来跟上。
- en: Performance tools
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能工具
- en: 'We''ll be using three main tools to validate and test the performance of the
    solution:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用三种主要工具来验证和测试解决方案的性能：
- en: '`zpool iostat`'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zpool iostat`'
- en: '`iostat`'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`iostat`'
- en: '**Flexible I/O tester** (**FIO**)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**灵活I/O测试工具**（**FIO**）'
- en: Each of these tools works at a different level. Let's now detail what each one
    does and how to understand the information that they provide.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具每个都在不同的级别上工作。现在让我们详细说明每个工具的作用以及如何理解它们提供的信息。
- en: The ZFS zpool iostat command
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ZFS zpool iostat命令
- en: ZFS works on the backend volume level; the `zpool iostat -v` command gives performance
    statistics for each of the members in the ZFS volume and statistics for the ZFS
    volume as a whole.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ZFS在后端卷级别上工作；`zpool iostat -v`命令为ZFS卷中的每个成员提供性能统计信息，并为整个ZFS卷提供统计信息。
- en: The command can provide real-time data by passing a number in seconds that it
    will iterate after that period of time has passed. For example, `zpool iostat
    -v 1` reports disk statistics each second. Here, the `-v` option shows each of
    the members of the pool and their respective data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令可以通过传递秒数来提供实时数据，它将在经过该时间段后迭代。例如，`zpool iostat -v 1`每秒报告磁盘统计信息。这里，`-v`选项显示池的每个成员及其各自的数据。
- en: 'This tool helps to present the performance at the lowest level possible because
    it shows data from each of the disks, from each of the nodes:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 该工具有助于以尽可能低的级别呈现性能，因为它显示了来自每个磁盘、每个节点的数据：
- en: '![](img/c22ef836-7633-4f8f-acdb-e7f1108b756c.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c22ef836-7633-4f8f-acdb-e7f1108b756c.png)'
- en: Note that we used the extra `-L` and `-P` options so that the absolute paths
    of the device's files or the **Universally Unique Identifier** (**UUID**) are
    printed; this is because we created the pool using the unique identifier of each
    of the disks.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用了额外的`-L`和`-P`选项，以便打印设备文件的绝对路径或**通用唯一标识符**（**UUID**）；这是因为我们使用每个磁盘的唯一标识符创建了池。
- en: 'From the preceding screenshot, we can see four main groups, as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的屏幕截图中，我们可以看到四个主要组，如下：
- en: '`pool`: This is created with each of the members.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pool`: 这是使用每个成员创建的。'
- en: '`capacity`: This is the amount of space that is allocated to each device.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`capacity`: 这是分配给每个设备的空间量。'
- en: '`operations`: This is the number of IOPSes that are done on each of the devices.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`operations`: 这是在每个设备上执行的IOPS数量。'
- en: '`bandwidth`: This is the throughput of each device.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bandwidth`: 这是每个设备的吞吐量。'
- en: In the first line, the command prints the statistics since the last boot. Remember
    that this tool helps to present the performance from a ZFS-pool level.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一行中，该命令打印自上次启动以来的统计信息。请记住，该工具有助于从ZFS池级别呈现性能。
- en: iostat
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: iostat
- en: As part of the `sysstat` package, `iostat` provides low-level performance statistics
    from each of the devices. `iostat` bypasses filesystems and volumes and presents
    the RAW performance data from each of the block devices in the system.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 作为`sysstat`软件包的一部分，`iostat`提供了来自每个设备的低级性能统计数据。`iostat`绕过文件系统和卷，并呈现系统中每个块设备的原始性能数据。
- en: 'The `iostat` tool can be run with options to alter the information that is
    printed onscreen, for example, `iostat -dxctm 1`. Let''s explore what each part
    does:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`iostat`工具可以使用选项运行，以更改在屏幕上打印的信息，例如，`iostat -dxctm 1`。让我们探讨每个部分的作用：'
- en: '`iostat`: This is the primary command.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`iostat`: 这是主要命令。'
- en: '`d`: This prints the device utilization.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`d`: 这打印设备利用率。'
- en: '`x`: This displays the extended device statistics.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x`: 这显示扩展设备统计信息。'
- en: '`c`: This displays the CPU utilization.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`c`: 这显示了CPU利用率。'
- en: '`t`: This displays the time for each report printed.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`t`: 这显示每个报告打印的时间。'
- en: '`m`: This ensures that the statistics will be displayed in MB/s.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`m`: 这确保统计数据以MB/s显示。'
- en: '`1`: This is the amount of time in seconds in which `iostat` prints data.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1`: 这是`iostat`打印数据的时间，以秒为单位。'
- en: 'In the following screenshot, you can see that `iostat` displays information
    in different columns:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的屏幕截图中，您可以看到`iostat`以不同的列显示信息：
- en: '![](img/ecf9dba7-7455-4f09-8839-5f6db3e3cf6c.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ecf9dba7-7455-4f09-8839-5f6db3e3cf6c.png)'
- en: 'There''s no need to go through all of the columns, but the most important ones
    are as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 没有必要浏览所有列，但最重要的列如下：
- en: '`Device`: This shows the block devices that are present on the system.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Device`: 这显示系统上存在的块设备。'
- en: '`r/s`: These are the read operations per second.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`r/s`: 这些是每秒的读取操作。'
- en: '`w/s`: These are the write operations per second.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`w/s`: 这些是每秒的写入操作。'
- en: '`rMB/s`: These are the MB/s read from the device.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rMB/s`: 这些是从设备读取的MB/s。'
- en: '`wMB/s`: These are the MB/s written to the device.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wMB/s`: 这些是写入设备的MB/s。'
- en: '`r_await`: This is the average time in milliseconds for read requests.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`r_await`: 这是读取请求的平均时间（毫秒）。'
- en: '`w_await`: This is the average time in milliseconds for write requests.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`w_await`: 这是写请求的平均时间（毫秒）。'
- en: The `r_await` and `w_await` columns in conjunction with the `avg-cpu %iowait`
    time are essential; this is because these metrics can help determine whether one
    of the devices has increased latency over the others. A high CPU `iowait` time
    means that the CPU is continuously waiting for I/O to complete, which, in turn,
    might mean that the block devices have high latency.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`r_await`和`w_await`列与`avg-cpu %iowait`时间一起是必不可少的；这是因为这些指标可以帮助确定是否有一个设备的延迟增加超过其他设备。高CPU
    `iowait`时间意味着CPU不断等待I/O完成，这反过来可能意味着块设备具有较高的延迟。'
- en: The `iostat` tool can be run on each of the nodes in the cluster, providing
    low-level statistics for each of the disks that make up the GlusterFS volume.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`iostat`工具可以在集群中的每个节点上运行，为组成GlusterFS卷的每个磁盘提供低级别统计信息。'
- en: Details on the rest of the columns can be found on the man page for `iostat`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 有关其他列的详细信息可以在`iostat`的man页面上找到。
- en: The FIO tester
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FIO测试员
- en: FIO is a benchmarking tool that is used to conduct performance testing by generating
    synthetic workloads and presenting a summary of the I/O metrics.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: FIO是一个基准测试工具，用于通过生成合成工作负载进行性能测试，并提供I/O指标摘要。
- en: Note that `fio` does not come by default on CentOS, but it is available in the
    base repository and can be installed by running `sudo yum install -y fio`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`fio`在CentOS上不是默认安装的，但它可以在基本存储库中找到，并且可以通过运行`sudo yum install -y fio`来安装。
- en: This tool is exceptionally helpful as it allows us to perform tests that are
    close to what the real workload of the system will be—by allowing the user to
    change parameters such as the block size, file size, and thread count. FIO can
    deliver data that is close to real-world performance. This level of customization
    can be potentially confusing as it provides many options for workload simulation,
    and some of these are not very intuitive at first.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个工具非常有用，因为它允许我们执行接近系统实际工作负载的测试——通过允许用户更改块大小、文件大小和线程数量等参数。FIO可以提供接近真实性能的数据。这种定制水平可能会让人感到困惑，因为它提供了许多工作负载模拟选项，一些选项一开始并不直观。
- en: 'The easiest way to perform testing with FIO is by creating a configuration
    file, which tells the software how to behave; a configuration file looks like
    this:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 使用FIO进行测试的最简单方法是创建一个配置文件，告诉软件如何行为；配置文件看起来像这样：
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s break it down so that we can understand how each part of the configuration
    file works:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下，以便了解配置文件的每个部分是如何工作的：
- en: '`[global]`: This denotes the configuration parameters that affect the entire
    test (parameters for individual files can be set).'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[global]`：这表示影响整个测试的配置参数（可以设置单个文件的参数）。'
- en: '`name=`: This is the name of the test; it can be anything meaningful.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name=`：这是测试的名称；可以是任何有意义的名称。'
- en: '`rw=randrw`: This tells FIO what type of I/O to perform; in this case, it does
    random reads and writes.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rw=randrw`：这告诉FIO要执行什么类型的I/O；在这种情况下，它执行随机读写。'
- en: '`rwmixread` and `rwmixwrite`: These tell FIO what percentage or mix of reads
    and writes to perform—in this case, it is a 50-50 mix.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rwmixread`和`rwmixwrite`：这告诉FIO执行读写的百分比或混合比例——在这种情况下，是50-50的混合比例。'
- en: '`group_reporting=1`: This is used to give statistics for the entire test rather
    than for each of the jobs.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`group_reporting=1`：这用于为整个测试提供统计信息，而不是为每个作业提供统计信息。'
- en: '`bs=1M`: This is the block size that FIO uses when performing the test; it
    can be changed to a value that mimics the workload intended.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bs=1M`：这是FIO在执行测试时使用的块大小；可以将其更改为模拟预期工作负载的值。'
- en: '`numjobs=4`: This controls how many threads are opened per file. Ideally, this
    can be used to match the number of users or threads that will be using the storage.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numjobs=4`：这控制每个文件打开多少个线程。理想情况下，可以用来匹配将使用存储的用户或线程的数量。'
- en: '`runtime=180`: This controls, in seconds, how long the test will run for.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`runtime=180`：这控制测试运行的时间，以秒为单位。'
- en: '`ioengine=libaio`: This controls the type of I/O engine to be used. The most
    common is `libaio` as it resembles most workloads.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ioengine=libaio`：这控制要使用的I/O引擎类型。最常见的是`libaio`，因为它最接近大多数工作负载。'
- en: '`iodepth=64`: This controls the I/O depth of the test; a higher number allows
    the storage device to be used at its fullest.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`iodepth=64`：这控制测试的I/O深度；较高的数字允许存储设备充分利用。'
- en: Finally, the file group controls how many files are created for the test and
    what their size will be. Certain settings, such as `iodepth`, can be added to
    this group that only affect the file where the parameter is defined. Another consideration
    is that `fio` opens a thread based on the `numjobs` parameter for each of the
    files. In the preceding configuration, it will open a total of 16 threads.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，文件组控制创建测试文件的数量和它们的大小。可以向该组添加某些设置，例如`iodepth`，这些设置仅影响定义了该参数的文件。另一个考虑因素是，`fio`根据`numjobs`参数为每个文件打开一个线程。在前面的配置中，它将打开总共16个线程。
- en: 'To run FIO, simply move into the directory where the mount point is located
    and point it to the configuration file, as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行FIO，只需进入挂载点所在的目录，并将其指向配置文件，如下所示：
- en: '[PRE1]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note that FIO requires root privileges, so make sure that FIO is run with `sudo`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，FIO需要root权限，因此请确保使用`sudo`来运行FIO。
- en: 'While FIO is running, it displays statistics such as throughput and IOPS:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在FIO运行时，它会显示吞吐量和IOPS等统计信息：
- en: '![](img/de6fd649-943a-4f3e-809d-96770f8fa49a.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de6fd649-943a-4f3e-809d-96770f8fa49a.png)'
- en: 'Once done, FIO reports the test statistics on screen. The main things to look
    for are the IOPS and **bandwidth** (**BW**) for both read and write operations:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，FIO会在屏幕上报告测试统计信息。要查找的主要内容是读写操作的IOPS和带宽。
- en: '![](img/ab8a1e8b-1fb9-4905-9803-8e2156cf7651.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ab8a1e8b-1fb9-4905-9803-8e2156cf7651.png)'
- en: From the test results, we can see that the GlusterFS volume can sustain about
    150 MB/s of both read and write operations simultaneously. We're off by 75 MB/s
    from the theoretical maximum performance of the cluster; in this specific case,
    we're hitting a network limit.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从测试结果中，我们可以看到GlusterFS卷可以同时维持约150MB/s的读写操作。我们与集群的理论最大性能相差75MB/s；在这种特定情况下，我们达到了网络限制。
- en: FIO can be extremely effective at validating performance and detecting problems; `fio`
    can be run on clients mounting the Gluster volume or directly on the bricks of
    each of the nodes. You can use FIO for testing existing solutions in order to
    validate performance needs; just make sure the settings in the FIO configuration
    are changed based on what needs to be tested.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: FIO在验证性能和检测问题方面非常有效；`fio`可以在挂载Gluster卷的客户端上运行，也可以直接在每个节点的存储单元上运行。您可以使用FIO来测试现有解决方案，以验证性能需求；只需确保根据需要测试的内容更改FIO配置中的设置。
- en: GlusterFS provides some tools to monitor performance from the perspective of
    volume. These can be found in the GlusterFS documentation page, under *Monitoring
    Workload*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: GlusterFS提供了一些工具，可以从卷的角度监视性能。这些工具可以在GlusterFS的文档页面上找到，位于*监视工作负载*下。
- en: Availability testing
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可用性测试
- en: Making sure that the cluster is able to tolerate a node going down is crucial
    because we can confirm that no downtime occurs if a node is lost.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 确保集群能够容忍节点宕机是至关重要的，因为我们可以确认如果节点丢失，不会发生停机。
- en: This can be done by forcibly shutting down one of the nodes while the others
    continue to serve data. To function as a synthetic workload, we can use FIO to
    perform a continuous test while one of the nodes is being shut down.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过强制关闭其中一个节点来完成，而其他节点继续提供数据。为了作为合成工作负载，我们可以使用FIO在关闭其中一个节点时执行连续测试。
- en: 'In the following screenshot, we can see that the `gfs2` node was not present,
    but the FIO test continued serving data as expected:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的屏幕截图中，我们可以看到`gfs2`节点不存在，但FIO测试继续按预期提供数据：
- en: '![](img/ac25b2de-f97a-4d29-bd05-06fdf2b45162.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ac25b2de-f97a-4d29-bd05-06fdf2b45162.png)'
- en: Scaling
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展
- en: Scaling this setup is relatively straightforward. As previously mentioned, we
    can either scale vertically, by adding more disks to each of the nodes, or scale
    horizontally, by adding more nodes to the cluster.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设置的扩展相对简单。如前所述，我们可以通过在每个节点上添加更多磁盘来纵向扩展，或者通过在集群中添加更多节点来横向扩展。
- en: Scaling vertically is considerably simpler than horizontally as it requires
    fewer resources. For example, a single disk can be added to the ZFS pool on each
    of the nodes—effectively increasing the available space by 256 GB if three 128
    GB disks are added.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 纵向扩展比横向扩展要简单得多，因为它需要更少的资源。例如，如果在每个节点上将单个磁盘添加到ZFS池中，则可以将可用空间增加256 GB，如果添加了三个128
    GB磁盘。
- en: 'Adding disks to the ZFS pool can be done with the following command:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令将磁盘添加到ZFS池中：
- en: '[PRE2]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: From the previous command, `brick1` is the name of the pool and `disk-id` is
    the UUID of the recently added disk or disks.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 从前一个命令中，`brick1`是池的名称，`disk-id`是最近添加的磁盘的UUID。
- en: Scaling horizontally requires the exact setup to be mirrored on a new node and
    then added to the cluster. This requires a new set of disks. The advantage is
    that the available space and performance will grow accordingly.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 横向扩展需要在新节点上镜像完全相同的设置，然后将其添加到集群中。这需要一组新的磁盘。优点是可用空间和性能将相应增长。
- en: Summary
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at an overview of the implementation done in the
    previous [Chapter 4](51a50d0f-7932-431e-ba32-c5d62d29b71e.xhtml), *Using GlusterFS
    on the Cloud Infrastructure*, so that we could have a fresh understanding of what
    was implemented in order to understand how we could test performance. Given the
    previous setup, the implementation should be capable of a theoretical 375 MB/s
    of throughput. We can validate this number with several tools that work at different
    levels.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们概述了前一章[第4章](51a50d0f-7932-431e-ba32-c5d62d29b71e.xhtml)中完成的实施，即*在云基础架构上使用GlusterFS*，以便我们可以对实施的内容有一个新的理解，以便了解我们如何测试性能。根据先前的设置，实施应该能够实现理论上的375
    MB/s吞吐量。我们可以使用几个在不同级别工作的工具来验证这个数字。
- en: For ZFS volumes, we can use the `zpool iostat` command, which provides data
    for each of the block devices that are part of the ZFS volume. `iostat` can be
    used to determine performance for all of the block devices present in the system.
    These commands can only be run on each of the nodes of the cluster. To be able
    to verify the actual performance of the implementation, we used the FIO tool,
    which can simulate specific workloads by changing the parameters of how I/O is
    performed. This tool can be used on each of the nodes on the brick level or on
    each of the Gluster clients on the GlusterFS volume to get a general overview
    of the performance that is achievable by the cluster.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于ZFS卷，我们可以使用`zpool iostat`命令，该命令提供ZFS卷的每个块设备的数据。`iostat`可用于确定系统中存在的所有块设备的性能。这些命令只能在集群的每个节点上运行。为了验证实现的实际性能，我们使用了FIO工具，该工具可以通过更改I/O执行方式的参数来模拟特定的工作负载。该工具可用于每个节点的砖级别，或者在GlusterFS卷上的每个Gluster客户端上，以获得集群可实现的性能的概述。
- en: We went through how we can perform availability testing by purposely shutting
    down one of the nodes while performing a test through FIO. Finally, scaling the
    solution can be done either vertically, by adding disks to each of the volumes
    in each of the nodes, or horizontally, by adding an entirely new node to the cluster. Your
    main takeaway from this chapter is to consider how the configuration that was
    implemented can be validated using widely available tools. These are just a set
    of tools. Many other tools might be available, which could be better for the solution
    that you're implementing.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了如何通过有意关闭其中一个节点并通过FIO执行测试来执行可用性测试。最后，可以通过纵向扩展（在每个节点的每个卷中添加磁盘）或横向扩展（在集群中添加一个全新节点）来扩展解决方案。您从本章的主要收获是考虑如何使用广泛可用的工具来验证已实施的配置。这只是一组工具。可能还有许多其他工具可用，这些工具可能更适合您正在实施的解决方案。
- en: In the next chapter, we'll jump into creating a highly-available self-healing
    architecture.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍创建高可用性自愈架构。
- en: Questions
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is MB/s?
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MB/s是什么？
- en: What is `zpool iostat`?
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`zpool iostat`是什么？'
- en: Where can I run `zpool iostat`?
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我在哪里可以运行`zpool iostat`？
- en: What is `iostat`?
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`iostat`是什么？'
- en: What does `r_await` mean?
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`r_await`是什么意思？'
- en: What is CPU IOWAIT time?
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CPU IOWAIT时间是什么？
- en: What is FIO?
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: FIO是什么？
- en: How can I run FIO?
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我怎样才能运行FIO？
- en: What is an FIO configuration file?
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: FIO配置文件是什么？
- en: How can I validate availability in a Gluster cluster?
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我怎样才能在Gluster集群中验证可用性？
- en: How can I scale vertically?
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我怎样才能纵向扩展？
- en: Further reading
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Learning Microsoft Azure Storage* by Mohamed Waly: [https://www.packtpub.com/big-data-and-business-intelligence/learning-microsoft-azure-storage](https://www.packtpub.com/big-data-and-business-intelligence/learning-microsoft-azure-storage)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '* Mohamed Waly的《学习Microsoft Azure存储》：[https://www.packtpub.com/big-data-and-business-intelligence/learning-microsoft-azure-storage](https://www.packtpub.com/big-data-and-business-intelligence/learning-microsoft-azure-storage)'

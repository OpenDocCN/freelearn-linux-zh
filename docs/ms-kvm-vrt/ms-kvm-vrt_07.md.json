["```\nyum -y install stratisd stratis-cli\nsystemctl enable --now stratisd\n```", "```\nmdadm --create /dev/md0 --verbose --level=10 --raid-devices=4 /dev/sdb /dev/sdc /dev/sdd /dev/sde --spare-devices=1 /dev/sdf2\nstratis pool create PacktStratisPool01 /dev/md0\nstratis pool add-cache PacktStratisPool01 /dev/sdg\nstratis pool add-cache PacktStratisPool01 /dev/sdg\nstratis fs create PackStratisPool01 PacktStratisXFS01\nmkdir /mnt/packtStratisXFS01\nmount /stratis/PacktStratisPool01/PacktStratisXFS01 /mnt/packtStratisXFS01\n```", "```\nyum -y install nfs-utils\nsystemctl enable --now nfs-server\n```", "```\n/mnt/packtStratisXFS01\t192.168.159.0/24(rw)\nexportfs -r\n```", "```\n/mnt/packtStratisXFS01\t192.168.159.0/24(rw,sync,root_squash)\n```", "```\n/mnt/packtStratisXFS01 192.168.159.0/255.255.255.0(rw,root_squash)\n```", "```\nyum -y install targetcli\nsystemctl enable --now target\n```", "```\nfirewall-cmd --permanent --add-port=3260/tcp ; firewall-cmd --reload\n```", "```\n    yum -y install iscsi-initiator-utils\n    ```", "```\n    InitiatorName=iqn.2019-12.com.packt:PacktStratis01\n    ```", "```\n<pool type='iscsi'>\n\u00a0\u00a0<name>MyiSCSIPool</name>\n\u00a0\u00a0<source>\n\u00a0\u00a0\u00a0\u00a0<host name='192.168.159.145'/>\n\u00a0\u00a0\u00a0\u00a0<device path='iqn.2003-01.org.linux-iscsi.packtiscsi01.x8664:sn.7b3c2efdbb11'/>\n\u00a0\u00a0</source>\n\u00a0\u00a0<initiator>\n\u00a0\u00a0\u00a0<iqn name='iqn.2019-12.com.packt:PacktStratis01' />\n</initiator>\n\u00a0\u00a0<target>\n\u00a0\u00a0\u00a0\u00a0<path>/dev/disk/by-path</path>\n\u00a0\u00a0</target>\n</pool>\n```", "```\nvirsh pool-define --file iSCSIPool.xml\nvirsh pool-start --pool MyiSCSIPool\nvirsh pool-autostart --pool MyiSCSIPool\n```", "```\nvirsh vol-list --pool MyiSCSIPool\n```", "```\nyum -y install epel-release*\nyum -y install centos-release-gluster7.noarch\nyum -y update\nyum -y install glusterfs-server\nsystemctl reboot\n```", "```\nmkfs.xfs /dev/sdb\nmkdir /gluster/bricks/1 -p\necho '/dev/sdb /gluster/bricks/1 xfs defaults 0 0' >> /etc/fstab\nmount -a\nmkdir /gluster/bricks/1/brick\nsystemctl disable firewalld\nsystemctl stop firewalld\nsystemctl start glusterd\nsystemctl enable glusterd\n```", "```\n192.168.159.147 gluster1\n192.168.159.148 gluster2\n192.168.159.149 gluster3\n```", "```\ngluster peer probe gluster1\ngluster peer probe gluster2\ngluster peer probe gluster3\ngluster peer status\n```", "```\ngluster volume create kvmgluster replica 3 \\ gluster1:/gluster/bricks/1/brick gluster2:/gluster/bricks/1/brick \\ gluster3:/gluster/bricks/1/brick \ngluster volume start kvmgluster\ngluster volume set kvmgluster auth.allow 192.168.159.0/24\ngluster volume set kvmgluster allow-insecure on\ngluster volume set kvmgluster storage.owner-uid 107\ngluster volume set kvmgluster storage.owner-gid 107\n```", "```\necho 'localhost:/kvmgluster /mnt glusterfs \\ defaults,_netdev,backupvolfile-server=localhost 0 0' >> /etc/fstab\nmount.glusterfs localhost:/kvmgluster /mnt\n```", "```\nwget \\ https://download.gluster.org/pub/gluster/glusterfs/6/LATEST/CentOS/gl\\ usterfs-rhel8.repo -P /etc/yum.repos.d\nyum install glusterfs glusterfs-fuse attr -y\nmount -t glusterfs -o context=\"system_u:object_r:virt_image_t:s0\" \\ gluster1:/kvmgluster /var/lib/libvirt/images/GlusterFS \n```", "```\n<pool type='dir'>\n\u00a0\u00a0<name>glusterfs-pool</name>\n\u00a0\u00a0<target>\n\u00a0\u00a0\u00a0\u00a0<path>/var/lib/libvirt/images/GlusterFS</path>\n\u00a0\u00a0\u00a0\u00a0<permissions>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<mode>0755</mode>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<owner>107</owner>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<group>107</group>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<label>system_u:object_r:virt_image_t:s0</label>\n\u00a0\u00a0\u00a0\u00a0</permissions>\n\u00a0\u00a0</target>\n</pool> \n```", "```\nvirsh pool-define --file gluster.xml\nvirsh pool-start --pool glusterfs-pool\nvirsh pool-autostart --pool glusterfs-pool\n```", "```\ngluster1:/kvmgluster\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0/var/lib/libvirt/images/GlusterFS \\ glusterfs\u00a0\u00a0\u00a0defaults,_netdev\u00a0\u00a00\u00a0\u00a00\n```", "```\ngluster volume set kvmgluster network.ping-timeout number\n```", "```\nyum -y update; reboot\n```", "```\nrpm -Uhv http://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-1-1.el7.noarch.rpm\nyum -y install ceph-deploy ceph ceph-radosgw\nuseradd cephadmin\necho \"cephadmin:ceph123\" | chpasswd\necho \"cephadmin ALL = (root) NOPASSWD:ALL\" | sudo tee /etc/sudoers.d/cephadmin\nchmod 0440 /etc/sudoers.d/cephadmin\n```", "```\nsed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config\nsystemctl stop firewalld\nsystemctl disable firewalld\nsystemctl mask firewalld\n```", "```\necho \"192.168.159.150 ceph-admin\" >> /etc/hosts\necho \"192.168.159.151 ceph-monitor\" >> /etc/hosts\necho \"192.168.159.152 ceph-osd1\" >> /etc/hosts\necho \"192.168.159.153 ceph-osd2\" >> /etc/hosts\necho \"192.168.159.154 ceph-osd3\" >> /etc/hosts\n```", "```\nssh-copy-id cephadmin@ceph-admin\nssh-copy-id cephadmin@ceph-monitor\nssh-copy-id cephadmin@ceph-osd1\nssh-copy-id cephadmin@ceph-osd2\nssh-copy-id cephadmin@ceph-osd3\n```", "```\nHost ceph-admin\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Hostname ceph-admin\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0User cephadmin\nHost ceph-monitor\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Hostname ceph-monitor\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0User cephadmin\nHost ceph-osd1\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Hostname ceph-osd1\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0User cephadmin\nHost ceph-osd2\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Hostname ceph-osd2\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0User cephadmin\nHost ceph-osd3\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Hostname ceph-osd3\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0User cephadmin\n```", "```\ncd /root\nmkdir cluster\ncd cluster\nceph-deploy new ceph-monitor\n```", "```\npublic network = 192.168.159.0/24\nosd pool default size = 2\n```", "```\nceph-deploy install ceph-admin ceph-monitor ceph-osd1 ceph-osd2 ceph-osd3\nceph-deploy mon create-initial\nceph-deploy gatherkeys ceph-monitor\nceph-deploy disk list ceph-osd1 ceph-osd2 ceph-osd3\nceph-deploy disk zap ceph-osd1:/dev/sdb\u00a0\u00a0ceph-osd2:/dev/sdb\u00a0\u00a0ceph-osd3:/dev/sdb\nceph-deploy osd prepare ceph-osd1:/dev/sdb ceph-osd2:/dev/sdb ceph-osd3:/dev/sdb\nceph-deploy osd activate ceph-osd1:/dev/sdb1 ceph-osd2:/dev/sdb1 ceph-osd3:/dev/sdb1\n```", "```\nceph osd pool create KVMpool 128 128\n```", "```\nceph auth get-or-create client.KVMpool mon 'allow r' osd 'allow rwx pool=KVMpool'\n```", "```\nkey = AQB9p8RdqS09CBAA1DHsiZJbehb7ZBffhfmFJQ==\n```", "```\n\u00a0\u00a0\u00a0<secret ephemeral='no' private='no'>\n\u00a0\u00a0\u00a0<usage type='ceph'>\n\u00a0\u00a0\u00a0\u00a0\u00a0<name>client.KVMpool secret</name>\n\u00a0\u00a0\u00a0</usage>\n</secret>\n```", "```\nvirsh secret-define --file secret.xml\n```", "```\nSecret 95b1ed29-16aa-4e95-9917-c2cd4f3b2791 created\n```", "```\nvirsh secret-set-value 95b1ed29-16aa-4e95-9917-c2cd4f3b2791 AQB9p8RdqS09CBAA1DHsiZJbehb7ZBffhfmFJQ==\n```", "```\n\u00a0\u00a0\u00a0<pool type=\"rbd\">\n\u00a0\u00a0\u00a0\u00a0\u00a0<source>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<name>KVMpool</name>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<host name='192.168.159.151' port='6789'/>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<auth username='KVMpool' type='ceph'>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<secret uuid='95b1ed29-16aa-4e95-9917-c2cd4f3b2791'/>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0</auth>\n\u00a0\u00a0\u00a0\u00a0\u00a0</source>\n\u00a0\u00a0\u00a0</pool>\n```", "```\nvirsh pool-define --file ceph.xml\nvirsh pool-start KVMpool\nvirsh pool-autostart KVMpool\nvirsh pool-list --details\n```", "```\n# dd if=/dev/zero of=/vms/dbvm_disk2.img bs=1G count=10\n```", "```\n    dd if=/dev/zero of=/vms/dbvm_disk2_seek.imgbs=1G seek=10 count=0\n    ```", "```\n# qemu-img info /vms/dbvm_disk2.img\nimage: /vms/dbvm_disk2.img\nfile format: raw\nvirtual size: 10G (10737418240 bytes)\ndisk size: 10G\n# qemu-img info /vms/dbvm_disk2_seek.img\nimage: /vms/dbvm_disk2_seek.img\nfile format: raw\nvirtual size: 10G (10737418240 bytes)\ndisk size: 10M\n```", "```\nssh -X host's address\n[remotehost]# virt-manager\n```", "```\nattach-disk domain source target [[[--live] [--config] | [--current]] | [--persistent]] [--targetbusbus] [--driver driver] [--subdriversubdriver] [--iothreadiothread] [--cache cache] [--type type] [--mode mode] [--sourcetypesourcetype] [--serial serial] [--wwnwwn] [--rawio] [--address address] [--multifunction] [--print-xml]\n```", "```\n# virsh attach-disk CentOS8 /vms/dbvm_disk2.img vdb --live --config\n```", "```\n# virsh domblklist CentOS8 --details\nType Device Target Source\n------------------------------------------------\nfile disk vda /var/lib/libvirt/images/fedora21.qcow2\nfile disk vdb /vms/dbvm_disk2_seek.img\n```", "```\n    # mkdir /iso\n    ```", "```\n    # chmod 700 /iso\n    # semanage fcontext -a -t virt_image_t \"/iso(/.*)?\"\n    ```", "```\n    iso_library to demonstrate how to create a storage pool that will hold ISO images, but you are free to use any name you wish.\n    ```", "```\n    # virsh pool-info iso_library\n    Name: iso_library\n    UUID: 959309c8-846d-41dd-80db-7a6e204f320e\n    State: running\n    Persistent: yes\n    Autostart: no\n    Capacity: 49.09 GiB\n    Allocation: 8.45 GiB\n    Available: 40.64 GiB\n    ```", "```\n    # virsh pool-refresh iso_library\n    Pool iso_library refreshed\n    # virsh vol-list iso_library\n    Name Path\n    ------------------------------------------------------------------\n    ------------\n    CentOS8-Everything.iso /iso/CentOS8-Everything.iso\n    CentOS7-EVerything.iso /iso/CentOS7-Everything.iso\n    RHEL8.iso /iso/RHEL8.iso\n    Win8.iso /iso/Win8.iso\n    ```", "```\nvirsh pool-destroy MyNFSpool\nvirsh pool-undefine MyNFSpool\n```", "```\n# virsh vol-create-as dedicated_storage vm_vol1 10G\n```", "```\n# virsh vol-info --pool dedicated_storage vm_vol1\nName: vm_vol1\nType: file\nCapacity: 1.00 GiB\nAllocation: 1.00 GiB\n```", "```\n# virsh vol-delete dedicated_storage vm_vol2\n```"]
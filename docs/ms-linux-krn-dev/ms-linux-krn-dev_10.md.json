["```\nstruct x86_platform_ops {\n        unsigned long (*calibrate_cpu)(void);\n        unsigned long (*calibrate_tsc)(void);\n        void (*get_wallclock)(struct timespec *ts);\n        int (*set_wallclock)(const struct timespec *ts);\n        void (*iommu_shutdown)(void);\n        bool (*is_untracked_pat_range)(u64 start, u64 end);\n        void (*nmi_init)(void);\n        unsigned char (*get_nmi_reason)(void);\n        void (*save_sched_clock_state)(void);\n        void (*restore_sched_clock_state)(void);\n        void (*apic_post_init)(void);\n        struct x86_legacy_features legacy;\n        void (*set_legacy_features)(void);\n};\n```", "```\nvoid __init setup_pit_timer(void)\n{\n        clockevent_i8253_init(true);\n        global_clock_event = &i8253_clockevent;\n}\n```", "```\nvoid __init clockevent_i8253_init(bool oneshot)\n{\n        if (oneshot)\n                i8253_clockevent.features |= CLOCK_EVT_FEAT_ONESHOT;\n        /*\n        * Start pit with the boot cpu mask. x86 might make it global\n        * when it is used as broadcast device later.\n        */\n        i8253_clockevent.cpumask = cpumask_of(smp_processor_id());\n\n        clockevents_config_and_register(&i8253_clockevent, PIT_TICK_RATE,\n                                        0xF, 0x7FFF);\n}\n#endif\n```", "```\nstruct clocksource {\n        u64 (*read)(struct clocksource *cs);\n        u64 mask;\n        u32 mult;\n        u32 shift;\n        u64 max_idle_ns;\n        u32 maxadj;\n#ifdef CONFIG_ARCH_CLOCKSOURCE_DATA\n        struct arch_clocksource_data archdata;\n#endif\n        u64 max_cycles;\n        const char *name;\n        struct list_head list;\n        int rating;\n        int (*enable)(struct clocksource *cs);\n        void (*disable)(struct clocksource *cs);\n        unsigned long flags;\n        void (*suspend)(struct clocksource *cs);\n        void (*resume)(struct clocksource *cs);\n        void (*mark_unstable)(struct clocksource *cs);\n        void (*tick_stable)(struct clocksource *cs);\n\n        /* private: */\n#ifdef CONFIG_CLOCKSOURCE_WATCHDOG\n        /* Watchdog related data, used by the framework */\n        struct list_head wd_list;\n        u64 cs_last;\n        u64 wd_last;\n#endif\n        struct module *owner;\n};\n```", "```\nstatic inline s64 clocksource_cyc2ns(u64 cycles, u32 mult, u32 shift)\n{\n        return ((u64) cycles * mult) >> shift;\n}\n```", "```\nvoid\nclocks_calc_mult_shift(u32 *mult, u32 *shift, u32 from, u32 to, u32 maxsec)\n{\n        u64 tmp;\n        u32 sft, sftacc= 32;\n\n        /*\n        * Calculate the shift factor which is limiting the conversion\n        * range:\n        */\n        tmp = ((u64)maxsec * from) >> 32;\n        while (tmp) {\n                tmp >>=1;\n                sftacc--;\n        }\n\n        /*\n        * Find the conversion shift/mult pair which has the best\n        * accuracy and fits the maxsec conversion range:\n        */\n        for (sft = 32; sft > 0; sft--) {\n                tmp = (u64) to << sft;\n                tmp += from / 2;\n                do_div(tmp, from);\n                if ((tmp >> sftacc) == 0)\n                        break;\n        }\n        *mult = tmp;\n        *shift = sft;\n}\n```", "```\nstruct clocksource *cs = &curr_clocksource;\ncycle_t start = cs->read(cs);\n/* things to do */\ncycle_t end = cs->read(cs);\ncycle_t diff = end \u2013 start;\nduration =  clocksource_cyc2ns(diff, cs->mult, cs->shift);\n```", "```\nu64 get_jiffies_64(void)\n{\n        unsigned long seq;\n        u64 ret;\n\n        do {\n                seq = read_seqbegin(&jiffies_lock);\n                ret = jiffies_64;\n        } while (read_seqretry(&jiffies_lock, seq));\n        return ret;\n}\n```", "```\n#define time_after(a,b)           \\\n       (typecheck(unsigned long, a) && \\\n        typecheck(unsigned long, b) && \\\n        ((long)((b) - (a)) < 0))\n#define time_before(a,b)       time_after(b,a)\n\n#define time_after_eq(a,b)     \\\n       (typecheck(unsigned long, a) && \\\n        typecheck(unsigned long, b) && \\\n        ((long)((a) - (b)) >= 0))\n#define time_before_eq(a,b)    time_after_eq(b,a)\n```", "```\nunsigned int jiffies_to_msecs(const unsigned long j)\n{\n#if HZ <= MSEC_PER_SEC && !(MSEC_PER_SEC % HZ)\n        return (MSEC_PER_SEC / HZ) * j;\n#elif HZ > MSEC_PER_SEC && !(HZ % MSEC_PER_SEC)\n        return (j + (HZ / MSEC_PER_SEC) - 1)/(HZ / MSEC_PER_SEC);\n#else\n# if BITS_PER_LONG == 32\n        return (HZ_TO_MSEC_MUL32 * j) >> HZ_TO_MSEC_SHR32;\n# else\n        return (j * HZ_TO_MSEC_NUM) / HZ_TO_MSEC_DEN;\n# endif\n#endif\n}\n\nunsigned int jiffies_to_usecs(const unsigned long j)\n{\n        /*\n        * Hz doesn't go much further MSEC_PER_SEC.\n        * jiffies_to_usecs() and usecs_to_jiffies() depend on that.\n        */\n        BUILD_BUG_ON(HZ > USEC_PER_SEC);\n\n#if !(USEC_PER_SEC % HZ)\n        return (USEC_PER_SEC / HZ) * j;\n#else\n# if BITS_PER_LONG == 32\n        return (HZ_TO_USEC_MUL32 * j) >> HZ_TO_USEC_SHR32;\n# else\n        return (j * HZ_TO_USEC_NUM) / HZ_TO_USEC_DEN;\n# endif\n#endif\n}\n\nstatic inline u64 jiffies_to_nsecs(const unsigned long j)\n{\n        return (u64)jiffies_to_usecs(j) * NSEC_PER_USEC;\n}\n```", "```\nstruct timespec {\n        __kernel_time_t  tv_sec;                   /* seconds */\n        long            tv_nsec;          /* nanoseconds */\n};\n#endif\n\nstruct timeval {\n        __kernel_time_t          tv_sec;           /* seconds */\n        __kernel_suseconds_t     tv_usec;  /* microseconds */\n};\n```", "```\nstruct tk_read_base {\n        struct clocksource        *clock;\n        cycle_t                  (*read)(struct clocksource *cs);\n        cycle_t                  mask;\n        cycle_t                  cycle_last;\n        u32                      mult;\n        u32                      shift;\n        u64                      xtime_nsec;\n        ktime_t                  base_mono;\n};\n```", "```\nstruct timekeeper {\n        struct tk_read_base       tkr;\n        u64                      xtime_sec;\n        unsigned long           ktime_sec;\n        struct timespec64 wall_to_monotonic;\n        ktime_t                  offs_real;\n        ktime_t                  offs_boot;\n        ktime_t                  offs_tai;\n        s32                      tai_offset;\n        ktime_t                  base_raw;\n        struct timespec64 raw_time;\n\n        /* The following members are for timekeeping internal use */\n        cycle_t                  cycle_interval;\n        u64                      xtime_interval;\n        s64                      xtime_remainder;\n        u32                      raw_interval;\n        u64                      ntp_tick;\n        /* Difference between accumulated time and NTP time in ntp\n        * shifted nano seconds. */\n        s64                      ntp_error;\n        u32                      ntp_error_shift;\n        u32                      ntp_err_mult;\n};\n```", "```\nstatic inline u64 timekeeping_delta_to_ns(struct tk_read_base *tkr, u64 delta)\n{\n        u64 nsec;\n\n        nsec = delta * tkr->mult + tkr->xtime_nsec;\n        nsec >>= tkr->shift;\n\n        /* If arch requires, add in get_arch_timeoffset() */\n        return nsec + arch_gettimeoffset();\n}\n\nstatic inline u64 timekeeping_get_ns(struct tk_read_base *tkr)\n{\n        u64 delta;\n\n        delta = timekeeping_get_delta(tkr);\n        return timekeeping_delta_to_ns(tkr, delta);\n}\n```", "```\nstatic u64 logarithmic_accumulation(struct timekeeper *tk, u64 offset,\n                                    u32 shift, unsigned int *clock_set)\n{\n        u64 interval = tk->cycle_interval << shift;\n        u64 snsec_per_sec;\n\n        /* If the offset is smaller than a shifted interval, do nothing */\n        if (offset < interval)\n                return offset;\n\n        /* Accumulate one shifted interval */\n        offset -= interval;\n        tk->tkr_mono.cycle_last += interval;\n        tk->tkr_raw.cycle_last  += interval;\n\n        tk->tkr_mono.xtime_nsec += tk->xtime_interval << shift;\n        *clock_set |= accumulate_nsecs_to_secs(tk);\n\n        /* Accumulate raw time */\n        tk->tkr_raw.xtime_nsec += (u64)tk->raw_time.tv_nsec << tk->tkr_raw.shift;\n        tk->tkr_raw.xtime_nsec += tk->raw_interval << shift;\n        snsec_per_sec = (u64)NSEC_PER_SEC << tk->tkr_raw.shift;\n        while (tk->tkr_raw.xtime_nsec >= snsec_per_sec) {\n                tk->tkr_raw.xtime_nsec -= snsec_per_sec;\n                tk->raw_time.tv_sec++;\n        }\n        tk->raw_time.tv_nsec = tk->tkr_raw.xtime_nsec >> tk->tkr_raw.shift;\n        tk->tkr_raw.xtime_nsec -= (u64)tk->raw_time.tv_nsec << tk->tkr_raw.shift;\n\n        /* Accumulate error between NTP and clock interval */\n        tk->ntp_error += tk->ntp_tick << shift;\n        tk->ntp_error -= (tk->xtime_interval + tk->xtime_remainder) <<\n                                                (tk->ntp_error_shift + shift);\n\n        return offset;\n}\n```", "```\nstruct clock_event_device {\n        void                    (*event_handler)(struct clock_event_device *);\n        int                     (*set_next_event)(unsigned long evt, struct clock_event_device *);\n        int                     (*set_next_ktime)(ktime_t expires, struct clock_event_device *);\n        ktime_t                  next_event;\n        u64                      max_delta_ns;\n        u64                      min_delta_ns;\n        u32                      mult;\n        u32                      shift;\n        enum clock_event_state    state_use_accessors;\n        unsigned int            features;\n        unsigned long           retries;\n\n        int                     (*set_state_periodic)(struct  clock_event_device *);\n        int                     (*set_state_oneshot)(struct clock_event_device *);\n        int                     (*set_state_oneshot_stopped)(struct clock_event_device *);\n        int                     (*set_state_shutdown)(struct clock_event_device *);\n        int                     (*tick_resume)(struct clock_event_device *);\n\n        void                    (*broadcast)(const struct cpumask *mask);\n        void                    (*suspend)(struct clock_event_device *);\n        void                    (*resume)(struct clock_event_device *);\n        unsigned long           min_delta_ticks;\n        unsigned long           max_delta_ticks;\n\n        const char               *name;\n        int                     rating;\n        int                     irq;\n        int                     bound_on;\n        const struct cpumask       *cpumask;\n        struct list_head  list;\n        struct module             *owner;\n} ____cacheline_aligned;\n```", "```\n#define CLOCK_EVT_FEAT_PERIODIC 0x000001\n#define CLOCK_EVT_FEAT_ONESHOT 0x000002\n#define CLOCK_EVT_FEAT_KTIME  0x000004\n```", "```\nenum tick_device_mode {\n TICKDEV_MODE_PERIODIC,\n TICKDEV_MODE_ONESHOT,\n};\n\nstruct tick_device {\n        struct clock_event_device *evtdev;\n        enum tick_device_mode mode;\n}\n```", "```\nstruct timer_list {\n        /*\n        * Every field that changes during normal runtime grouped to the\n        * same cacheline\n        */\n        struct hlist_node entry;\n        unsigned long           expires;\n        void                    (*function)(unsigned long);\n        unsigned long           data;\n        u32                      flags;\n\n#ifdef CONFIG_LOCKDEP\n        struct lockdep_map        lockdep_map;\n#endif\n};\n```", "```\nint del_timer(struct timer_list *timer)\n{\n        struct tvec_base *base;\n        unsigned long flags;\n        int ret = 0;\n\n        debug_assert_init(timer);\n\n        timer_stats_timer_clear_start_info(timer);\n        if (timer_pending(timer)) {\n                base = lock_timer_base(timer, &flags);\n                if (timer_pending(timer)) {\n                        detach_timer(timer, 1);\n                        if (timer->expires == base->next_timer &&\n                            !tbase_get_deferrable(timer->base))\n                                base->next_timer = base->timer_jiffies;\n                        ret = 1;\n                }\n                spin_unlock_irqrestore(&base->lock, flags);\n        }\n\n        return ret;\n}\n\nint del_timer_sync(struct timer_list *timer)\n{\n#ifdef CONFIG_LOCKDEP\n        unsigned long flags;\n\n        /*\n        * If lockdep gives a backtrace here, please reference\n        * the synchronization rules above.\n        */\n        local_irq_save(flags);\n        lock_map_acquire(&timer->lockdep_map);\n        lock_map_release(&timer->lockdep_map);\n        local_irq_restore(flags);\n#endif\n        /*\n        * don't use it in hardirq context, because it\n        * could lead to deadlock.\n        */\n        WARN_ON(in_irq());\n        for (;;) {\n                int ret = try_to_del_timer_sync(timer);\n                if (ret >= 0)\n                        return ret;\n                cpu_relax();\n        }\n}\n\n#define del_singleshot_timer_sync(t) del_timer_sync(t)\n```", "```\nRESOURCE_DEALLOCATE() here could be any relevant resource deallocation routine:\n```", "```\n...\ndel_timer(&t_obj);\nRESOURCE_DEALLOCATE();\n....\n```", "```\nstatic __latent_entropy void run_timer_softirq(struct softirq_action *h)\n{\n        struct timer_base *base = this_cpu_ptr(&timer_bases[BASE_STD]);\n\n        base->must_forward_clk = false;\n\n        __run_timers(base);\n        if (IS_ENABLED(CONFIG_NO_HZ_COMMON) && base->nohz_active)\n                __run_timers(this_cpu_ptr(&timer_bases[BASE_DEF]));\n}\n```", "```\nstatic inline void ndelay(unsigned long x)\n{\n        udelay(DIV_ROUND_UP(x, 1000));\n}\n```", "```\nstatic void\nia64_itc_udelay (unsigned long usecs)\n{\n        unsigned long start = ia64_get_itc();\n        unsigned long end = start + usecs*local_cpu_data->cyc_per_usec;\n\n        while (time_before(ia64_get_itc(), end))\n                cpu_relax();\n}\n\nvoid (*ia64_udelay)(unsigned long usecs) = &ia64_itc_udelay;\n\nvoid\nudelay (unsigned long usecs)\n{\n        (*ia64_udelay)(usecs);\n}\n```"]
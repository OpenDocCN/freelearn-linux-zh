- en: Memory Management Internals - Essentials
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 内存管理内部 - 基本要点
- en: Kernel internals, especially regarding memory management, is a vast and complex
    topic. In this book, I do not intend to delve into the deep, gory details of kernel
    memory internals. At the same time, I would like to provide sufficient – and definitely required
    – background knowledge for a budding kernel or device driver developer like you
    to successfully tackle this key topic.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 内核内部，特别是关于内存管理的部分，是一个广阔而复杂的主题。在本书中，我不打算深入研究内核内存的细节。与此同时，我希望为像您这样的新兴内核或设备驱动程序开发人员提供足够的背景知识，以成功地解决这一关键主题。
- en: Accordingly, this chapter will help you understand to sufficient depth the internals
    of how memory management is performed on the Linux OS; this includes delving into
    the **Virtual Memory** (**VM**) split, examining both the user-mode and kernel
    segment of the process to a good level of depth, and covering the basics of how
    the kernel manages physical memory. In effect, you will come to  understand the memory
    maps – both virtual and physical – of the process and the system.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本章将帮助您充分了解Linux操作系统上内存管理是如何执行的；这包括深入研究虚拟内存（VM）分割，以及对进程的用户模式和内核段进行深入的检查，以及覆盖内核如何管理物理内存的基础知识。实际上，您将了解进程和系统的内存映射
    - 虚拟和物理。
- en: This background knowledge will go a long way in helping you correctly and efficiently
    manage dynamic kernel memory (with a focus on writing kernel or driver code using
    the **Loadable Kernel Module** (**LKM**) framework; this aspect - dynamic memory
    management - in a practical fashion is the focal point of the next two chapters
    in the book). As an important side benefit, armed with this knowledge, you will
    find yourself becoming more proficient at the debugging of both user and kernel-space
    code. (The importance of this cannot be overstated! Debugging code is both an
    art and a science, as well as a reality.)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这些背景知识将在帮助您正确和高效地管理动态内核内存方面发挥重要作用（重点是使用**可加载内核模块**（LKM）框架编写内核或驱动程序代码；这方面 - 动态内存管理
    - 在本书的接下来的两章中是重点）。作为一个重要的附带好处，掌握了这些知识，您将发现自己在调试用户和内核空间代码方面变得更加熟练。（这一点的重要性不言而喻！调试代码既是一门艺术，也是一门科学，也是一种现实。）
- en: 'In this chapter, the areas we will cover include the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Understanding the VM split
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解虚拟内存分割
- en: Examining the process VAS
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查进程VAS
- en: Examining the kernel segment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查内核段
- en: Randomizing the memory layout – [K]ASLR
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机化内存布局 - [K]ASLR
- en: Physical memory
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物理内存
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: I assume that you have gone through [Chapter 1](ad75db43-a1a2-4f3f-92c7-a544f47baa23.xhtml), *Kernel
    Workspace Setup*, and have appropriately prepared a guest VM running Ubuntu 18.04
    LTS (or a later stable release) and installed all the required packages. If not,
    I recommend you do this first. To get the most out of this book, I strongly recommend
    you first set up the workspace environment, including cloning this book's GitHub
    repository for the code ([https://github.com/PacktPublishing/Linux-Kernel-Programming](https://github.com/PacktPublishing/Linux-Kernel-Programming)),
    and work on it in a hands-on fashion.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设您已经阅读了[第1章](ad75db43-a1a2-4f3f-92c7-a544f47baa23.xhtml)，*内核工作空间设置*，并已经适当地准备了运行Ubuntu
    18.04 LTS（或更高版本）的虚拟机，并安装了所有必需的软件包。如果没有，我建议您先这样做。为了充分利用本书，我强烈建议您首先设置工作环境，包括克隆本书的GitHub代码库（[https://github.com/PacktPublishing/Linux-Kernel-Programming](https://github.com/PacktPublishing/Linux-Kernel-Programming)），并以实际操作的方式进行工作。
- en: I assume that you are familiar with basic virtual memory concepts, the user-mode
    process **Virtual Address Space** (**VAS**) layout of segments, user-and kernel-mode stacks,
    the task structure, and so on. If you're unsure on this footing, I strongly suggest
    you read the preceding chapter first.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设您熟悉基本的虚拟内存概念，用户模式进程**虚拟地址空间**（VAS）段的布局，用户和内核模式的堆栈，任务结构等。如果您对此不确定，我强烈建议您先阅读前一章。
- en: Understanding the VM split
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解虚拟内存分割
- en: 'In this chapter, we will broadly be looking at how the Linux kernel manages
    memory in two ways:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将广泛地研究Linux内核以两种方式管理内存：
- en: The virtual memory-based approach, where memory is virtualized (the usual case)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于虚拟内存的方法，其中内存是虚拟化的（通常情况）
- en: A view of how the kernel actually organizes physical memory (RAM pages)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看内核实际如何组织物理内存（RAM页面）
- en: First, let's begin with the virtual memory view, and then discuss physical memory
    organization later in the chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从虚拟内存视图开始，然后在本章后面讨论物理内存组织。
- en: 'As we saw earlier in the previous chapter, in the *Understanding the basics
    of the process Virtual Address Space (VAS)* section, a key property of the process,
    VAS, is that it is completely self-contained, a sandbox. You cannot look outside
    the box. In [Chapter 6](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml), *Kernel Internals
    Essentials – Processes and Threads*, Figure 6.2, we saw that the process VAS ranges
    from virtual address `0` to what we simply termed the high address. What is the
    actual value of this high address? Obviously, it''s the highest extent of the
    VAS and thus depends on the number of bits used for addressing:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一章中所看到的，在*理解进程虚拟地址空间（VAS）的基础*部分，进程VAS的一个关键属性是它是完全自包含的，一个沙盒。你不能看到盒子外面。在[第6章](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml)，*内核内部基本要点
    - 进程和线程*，图6.2中，我们看到进程VAS范围从虚拟地址`0`到我们简单地称为高地址。这个高地址的实际值是多少？显然，这是VAS的最高范围，因此取决于用于寻址的位数：
- en: On a Linux OS running on a 32-bit processor (or compiled for 32-bit), the highest
    virtual address will be *2^(32) = 4 GB*.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在运行在32位处理器上的Linux操作系统（或为32位编译）上，最高虚拟地址将是*2^(32) = 4 GB*。
- en: On a Linux OS running on (and compiled for) a 64-bit processor, the highest
    virtual address will be *2^(64) = 16 EB.* (EB is short for exabyte. Believe me,
    it's an enormousquantity. 16 EB is equivalent to the number *16 x 10^(18).*)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在运行在（并为）64位处理器编译的Linux操作系统上，最高虚拟地址将是*2^(64)=16 EB*。（EB是exabyte的缩写。相信我，这是一个巨大的数量。16
    EB相当于数字*16 x 10^(18)。）
- en: For simplicity, to keep the numbers manageable, let's focus for now on the 32-bit
    address space (we will certainly cover 64-bit addressing as well). So, according
    to our discussions, on a 32-bit system, the process VAS is from 0 to 4 GB – this
    region consists of empty space (unused regions, called **sparse regions** or **holes**)
    and valid regions of memory commonly termed **segments** (or more correctly, **mappings**)
    – text, data, library, and stack (all of this having been covered in some detail
    in [Chapter 6](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml), *Kernel Internals
    Essentials – Processes and Threads*).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，为了使数字易于管理，让我们现在专注于32位地址空间（我们肯定也会涵盖64位寻址）。因此，根据我们的讨论，在32位系统上，进程VAS从0到4
    GB-这个区域包括空白空间（未使用的区域，称为**稀疏区域**或**空洞**）和通常称为**段**（或更正确地说是**映射**）的内存有效区域-文本、数据、库和堆栈（所有这些在[第6章](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml)中已经有了详细的介绍，*内核内部要点-进程和线程*）。
- en: On our journey to understanding virtual memory, it's useful to take up the well-known
    `Hello, world` C program and understand its inner workings on a Linux system;
    this is what the next section covers!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们理解虚拟内存的旅程中，拿出众所周知的`Hello, world` C程序，并在Linux系统上理解它的内部工作是很有用的；这就是下一节要讨论的内容！
- en: Looking under the hood – the Hello, world C program
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深入了解-Hello, world C程序
- en: 'Right, is there anyone here who knows how to code the canonical `Hello, world`
    C program? Okay, very amusing, let''s check out the one meaningful line therein:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对了，这里有谁知道如何编写经典的`Hello, world` C程序吗？好的，非常有趣，让我们来看看其中有意义的一行：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The process is calling the `printf(3)` function. Have you written the code of
    the `printf()`? "No, of course not," you say, "it's within the standard `libc` C
    library, typically `glibc` (GNU `libc`) on Linux." But hang on, unless the code
    and data of `printf` (and similarly all other library APIs) is actually within
    the process VAS, how can we ever access it? (Recall, you can't look *outside the
    box*!) For that, the code (and data) of `printf(3)` (in fact, of the `glibc` library)
    must be mapped within the process *box* – the process VAS. It is indeed mapped
    within the process VAS, in the library segments or mappings (as we saw in [Chapter
    6](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml), *Kernel Internals Essentials –
    Processes and Threads*, *F**igure 6.1*). How did this happen?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 该进程正在调用`printf(3)`函数。你写过`printf()`的代码吗？“当然没有”，你说，“它在标准的`libc` C库中，通常是Linux上的`glibc`（GNU
    `libc`）。”但是等等，除非`printf`（以及所有其他库API）的代码和数据实际上在进程VAS中，我们怎么能访问它呢？（记住，你不能看*盒子外*！）为此，`printf(3)`的代码（和数据）（实际上是`glibc`库的）必须在进程*盒子*内——进程VAS内被映射。它确实被映射到了进程VAS中，在库段或映射中（正如我们在[第6章](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml)中看到的，*内核内部要点-进程和线程*，*图6.1*）。这是怎么发生的？
- en: The reality is that on application startup, as part of the C runtime environment
    setup, there is a small **Executable and Linkable Format** (**ELF**) binary (embedded
    into your `a.out` binary executable file) called the **loader **(`ld.so` or `ld-linux.so`).
    It is given control early. It detects all required shared libraries and memory
    maps all of them – the library text (code) and data segments – into the process
    VAS by opening the library file(s) and issuing the `mmap(2)` system call. So,
    now, once the code and data of the library are mapped within the process VAS,
    the process can indeed access it, and thus – wait for it – the `printf()` API
    can be successfully invoked! (We've skipped the gory details of memory mapping
    and linkage here).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，在应用程序启动时，作为C运行时环境设置的一部分，有一个小的**可执行和可链接格式**（**ELF**）二进制文件（嵌入到你的`a.out`二进制可执行文件中）称为**加载器**（`ld.so`或`ld-linux.so`）。它很早就获得了控制权。它检测所有需要的共享库，并通过打开库文件并发出`mmap(2)`系统调用将它们全部内存映射到进程VAS中-库文本（代码）和数据段。因此，一旦库的代码和数据被映射到进程VAS中，进程就可以访问它，因此-等待它-`printf()`
    API可以成功调用！（我们在这里跳过了内存映射和链接的血腥细节）。
- en: 'Further verifying this, the `ldd(1)` script (the following output is from an
    x86_64 system) reveals that this is indeed the case:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步验证这一点，`ldd(1)`脚本（以下输出来自x86_64系统）显示确实如此：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A few quick points to note:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一些要点：
- en: 'Every single Linux process – automatically and by default – links to a minimum
    of two objects: the `glibc` shared library and the program loader (no explicit
    linker switch is required).'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个Linux进程-自动且默认-链接到至少两个对象：`glibc`共享库和程序加载器（不需要显式的链接器开关）。
- en: The name of the loader program varies with the architecture. Here, on our x86_64
    system, it's `ld-linux-x86-64.so.2`.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载程序的名称因架构而异。在我们的x86_64系统上，它是`ld-linux-x86-64.so.2`。
- en: In the preceding `ldd` output, the address within parentheses on the right is
    the virtual address of the location of the mapping. For example, in the preceding
    output, `glibc` is mapped into our process VAS at the **User Virtual Address**
    (**UVA**), which equals `0x00007feb7b85b000`. Note that it's runtime dependent
    (it also varies due to **Address Space Layout Randomization** (**ASLR**) semantics
    (seen later)).
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在前面的`ldd`输出中，括号中的地址是映射位置的虚拟地址。例如，在前面的输出中，`glibc`被映射到我们的进程VAS的**用户虚拟地址**（**UVA**），等于`0x00007feb7b85b000`。请注意，这是运行时相关的（也因为**地址空间布局随机化**（**ASLR**）语义而变化（稍后会看到））。
- en: For security reasons (and on architectures besides x86), it's considered better
    to use the `objdump(1)` utility to look up details like these.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出于安全原因（以及在除x86之外的架构上），最好使用`objdump(1)`实用程序来查找这类细节。
- en: Try performing `strace(1)` on the `Hello, world` binary executable and you will
    see numerous `mmap()` system calls, mapping in `glibc` (and other) segments!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试对`Hello, world`二进制可执行文件执行`strace(1)`，你会看到大量的`mmap()`系统调用，映射`glibc`（和其他）段！
- en: Let's further examine our simple `Hello, world`application more deeply.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地研究我们简单的`Hello, world`应用程序。
- en: Going beyond the printf() API
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 超越printf() API
- en: As you will know, the `printf(3)` API translates to the `write(2)` system call,
    which of course writes the `"Hello, world"` string to `stdout` (by default, the
    terminal window or the console device).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所知，`printf(3)` API 转换为 `write(2)` 系统调用，这当然会将 `"Hello, world"` 字符串写入 `stdout`（默认情况下是终端窗口或控制台设备）。
- en: 'We also understand that as `write(2)` is a system call, this implies that the
    current process running this code – the process context – must now switch to kernel
    mode and run the kernel code of `write(2)` (monolithic kernel architecture)! Indeed
    it does. But hang on a second: the kernel code of `write(2)` is in kernel VAS (refer
    to [Chapter 6](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml), *Kernel Internals
    Essentials – Processes and Threads*, Figure 6.1). The point here is if the kernel
    VAS is outside the box, then how in the world are we going to call it?'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也明白，由于`write(2)`是一个系统调用，这意味着运行此代码的当前进程-进程上下文-现在必须切换到内核模式并运行`write(2)`的内核代码（单内核架构）！确实如此。但等一下：`write(2)`的内核代码在内核VAS中（参见[第6章](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml)，*内核内部要点-进程和线程*，图6.1）。关键在于，如果内核VAS在盒子外面，那么我们怎么调用它呢？
- en: Well, it could be done by placing the kernel in a separate 4 GB VAS, but this
    approach results in very slow context switching, so it's simply not done.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，可以通过将内核放在单独的4GB VAS中来完成，但这种方法会导致非常缓慢的上下文切换，所以不会这样做。
- en: 'The way it is engineered is like this: both user and kernel VASes live in the
    same ''box'' – the available VAS. How exactly? By *splitting* the available address
    space between the user and kernel in some `User:Kernel :: u:k` ratio. This is
    called the** VM split **(the ratio `u:k` being typically expressed in gigabytes,
    terabytes, or even petabytes).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '它的工程方式是这样的：用户和内核VAS都存在于同一个''盒子''中-可用VAS。具体是如何实现的呢？通过*分割*可用地址空间，将用户和内核分配在某个`User:Kernel
    :: u:k`比例中。这被称为**VM分割**（比例`u:k`通常以GB、TB甚至PB表示）。'
- en: 'The following diagram is representative of a 32-bit Linux process having a *2:2* VM
    split (in gigabytes); that is, the total 4 GB process VAS is split into 2 GB of
    user space and 2 GB of kernel-space. This is often the typical VM split on an
    ARM-32 system running the Linux OS:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表代表了运行Linux操作系统的ARM-32系统上具有*2:2* VM分割（以GB为单位）的32位Linux进程；即，总共4GB的进程VAS被分割为2GB的用户空间和2GB的内核空间。这通常是运行Linux操作系统的ARM-32系统上的典型VM分割。
- en: '![](img/ece3c732-866d-41d8-af9b-ac0a48e4d774.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ece3c732-866d-41d8-af9b-ac0a48e4d774.png)'
- en: 'Figure 7.1 – User:Kernel :: 2:2 GB VM split on an ARM-32 system running Linux'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '图7.1- User:Kernel :: 2:2 GB VM split on an ARM-32 system running Linux'
- en: 'So, now that the kernel VAS is within the box, it''s suddenly clear and critical
    to understand this: when a user-mode process or thread issues a system call, there
    is a context switch to the kernel''s 2 GB VAS (various CPU registers, including
    the stack pointer, get updated) within the very same process''s VAS. The thread
    issuing the system call now runs its kernel code in process context in privileged
    kernel mode (and works on kernel-space data). When done, it returns from the system
    call, context switching back into unprivileged user mode, and is now running user-mode
    code within the first 2 GB VAS.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在内核VAS在盒子内，突然清楚并且至关重要的是要理解这一点：当用户模式进程或线程发出系统调用时，会发生上下文切换到内核的2GB VAS（包括各种CPU寄存器，包括堆栈指针在内的寄存器会得到更新），在同一个进程的VAS内。发出系统调用的线程现在以特权内核模式在进程上下文中运行其内核代码（并且处理内核空间数据）。完成后，它从系统调用返回，上下文切换回非特权用户模式，并且现在在第一个2GB
    VAS内运行用户模式代码。
- en: The exact virtual address where the kernel VAS – also known as the **kernel
    segment*** –* begins is typically represented via the `PAGE_OFFSET` macro within
    the kernel. We will examine this, and some other key macros as well, in the *Macros
    and variables describing the kernel segment layout *section.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 内核VAS的确切虚拟地址-也称为**内核段**-通常通过内核中的`PAGE_OFFSET`宏表示。我们将在*描述内核段布局的宏和变量*部分中进一步研究这一点，以及其他一些关键的宏。
- en: 'Where is this decision regarding the precise location and size of the VM split
    taken? Ah, on 32-bit Linux, it''s a kernel build-time configurable. It''s done
    within the kernel build as part of the `make [ARCH=xxx] menuconfig` procedure
    – for example, when configuring the kernel for a Broadcom BCM2835 (or the BCM2837)
    **System on Chip** (**SoC**) (the Raspberry Pi being a popular board with this
    very SoC). Here''s a snippet from the official kernel configuration file (the
    output is from the Raspberry Pi console):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 关于VM分割的确切位置和大小的决定是在哪里做出的呢？啊，在32位Linux上，这是一个内核构建时可配置的。它是在内核构建中作为`make [ARCH=xxx]
    menuconfig`过程的一部分完成的-例如，当为Broadcom BCM2835（或BCM2837）**SoC**（Raspberry Pi是一个搭载这个SoC的热门开发板）配置内核时。以下是来自官方内核配置文件的片段（输出来自Raspberry
    Pi控制台）：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As seen in the preceding snippet, the `CONFIG_VMSPLIT_2G` kernel config option
    is set to `y` implying that the default VM split is `user:kernel :: 2:2`. For
    32-bit architectures, the VM split location is **tunable** (as can be seen in
    the preceding snippet, `CONFIG_VMSPLIT_[1|2|3]G`; `CONFIG_PAGE_OFFSET` gets set
    accordingly). With a 2:2 VM split, `PAGE_OFFSET` is literally halfway, at the
    virtual address `0x8000 0000` (2 GB)!'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '如前面的片段所示，`CONFIG_VMSPLIT_2G`内核配置选项设置为`y`，意味着默认的VM分割是`user:kernel :: 2:2`。对于32位架构，VM分割位置是**可调整的**（如前面的片段中所示，`CONFIG_VMSPLIT_[1|2|3]G`；`CONFIG_PAGE_OFFSET`相应地设置）。对于2:2的VM分割，`PAGE_OFFSET`实际上是在虚拟地址`0x8000
    0000`（2GB）的中间位置！'
- en: The default VM split for the IA-32 processor (the Intel x86-32) is 3:1 (GB).
    Interestingly, the (ancient) Windows 3.x OS running on the IA-32 had the same
    VM split, showing that these concepts are essentially OS-agnostic. Later in this
    chapter, we will cover several more architectures and their VM split, in addition
    to other details.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: IA-32处理器（Intel x86-32）的默认VM分割是3:1（GB）。有趣的是，运行在IA-32上的（古老的）Windows 3.x操作系统具有相同的VM分割，这表明这些概念基本上与操作系统无关。在本章的后面，我们将涵盖几种更多的架构及其VM分割，以及其他细节。
- en: Configuring the VM split is not directly possible for 64-bit architectures.
    So, now that we understand the VM split on 32-bit systems, let's now move on to
    examining how it's done on 64-bit systems.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 无法直接为64位架构配置VM分割。因此，现在我们了解了32位系统上的VM分割，让我们继续研究如何在64位系统上进行VM分割。
- en: VM split on 64-bit Linux systems
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 64位Linux系统上的VM分割
- en: First off, it is worth noting that on 64-bit systems, all 64 bits are not used
    for addressing. On a standard or typical Linux OS configuration for the x86_64
    with a (typical) 4 KB page size, we use (the **Least Significant Bit** (**LSB**))
    48 bits for addressing. Why not the full 64 bits? It's simply too much! No existing
    computer comes close to having even half of the full *2**^(64)** = 18,446,744,073,709,551,616* bytes,
    which is equivalent to 16 EB (that's 16,384 petabytes) of RAM!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 首先值得注意的是，在64位系统上，并非所有64位都用于寻址。在标准或典型的x86_64 Linux OS配置中，使用（**最低有效位**（**LSB**））48位进行寻址。为什么不使用全部64位？因为太多了！没有现有的计算机接近拥有甚至一半的完整*2**^(64)** = 18,446,744,073,709,551,616* 字节，相当于16
    EB（即16,384 PB）的RAM！
- en: '"Why," you might well wonder, "do we equate this with RAM?". Please read on
    – more material needs to be covered before this becomes clear. The *Examining
    the kernel segment* section  is where you will understand this fully.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: “为什么”，您可能会想，“我们为什么将其等同于RAM？”。请继续阅读 - 在此变得清晰之前，需要涵盖更多内容。在*检查内核段*部分，您将完全理解这一点。
- en: Virtual addressing and address translation
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟寻址和地址转换
- en: Before diving further into these details, it's very important to clearly understand
    a few key points.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步深入了解这些细节之前，非常重要的是清楚地理解一些关键点。
- en: 'Consider a small and typical code snippet from a C program:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑来自C程序的一个小而典型的代码片段：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The address you see the `printf()` emit is a virtual address and not a physical
    one. We distinguish between two kinds of virtual addresses:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 您看到`printf()`发出的地址是虚拟地址而不是物理地址。我们区分两种虚拟地址：
- en: If you run this code in a user space process, the address of variable `i` that
    you will see is a UVA.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在用户空间进程中运行此代码，您将看到变量`i`的地址是UVA。
- en: If you run this code within the kernel, or a kernel module (of course, you'd
    then use the `printk()` API), the address of variable `i` you will see is a **Kernel
    Virtual Address** (**KVA**).
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在内核中运行此代码，或者在内核模块中运行此代码（当然，您将使用`printk()` API），您将看到变量`i`的地址是**内核虚拟地址**（**KVA**）。
- en: 'Next, a virtual address is not an absolute value (an offset from `0`); it''s
    actually a *bitmask*:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，虚拟地址不是绝对值（相对于`0`的偏移量）；它实际上是*位掩码*：
- en: On a 32-bit Linux OS, the 32 available bits are divided into what's called the
    **Page Global Directory** (**PGD**) value, the **Page Table** (**PT**) value,
    and the offset.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在32位Linux操作系统上，32个可用位被分为**页全局目录**（**PGD**）值，**页表**（**PT**）值和偏移量。
- en: These become indices via which the **MMU** (the **Memory Management Unit** that's
    within the silicon of modern microprocessors), with access to the kernel page
    tables for the current process context, performs address translation.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些成为**MMU**（现代微处理器硅片内部的**内存管理单元**）进行地址转换的索引。
- en: We do not intend on covering the deep details on MMU-level address translation
    here. It's also very arch-specific. Do refer to the *Further reading* section
    for useful links on this topic.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不打算在这里详细介绍MMU级别的地址转换。这也非常与架构相关。请参考*进一步阅读*部分，了解有关此主题的有用链接。
- en: As might be expected, on a 64-bit system, even with 48-bit addressing, there
    will be more fields within the virtual address bitmask.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如预期的那样，在64位系统上，即使使用48位寻址，虚拟地址位掩码中将有更多字段。
- en: 'Okay, if this 48-bit addressing is the typical case on the x86_64 processor,
    then how are the bits in a 64-bit virtual address laid out? What happens to the
    unused 16 MSB bits? The following figure answers the question; it''s a representation
    of the breakup of a virtual address on an x86_64 Linux system:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，如果这种48位寻址是x86_64处理器上的典型情况，那么64位虚拟地址中的位是如何布局的？未使用的16位MSB会发生什么？以下图解答了这个问题；这是x86_64
    Linux系统上虚拟地址的分解表示：
- en: '![](img/dddfab92-3acb-47e2-bcb9-110da524f813.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dddfab92-3acb-47e2-bcb9-110da524f813.png)'
- en: Figure 7.2 – Breakup of a 64-bit virtual address on the Intel x86_64 processor
    with 4 KB pages
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 - 在具有4 KB页面的Intel x86_64处理器上分解64位虚拟地址
- en: 'Essentially, with 48-bit addressing, we use bits 0 to 47 (the LSB 48 bits)
    and ignore the **Most Significant Bit** (**MSB**) 16 bits, treating it much as
    a sign extension. Not so fast though; the value of the unused sign-extended MSB
    16 bits varies with the address space you are in:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，使用48位寻址，我们使用0到47位（LSB 48位）并忽略**最高有效位**（**MSB**）的16位，将其视为符号扩展。不过，未使用的符号扩展MSB
    16位的值随着您所在的地址空间而变化：
- en: '**Kernel VAS**: MSB 16 bits are always set to `1`.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内核VAS**：MSB 16位始终设置为`1`。'
- en: '**User VAS**: MSB 16 bits are always set to `0`.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户VAS**：MSB 16位始终设置为`0`。'
- en: 'This is useful information! Knowing this, by merely looking at a (full 64-bit)
    virtual address, you can therefore tell whether it''s a KVA or a UVA:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这是有用的信息！知道这一点，仅通过查看（完整的64位）虚拟地址，您因此可以判断它是KVA还是UVA：
- en: KVAs on a 64-bit Linux system always follow the format `0xffff .... .... ....`.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 64位Linux系统上的KVA始终遵循格式`0xffff .... .... ....`。
- en: UVAs always have the format `0x0000 .... .... ....`.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: UVA始终具有格式`0x0000 .... .... ....`。
- en: '**A word of caution**: the preceding format holds true only for processors
    (MMUs, really) that self-define virtual addresses as being KVAs or UVAs; the x86
    and ARM family of processors do fall in this bracket.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告**：前面的格式仅适用于将虚拟地址自定义为KVA或UVA的处理器（实际上是MMU）； x86和ARM系列处理器属于这一范畴。'
- en: As can now be seen (and I reiterate here), the reality is that virtual addresses
    are not absolute addresses (absolute offsets from zero, as you might have mistakenly
    imagined) but are actually bitmasks. The fact is that memory management is a complex
    area where the work is shared:**the OS is in charge of creating and manipulating
    the paging tables of each process, the toolchain (compiler) generates virtual
    addresses, and it's the processor MMU that actually performs runtime address translation,
    translating a given (user or kernel) virtual address to a physical (RAM) address!**
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以看到（我在这里重申），事实是虚拟地址不是绝对地址（绝对偏移量从零开始，正如你可能错误地想象的那样），而是实际上是位掩码。事实上，内存管理是一个复杂的领域，工作是共享的：**操作系统负责创建和操作每个进程的分页表，工具链（编译器）生成虚拟地址，而处理器MMU实际上执行运行时地址转换，将给定的（用户或内核）虚拟地址转换为物理（RAM）地址！**
- en: We will not delve into further details regarding hardware paging (and various
    hardware acceleration technologies, such as the **Translation Lookaside Buffer**
    (**TLB**) and CPU caches) in this book. This particular topic is well covered
    by various other excellent books and reference sites that are mentioned in the *Further
    reading *section of this chapter.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在本书中深入讨论硬件分页（以及各种硬件加速技术，如**转换旁路缓冲**（**TLB**）和CPU缓存）。这个特定的主题已经被其他一些优秀的书籍和参考网站很好地涵盖，这些书籍和网站在本章的*进一步阅读*部分中提到。
- en: 'Back to the VAS on a 64-bit processor. The available VAS on a 64-bit system
    is a simply gigantic *2**^(64) **= 16 EB* (*16 x 10**^(18)* bytes!). The story
    goes that when AMD engineers were first porting the Linux kernel to the x86_64
    (or AMD64) 64-bit processor, they would have had to decide how to lay out the
    process and kernel segments within this enormous VAS. The decision reached has
    more or less remained identical, even on today''s x86_64 Linux OS. This enormous
    64-bit VAS is split as follows. Here, we assume 48-bit addressing with a 4 KB
    page size:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 回到64位处理器上的VAS。64位系统上可用的VAS是一个巨大的*2**^(64) **= 16 EB*（*16 x 10**^(18)*字节！）。故事是这样的，当AMD工程师首次将Linux内核移植到x86_64（或AMD64）64位处理器时，他们必须决定如何在这个巨大的VAS中布置进程和内核段。即使在今天的x86_64
    Linux操作系统上，这个巨大的64位VAS的划分基本上保持不变。这个巨大的64位VAS划分如下。在这里，我们假设48位寻址和4 KB页面大小：
- en: 'Canonical lower half, for 128 TB: User VAS and virtual address ranges from `0x0` to `0x0000
    7fff ffff ffff`'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规范的下半部分，128 TB：用户VAS和虚拟地址范围从`0x0`到`0x0000 7fff ffff ffff`
- en: 'Canonical upper half, for 128 TB: Kernel VAS and virtual address ranges from `0xffff
    8000 0000 0000` to `0xffff ffff ffff ffff`'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规范的上半部分，128 TB：内核VAS和虚拟地址范围从`0xffff 8000 0000 0000`到`0xffff ffff ffff ffff`
- en: The word *canonical* effectively means *as per the law* or as *per common convention*.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*规范*这个词实际上意味着*根据法律*或*根据共同惯例*。'
- en: 'This 64-bit VM split on an x86_64 platform can be seen in the following figure:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在x86_64平台上可以看到这个64位VM分割，如下图所示：
- en: '![](img/8868926d-f362-4c96-9cad-544cfd2b7a4f.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8868926d-f362-4c96-9cad-544cfd2b7a4f.png)'
- en: 'Figure 7.3 – The Intel x86_64 (or AMD64) 16 EB VAS layout (48-bit addressing);
    VM split is User : Kernel :: 128 TB : 128 TB'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '图7.3 - Intel x86_64（或AMD64）16 EB VAS布局（48位寻址）；VM分割是用户：内核:: 128 TB：128 TB'
- en: In the preceding figure,the in-between unused region – a hole or sparse region
    – is also called the **non-canonical addresses** region. Interestingly, with the
    48-bit addressing scheme, the vast majority of the VAS is left unused. This is
    why we term the VAS as being very sparse.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，中间未使用的区域 - 空洞或稀疏区域 - 也称为**非规范地址**区域。有趣的是，使用48位寻址方案，绝大多数VAS都未被使用。这就是为什么我们称VAS非常稀疏。
- en: The preceding figure is certainly not drawn to scale! Always keep in mind that
    this is all *virtual* memory space, not physical.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 上图显然不是按比例绘制的！请记住，这一切都是*虚拟*内存空间，而不是物理内存。
- en: 'To round off our discussion on the VM split, some common `user:kernel` VM split
    ratios for different CPU architectures are shown in the following figure (we assume
    an MMU page size of 4 KB):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了结束我们对VM分割的讨论，以下图表显示了不同CPU架构的一些常见`用户：内核`VM分割比例（我们假设MMU页面大小为4 KB）：
- en: '![](img/dae425dd-f5cb-4492-a14b-ad846c342920.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dae425dd-f5cb-4492-a14b-ad846c342920.png)'
- en: Figure 7.4 – Common user:kernel VM split ratios for different CPU architectures
    (for 4 KB page size)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 - 不同CPU架构的常见用户：内核VM分割比例（4 KB页面大小）
- en: 'We highlight the third row in bold red as it''s considered the common case:
    running Linux on the x86_64 (or AMD64) architecture, with a `user:kernel :: 128
    TB:128 TB` VM split. Also, be careful when reading the table: the numbers in the
    sixth and eighth columns, End vaddr, are single 64-bit quantities each and not
    two numbers. The number may have simply wrapped around. So, for example, in the
    x86_64 row, column 6, it''s the *single* number `0x0000 7fff ffff ffff` and not
    two numbers.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '我们用粗体红色突出显示第三行，因为它被认为是常见情况：在x86_64（或AMD64）架构上运行Linux，使用`用户：内核:: 128 TB:128
    TB` VM分割。在阅读表格时要小心：第六列和第八列的数字，结束vaddr，每个都是单个64位数量，而不是两个数字。数字可能只是简单地绕回去了。因此，例如，在x86_64行中，第6列是*单个*数字`0x0000
    7fff ffff ffff`而不是两个数字。'
- en: The third column, Addr Bits, shows us that, on 64-bit processors, no real-world
    processor actually uses all 64 bits for addressing.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第三列，地址位，告诉我们，在64位处理器上，实际上没有真正的处理器使用所有64位进行寻址。
- en: 'Under the x86_64, there are two VM splits shown in the preceding table:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在x86_64下，上表显示了两个VM分割：
- en: 'The first one, 128 TB : 128 TB (4-level paging) is the typical VM split being
    used on Linux x86_64-bit systems as of today (embedded laptops, PCs, workstations,
    and servers). It limits thephysical address space to 64 TB (of RAM).'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个，128 TB：128 TB（4级分页）是今天在Linux x86_64位系统上使用的典型VM分割（嵌入式笔记本电脑，个人电脑，工作站和服务器）。它将物理地址空间限制为64
    TB（RAM）。
- en: 'The second one, 64 PB : 64 PB, is, as of the time of writing at least, still
    purely theoretical; it comes with support for what is called 5-level paging from
    4.14 Linux; the assigned VASes (56-bit addressing; a total of 128 petabytes of
    VAS and 4 PB of physical address space!) is so enormous that, as of the time of
    writing, no actual computer is (yet) using it.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个，64 PB：64 PB，截至目前为止，仍然纯理论；它支持所谓的5级分页，从4.14版Linux开始；分配的VAS（56位寻址；总共128PB的VAS和4PB的物理地址空间！）是如此巨大，以至于截至目前为止，没有实际的计算机（尚未）使用它。
- en: Note that the two rows for the AArch64 (ARM-64) architecture running on Linux
    are merely representative. The BSP vendor or platform team working on the product
    could well use differing splits. As an interesting aside, the VM split on the
    (old) Windows 32-bit OS is 2:2 (GB).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，运行在Linux上的AArch64（ARM-64）架构的两行仅仅是代表性的。正在开发产品的BSP供应商或平台团队可能会使用不同的分割。有趣的是，（旧）Windows
    32位操作系统上的VM分割是2:2（GB）。
- en: What's actually residing within the kernel VAS, or as it's commonly called,
    the kernel segment? All kernel code, data structures (including the task structures,
    the lists, the kernel-mode stacks, paging tables, and so on), device drivers,
    kernel modules, and so on are within here (as the lower half of *Figure 6.7* in [Chapter
    6](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml), *Kernel Internals Essentials –
    Processes and Threads*, showed; we cover precisely this in some detail in the
    *Understanding* *the kernel segment* section).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上驻留在内核VAS中的是什么，或者通常所说的内核段？所有内核代码、数据结构（包括任务结构、列表、内核模式堆栈、分页表等等）、设备驱动程序、内核模块等等都在这里（正如[第6章](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml)中*内核内部要点
    - 进程和线程*的*图6.7*的下半部分所显示的；我们在*理解内核段*部分中详细介绍了这一点）。
- en: It's important to realize that, as a performance optimization on Linux, kernel
    memory is always non-swappable; that is, kernel memory can never be paged out
    to a swap partition. User space memory pages are always candidates for paging,
    unless locked (see the `mlock[all](2)` system calls).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要意识到，在Linux上，作为性能优化，内核内存始终是不可交换的；也就是说，内核内存永远不会被换出到交换分区。用户空间内存页总是可以进行分页，除非被锁定（参见`mlock[all](2)`系统调用）。
- en: With this background, you're now in a position to understand the full process
    VAS layout. Read on.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个背景，您现在可以理解完整的进程VAS布局。继续阅读。
- en: The process VAS – the full view
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进程VAS - 完整视图
- en: 'Once again, refer to *Figure 7.1*; it shows the actual process VAS layout for
    a single 32-bit process. The reality, of course – and this is key – is that **all
    processes alive on the system have their own unique user-mode VAS but share the
    same kernel segment***.* For some contrast from *Figure 7.1*, which showed a 2:2
    (GB) VM split, the following figure shows the actual situation for a typical IA-32
    system, with a 3:1 (GB) VM split:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 再次参考*图7.1*；它显示了单个32位进程的实际进程VAS布局。当然，现实情况是 - 这是关键的 - **系统上所有活动的进程都有自己独特的用户模式VAS，但共享相同的内核段**。与*图7.1*形成对比的是，它显示了2:2（GB）的VM分割，下图显示了典型IA-32系统的实际情况，其中有3:1（GB）的VM分割：
- en: '![](img/1302b8d9-478a-494d-b527-6556b8cc5c78.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1302b8d9-478a-494d-b527-6556b8cc5c78.png)'
- en: Figure 7.5 – Processes have a unique user VAS but share the kernel segment (32-bit
    OS); IA-32 with a 3:1 VM split
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 - 进程具有独特的用户VAS，但共享内核段（32位操作系统）；IA-32的VM分割为3:1
- en: Notice in the preceding figure how the address space reflects a 3:1 (GB) VM
    split. The user address space extends from `0` to `0xbfff ffff` (`0xc000 0000` is
    the 3 GB mark; this is what the `PAGE_OFFSET` macro is set to), and the kernel
    VAS extends from `0xc000 0000` (3 GB) to `0xffff ffff` (4 GB).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的图中，地址空间反映了3:1（GB）的VM分割。用户地址空间从`0`扩展到`0xbfff ffff`（`0xc000 0000`是3GB标记；这是`PAGE_OFFSET`宏的设置），内核VAS从`0xc000
    0000`（3GB）扩展到`0xffff ffff`（4GB）。
- en: Later in this chapter, we will cover the usage of a useful utility called `procmap`. It
    will help you literally visualize the VASes, both kernel and user VASes, in detail,
    similar to how our preceding diagrams have been showing.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面，我们将介绍一个有用的实用程序`procmap`的用法。它将帮助您详细可视化VAS，包括内核和用户VAS，类似于我们之前的图表所显示的方式。
- en: 'A few things to note:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的几点：
- en: For the example shown in Figure 7.5, the value of `PAGE_OFFSET` is `0xc000 0000`.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图7.5中显示的示例中，`PAGE_OFFSET`的值为`0xc000 0000`。
- en: The figures and numbers we have shown here are not absolute and binding across
    all architectures; they tend to be very arch-specific and many highly vendor-customized
    Linux systems may change them.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们在这里展示的图表和数字并不是所有架构上的绝对和约束性的；它们往往是非常特定于架构的，许多高度定制的Linux系统可能会改变它们。
- en: '*Figure 7.5* details the VM layout on a 32-bit Linux OS. On 64-bit Linux, the *concepts *remain
    identical, it''s just the numbers that (significantly) change. As shown in some
    detail in the preceding sections, the VM split on an x86_64 (with 48-bit addressing) Linux
    system becomes `User : Kernel :: 128 TB : 128 TB`.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图7.5*详细介绍了32位Linux操作系统上的VM布局。在64位Linux上，*概念*保持不变，只是数字（显著）变化。正如前面的章节中所详细介绍的，x86_64（带48位寻址）Linux系统上的VM分割变为`User：Kernel
    :: 128 TB：128 TB`。'
- en: Now that the fundamentals of the virtual memory layout of a process are understood,
    you will find that it greatly helps in deciphering and making progress in difficult-to-debug
    situations. As usual, there's still more to it; sections follow on the user space
    and kernel-space memory map (the kernel segment), and some coverage on the physical
    memory map as well. Read on!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，一旦理解了进程的虚拟内存布局的基本原理，您会发现它在解密和在难以调试的情况下取得进展方面非常有帮助。像往常一样，还有更多内容；接下来的部分将介绍用户空间和内核空间内存映射（内核段），以及一些关于物理内存映射的内容。继续阅读！
- en: Examining the process VAS
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查进程VAS
- en: We have already covered the layout – the segments or mappings – that every process's
    VAS is made up of (see the *Understanding the basics of* *the process **Virtual
    Address Space (VAS)* section in [Chapter 6](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml)*,* *Kernel
    Internals Essentials – Processes and Threads*). We learned that the process VAS
    consists of various mappings or segments, and among them are text (code), data
    segments, library mappings, and at least one stack. Here, we expand greatly on
    that discussion.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了每个进程VAS由哪些段或映射组成（参见[第6章](e13fb379-a77f-4ba5-9de6-d6707b0214e6.xhtml)中的*理解进程虚拟地址空间（VAS）基础知识*部分）。我们了解到进程VAS包括各种映射或段，其中包括文本（代码）、数据段、库映射，以及至少一个堆栈。在这里，我们将对此进行更详细的讨论。
- en: Being able to dive deep into the kernel and see various runtime values is an
    important skill for a developer like you, as well as the user, QA, sysadmin, DevOps,
    and so on. The Linux kernel provides us with an amazing interface to do precisely
    this – it's, you guessed it, the `proc` filesystem (`procfs`).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 能够深入内核并查看各种运行时值是开发人员像您这样的重要技能，以及用户、QA、系统管理员、DevOps等。Linux内核为我们提供了一个令人惊叹的接口来做到这一点
    - 这就是，你猜对了，`proc`文件系统（`procfs`）。
- en: 'This is always present on Linux (at least it should be) and is mounted under `/proc`*.* The `procfs`system
    has two primary jobs:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这在Linux上始终存在（至少应该存在），并且挂载在`/proc`下。`procfs`系统有两个主要作用：
- en: To provide a unified set of (pseudo or virtual) files and directories, enabling
    you to look deep into the kernel and hardware internal details.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供一组统一的（伪或虚拟）文件和目录，使您能够深入了解内核和硬件的内部细节。
- en: To provide a unified set of root-writeable files, allowing the sysad to modify
    key kernel parameters. These are present under `/proc/sys/` and are termed `sysctl` – they
    are the tuning knobs of the Linux kernel.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供一组统一的可写根文件，允许系统管理员修改关键的内核参数。这些文件位于`/proc/sys/`下，并被称为`sysctl` - 它们是Linux内核的调整旋钮。
- en: Familiarity with the `proc` filesystem is indeed a must. I urge you to check
    it out, and read the excellent man page on `proc(5)` as well. For example, simply
    doing `cat /proc/PID/status` (where `PID` is, of course, the unique process identifier
    of a given process or thread) yields a whole bunch of useful details from the
    process or thread's task structure!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉`proc`文件系统确实是必须的。我建议您查看一下，并阅读关于`proc(5)`的优秀手册页。例如，简单地执行`cat /proc/PID/status`（其中`PID`当然是给定进程或线程的唯一进程标识符）会产生一大堆有用的进程或线程任务结构的细节！
- en: Conceptually similar to `procfs` is the `sysfs` filesystem, mounted under `/sys` (and
    under it `debugfs`*,* typicallymounted at `/sys/kernel/debug`). `sysfs` is a representation
    of 2.6 Linux's new device and driver model; it exposes a tree of all devices on
    the system, as well as several kernel-tuning knobs.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在概念上类似于`procfs`的是`sysfs`文件系统，它挂载在`/sys`下（在其下是`debugfs`，通常挂载在`/sys/kernel/debug`）。`sysfs`是2.6
    Linux新设备和驱动程序模型的表示；它公开了系统上所有设备的树形结构，以及几个内核调整旋钮。
- en: Examining the user VAS in detail
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 详细检查用户VAS
- en: 'Let''s begin by checking out the user VAS of any given process. A pretty detailed
    map of the user VAS is made available via `procfs`, particularly via the `/proc/PID/maps`
    pseudo-file. Let''s learn how to use this interface to peek into a process''s
    user space memory map. We will see two ways:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从检查任何给定进程的用户VAS开始。用户VAS的相当详细的映射可以通过`procfs`获得，特别是通过`/proc/PID/maps`伪文件。让我们学习如何使用这个接口来窥视进程的用户空间内存映射。我们将看到两种方法：
- en: Directly via the `procfs` interface's `/proc/PID/maps` pseudo-file
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 直接通过`procfs`接口的`/proc/PID/maps`伪文件
- en: Using a few useful frontends (making the output more human-digestible)
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一些有用的前端（使输出更易于理解）
- en: Let's start with the first one.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一个开始。
- en: Directly viewing the process memory map using procfs
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 直接使用procfs查看进程内存映射
- en: 'Looking up the internal process details of any arbitrary process does require root access,
    whereas looking up details of a process under your ownership (including the caller
    process itself) does not. So, as a simple example, we will look up the calling
    process''s VAS by using the `self` keyword in place of the PID. The following
    screenshot shows this (on an x86_64 Ubuntu 18.04 LTS guest):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 查找任意进程的内部进程细节需要`root`访问权限，而查找自己拥有的进程的细节（包括调用进程本身）则不需要。因此，举个简单的例子，我们将使用`self`关键字来查找调用进程的VAS，而不是PID。以下屏幕截图显示了这一点（在x86_64
    Ubuntu 18.04 LTS客户机上）：
- en: '![](img/f77f3aa4-e443-40d4-8b06-25d1ad178bed.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f77f3aa4-e443-40d4-8b06-25d1ad178bed.png)'
- en: Figure 7.6 – Output of the cat /proc/self/maps command
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 - cat /proc/self/maps命令的输出
- en: In the preceding screenshot, you can actually see the user VAS of the `cat` process
    – a veritable memory map of the user VAS of that process! Also, notice that the
    preceding `procfs` output is sorted in ascending order by (user) virtual address
    (UVA).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的屏幕截图中，您实际上可以看到`cat`进程的用户VAS - 该进程的用户VAS的实际内存映射！还要注意，前面的`procfs`输出是按（用户）虚拟地址（UVA）升序排序的。
- en: Familiarity with using the powerful `mmap(2)` system call will help greatly
    in understanding further discussions. Do (at least) browse through its man page.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉使用强大的`mmap(2)`系统调用将有助于更好地理解后续的讨论。至少要浏览一下它的手册页。
- en: Interpreting the /proc/PID/maps output
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解释/proc/PID/maps输出
- en: To interpret the output of Figure 7.6, read it one line at a time. **Each line
    represents a segment or mapping of the user-mode VAS** of the process in question
    (in the preceding example, it's of the `cat` process). Each line consists of the
    following fields.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 要解释图7.6的输出，请逐行阅读。**每行代表了进程的用户模式VAS的一个段或映射**（在前面的示例中，是`cat`进程的）。每行包括以下字段。
- en: 'To make it easier, I will show just a single line of output whose fields we
    will label and refer to in the following notes:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更容易，我将只展示一行输出，我们将在接下来的注释中标记并引用这些字段：
- en: '[PRE4]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, the entire line represents a segment, or more correctly, a *mapping* within
    the process (user) VAS. `uva` is the user virtual address. `start_uva` and `end_uva`
    for each segment are displayed as the first two fields (or columns). Thus, the
    length of the mapping (segment) is easily calculated (`end_uva`–`start_uva` bytes).
    Thus, in the preceding line, `start_uva` is `0x555d83b65000` and `end_uva` is
    `0x555d83b6d000` (and the length can be calculated to be 32 KB); but, what is
    this segment? Do read on...
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，整行表示进程（用户）VAS中的一个段，或更正确地说，是一个*映射*。`uva`是用户虚拟地址。每个段的`start_uva`和`end_uva`显示为前两个字段（或列）。因此，映射（段）的长度可以轻松计算（`end_uva`-`start_uva`字节）。因此，在前面的行中，`start_uva`是`0x555d83b65000`，`end_uva`是`0x555d83b6d000`（长度可以计算为32
    KB）；但是，这个段是什么？请继续阅读...
- en: 'The third field, `r-xp`, is actually a combination of two pieces of information:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个字段`r-xp`实际上是两个信息的组合：
- en: The first three letters represent the mode (permissions) of the segment (in
    the usual `rwx` notation).
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前三个字母表示段（通常以`rwx`表示）的模式（权限）。
- en: The next letter represents whether the mapping is a private one (`p`) or a shared
    one (`s`). Internally, this is set up by the fourth parameter to the `mmap(2)` system
    call, `flags`; it's really **the** `mmap(2)` **system call that is internally
    responsible for creating every segment or mapping within a process!**
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个字母表示映射是私有的（`p`）还是共享的（`s`）。在内部，这是由`mmap(2)`系统调用的第四个参数`flags`设置的；实际上是**`mmap(2)`**系统调用在内部负责创建进程中的每个段或映射！
- en: So, for the preceding sample segment shown, the third field being the value
    `r-xp`, we can now tell it's a text (code) segment and is a private mapping (as
    expected).
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，对于前面显示的示例段，第三个字段的值为`r-xp`，我们现在可以知道它是一个文本（代码）段，并且是一个私有映射（如预期的那样）。
- en: The fourth field `start-off` (here, it's the value `0`) is the start offset
    from the beginning of the file whose contents has been mapped into the process
    VAS. Obviously, this value is only valid for file mappings. You can tell whether
    the current segment is a file mapping by glancing at the penultimate (sixth) field.
    For mappings that are not file-mapped – called **anonymous mappings** – it's always
    `0` (examples would be the mappings representing the heap or stack segments).
    In our preceding example line, it's a file mapping (that of `/bin/cat`) and the
    offset from the beginning of that file is `0` bytes (the length of the mapping,
    as we calculated in the preceding paragraph, is 32 KB).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个字段`start-off`（这里是值`0`）是从已映射到进程VAS的文件开头的起始偏移量。显然，此值仅对文件映射有效。您可以通过查看倒数第二个（第六个）字段来判断当前段是否是文件映射。对于不是文件映射的映射
    - 称为**匿名映射** - 它始终为`0`（例如表示堆或栈段的映射）。在我们之前的示例行中，这是一个文件映射（`/bin/cat`），从该文件开头的偏移量为`0`字节（如我们在前一段中计算的映射长度为32
    KB）。
- en: The fifth field (`08:01`) is of the form `mj:mn`, where `mj` is the major number
    and `mn` is the minor number of the device file where the image resides. Similar
    to the fourth field, it's only valid for file mappings, else it's simply shown
    as `00:00`; in our preceding example line, it's a file mapping (that of `/bin/cat`),
    and the major and minor numbers (of the *device* that the file resides on) are `8` and `1`,
    respectively.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 第五个字段（`08:01`）的格式为`mj:mn`，其中`mj`是设备文件的主编号，`mn`是映像所在设备文件的次编号。与第四个字段类似，它仅对文件映射有效，否则显示为`00:00`；在我们之前的示例行中，这是一个文件映射（`/bin/cat`），设备文件的主编号和次编号（文件所在的*设备*）分别为`8`和`1`。
- en: 'The sixth field (`524313`) represents the inode number of the image file –
    the file whose contents are being mapped into the process VAS. The inode is the
    key data structure of the **VFS (Virtual FileSystem)**; it holds all metadata
    of the file object, everything except for its name (which is in the directory
    file). Again, this value is only valid for file mappings and simply shows as `0`
    otherwise. This is, in fact, a quick way to tell whether the mapping is file-mapped
    or an anonymous mapping! In our preceding example mapping, clearly it''s a file
    mapping (that of `/bin/cat`), and the inode number is `524313`. Indeed, we can
    confirm this:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 第六个字段（`524313`）表示映像文件的索引节点号 - 正在映射到进程VAS的文件的内容。索引节点是**VFS（虚拟文件系统）**的关键数据结构；它保存文件对象的所有元数据，除了其名称（名称在目录文件中）。同样，此值仅对文件映射有效，否则显示为`0`。实际上，这是一种快速判断映射是文件映射还是匿名映射的方法！在我们之前的示例映射中，显然是文件映射（`/bin/cat`），索引节点号是`524313`。事实上，我们可以确认：
- en: '[PRE5]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The seventh and last field represents the pathname of the file whose contents
    are being mapped into the user VAS. Here, as we're viewing the memory map of the `cat(1)` process,
    the pathname (for the file-mapped segments) is `/bin/cat`, of course. If the mapping
    represents a file, the file's inode number (the sixth field) shows up as a positive
    quantity; if not – meaning it's a pure memory or anonymous mapping with no backing
    store – the inode number shows up as `0` and this field will be empty.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 第七个和最后一个字段表示正在映射到用户VAS的文件的路径名。在这里，因为我们正在查看`cat(1)`进程的内存映射，路径名（对于文件映射的段）当然是`/bin/cat`。如果映射表示文件，则文件的索引节点号（第六个字段）显示为正值；如果不是
    - 意味着是没有后备存储的纯内存或匿名映射 - 索引节点号显示为`0`，此字段将为空。
- en: 'It should by now be obvious, but we will point this out nevertheless – it is
    a key point: all the preceding addresses seen are virtual, not physical. Furthermore,
    they only belong to user space, hence they are termed UVAs and are always accessed
    (and translated) via the unique paging tables for that process. Also, the preceding
    screenshot was taken on a 64-bit (x86_64) Linux guest. Hence, here, we see 64-bit
    virtual addresses.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应该很明显了，但我们仍然会指出这一点 - 这是一个关键点：前面看到的所有地址都是*虚拟*地址，而不是物理地址。此外，它们仅属于用户空间，因此被称为*UVA*，并且始终通过该进程的唯一分页表访问（和转换）。此外，前面的屏幕截图是在64位（x86_64）Linux客户机上拍摄的。因此，在这里，我们看到64位虚拟地址。
- en: Though the way the virtual addresses are displayed isn't as a full 64-bit number
    – for example, as `0x555d83b65000` and not as `0x0000555d83b65000` – I want you
    to notice how, because it's a **user virtual address** (a **UVA**), the MSB 16
    bits are zero!
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然虚拟地址的显示方式不是完整的64位数字 - 例如，显示为`0x555d83b65000`而不是`0x0000555d83b65000` - 但我希望您注意到，因为它是**用户虚拟地址**（**UVA**），最高16位为零！
- en: Right, that covers how to interpret a particular segment or mapping, but there
    seems to be a few strange ones – the `vvar`, `vdso`, and `vsyscall` mappings.
    Let's see what they mean.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，这涵盖了如何解释特定段或映射，但似乎还有一些奇怪的 - `vvar`，`vdso`和`vsyscall`映射。让我们看看它们的含义。
- en: The vsyscall page
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: vsyscall页面
- en: 'Did you notice something a tad unusual in the output of Figure 7.6? The very
    last line there – the so-called `vsyscall` entry – maps a kernel page (by now,
    you know how we can tell: the MSB 16 bits of its start and end virtual addresses
    are set). Here, we just mention the fact that this is an (old) optimization for
    performing system calls. It works by alleviating the need to actually switch to
    kernel mode for a small subset of syscalls that don''t really need to.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 您是否注意到图7.6的输出中有一些不太寻常的东西？那里的最后一行 - 所谓的`vsyscall`条目 - 映射了一个内核页面（到目前为止，您知道我们如何判断：其起始和结束虚拟地址的最高16位被设置）。在这里，我们只提到这是一个（旧的）用于执行系统调用的优化。它通过减轻对于一小部分不真正需要的系统调用而实际上不需要切换到内核模式来工作。
- en: Currently, on the x86, these include the `gettimeofday(2)`, `time(2)`, and `getcpu(2)`
    system calls. Indeed, the `vvar` and `vdso` (aka vDSO) mappings above it are (slightly)
    modern variations on the same theme. If you are interested in finding out more
    about this, visit the *Further reading *section for this chapter.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，在x86上，这些包括`gettimeofday(2)`，`time(2)`和`getcpu(2)`系统调用。实际上，上面的`vvar`和`vdso`（又名vDSO）映射是同一主题的现代变体。如果您对此感兴趣，可以访问本章的*进一步阅读*部分了解更多信息。
- en: So, you've now seen how to examine the user space memory map of any given process
    by directly reading and interpreting the output of the `/proc/PID/maps` (pseudo)
    file for the process with PID. There are other convenient frontends to do so;
    we'll now check out a few.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您现在已经看到了如何通过直接阅读和解释`/proc/PID/maps`（伪）文件的输出来检查任何给定进程的用户空间内存映射。还有其他方便的前端可以这样做；我们现在将检查一些。
- en: Frontends to view the process memory map
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查看进程内存映射的前端
- en: Besides the raw or direct format via `/proc/PID/maps` (which we saw how to interpret
    in the previous section), there are some wrapper utilities that help us more easily
    interpret the user-mode VAS. Among them are the additional (raw) `/proc/PID/smaps`pseudo-file,
    the `pmap(1)` and `smem(8)` utilities, and my own simple utility (christened `procmap`).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过`/proc/PID/maps`（我们在上一节中看到如何解释）的原始或直接格式外，还有一些包装实用程序可以帮助我们更轻松地解释用户模式VAS。其中包括额外的（原始）`/proc/PID/smaps`伪文件，`pmap(1)`和`smem(8)`实用程序，以及我自己的简单实用程序（名为`procmap`）。
- en: The kernel provides detailed information on each segment or mapping via the `/proc/PID/smaps`pseudo-file
    under `proc`. Do try `cat /proc/self/smaps` to see this for yourself. You will
    notice that for each segment (mapping), a good amount of detail information is
    provided on it. The man page on `proc(5)` helps explain the many fields seen.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 内核通过`/proc/PID/smaps`伪文件在`proc`下提供了每个段或映射的详细信息。尝试`cat /proc/self/smaps`来查看这些信息。您会注意到对于每个段（映射），都提供了大量详细信息。`proc(5)`的man页面有助于解释所见到的许多字段。
- en: 'For both the `pmap(1)` and `smem(8)` utilities, I refer you to the man pages
    on them for details. For example, with `pmap(1)`, the man page informs us of the
    more verbose `-X` and `-XX` options:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`pmap(1)`和`smem(8)`实用程序，我建议您查阅它们的man页面以获取详细信息。例如，对于`pmap(1)`，man页面告诉我们更详细的`-X`和`-XX`选项：
- en: '[PRE6]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Regarding the `smem(8)` utility, the fact is that it does *not* show you the
    process VAS; rather, it''s more about answering an FAQ: namely, ascertaining which
    process is taking up the most physical memory. It uses metrics such as **Resident
    Set Size** (**RSS**), **P****roportional Set Size** (**PSS**), and **U****nique
    Set Size** (**USS**) to throw up a clearer picture. I will leave the further exploration
    of these utilities as an exercise to you, dear reader!'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 关于`smem(8)`实用程序，事实是它*不*显示进程VAS；相反，它更多地是回答一个常见问题：即确定哪个进程占用了最多的物理内存。它使用诸如**Resident
    Set Size**（**RSS**），**Proportional Set Size**（**PSS**）和**Unique Set Size**（**USS**）等指标来呈现更清晰的图片。我将把进一步探索这些实用程序作为一个练习留给您，亲爱的读者！
- en: Now, let's move on to exploring how we can use a useful utility – `procmap`
    – to view in quite a bit of detail both the kernel and user memory map of any
    given process.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续探讨如何使用一个有用的实用程序 - `procmap` - 以相当详细的方式查看任何给定进程的内核和用户内存映射。
- en: The procmap process VAS visualization utility
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: procmap进程VAS可视化实用程序
- en: 'As a small learning and teaching (and helpful during debug!) project, I have
    authored and hosted a small project on GitHub going by the name of `procmap`*,* available
    here: [https://github.com/kaiwan/procmap](https://github.com/kaiwan/procmap) (do
    `git clone` it). A snippet from its `README.md` file helps explain its purpose:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个小型的学习和教学（以及在调试期间有帮助！）项目，我编写并托管了一个名为`procmap`的小型项目，可以在GitHub上找到：[https://github.com/kaiwan/procmap](https://github.com/kaiwan/procmap)（使用`git
    clone`进行克隆）。其`README.md`文件的一部分有助于解释其目的：
- en: '[PRE7]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'An aside: at the time of writing this material (April/May 2020), the COVID-19
    pandemic is in full swing across most of the globe. Similar to the earlier *SETI@home*
    project ([https://setiathome.berkeley.edu/](https://setiathome.berkeley.edu/)),
    the *Folding@home* project ([https://foldingathome.org/category/covid-19/](https://foldingathome.org/category/covid-19/))
    is a distributed computing project that leverages internet-connected home (or
    any) computers to help simulate and solve problems related to COVID-19 treatments
    (among finding cures for several other serious diseases that affect us). You can
    download the software from [https://foldingathome.org/start-folding/](https://foldingathome.org/start-folding/)
    (install it, and it runs during your system''s idle cycles). I did just this;
    here''s the FAH viewer (a nice GUI showing protein molecules!) process running
    on my (native) Ubuntu Linux system:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句：在撰写本材料时（2020年4月/5月），COVID-19大流行席卷全球大部分地区。类似于早期的*SETI@home*项目（[https://setiathome.berkeley.edu/](https://setiathome.berkeley.edu/)），*Folding@home*项目（[https://foldingathome.org/category/covid-19/](https://foldingathome.org/category/covid-19/)）是一个分布式计算项目，利用互联网连接的家用（或任何）计算机来帮助模拟和解决与COVID-19治疗相关的问题（以及找到治愈我们的其他严重疾病）。您可以从[https://foldingathome.org/start-folding/](https://foldingathome.org/start-folding/)下载软件（安装它，并在系统空闲时运行）。我就是这样做的；这是在我的（本机）Ubuntu
    Linux系统上运行的FAH查看器（一个漂亮的GUI显示蛋白质分子！）进程的部分截图：
- en: '[PRE8]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Alright, let''s interrogate its VAS using the `procmap` utility. How do we
    invoke it? Simple, see what follows (due to a lack of space, I won''t show all
    the information, caveats, and more here; do try it out yourself):'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，让我们使用`procmap`实用程序来查询它的VAS。我们如何调用它？简单，看看接下来的内容（由于空间不足，我不会在这里显示所有信息、警告等；请自行尝试）：
- en: '[PRE9]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Do note that this `procmap` utility is not the same as the `procmap` utility
    provided by BSD Unix. Also, it depends upon the `bc(1)` and `smem(8)` utilities;
    please ensure they're installed.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个`procmap`实用程序与BSD Unix提供的`procmap`实用程序不同。它还依赖于`bc(1)`和`smem(8)`实用程序；请确保它们已安装。
- en: When I run the `procmap` utility with only `--pid=<PID>`, it will display both
    the kernel and user space VASes of the given process. Now, as we have not yet
    covered the details regarding the kernel VAS (or segment), I won't show the kernel-space
    detailed output here; let's defer that to the upcoming section, *Examining the
    kernel segment*. As we proceed, you will find partial screenshots of only the
    user VAS output from the `procmap` utility. The complete output can be quite lengthy,
    depending, of course, on the process in question; do try it out for yourself.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当我只使用`--pid=<PID>`运行`procmap`实用程序时，它将显示给定进程的内核和用户空间VAS。现在，由于我们尚未涵盖有关内核VAS（或段）的详细信息，我不会在这里显示内核空间的详细输出；让我们把它推迟到即将到来的部分，*检查内核段*。随着我们的进行，您将发现`procmap`实用程序的部分截图仅显示用户VAS输出。完整的输出可能会相当冗长，当然取决于所涉及的进程；请自行尝试。
- en: 'As you''ll see, it attempts to provide a basic visualization of the complete
    process memory map – both kernel and user space VAS in a vertically tiled format
    (as mentioned, here we just display truncated screenshots):'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您将看到的，它试图以垂直平铺的格式提供完整进程内存映射的基本可视化 – 包括内核和用户空间VAS（如前所述，这里我们只显示截断的截图）：
- en: '![](img/edb272b2-8d74-4973-8c40-3e77350e541a.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/edb272b2-8d74-4973-8c40-3e77350e541a.png)'
- en: 'Figure 7.7 – Partial screenshot: the first line of the kernel VAS output from
    the procmap utility'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 部分截图：从procmap实用程序的内核VAS输出的第一行
- en: 'Notice, from the preceding (partial) screenshot, a few things:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，从前面（部分）截图中，有一些事情：
- en: The `procmap` (Bash) script auto-detects that we're running on an x86_64 64-bit
    system.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`procmap` （Bash）脚本自动检测到我们正在运行的是x86_64 64位系统。'
- en: Though we're not focused on it right now, the output of the kernel VAS appears
    first; this is natural as we show the output ordered by descending virtual address
    (Figures 7.1, 7.3 and 7.5 reiterate this)
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然我们现在不专注于它，但内核VAS的输出首先出现；这是自然的，因为我们按照虚拟地址降序显示输出（图7.1、7.3和7.5重申了这一点）
- en: You can see that the very first line (after the `KERNEL VAS` header) corresponds
    to a KVA at the very top of the VAS – the value `0xffff ffff ffff ffff` (as we're
    on 64-bit).
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以看到第一行（在`KERNEL VAS`标题之后）对应于VAS的顶部 – 值为`0xffff ffff ffff ffff`（因为我们是64位）。
- en: 'Moving on to the next part of the `procmap` output, let''s look at a truncated
    view of the upper end of the user VAS of the `FAHViewer` process:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 继续看 `procmap` 输出的下一部分，让我们看一下`FAHViewer` 进程的用户VAS的上端的截断视图：
- en: '![](img/67891589-c023-45fe-a478-ec7f6546e983.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/67891589-c023-45fe-a478-ec7f6546e983.png)'
- en: 'Figure 7.8 – Partial screenshot: first few lines (high end) of the user VAS
    output from the procmap utility'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – 部分截图：procmap实用程序的用户VAS输出的前几行（高端）
- en: Figure 7.8 is a partial screenshot of the `procmap`output, and shows the user
    space VAS; at the very top of it, you can see the (high) end UVA.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8是`procmap`输出的部分截图，显示了用户空间VAS；在其顶部，您可以看到（高）端UVA。
- en: On our x86_64 system (recall, this is arch-dependent), the (high) `end_uva` value
    is
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的x86_64系统上（请记住，这是与架构相关的），（高）`end_uva`值是
- en: '`0x0000 7fff ffff ffff` and `start_uva` is, of course, `0x0`. How does `procmap` figure
    out the precise address values? Ah, it''s fairly sophisticated: for the kernel-space
    memory information, it uses a kernel module (an LKM!) to query the kernel and
    sets up a config file depending on the system architecture; user space details,
    of course, come from the `/proc/PID/maps` direct `procfs` pseudo-file.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`0x0000 7fff ffff ffff` 和 `start_uva` 当然是 `0x0`。`procmap` 如何找出精确的地址值呢？哦，它相当复杂：对于内核空间内存信息，它使用一个内核模块（一个LKM！）来查询内核，并根据系统架构设置一个配置文件；用户空间的细节当然来自 `/proc/PID/maps` 直接的 `procfs` 伪文件。'
- en: As an aside, the kernel component of `procmap`, a kernel module, sets up a way
    to interface with user space – the `procmap` scripts – by creating and setting
    up a `debugfs` (pseudo) file.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '顺便说一句，`procmap`的内核组件，一个内核模块，建立了一种与用户空间进行交互的方式 – 通过创建和设置一个`debugfs`（伪）文件的`procmap`脚本。 '
- en: 'The following screenshot shows a partial screenshot of the low end of the user
    mode VAS for the process, right down to the lowest UVA, `0x0`:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图显示了进程用户模式VAS的低端的部分截图，直到最低的UVA `0x0`：
- en: '![](img/18776659-6b89-4ec7-a08a-0a6ea6afadea.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18776659-6b89-4ec7-a08a-0a6ea6afadea.png)'
- en: 'Figure 7.9 – Partial screenshot: last few lines (low end) of the user VAS output
    from the procmap utility'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 - 部分截图：进程用户VAS输出的最后几行（低端）来自procmap实用程序
- en: The last mapping, a single page, is, as expected, the null trap page (from UVA `0x1000`
    to `0x0`; we will explain its purpose in the upcoming *The null trap page* section).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个映射，一个单页，如预期的那样，是空指针陷阱页（从UVA `0x1000`到`0x0`；我们将在即将到来的*空指针陷阱页*部分中解释其目的）。
- en: The `procmap` utility, then, if enabled in its config file, calculates and displays
    a few statistics; this includes the sizes of both the kernel and user-mode VASes,
    the amount of user space memory taken up by sparse regions (on 64-bit, as in the
    preceding example, it's usually the vast majority of the space!) as an absolute
    number and a percentage, the amount of physical RAM reported, and finally, the
    memory usage details for this particular process as reported by the `ps(1)` and
    `smem(8)` utilities.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`procmap`实用程序（如果在其配置文件中启用）会计算并显示一些统计信息；这包括内核和用户模式VAS的大小，64位系统上稀疏区域占用的用户空间内存量（通常是空间的绝大部分！）的绝对数量和百分比，报告的物理RAM量，最后，由`ps(1)`和`smem(8)`实用程序报告的此特定进程的内存使用详细信息。
- en: You will find, in general, on a 64-bit system (see Figure 7.3), that the *sparse*
    (empty) memory regions of the process VAS take up close to 100% of the available
    address space! (It's often a number such as 127.99[...] TB of VAS out of the 128
    TB available.) This implies that 99.99[...]% of the memory space is sparse (empty)!
    This is the reality of the simply enormous VAS on a 64-bit system. Only a tiny
    fraction of the gigantic 128 TB of VAS (as this is the case on the x86_64) is
    actually in use. Of course, the actual amounts of sparse and used VAS depend on
    the size of the particular application process.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，在64位系统上（参见图7.3），进程VAS的*稀疏*（空）内存区域占用了可用地址空间的接近100%！（通常是诸如127.99[...] TB的VAS占用了128
    TB可用空间的情况。）这意味着99.99[...]%的内存空间是稀疏的（空的）！这就是64位系统上巨大的VAS的现实。实际上，巨大的128 TB的VAS（就像在x86_64上一样）中只有一小部分被使用。当然，稀疏和已使用的VAS的实际数量取决于特定应用程序进程的大小。
- en: Being able to clearly visualize the process VAS can aid greatly when debugging
    or analyzing issues at a deeper level.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 能够清晰地可视化进程VAS在深层次调试或分析问题时可以提供很大帮助。
- en: If you're reading this book in its hardcopy format, be sure to download the
    full-color PDF of diagrams/figures from the publisher's website: [https://static.packt-cdn.com/downloads/9781789953435_ColorImages.pdf](_ColorImages.pdf).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在阅读本书的实体版本，请务必从出版商的网站下载图表/图像的全彩PDF：[https://static.packt-cdn.com/downloads/9781789953435_ColorImages.pdf](_ColorImages.pdf)。
- en: You will also see that the statistics printed out at the end of the output (if
    enabled) show the number of **Virtual Memory Areas** (**VMAs**) set up for the
    target process. The following section briefly explains what a VMA is. Let's get
    to it!
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 您还会看到输出末尾（如果启用）打印出的统计信息显示了目标进程设置的**虚拟内存区域**（**VMAs**）的数量。接下来的部分简要解释了VMA是什么。让我们开始吧！
- en: Understanding VMA basics
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解VMA的基础知识
- en: 'In the output of `/proc/PID/maps`, each line of the output is actually extrapolated
    from a kernel metadata structure called a VMA. It''s quite straightforward, really:
    the kernel uses the VMA data structure to abstract what we have been calling a segment or mapping.
    Thus, for every single segment in the user VAS, there is a VMA object maintained
    by the OS. Please realize that only user space segments or mappings are governed
    by the kernel metadata structure called the VMA; the kernel segment itself has
    no VMAs.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在`/proc/PID/maps`的输出中，实际上每行输出都是从一个称为VMA的内核元数据结构中推断出来的。这实际上非常简单：内核使用VMA数据结构来抽象我们所说的段或映射。因此，在用户VAS中的每个段都有一个由操作系统维护的VMA对象。请意识到，只有用户空间段或映射受到称为VMA的内核元数据结构的管理；内核段本身没有VMA。
- en: So, how many VMAs will a given process have? Well, it's equal to the number
    of mappings (segments) in its user VAS. In our example with the *FAHViewer* process,
    it happened to have 206 segments or mappings, implying that there are 206 VMA
    metadata objects – representing the 206 user space segments or mappings – for
    this process in kernel memory.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，给定进程会有多少个VMA？嗯，它等于其用户VAS中的映射（段）数量。在我们的*FAHViewer*进程示例中，它恰好有206个段或映射，这意味着内核内存中为该进程维护了206个VMA元数据对象，代表了206个用户空间段或映射。
- en: 'Programmatically speaking, the kernel maintains a VMA "chain" (which is actually
    a red-black tree data structure for efficiency reasons) via the task structure
    rooted at `current->mm->mmap`. Why is the pointer called `mmap`? It''s very deliberate:
    every time an `mmap(2)` system call – that is, a memory mapping operation – is
    performed, the kernel generates a mapping (or "segment") within the calling process''s
    (that is, within `current` instances) VAS and a VMA object representing it.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 从编程的角度来看，内核通过根据`current->mm->mmap`的任务结构维护VMA“链”（实际上是红黑树数据结构，出于效率原因）来进行管理。为什么指针称为`mmap`？这是非常有意义的：每次执行`mmap(2)`系统调用（即内存映射操作）时，内核都会在调用进程的（即在`current`实例内）VAS中生成一个映射（或“段”）和代表它的VMA对象。
- en: 'The VMA metadata structure is akin to an umbrella encompassing the mapping
    and includes all required information for the kernel to perform various kinds
    of memory management: servicing page faults (very common), caching the contents
    of a file during I/O into (or out of) the kernel page cache, and so on.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: VMA元数据结构类似于一个包含映射的伞，包括内核执行各种内存管理操作所需的所有信息：处理页面错误（非常常见），在I/O期间将文件内容缓存到（或从）内核页缓存中等等。
- en: Page fault handling is a very important OS activity, whose algorithm makes up
    quite a bit of usage of the kernel VMA objects; in this book, though, we don't
    delve into these details as it's largely transparent to kernel module/driver authors.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 页面错误处理是一个非常重要的操作系统活动，其算法占用了相当大一部分内核VMA对象的使用；然而，在本书中，我们不深入讨论这些细节，因为对内核模块/驱动程序的作者来说，这些细节基本上是透明的。
- en: 'Just to give you a feel for it, we will show a few members of the kernel VMA
    data structure in the following snippet; the comments alongside help explain their
    purpose:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您感受一下，我们将在下面的片段中展示内核VMA数据结构的一些成员；旁边的注释有助于解释它们的目的：
- en: '[PRE10]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'It should now be clearer as to how `cat /proc/PID/maps` really works under
    the hood: when the user space does, say, `cat /proc/self/maps`, a `read(2)` system
    call is issued by `cat`; this results in the `cat` process switching to kernel
    mode and running the `read(2)` system call code within the kernel with kernel
    privileges. Here, the kernel **Virtual Filesystem Switch** (**VFS**) redirects
    control to the appropriate `procfs` callback handler (function). This code iterates
    (loops) over every VMA metadata structures (for `current`, which is our `cat`
    process, of course), sending relevant information back to user space. The `cat`
    process then faithfully dumps the data received via the read to `stdout`, and
    thus we see it: all the segments or mappings of the process – in effect, the memory
    map of the user-mode VAS!'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应该更清楚了`cat /proc/PID/maps`是如何在底层工作的：当用户空间执行`cat /proc/self/maps`时，`cat`发出了一个`read(2)`系统调用；这导致`cat`进程切换到内核模式，并在内核中以内核特权运行`read(2)`系统调用代码。在这里，内核**虚拟文件系统开关**（**VFS**）将控制权重定向到适当的`procfs`回调处理程序（函数）。这段代码遍历了每个VMA元数据结构（对于`current`，也就是我们的`cat`进程），将相关信息发送回用户空间。`cat`进程然后忠实地将通过读取接收到的数据转储到`stdout`，因此我们看到了它：进程的所有段或映射
    - 实际上是用户模式VAS的内存映射！
- en: Right, with this, we conclude this section, where we have covered details on
    examining the process user VAS. This knowledge helps not only with understanding
    the precise layout of user-mode VAS but also with debugging user space memory
    issues!
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，通过这一部分，我们总结了检查进程用户VAS的细节。这种知识不仅有助于理解用户模式VAS的精确布局，还有助于调试用户空间内存问题！
- en: Now, let's move on to understanding another critical aspect of memory management
    – the detailed layout of the kernel VAS, in other words, the kernel segment.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续理解内存管理的另一个关键方面 - 内核VAS的详细布局，换句话说，内核段。
- en: Examining the kernel segment
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查内核段
- en: As we have talked about in the preceding chapter, and as seen in *Figure 7.5*,
    it's really critical to understand that all processes have their own unique user
    VAS but share the kernel space – what we call the kernel segment or kernel VAS.
    Let's begin this section by starting to examine some common (arch-independent)
    regions of the kernel segment.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一章中讨论过的，以及在*图7.5*中所见，非常重要的是要理解所有进程都有自己独特的用户VAS，但共享内核空间 - 我们称之为内核段或内核VAS。让我们开始这一部分，从开始检查内核段的一些常见（与架构无关）区域。
- en: 'The kernel segment''s memory layout is very arch (CPU)-dependent. Nevertheless,
    all architectures share some commonalities. The following basic diagram represents
    both the user VAS and the kernel segment (in a horizontally tiled format), as
    seen on an x86_32 with a 3:1 VM split:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 内核段的内存布局非常依赖于架构（CPU）。然而，所有架构都有一些共同点。下面的基本图表代表了用户VAS和内核段（以水平平铺的格式），在x86_32上以3:1的VM分割中看到：
- en: '![](img/9192b519-60a5-4db3-933a-a492f3036266.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9192b519-60a5-4db3-933a-a492f3036266.png)'
- en: Figure 7.10 – User and kernel VASes on an x86_32 with a 3:1 VM split with focus
    on the lowmem region
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 - 在x86_32上以3:1 VM分割为焦点的用户和内核VAS
- en: 'Let''s go over each region one by one:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个地过一遍每个区域：
- en: '**The user mode VAS**: This is the user VAS; we have covered it in detail in
    the preceding chapter as well as earlier sections in this chapter; in this particular
    example, it takes 3 GB of VAS (UVAs from `0x0` to `0xbfff ffff`).'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户模式VAS**：这是用户VAS；我们在前一章和本章的早些部分详细介绍了它；在这个特定的例子中，它占用了3GB的VAS（从`0x0`到`0xbfff
    ffff`）。'
- en: All that follows belongs to kernel VAS or the kernel segment; in this particular
    example, it takes 1 GB of VAS (KVAs from `0xc000 0000` to `0xffff ffff`); let's
    examine individual portions of it now.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有接下来的内容都属于内核VAS或内核段；在这个特定的例子中，它占用了1GB的VAS（从`0xc000 0000`到`0xffff ffff`）；现在让我们逐个部分来检查它。
- en: '**The lowmem region**: This is where platform (system) RAM direct-maps into
    the kernel. (We will cover this key topic in more detail in the *Direct-mapped
    RAM and address translation* section*.* If you feel it helps, you can read that
    section first and then return here). Skipping a bit ahead for now, let''s just
    understand that the base location in the kernel segment where platform RAM is
    mapped is specified by a kernel macro called `PAGE_OFFSET`. The precise value
    of this macro is very arch-dependent; we will leave this discussion to a later
    section. For now, we ask you to just take it on faith that on the IA-32 with a
    3:1 (GB) VM split, the value of `PAGE_OFFSET` is `0xc000 0000`.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低端内存区域**：这是平台（系统）RAM直接映射到内核的地方。（我们将在*直接映射RAM和地址转换*部分更详细地介绍这个关键主题。如果有帮助的话，您可以先阅读该部分，然后再回到这里）。现在先跳过一点，让我们先了解一下内核段中平台RAM映射的基本位置，这个位置由一个名为`PAGE_OFFSET`的内核宏指定。这个宏的精确值非常依赖于架构；我们将把这个讨论留到后面的部分。现在，我们要求您只是相信，在具有3:1（GB）VM分割的IA-32上，`PAGE_OFFSET`的值是`0xc000
    0000`。'
- en: The length or size of the kernel lowmem region is equal to the amount of RAM
    on the system. (Well, at least the amount of RAM as seen by the kernel; enabling
    the kdump facility, for example, has the OS reserve some RAM early). The virtual
    addresses that make up this region are termed **kernel logical addresses** as
    they are at a fixed offset from their physical counterparts. The core kernel and
    device drivers can allocate (physically contiguous!) memory from this region via
    various APIs (we cover precisely these APIs in detail in the following two chapters).
    The kernel static text (code), data, and BSS (uninitialized data) memory also
    resides within this lowmem region.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 内核低内存区域的长度或大小等于系统上的RAM量。（至少是内核看到的RAM量；例如，启用kdump功能会让操作系统提前保留一些RAM）。构成这个区域的虚拟地址被称为**内核逻辑地址**，因为它们与它们的物理对应物有固定的偏移量。核心内核和设备驱动程序可以通过各种API（我们将在接下来的两章中详细介绍这些API）从这个区域分配（物理连续的）内存。内核静态文本（代码）、数据和BSS（未初始化数据）内存也驻留在这个低内存区域内。
- en: '**The kernel vmalloc region**: This is a region of the kernel VAS that is completely
    virtual. Core kernel and/or device driver code can allocate virtually contiguous
    memory from this region using the `vmalloc()` (and friends) API. Again, we will
    cover this in detail in [Chapter 8](e78245d1-5a99-4b9e-a98c-cb16b15f3bee.xhtml),
    *Kernel Memory Allocation for Module Authors Part 1*, and [Chapter 9](dbb888a2-8145-4132-938c-1313a707b2f2.xhtml),
    *Kernel Memory Allocation for Module Authors Part 2*. This is also the so-called
    `ioremap` space.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内核vmalloc区域**：这是内核VAS的一个完全虚拟的区域。核心内核和/或设备驱动程序代码可以使用`vmalloc()`（和其他类似的）API从这个区域分配虚拟连续的内存。同样，我们将在[第8章](e78245d1-5a99-4b9e-a98c-cb16b15f3bee.xhtml)和[第9章](dbb888a2-8145-4132-938c-1313a707b2f2.xhtml)中详细介绍这一点，即*模块作者的内核内存分配第1部分*和*模块作者的内核内存分配第2部分*。这也是所谓的`ioremap`空间。'
- en: '**The kernel modules space**: A region of kernel VAS is set aside for memory
    taken up by the static text and data of **Loadable Kernel Modules** (**LKMs**).
    When you perform `insmod(8)`, the underlying kernel code of the resulting `[f]init_module(2)`
    system call allocates memory from this region (typically via the `vmalloc()` API)
    and loads the kernel module''s (static) code and data there.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核模块空间：内核VAS的一个区域被留出来，用于存放**可加载内核模块**（**LKMs**）的静态文本和数据所占用的内存。当您执行`insmod(8)`时，生成的`[f]init_module(2)`系统调用的底层内核代码会从这个区域分配内存（通常通过`vmalloc()`
    API），并将内核模块的（静态）代码和数据加载到那里。
- en: The preceding figure (Figure 7.10) is deliberately left simplistic and even
    a bit vague as the exact kernel virtual memory layout is very arch-dependent.
    We'll put off the temptation to draw a detailed diagram for a bit. Instead, to
    make this discussion less pedantic and more practical and useful, we'll present,
    in a soon-to-come section, a kernel module that queries and prints relevant information
    regarding the kernel segment layout. Only then, once we have actual values for
    various regions of the kernel segment for a particular architecture, will we present
    a detailed diagram depicting this.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图（图7.10）故意保持简单甚至有点模糊，因为确切的内核虚拟内存布局非常依赖于架构。我们将暂时抑制绘制详细图表的冲动。相反，为了使这个讨论不那么学究，更实用和有用，我们将在即将到来的一节中介绍一个内核模块，该模块查询并打印有关内核段布局的相关信息。只有在我们对特定架构的内核段的各个区域有了实际值之后，我们才会呈现详细的图表。
- en: 'Pedantically (as can be seen in Figure 7.10), the addresses belonging to the lowmem
    region are termed kernel logical addresses (they''re at a fixed offset from their
    physical counterparts), whereas the addresses for the remainder of the kernel
    segment are termed KVAs. Though this distinction is made here, please realize
    that, for all practical purposes, it''s a rather pedantic one: we will often simply
    refer to all addresses within the kernel segment as KVAs.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 学究地（如图7.10所示），属于低内存区域的地址被称为内核逻辑地址（它们与它们的物理对应物有固定的偏移量），而内核段的其余地址被称为KVA。尽管在这里做出了这样的区分，请意识到，实际上，这是一个相当学究的区分：我们通常会简单地将内核段内的所有地址称为KVA。
- en: 'Before that, there are several other pieces of information to cover. Let''s
    begin with another peculiarity, mostly brought about by the limitations of a 32-bit
    architecture: the so-called high memory region of the kernel segment.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之前，还有几个其他信息要涵盖。让我们从另一个特殊情况开始，这主要是由32位架构的限制带来的：内核段的所谓高内存区域。
- en: High memory on 32-bit systems
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 32位系统上的高内存
- en: Regarding the kernel lowmem region that we briefly discussed previously, an
    interesting observation ensues. On a 32-bit system with, say, a 3:1 (GB) VM split
    (just as Figure 7.10 depicts), a system with (say) 512 MB of RAM will have its
    512 MB RAM direct-mapped into the kernel starting at `PAGE_OFFSET` (3 GB or KVA `0xc000
    0000`). This is quite clear.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 关于我们之前简要讨论过的内核低内存区域，有一个有趣的观察结果。在一个32位系统上，例如，3:1（GB）的VM分割（就像图7.10所描述的那样），拥有（例如）512
    MB RAM的系统将其512 MB RAM直接映射到从`PAGE_OFFSET`（3 GB或KVA `0xc000 0000`）开始的内核中。这是非常清楚的。
- en: 'But think about it: what would happen if the system has a lot more RAM, say,
    2 GB? Now, it''s obvious that we cannot direct-map the whole of the RAM into the
    lowmem region. It just cannot fit (as, in this example, the entire available kernel
    VAS is just a gigabyte and RAM is 2 gigabytes)! So, on a 32-bit Linux OS, a certain
    amount of memory (typically 768 MB on the IA-32) is allowed to be direct-mapped and
    thus falls into the lowmem region. The remaining RAM is *indirectly mapped* into
    another memory zone called `ZONE_HIGHMEM` (we think of it as a high-memory region
    or *zone* as opposed to lowmem; more on memory zones follows in a later section, *Zones*).
    More correctly, as the kernel now finds it impossible to direct-map all physical
    memory at once, it sets up a (virtual) region where it can set up and use temporary
    virtual mappings of that RAM. This is the so-called high-memory region.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 但是想一想：如果系统有更多的RAM，比如2GB，会发生什么？现在很明显，我们无法将整个RAM直接映射到lowmem区域。它根本就放不下（例如，在这个例子中，整个可用的内核VAS只有1GB，而RAM是2GB）！因此，在32位Linux操作系统上，允许将一定数量的内存（通常是IA-32上的768MB）直接映射，因此落入lowmem区域。剩下的RAM则*间接映射*到另一个内存区域，称为`ZONE_HIGHMEM`（我们认为它是一个高内存区域或*区域*，与lowmem相对；关于内存区域的更多信息将在后面的部分*区域*中介绍）。更准确地说，由于内核现在发现不可能一次性直接映射所有物理内存，它设置了一个（虚拟）区域，可以在其中设置和使用该RAM的临时虚拟映射。这就是所谓的高内存区域。
- en: Don't get confused by the phrase "high memory"; one, it's not necessarily placed
    "high" in the kernel segment, and two, this is not what the `high_memory` global
    variable represents – it (`high_memory`) represents the upper bound of the kernel's 
    lowmem region. More on this follows in a later section, *Macros and variables
    describing the kernel segment layout*.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 不要被“高内存”这个词所迷惑；首先，它不一定放在内核段的“高”位置，其次，这并不是`high_memory`全局变量所代表的 - 它（`high_memory`）代表了内核的lowmem区域的上限。关于这一点，后面的部分会有更多介绍，*描述内核段布局的宏和变量*。
- en: 'Nowadays, though (and especially with 32-bit systems being used more and more
    infrequently), these concerns completely disappear on 64-bit Linux. Think about
    it: on 64-bit Linux, the kernel segment size is a whopping 128 TB (!) on the x86_64\.
    No single system in existence has anywhere close to this much RAM. Hence, all
    platform RAM can indeed (easily) be direct-mapped into the kernel segment and
    the need for `ZONE_HIGHMEM` (or equivalent) disappears.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现在（特别是32位系统越来越少使用），这些问题在64位Linux上完全消失了。想想看：在64位Linux上，x86_64的内核段大小达到了128
    TB（！）。目前没有任何系统的RAM接近这么多。因此，所有平台的RAM确实（轻松地）可以直接映射到内核段，而`ZONE_HIGHMEM`（或等效）的需求也消失了。
- en: Again, the kernel documentation provides details on this "high-memory" region.
    Take a look if interested: [https://www.kernel.org/doc/Documentation/vm/highmem.txt](https://www.kernel.org/doc/Documentation/vm/highmem.txt).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，内核文档提供了有关这个“高内存”区域的详细信息。如果感兴趣，请查看：[https://www.kernel.org/doc/Documentation/vm/highmem.txt](https://www.kernel.org/doc/Documentation/vm/highmem.txt)。
- en: Okay, let's now tackle the thing we've been waiting to do – writing a kernel
    module (an LKM) to delve into some details regarding the kernel segment.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在让我们来做我们一直在等待的事情 - 编写一个内核模块（LKM）来深入了解内核段的一些细节。
- en: Writing a kernel module to show information about the kernel segment
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写一个内核模块来显示有关内核段的信息
- en: 'As we have learned, the kernel segment consists of various regions. Some are
    common to all architectures (arch-independent): they include the lowmem region
    (which contains, among other things, the uncompressed kernel image – its code,
    data, BSS), the kernel modules region, `vmalloc`/`ioremap` regions, and so on.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所了解的，内核段由各种区域组成。有些是所有架构（与架构无关）共有的：它们包括lowmem区域（其中包含未压缩的内核映像 - 其代码、数据、BSS等）、内核模块区域、`vmalloc`/`ioremap`区域等。
- en: The precise location within the kernel segment where these regions lie, and
    indeed which regions may be present, is very arch (CPU)-dependent. To help understand
    and pin it down for any given system, let's develop a kernel module that queries
    and prints various details regarding the kernel segment (in fact, if asked to,
    it also prints some useful user space memory details).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这些区域在内核段中的精确位置，以及可能存在的区域，都与特定的架构（CPU）有关。为了帮助理解并针对任何给定的系统进行固定，让我们开发一个内核模块，查询并打印有关内核段的各种细节（实际上，如果需要，它还会打印一些有用的用户空间内存细节）。
- en: Viewing the kernel segment on a Raspberry Pi via dmesg
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过dmesg查看树莓派上的内核段
- en: 'Before jumping into and analyzing the code for such a kernel module, the fact
    is that something pretty similar to what we''re attempting here – printing the
    location and size of various interesting regions within the kernel segment/VAS
    – is already performed at early boot on the popular Raspberry Pi (ARM) Linux kernel. In
    the following snippet, we show the relevant output from the kernel log when the
    Raspberry Pi 3 B+ (running the stock (default) 32-bit Raspberry Pi OS) boots:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在跳入并分析这样一个内核模块的代码之前，事实上，类似于我们在这里尝试的事情 - 打印内核段/VAS中各种有趣区域的位置和大小 - 已经在流行的树莓派（ARM）Linux内核的早期引导时执行。在下面的片段中，我们展示了树莓派3
    B+（运行默认的32位树莓派OS）启动时内核日志的相关输出：
- en: '[PRE11]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It's important to note that these preceding prints are very specific to the
    OS and device. The default Raspberry Pi 32-bit OS prints this information out,
    while others may not: **YMMV** (**Your Mileage May Vary**!). For example, with
    the standard 5.4 kernel for Raspberry Pi that I built and ran on the device, these
    informative prints weren't present. On recent kernels (as seen in the preceding
    logs on the 4.19.97-v7+ Raspberry Pi OS kernel), for security reasons – that of
    preventing kernel information leakage – many early `printk` functions will not
    display a "real" kernel address (pointer) value; you might simply see it prints
    the `0x(ptrval)` string.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，前面的打印非常特定于操作系统和设备。默认的树莓派32位操作系统会打印这些信息，而其他操作系统可能不会：**YMMV**（**你的情况可能有所不同**！）。例如，我在设备上构建和运行的标准的树莓派5.4内核中，这些信息性的打印是不存在的。在最近的内核版本中（如在4.19.97-v7+树莓派操作系统内核的前面日志中所见），出于安全原因
    - 防止内核信息泄漏 - 许多早期的`printk`函数不会显示“真实”的内核地址（指针）值；你可能只会看到它打印了`0x(ptrval)`字符串。
- en: This **`0x(ptrval)`** output implies that the kernel is deliberately not showing
    even a hashed printk (recall the `%pK` format specifier from [Chapter 5](408b6f9d-42dc-4c59-ab3d-1074d595f9e2.xhtml),
    *Writing Your First Kernel Module – LKMs Part 2*) as the system entropy is not
    yet high enough. If you insist on seeing a (weakly) hashed printk, you can always
    pass the `debug_boot_weak_hash` kernel parameter at boot (look up details on kernel
    boot parameters here: [https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html](https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html)).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这个**`0x(ptrval)`**输出意味着内核故意不显示甚至是散列的printk（回想一下[第5章](408b6f9d-42dc-4c59-ab3d-1074d595f9e2.xhtml)，*编写你的第一个内核模块
    - LKMs第2部分*中的`%pK`格式说明符），因为系统熵还不够高。如果你坚持要看到一个（弱）散列的printk，你可以在启动时传递`debug_boot_weak_hash`内核参数（在这里查找内核启动参数的详细信息：[https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html](https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html)）。
- en: Interestingly, (as mentioned in the preceding information box), the code that
    prints this `Virtual kernel memory layout :`information is very specific to the
    Raspberry Pi kernel patches! It can be found in the Raspberry Pi kernel source
    tree here: [https://github.com/raspberrypi/linux/blob/rpi-5.4.y/arch/arm/mm/init.c](https://github.com/raspberrypi/linux/blob/rpi-5.4.y/arch/arm/mm/init.c).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，（如前面信息框中提到的），打印这个`Virtual kernel memory layout :`信息的代码非常特定于树莓派内核补丁！它可以在树莓派内核源代码树中找到：[https://github.com/raspberrypi/linux/blob/rpi-5.4.y/arch/arm/mm/init.c](https://github.com/raspberrypi/linux/blob/rpi-5.4.y/arch/arm/mm/init.c)。
- en: Now, in order for you to query and print similar information, you must first
    get familiar with some key kernel macros and globals.; let's do so in the next
    section.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了查询和打印类似的信息，你必须首先熟悉一些关键的内核宏和全局变量；我们将在下一节中这样做。
- en: Macros and variables describing the kernel segment layout
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 描述内核段布局的宏和变量
- en: 'To write a kernel module that displays relevant kernel segment information,
    we need to know how exactly to interrogate the kernel with regard to these details.
    In this section, we will briefly describe a few key macros and variables within
    the kernel representing the memory of the kernel segment (on most architectures,
    in descending order by KVA):'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 要编写一个显示相关内核段信息的内核模块，我们需要知道如何询问内核这些细节。在本节中，我们将简要描述内核中表示内核段内存的一些关键宏和变量（在大多数架构上，按KVA降序排列）：
- en: '**The vector table** is a common OS data structure – it''s an array of function
    pointers (aka a switching or jump table). It is arch-specific: ARM-32 uses it
    to initialize its vectors such that when a processor exception or mode change
    (such as an interrupt, syscall, page fault, MMU abort, and so on) occurs, the
    processor knows what code to run:'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向量表** 是一个常见的操作系统数据结构 - 它是一个函数指针数组（也称为切换表或跳转表）。它是特定于架构的：ARM-32使用它来初始化它的向量，以便当处理器发生异常或模式更改（如中断，系统调用，页错误，MMU中止等）时，处理器知道要运行的代码：'
- en: '| **Macro or variable** | **Interpretation** |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| **宏或变量** | **解释** |'
- en: '| `VECTORS_BASE` | Typically ARM-32 only; start KVA of a kernel vector table
    spanning 1 page |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| `VECTORS_BASE` | 通常仅适用于ARM-32；内核向量表的起始KVA，跨越1页 |'
- en: '**The fix map region** is a range of compile-time special or reserved virtual
    addresses; they are employed at boot time to fix, into the kernel segment, required
    kernel elements that must have memory available for them. Typical examples include
    the setup of initial kernel page tables, early `ioremap` and `vmalloc` regions,
    and so on. Again, it''s an arch-dependent region and is thus used differently
    on different CPUs:'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**fix map区域** 是一系列编译时的特殊或保留的虚拟地址；它们在启动时被用来修复内核段中必须为其提供内存的必需内核元素。典型的例子包括初始化内核页表，早期的`ioremap`和`vmalloc`区域等。同样，它是一个与架构相关的区域，因此在不同的CPU上使用方式不同：'
- en: '| **Macro or variable** | **Interpretation** |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| **宏或变量** | **解释** |'
- en: '| `FIXADDR_START` | Start KVA of the kernel fixmap region spanning `FIXADDR_SIZE`
    bytes |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| `FIXADDR_START` | 内核fixmap区域的起始KVA，跨越`FIXADDR_SIZE`字节 |'
- en: '**Kernel modules** are allocated memory – for their static text and data –
    within a specific range in the kernel segment. The precise location of the kernel
    module region varies with the architecture. On ARM 32-bit systems, in fact, it''s
    placed just above the user VAS; while on 64-bit, it''s usually higher up in the
    kernel segment:'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内核模块** 在内核段中的特定范围内分配内存 - 用于它们的静态文本和数据。内核模块区域的精确位置因架构而异。在ARM 32位系统上，实际上是放在用户VAS的正上方；而在64位系统上，通常放在内核段的更高位置：'
- en: '| **Kernel modules (LKMs) region** | **Memory allocated from here for static
    code + data of LKMs** |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| **内核模块（LKMs）区域** | **从这里分配内存用于LKMs的静态代码+数据** |'
- en: '| **`MODULES_VADDR`** | Start KVA of the kernel modules region |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| **`MODULES_VADDR`** | 内核模块区域的起始KVA |'
- en: '| `MODULES_END` | End KVA of kernel modules region; size is `MODULES_END - MODULES_VADDR`
    |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| `MODULES_END` | 内核模块区域的结束KVA；大小为`MODULES_END - MODULES_VADDR` |'
- en: '**KASAN***:* The modern kernel (4.0 onward for x86_64, 4.4 for ARM64) employs
    a powerful mechanism to detect and report memory issues. It''s based on the user
    space **Address SANitizer** *(***ASAN***)* code base and is thus called **Kernel
    Address SANitizer** (**KASAN**)*.* Its power lies in ably (via compile-time instrumentation)
    detecting memory issues such as **Use After Free** (**UAF**) and **Out Of Bounds**
    (**OOB**) access (including buffer over/under flows). It, however, works *only
    on 64-bit Linux* and requires a rather large **shadow memory region** (of a size
    that is one-eighth that of the kernel VAS, whose extents we show if it''s enabled).
    It''s a kernel configuration feature (`CONFIG_KASAN`) and is typically enabled
    only for debug purposes (but it''s really crucial to keep it enabled during debug
    and testing!):'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KASAN***:* 现代内核（从x86_64的4.0版本开始，ARM64的4.4版本开始）采用了一种强大的机制来检测和报告内存问题。它基于用户空间**地址SANitizer**（***ASAN***）代码库，因此被称为**内核地址SANitizer**（**KASAN**）。它的强大之处在于能够（通过编译时的插装）检测内存问题，如**释放后使用**（**UAF**）和**越界**（**OOB**）访问（包括缓冲区溢出/溢出）。但是，它仅在64位Linux上工作，并且需要一个相当大的**阴影内存区域**（大小为内核VAS的八分之一，如果启用则显示其范围）。它是一个内核配置功能（`CONFIG_KASAN`），通常仅用于调试目的（但在调试和测试期间保持启用非常关键！）：'
- en: '| **KASAN shadow memory region (only 64-bit)** | **[Optional] (only on 64-bit
    and only if CONFIG_KASAN is defined; see more as follows)** |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| **KASAN阴影内存区域（仅适用于64位）** | **[可选]（仅在64位且仅在CONFIG_KASAN定义的情况下；请参见以下更多信息）**
    |'
- en: '| `KASAN_SHADOW_START` | Start KVA of the KASAN region |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| `KASAN_SHADOW_START` | KASAN区域的KVA起始 |'
- en: '| `KASAN_SHADOW_END` | End KVA of the KASAN region; size is `KASAN_SHADOW_END
    - KASAN_SHADOW_START` |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| `KASAN_SHADOW_END` | KASAN区域的KVA结束；大小为`KASAN_SHADOW_END - KASAN_SHADOW_START`
    |'
- en: '**The vmalloc region** is the space from where memory for the `vmalloc()` (and
    friends) APIs are allocated; we will cover various memory allocation APIs in detail
    in the next two chapters:'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**vmalloc区域**是为`vmalloc()`（及其相关函数）分配内存的空间；我们将在接下来的两章节中详细介绍各种内存分配API：'
- en: '| **The vmalloc region** | **For memory allocated via vmalloc() and friends**
    |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| **vmalloc区域** | **用于通过vmalloc()和相关函数分配的内存** |'
- en: '| **`VMALLOC_START`** | Start KVA of the `vmalloc` region |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| **`VMALLOC_START`** | `vmalloc`区域的KVA起始 |'
- en: '| `VMALLOC_END` | End KVA of the `vmalloc` region; size is `VMALLOC_END - VMALLOC_START`
    |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| `VMALLOC_END` | `vmalloc`区域的结束KVA；大小为`VMALLOC_END - VMALLOC_START` |'
- en: '**The** **lowmem region** – direct-mapped RAM into the kernel segment on a
    `1:1 :: physical page frame:kernel page` basis – is in fact the region where the
    Linux kernel maps and manages (typically) all RAM. Also, it''s often set up as
    `ZONE_NORMAL` within the kernel (we will cover zones as well, a bit later):'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低内存区域** - 根据`1:1 ::物理页框:内核页`的基础，直接映射到内核段的RAM区域 - 实际上是Linux内核映射和管理（通常）所有RAM的区域。此外，它通常在内核中设置为`ZONE_NORMAL`（稍后我们还将介绍区域）：'
- en: '| **Lowmem region** | **Direct-mapped memory region** |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| **低内存区域** | **直接映射内存区域** |'
- en: '| `PAGE_OFFSET` | Start KVA of the lowmem region; also represents the start
    of the kernel segment on some architectures and is (often) the VM split value
    on 32-bit. |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| `PAGE_OFFSET` | 低内存区域的KVA起始；也代表某些架构上内核段的起始，并且（通常）是32位上的VM分割值。 |'
- en: '| `high_memory` | End KVA of the lowmem region, upper bound of direct-mapped
    memory; in effect, this value minus `PAGE_OFFSET` is the amount of (platform)
    RAM on the system (careful, this is not necessarily the case on all arches though);
    not to be confused with `ZONE_HIGHMEM`. |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| `high_memory` | 低内存区域的结束KVA，直接映射内存的上限；实际上，这个值减去`PAGE_OFFSET`就是系统上RAM的数量（注意，这并不一定适用于所有架构）；不要与`ZONE_HIGHMEM`混淆。
    |'
- en: '**The** **highmem region** or zone is an optional region. It might exist on
    some 32-bit systems (typically, where the amount of RAM present is greater than
    the size of the kernel segment itself). It''s often set up as `ZONE_HIGHMEM` in
    this case (we will cover zones a bit later. Also, you can refer back to more on
    this highmem region in the earlier section entitled *High memory on 32-bit systems*):'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高内存区域**或区域是一个可选区域。它可能存在于一些32位系统上（通常是当RAM的数量大于内核段本身的大小时）。在这种情况下，它通常设置为`ZONE_HIGHMEM`（稍后我们将介绍区域）。此外，您可以在之前的标题为*32位系统上的高内存*的部分中了解更多关于这个高内存区域的信息：'
- en: '| **Highmem region (only possible on 32-bit)** | **[Optional] HIGHMEM may be
    present on some 32-bit systems** |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| **高内存区域（仅适用于32位）** | **[可选] 在一些32位系统上可能存在HIGHMEM** |'
- en: '| `PKMAP_BASE` | Start KVA of the highmem region, runs until `LAST_PKMAP` pages;
    represents the kernel mapping of so-called high-memory pages (older, only possible
    on 32-bit) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| `PKMAP_BASE` | 高内存区域的KVA起始，直到`LAST_PKMAP`页；表示所谓的高内存页的内核映射（较旧，仅适用于32位） |'
- en: 'The (uncompressed) **kernel image** itself – its code, `init`, and data regions
    – are private symbols and thus unavailable to kernel modules; we don''t attempt
    to print them:'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核镜像本身（未压缩）- 其代码、`init`和数据区域 - 是私有符号，因此对内核模块不可用；我们不尝试打印它们：
- en: '| **Kernel (static) image** | **The content of the uncompressed kernel image
    (see the following); not exported and thus unavailable to modules** |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| **内核（静态）镜像** | **未压缩内核镜像的内容（请参见以下）；不导出，因此对模块不可用** |'
- en: '| `_text, _etext` | Start and end KVAs (respectively) of the kernel text (code)
    region |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| `_text, _etext` | 内核文本（代码）区域的起始和结束KVA（分别） |'
- en: '| `__init_begin, __init_end` | Start and end KVAs (respectively) of the kernel
    `init` section region |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| `__init_begin, __init_end` | 内核`init`部分区域的起始和结束KVA（分别） |'
- en: '| `_sdata, _edata` | Start and end KVAs (respectively) of the kernel static
    data region |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| `_sdata, _edata` | 内核静态数据区域的起始和结束KVA（分别） |'
- en: '| `__bss_start, __bss_stop` | Start and end KVAs (respectively) of the kernel
    BSS (uninitialized data) region |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| `__bss_start, __bss_stop` | 内核BSS（未初始化数据）区域的起始和结束KVA（分别） |'
- en: '**The user VAS**: The last item, of course, is the process user VAS. It''s
    below the kernel segment (when ordered by descending virtual address), and is
    of size `TASK_SIZE` bytes. It was discussed in detail earlier in this chapter:'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户VAS**：最后一项当然是进程用户VAS。它位于内核段的下方（按虚拟地址降序排列），大小为`TASK_SIZE`字节。在本章的前面部分已经详细讨论过：'
- en: '| **User VAS** | **User Virtual Address Space (VAS)** |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| **用户VAS** | **用户虚拟地址空间（VAS）** |'
- en: '| (User-mode VAS follows)`TASK_SIZE` | (Examined in detail earlier via `procfs`
    or our `procmap` utility script); the kernel macro `TASK_SIZE` represents the
    size of the user VAS (bytes). |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|（用户模式VAS如下）`TASK_SIZE` |（通过`procfs`或我们的`procmap`实用程序脚本之前详细检查过）；内核宏`TASK_SIZE`表示用户VAS的大小（字节）。'
- en: Well, that's that; we've seen several kernel macros and variables that, in effect,
    describe the kernel VAS.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，我们已经看到了几个内核宏和变量，实际上描述了内核VAS。
- en: 'Moving on to the code of our kernel module, you''ll soon see that its `init`
    method calls two functions (that matter):'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 继续我们的内核模块的代码，很快您将看到它的`init`方法调用了两个重要的函数：
- en: '`show_kernelseg_info()`, which prints relevant kernel segment details'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`show_kernelseg_info()`，打印相关的内核段细节'
- en: '`show_userspace_info()`, which prints relevant user VAS details (it''s optional,
    decided via a kernel parameter)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`show_userspace_info()`，打印相关的用户VAS细节（这是可选的，通过内核参数决定）'
- en: We will start by describing the kernel segment function and seeing its output.
    Also, the way the Makefile is set up, it links into the object file of our kernel
    library code, `klib_llkd.c`*,* and generates a kernel module object called `show_kernel_seg.ko`.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从描述内核段函数并查看其输出开始。此外，Makefile的设置方式是，它链接到我们的内核库代码的对象文件`klib_llkd.c`*，并生成一个名为`show_kernel_seg.ko`的内核模块对象。
- en: Trying it out – viewing kernel segment details
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 试一下 - 查看内核段细节
- en: 'For clarity, we will show only relevant parts of the source code in this section.
    Do clone and use the complete code from this book''s GitHub repository. Also,
    recall the `procmap` utility mentioned earlier; it has a kernel component, an
    LKM, which indeed does a similar job to this one – making kernel-level information
    available to user space. With it being more sophisticated, we won''t delve into
    its code here; seeing the code of the following demo kernel module `show_kernel_seg` is
    more than sufficient here:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰起见，我们将在本节中仅显示源代码的相关部分。 请从本书的GitHub存储库中克隆并使用完整的代码。 还要记住之前提到的`procmap`实用程序；
    它有一个内核组件，一个LKM，它确实与此类似 - 使内核级信息可用于用户空间。 由于它更复杂，我们不会在这里深入研究它的代码； 看到以下演示内核模块`show_kernel_seg`的代码在这里已经足够了：
- en: '[PRE12]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The preceding code snippet displays the extents of the ARM vector table. Of
    course, it's conditional. The output only occurs on an ARM-32 – hence the `#ifdef
    CONFIG_ARM` preprocessor directive. (Also, our use of the `%px` printk format
    specifier ensures the code is portable.)
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段显示了ARM向量表的范围。 当然，这是有条件的。 输出仅在ARM-32上发生 - 因此有`#ifdef CONFIG_ARM`预处理指令。（此外，我们使用`%px`
    printk格式说明符确保代码是可移植的。）
- en: 'The `SHOW_DELTA_*()` macros used here in this demo kernel module are defined
    in our `convenient.h` header and are helpers that enable us to easily display
    the low and high values passed to it, calculate the delta (the difference) between
    the two quantities passed, and display it; here''s the relevant code:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个演示内核模块中使用的`SHOW_DELTA_*()`宏在我们的`convenient.h`头文件中定义，并且是帮助程序，使我们能够轻松显示传递给它的低值和高值，计算两个数量之间的差异，并显示它；
    这是相关的代码：
- en: '[PRE13]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the following code, we show the code snippet that emits `printk` functions
    describing the following region extents:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们展示了发出`printk`函数描述以下区域范围的代码片段：
- en: Kernel module region
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核模块区域
- en: (Optional) KASAN region
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （可选）KASAN区域
- en: The vmalloc region
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vmalloc区域
- en: The lowmem, and a possible highmem, region
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低内存和可能的高内存区域
- en: 'Regarding the kernel modules region, as explained in the detailed comment in
    the following source, we try and keep the order as by descending KVAs:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 关于内核模块区域，如下面源代码中的详细注释所解释的那样，我们尝试保持按降序KVAs的顺序：
- en: '[PRE14]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s build and insert our LKM on the ARM-32 Raspberry Pi 3 B+; the following
    screenshot shows it being set up and then the kernel log:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在ARM-32 Raspberry Pi 3 B+上构建和插入我们的LKM； 以下屏幕截图显示了它的设置，然后是内核日志：
- en: '![](img/070ece9e-7253-4bba-9001-3cc1b3a69ea3.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](img/070ece9e-7253-4bba-9001-3cc1b3a69ea3.png)'
- en: Figure 7.11 – Output from the show_kernel_seg.ko LKM on a Raspberry Pi 3B+ running
    stock Raspberry Pi 32-bit Linux
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 - 在运行标准Raspberry Pi 32位Linux的Raspberry Pi 3B+上显示show_kernel_seg.ko LKM的输出
- en: As expected, the output we receive regarding the kernel segment perfectly matches
    what the stock Raspberry Pi kernel itself prints at boot (you can refer back to
    the *Viewing the kernel segment on a Raspberry Pi via dmesg* section to verify
    this). As can be deciphered from the value of `PAGE_OFFSET` (the KVA `0x8000 0000`
    in Figure 7.11), our Raspberry Pi's kernel's VM split is configured as 2:2 (GB)
    (as the hexadecimal value `0x8000 0000` is 2 GB in decimal base. Interestingly,
    the default Raspberry Pi 32-bit OS on the more recent Raspberry Pi 4 Model B device
    is configured with a 3:1 (GB) VM split).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们收到的关于内核段的输出完全匹配标准Raspberry Pi内核在启动时打印的内容（您可以参考*通过dmesg查看Raspberry
    Pi上的内核段*部分来验证这一点）。 从`PAGE_OFFSET`的值（图7.11中的KVA `0x8000 0000`）可以解释出来，我们的Raspberry
    Pi的内核的VM分割配置为2:2（GB）（因为十六进制值`0x8000 0000`在十进制基数中为2 GB。有趣的是，更近期的Raspberry Pi 4
    Model B设备上的默认Raspberry Pi 32位操作系统配置为3:1（GB）VM分割）。
- en: Technically, on ARM-32 systems, at least, user space is slightly under 2 GB
    (*2 GB – 16 MB = 2,032 MB*) as this 16 MB is taken as the *kernel module region*
    just below `PAGE_OFFSET`; indeed, exactly this can be seen in Figure 7.11 (the
    kernel module region here spans from `0x7f00 0000` to `0x8000 0000 `for 16 MB).
    Also, as you'll soon see, the value of the `TASK_SIZE` macro – the size of the
    user VAS – reflects this fact as well.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，在ARM-32系统上，至少用户空间略低于2 GB（*2 GB - 16 MB = 2,032 MB*），因为这16 MB被视为*内核模块区域*，就在`PAGE_OFFSET`下面；确实，这可以在图7.11中看到（这里的内核模块区域跨越了`0x7f00
    0000`到`0x8000 0000`的16 MB）。 此外，正如您很快将看到的，`TASK_SIZE`宏的值 - 用户VAS的大小 - 也反映了这一事实。
- en: 'We present much of this information in the following diagram:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在以下图表中展示了大部分这些信息：
- en: '![](img/caf083f0-6a88-4a75-a8e2-2a320b45d7e9.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](img/caf083f0-6a88-4a75-a8e2-2a320b45d7e9.png)'
- en: Figure 7.12 – The complete VAS of a process on ARM-32 (Raspberry Pi 3B+) with
    a 2:2 GB VM split
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 - Raspberry Pi 3B+上ARM-32进程的完整VAS，具有2:2 GB VM分割
- en: Do note that due to variations in differing models, the amount of usable RAM,
    or even the device tree, the layout shown in Figure 7.12 may not precisely match
    that on the Raspberry Pi you have.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于不同型号之间的差异、可用RAM的数量，甚至设备树的不同，图7.12中显示的布局可能与您拥有的树莓派上的布局并不完全匹配。
- en: Okay, now you know how to print relevant kernel segment macros and variables
    within a kernel module, helping you understand the kernel VM layout on any Linux
    system! In the following section, we will attempt to "see" (visualize) the kernel
    VAS, this time via our `procmap` utility.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，现在您知道如何在内核模块中打印相关的内核段宏和变量，帮助您了解任何Linux系统上的内核VM布局！在接下来的部分中，我们将尝试通过我们的`procmap`实用程序“看”（可视化）内核VAS。
- en: The kernel VAS via procmap
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过procmap的内核VAS
- en: 'Okay, this is interesting: the view of the memory map layout seen in some detail
    in the preceding figure is exactly what our aforementioned `procmap` utility provides!
    As promised earlier, let''s now see screenshots of the kernel VAS when running
    `procmap` (earlier, we showed screenshots of the user VAS).'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，这很有趣：在前面的图中以某些细节看到的内存映射布局的视图正是我们前面提到的`procmap`实用程序提供的！正如之前承诺的，现在让我们看一下运行`procmap`时内核VAS的截图（之前，我们展示了用户VAS的截图）。
- en: 'To keep in sync with the immediate discussion, we will now show screenshots
    of `procmap` providing a "visual" view of the kernel VAS on the very same Raspberry
    Pi 3B+ system (we could specify the  `--only-kernel` switch to show only the kernel
    VAS; we don''t do so here, though). As we have to run `procmap` on some process,
    we arbitrarily choose *systemd* PID `1`; we also use the `--verbose` option switch.
    However, it seems to fail:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与即时讨论保持同步，我们现在将展示`procmap`在同一台树莓派3B+系统上提供内核VAS的“视觉”视图的截图（我们可以指定`--only-kernel`开关来仅显示内核VAS；尽管我们在这里没有这样做）。由于我们必须在某个进程上运行`procmap`，我们任意选择*systemd*
    PID `1`；我们还使用`--verbose`选项开关。然而，似乎失败了：
- en: '![](img/0a141b0f-4c58-4675-954e-48d70c8ae89d.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a141b0f-4c58-4675-954e-48d70c8ae89d.png)'
- en: Figure 7.13 – Truncated screenshot showing the procmap kernel module build failing
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 - 显示procmap内核模块构建失败的截图
- en: 'Why did it fail to build the kernel module (that''s part of the `procmap` project)?
    I mention this in the project''s `README.md` file ([https://github.com/kaiwan/procmap/blob/master/README.md#procmap](https://github.com/kaiwan/procmap/blob/master/README.md#procmap)):'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么构建内核模块失败了（这是`procmap`项目的一部分）？我在项目的`README.md`文件中提到了这一点（[https://github.com/kaiwan/procmap/blob/master/README.md#procmap](https://github.com/kaiwan/procmap/blob/master/README.md#procmap)）：
- en: '[PRE15]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The kernel headers package for our *custom* 5.4 kernel (for the Raspberry Pi)
    isn't available, hence it fails. While you can conceivably copy in the entire
    5.4 Raspberry Pi kernel source tree onto the device and set up the `/lib/module/<kver>/build`
    symbolic link, this isn't considered the right way to do so. So, what is? *Cross-compiling*
    the `procmap` kernel module for the Raspberry Pi from your host, of course! We
    have covered the details on cross-compiling the kernel itself for the Raspberry
    Pi here in [Chapter 3](93e5c09d-6c80-47e7-91ab-d3f3f25d00e1.xhtml), *Building
    the 5.x Linux Kernel from Source - Part 2*, in the *Kernel Build for the Raspberry
    Pi* section; it, of course, applies to cross-compiling kernel modules as well.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的*自定义*5.4内核（用于树莓派）的内核头文件包不可用，因此失败了。虽然您可以想象地将整个5.4树莓派内核源树复制到设备上，并设置`/lib/module/<kver>/build`符号链接，但这并不被认为是正确的做法。那么，正确的做法是什么？当然是从主机上*交叉编译*树莓派的`procmap`内核模块！我们在这里的[第3章](93e5c09d-6c80-47e7-91ab-d3f3f25d00e1.xhtml)中涵盖了有关从源代码构建树莓派内核的交叉编译的详细信息，*构建5.x
    Linux内核的第2部分*，在*树莓派的内核构建*部分；当然，这也适用于交叉编译内核模块。
- en: 'I want to stress this point: the `procmap` kernel module build on the Raspberry
    Pi only fails due to the lack of a Raspberry Pi-supplied kernel headers package
    when running a custom kernel. If you are happy to work with the stock (default)
    Raspberry Pi kernel (earlier called Raspbian OS), the kernel headers package is
    certainly installable (or already installed) and everything will work. Similarly,
    on your typical x86_64 Linux distribution, the `procmap.ko` kernel module gets
    cleanly built and inserted at runtime. Do read the `procmap` project''s `README.md` file
    in detail; within it, the section labeled *IMPORTANT: Running procmap on systems
    other than x86_64* details how to cross-compile the `procmap` kernel module.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '我想强调一点：树莓派上的`procmap`内核模块构建仅因为在运行自定义内核时缺少树莓派提供的内核头文件包而失败。如果您愿意使用默认的树莓派内核（之前称为Raspbian
    OS），那么内核头文件包肯定是可安装的（或已安装），一切都将正常工作。同样，在您典型的x86_64 Linux发行版上，`procmap.ko`内核模块可以在运行时得到干净地构建和插入。请仔细阅读`procmap`项目的`README.md`文件；其中，标有*IMPORTANT:
    Running procmap on systems other than x86_64*的部分详细说明了如何交叉编译`procmap`内核模块。'
- en: Once you successfully cross-compile the `procmap` kernel module on your host
    system, copy across the `procmap.ko` kernel module (via `scp(1)`, perhaps) to
    the device and place it under the `procmap/procmap_kernel` directory; now you're
    ready to go!
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您成功在主机系统上交叉编译了`procmap`内核模块，通过`scp(1)`将`procmap.ko`内核模块复制到设备上，并将其放置在`procmap/procmap_kernel`目录下；现在您已经准备好了！
- en: 'Here''s the copied-in kernel module (on the Raspberry Pi):'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这是复制到树莓派上的内核模块：
- en: '[PRE16]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: (You can also run the `modinfo(8)` utility on it to verify that it's built for
    ARM.)
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: （您也可以在其上运行`modinfo(8)`实用程序，以验证它是否为ARM构建。）
- en: 'With this in place, let''s retry our `procmap` run to display the kernel VAS
    details:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，让我们重试一下我们的`procmap`运行，以显示内核VAS的详细信息：
- en: '![](img/2e812f0d-2abf-4b94-bc9c-9d82b5ea570c.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2e812f0d-2abf-4b94-bc9c-9d82b5ea570c.png)'
- en: Figure 7.14 – Truncated screenshot showing the procmap kernel module successfully
    inserted and various system details
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 - 显示procmap内核模块成功插入和各种系统详细信息的截图
- en: It does work now! As we've specified the `verbose` option to `procmap`, you
    get to see its detailed progress, as well as – quite usefully – various kernel
    variables/macros of interest and their current value.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 现在它确实起作用了！由于我们已经将`verbose`选项指定给`procmap`，因此您可以看到它的详细进展，以及非常有用的各种感兴趣的内核变量/宏及其当前值。
- en: 'Okay, let''s continue and view what we''re really after – the "visual map"
    of the kernel VAS on the Raspberry Pi 3B+, in descending order by KVA; the following
    screenshot captures this output from `procmap`:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们继续查看我们真正想要的内容-树莓派3B+上内核VAS的“可视地图”，按KVA降序排列；以下截图捕获了`procmap`的输出：
- en: '![](img/b7c5a153-2b47-422b-a128-b907bd31718d.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b7c5a153-2b47-422b-a128-b907bd31718d.png)'
- en: Figure 7.15 – Partial screenshot of our procmap utility's output showing the
    complete kernel VAS (Raspberry Pi 3B+ with 32-bit Linux)
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15-我们的procmap实用程序输出的部分截图，显示了树莓派3B+上完整的内核VAS（32位Linux）
- en: 'The complete kernel VAS – from `end_kva` (value `0xffff ffff`) right to the
    start of the kernel, `start_kva` (`0x7f00 0000`, which, as you can see, is the
    kernel module region) – is displayed. Notice (in green color) the label on the
    right of certain key addresses denoting what they are! For completeness, we also
    included in the preceding screenshot the kernel-user boundary (and the upper portion
    of the user VAS below the kernel segment, just as we have been saying all along!).
    As the preceding output is on a 32-bit system, the user VAS immediately follows
    the kernel segment. On a 64-bit system though, there is an (enormous!) "non-canonical"
    sparse region between the start of the kernel segment and the top of the user
    VAS. On the x86_64 (as we have already discussed), it spans the vast majority
    of the VAS: 16,383.75 petabytes (out of a total VAS of 16,384 petabytes)!'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的内核VAS-从`end_kva`（值为`0xffff ffff`）右到内核的开始，`start_kva`（`0x7f00 0000`，正如你所看到的，是内核模块区域）-被显示出来。请注意（绿色）标签右侧的某些关键地址的标注！为了完整起见，我们还在前面的截图中包括了内核-用户边界（以及用户VAS的上部分，就像我们一直在说的那样！）。由于前面的输出是在32位系统上，用户VAS紧随内核段。然而，在64位系统上，内核段和用户VAS之间有一个（巨大的！）“非规范”稀疏区域。在x86_64上（正如我们已经讨论过的），它跨越了VAS的绝大部分：16,383.75拍字节（总VAS为16,384拍字节）！
- en: I will leave it as an exercise to you to run this `procmap` project and carefully
    study the output (on your x86_64 or whichever box or VM). It also works well on
    a BeagleBone Black embedded board with a 3:1 VM split, showing details as expected.
    FYI, this forms an assignment.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我将把运行这个`procmap`项目的练习留给你，仔细研究你的x86_64或其他盒子或虚拟机上的输出。它在带有3:1虚拟机分割的BeagleBone Black嵌入式板上也能正常工作，显示了预期的详细信息。顺便说一句，这构成了一个作业。
- en: 'I also provide a solution in the form of three (large, stitched-together) screenshots
    of `procmap`''s output on a native x86_64 system, a BeagleBone Black (AArch32)
    board, and the Raspberry Pi running a 64-bit OS (AArch64) here: `solutions_to_assgn/ch7`.
    Studying the code of `procmap`*,* and, especially relevant here, its kernel module
    component, will certainly help. It''s open source, after all!'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我还提供了一个解决方案，以三个（大的、拼接在一起的）`procmap`输出的截图形式，分别是在本机x86_64系统、BeagleBone Black（AArch32）板和运行64位操作系统（AArch64）的树莓派上：`solutions_to_assgn/ch7`。研究`procmap`的代码*，*特别是它的内核模块组件，肯定会有所帮助。毕竟它是开源的！
- en: Let's finish this section by glancing at the user segment view that our earlier
    demo kernel module – `ch7/show_kernel_seg` – provides.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过查看我们之前的演示内核模块`ch7/show_kernel_seg`提供的用户段视图来完成本节。
- en: Trying it out – the user segment
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尝试一下-用户段
- en: 'Now, let''s go back to our `ch7/show_kernel_seg`LKM demo program. We have provided
    a kernel module parameter named `show_uservas`(defaulting to the value `0`); when
    set to `1`, some details regarding the process context''s *user space* are displayed
    as well. Here''s the definition of the module parameter:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到我们的`ch7/show_kernel_seg`LKM演示程序。我们提供了一个名为`show_uservas`的内核模块参数（默认值为`0`）；当设置为`1`时，还会显示有关进程上下文的*用户空间*的一些详细信息。以下是模块参数的定义：
- en: '[PRE17]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Right, on the same device (our Raspberry Pi 3 B+), let''s again run our `show_kernel_seg`
    kernel module, this time requesting it to display user space details as well (via
    the aforementioned parameter). The following screenshot shows the complete output:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，在同一设备上（我们的树莓派3 B+），让我们再次运行我们的`show_kernel_seg`内核模块，这次请求它也显示用户空间的详细信息（通过前面提到的参数）。以下截图显示了完整的输出：
- en: '![](img/a2b47043-b85d-4761-ac81-71b11c9a10a2.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2b47043-b85d-4761-ac81-71b11c9a10a2.png)'
- en: Figure 7.16 – Screenshot of our show_kernel_seg.ko LKM's output showing both
    kernel and user VAS details when running on a Raspberry Pi 3B+ with the stock
    Raspberry Pi 32-bit Linux OS
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16-我们的show_kernel_seg.ko LKM的输出截图，显示了在树莓派3B+上运行时内核和用户VAS的详细信息，带有树莓派32位Linux操作系统
- en: This is useful; we can now literally see a (more or less) complete memory map
    of the process – both the so-called "upper (canonical) half" kernel-space as well
    as the "lower (canonical) half" user space – in one shot (yes, that's right, even
    though the `procmap` project shows this better and in more detail).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有用；我们现在可以看到进程的（或多或少）完整的内存映射-所谓的“上（规范）半”内核空间以及“下（规范）半”用户空间-一次性看清楚（是的，没错，尽管`procmap`项目显示得更好，更详细）。
- en: 'I will leave it as an exercise to you to run this kernel module and carefully
    study the output on your x86_64, or whichever box or VM. Do carefully go through
    the code as well. We printed the user space details that you see in the preceding
    screenshot, such as the segment start and end addresses, by dereferencing the 
    `mm_struct` structure (the task structure member named `mm`) from `current`. Recall,
    `mm` is the abstraction of the user mapping of the process. A small snippet of
    the code that does this is as follows:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我将把运行这个内核模块的练习留给你，仔细研究你的x86_64或其他盒子或虚拟机上的输出。也要仔细阅读代码。我们通过从`current`中解引用`mm_struct`结构（名为`mm`的任务结构成员）打印了你在前面截图中看到的用户空间详细信息的代码段。回想一下，`mm`是进程用户映射的抽象。执行此操作的代码片段如下：
- en: '[PRE18]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Remember the so-called null trap page at the very beginning of the user VAS?
    (Again, `procmap`'s output – see *Figure 7.9* – shows the null trap page.) Let's
    see what it's for in the following section.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得用户VAS开头的所谓空陷阱页面吗？（再次，`procmap`的输出-参见*图7.9*显示了空陷阱页面。）让我们在下一节中看看它是用来做什么的。
- en: The null trap page
  id: totrans-331
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 空陷阱页面
- en: 'Did you notice how the preceding diagrams (Figure 7.9) and, in and Figure 7.12,
    at the extreme left edge (albeit very small!), a single page at the very beginning
    of the user space, named the **null trap** page? What is it? That''s easy: virtual
    page `0` is given no permissions (at the hardware MMU/PTE level). Thus, any access
    to this page, be it `r`, `w`, or `x`  (read/write/execute), will result in the
    MMU raising what is called a fault or exception. This will have the processor
    jump to an OS handler routine (the fault handler). It runs, killing the culprit
    trying to access a memory region with no permissions!'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 您是否注意到前面的图表（图7.9）和图7.12中，极左边（尽管非常小！）用户空间开头的单个页面，名为**null trap**页面？这是什么？很简单：虚拟页面`0`在硬件MMU/PTE级别上没有权限。因此，对该页面的任何访问，无论是`r`，`w`还是`x`（读/写/执行），都将导致MMU引发所谓的故障或异常。这将使处理器跳转到OS处理程序（故障处理程序）。它运行，杀死试图访问没有权限的内存区域的罪犯！
- en: 'It''s very interesting indeed: the OS handler mentioned previously runs in
    process context, and guess what `current`is: why, it''s the process (or thread)
    that initiated this bad `NULL` pointer lookup! Within the fault handler code,
    the `SIGSEGV` signal is delivered to the faulting process (`current`), causing
    it to die (via a segfault). In a nutshell, this is how the well-known `NULL` pointer
    dereference bug is caught by the OS.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 非常有趣：先前提到的OS处理程序实际上在进程上下文中运行，猜猜`current`是什么：哦，它是启动这个坏`NULL`指针查找的进程（或线程）！在故障处理程序代码中，`SIGSEGV`信号被传递给故障进程（`current`），导致其死亡（通过段错误）。简而言之，这就是OS如何捕获众所周知的`NULL`指针解引用错误的方式。
- en: Viewing kernel documentation on the memory layout
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查看内核文档中的内存布局
- en: 'Back to the kernel segment; obviously, with a 64-bit VAS, the kernel segment
    is *much* larger than on 32-bit. As we saw earlier, it''s typically 128 TB on
    the x86_64\. Study again the VM split table shown previously (Figure 7.4 in the
    section *VM split on 64-bit Linux systems*); there, the fourth column is the VM
    split for different architectures. You can see how on the 64-bit Intel/AMD and
    AArch64 (ARM64), the numbers are much larger than for their 32-bit counterparts.
    For arch-specific details, we refer you to the ''official'' kernel documentation
    on the process virtual memory layout here:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 回到内核段；显然，对于64位VAS，内核段比32位的要大得多。正如我们之前看到的，对于x86_64，它通常是128 TB。再次研究先前显示的VM分割表（图7.4中的*64位Linux系统上的VM分割*部分）；在那里，第四列是不同架构的VM分割。您可以看到在64位Intel/AMD和AArch64（ARM64）上，这些数字比32位的大得多。有关特定于架构的详细信息，我们建议您参考此处有关进程虚拟内存布局的“官方”内核文档：
- en: '| **Architecture** | **Documentation location in kernel source tree** |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| **架构** | **内核源树中的文档位置** |'
- en: '| ARM-32 | `Documentation/arm/memory.txt`. |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| ARM-32 | `Documentation/arm/memory.txt`。'
- en: '| AArch64 | `Documentation/arm64/memory.txt`. |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| AArch64 | `Documentation/arm64/memory.txt`。'
- en: '| x86_64 | `Documentation/x86/x86_64/mm.txt` Note: this document''s readability
    was vastly improved recently (as of the time of writing) with commit `32b8976`
    for Linux 4.20: [https://github.com/torvalds/linux/commit/32b89760ddf4477da436c272be2abc016e169031](https://github.com/torvalds/linux/commit/32b89760ddf4477da436c272be2abc016e169031).
    I recommend you browse through this file: [https://www.kernel.org/doc/Documentation/x86/x86_64/mm.txt](https://www.kernel.org/doc/Documentation/x86/x86_64/mm.txt).
    |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| x86_64 | `Documentation/x86/x86_64/mm.txt` 注意：此文档的可读性最近得到了极大改善（截至撰写时）Linux
    4.20的提交`32b8976`：[https://github.com/torvalds/linux/commit/32b89760ddf4477da436c272be2abc016e169031](https://github.com/torvalds/linux/commit/32b89760ddf4477da436c272be2abc016e169031)。我建议您浏览此文件：[https://www.kernel.org/doc/Documentation/x86/x86_64/mm.txt](https://www.kernel.org/doc/Documentation/x86/x86_64/mm.txt)。'
- en: At the risk of repetition, I urge you to try out this `show_kernel_seg` kernel
    module – and, even better, the `procmap` project ([https://github.com/kaiwan/procmap](https://github.com/kaiwan/procmap))
    – on different Linux systems and study the output. You can then literally see
    the "memory map" – the complete process VAS – of any given process, which includes
    the kernel segment! This understanding is critical when working with and/or debugging
    issues at the system layer.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 冒着重复的风险，我敦促您尝试这个`show_kernel_seg`内核模块 - 更好的是，`procmap`项目（[https://github.com/kaiwan/procmap](https://github.com/kaiwan/procmap)）-
    在不同的Linux系统上并研究输出。然后，您可以直接看到任何给定进程的“内存映射” - 完整的进程VAS - 包括内核段！在处理和/或调试系统层问题时，这种理解至关重要。
- en: Again, at the risk of overstating it, the previous two sections – covering the
    detailed examination of the *user and kernel VASes* – are very important indeed.
    Do take the time required to go over them and work on the sample code and assignments.
    Great going!
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 再次冒着过度陈述的风险，前两节 - 涵盖对*用户和内核VASes*进行详细检查 - 确实非常重要。确保花费足够的时间来研究它们并处理示例代码和作业。做得好！
- en: Moving along on our journey through the Linux kernel's memory management, let's
    now check out another interesting topic – that of the [K]ASLR protection-via-memory-layout-randomization
    feature. Read on!
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们通过Linux内核内存管理的旅程中继续前进，现在让我们来看看另一个有趣的主题 - [K]ASLR通过内存布局随机化功能的保护。继续阅读！
- en: Randomizing the memory layout – KASLR
  id: totrans-343
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机化内存布局 - KASLR
- en: In infosec circles, it's a well-known fact that, with **proc filesystem** (**procfs**)
    and various powerful tools at their disposal, a malicious user, knowing in advance
    the precise location (virtual addresses) of various functions and/or globals with
    a process's VAS, could devise an attack to exploit and ultimately compromise a
    given system. Thus, for security, to make it impossible (or at least difficult)
    for attackers to rely on "known" virtual addresses, user space as well as kernel
    space supports **ASLR (Address Space Layout Randomization) **and **KASLR (Kernel
    ASLR)** techniques (often pronounced *Ass-**ler* / *Kass-ler*).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在信息安全圈中，众所周知的事实是，利用**proc文件系统**（procfs）和各种强大的工具，恶意用户可以预先知道进程VAS中各种函数和/或全局变量的精确位置（虚拟地址），从而设计攻击并最终
    compromise给定系统。因此，为了安全起见，为了使攻击者无法依赖于“已知”虚拟地址，用户空间以及内核空间支持**ASLR（地址空间布局随机化）**和**KASLR（内核ASLR）**技术（通常发音为*Ass-**ler*
    / *Kass-ler*）。
- en: The keyword here is *randomization:* this feature, when enabled, *changes the
    location* of portions of the process (and kernel) memory layout in terms of absolute
    numbers as it *offsets portions of memory* from a given base address by a random
    (page-aligned) quantity. What "portions of memory" exactly are we talking about?
    With respect to user space mappings (we will talk about KASLR later), the starting
    addresses of shared libraries (their load address), `mmap(2)`-based allocations
    (remember, any `malloc()` function (`/calloc/realloc`*)* above 128 KB becomes
    an `mmap`-based allocation, not off the heap), stack start, the heap, and the
    vDSO page; all of these can be randomized at process run (launch) time.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键词是*随机化:* 当启用此功能时，它会*改变*进程（和内核）内存布局的部分位置，以绝对数字来说，它会通过随机（页面对齐）数量*偏移内存的部分*从给定的基址。我们到底在谈论哪些“内存部分”？关于用户空间映射（稍后我们将讨论KASLR），共享库的起始地址（它们的加载地址），`mmap(2)`-based分配（记住，任何`malloc()`函数（`/calloc/realloc`*）*超过128
    KB都会成为`mmap`-based分配，而不是堆外分配），堆栈起始位置，堆和vDSO页面；所有这些都可以在进程运行（启动）时被随机化。
- en: Hence, an attacker cannot depend on, say, a `glibc` function (such as `system(3)`)
    being mapped at a particular fixed UVA in any given process; not only that, the
    location will vary every time the process runs! Before ASLR, and on systems where
    ASLR is unsupported or turned off, the location of symbols can be ascertained
    in advance for a given architecture and software version (procfs plus utilities
    like `objdump`, `readelf`, `nm`, and so on make this quite easy).
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，攻击者不能依赖于，比如说，`glibc`函数（比如`system(3)`）在任何给定进程中被映射到特定的固定UVA；不仅如此，位置每次进程运行时都会变化！在ASLR之前，以及在不支持或关闭ASLR的系统上，可以提前确定给定架构和软件版本的符号位置（procfs加上诸如`objdump`、`readelf`、`nm`等实用程序使这变得非常容易）。
- en: It's key to realize that [K]ASLR is merely a statistical protection. In fact,
    typically, not many bits are available for randomization and thus the entropy
    isn't very good. This implies that the page-sized offsets are not too many, even
    on 64-bit systems, thus leading to a possibly weakened implementation.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于要意识到[K]ASLR只是一种统计保护。事实上，通常情况下，并没有太多比特可用于随机化，因此熵并不是很好。这意味着即使在64位系统上，页面大小的偏移量也不是很多，因此可能导致实现受到削弱。
- en: Let's now briefly look at a few more details regarding both user mode and kernel-mode
    ASLR (the latter being referred to as KASLR); the following sections cover these
    areas, respectively.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们简要地看一下关于用户模式和内核模式ASLR（后者被称为KASLR）的更多细节；以下各节分别涵盖了这些领域。
- en: User-mode ASLR
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户模式ASLR
- en: User-mode ASLR is usually what is meant by the term ASLR. It being enabled implies
    this protection to be available on the user space mapping of every process. Effectively,
    ASLR being enabled implies that the absolute memory map of user-mode processes
    will vary every time they're run.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 通常所说的ASLR指的是用户模式ASLR。它的启用意味着这种保护在每个进程的用户空间映射上都是可用的。实际上，ASLR的启用意味着用户模式进程的绝对内存映射每次运行时都会有所变化。
- en: ASLR has been supported on Linux for a very long time (since 2005 on 2.6.12).
    The kernel has a tunable pseudo-file within procfs, to query and set (as root)
    the ASLR status; here it is: `/proc/sys/kernel/randomize_va_space`.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ASLR在Linux上已经得到支持很长时间了（自2.6.12以来）。内核在procfs中有一个可调的伪文件，可以查询和设置（作为root）ASLR的状态；在这里：`/proc/sys/kernel/randomize_va_space`。
- en: 'It can have three possible values; the three values and their meaning are shown
    in the following table:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以有三个可能的值；这三个值及其含义如下表所示：
- en: '| **Tunable value** | **Interpretation of this value in `/proc/sys/kernel/randomize_va_space`**
    |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| **可调值** | **在`/proc/sys/kernel/randomize_va_space`中对该值的解释** |'
- en: '| `0` | (User mode) ASLR turned OFF; or can be turned off by passing the kernel
    parameter `norandmaps` at boot. |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| `0` | （用户模式）ASLR已关闭；或者可以通过在启动时传递内核参数`norandmaps`来关闭。|'
- en: '| `1` | (User mode) ASLR is ON: `mmap(2)` based allocations, the stack, and
    the vDSO page is randomized. It also implies that shared library load locations
    and shared memory segments are randomized. |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| `1` | （用户模式）ASLR已开启：基于`mmap(2)`的分配，堆栈和vDSO页面被随机化。这也意味着共享库加载位置和共享内存段被随机化。|'
- en: '| `2` | (User mode) ASLR is ON: all of the preceding (value `1`) *plus* the
    heap location is randomized (since 2.6.25); this is the OS value by default. |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| `2` | （用户模式）ASLR已开启：所有前述（值`1`）*加上*堆位置被随机化（自2.6.25起）；这是默认的操作系统值。|'
- en: '(As noted in an earlier section, *The vsyscall page*, the vDSO page is a system
    call optimization, allowing some frequently issued system calls (`gettimeofday(2)`
    being a typical one) to be invoked with less overhead. If interested, you can
    look up more details on the man page on vDSO(7) here: [https://man7.org/linux/man-pages/man7/vdso.7.html](https://man7.org/linux/man-pages/man7/vdso.7.html).[)](https://man7.org/linux/man-pages/man7/vdso.7.html)'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: （正如前面的一节中所指出的，*vsyscall页面*，vDSO页面是一种系统调用优化，允许一些频繁发出的系统调用（`gettimeofday(2)`是一个典型的例子）以更少的开销来调用。如果感兴趣，您可以在这里查看有关vDSO(7)的man页面的更多详细信息：[https://man7.org/linux/man-pages/man7/vdso.7.html](https://man7.org/linux/man-pages/man7/vdso.7.html)。[)](https://man7.org/linux/man-pages/man7/vdso.7.html)
- en: User-mode ASLR can be turned *off* at boot by passing the `norandmaps` parameter
    to the kernel (via the bootloader).
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 用户模式ASLR可以通过在启动时通过引导加载程序向内核传递`norandmaps`参数来关闭。
- en: KASLR
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KASLR
- en: Similar to (user) ASLR – and, more recently, from the 3.14 kernel onward – even
    *kernel* VAS can be randomized (to some extent) by having KASLR enabled. Here,
    the base location of the kernel and module code within the kernel segment will
    be randomized by a page-aligned random offset from the base of RAM. This remains
    in effect for that session; that is, until a power cycle or reboot.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于（用户）ASLR - 而且，更近期的是从3.14内核开始 - 甚至*内核*VAS也可以通过启用KASLR来随机化（在某种程度上）。在这里，内核和内核段内的模块代码的基本位置将通过与RAM基址的页面对齐随机偏移量而被随机化。这将在该会话中保持有效；也就是说，直到重新上电或重启。
- en: 'Several kernel configuration variables exist, enabling the platform developer
    to enable or disable these randomization options. As an example specific to the
    x86, the following is quoted directly from `Documentation/x86/x86_64/mm.txt`:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多个内核配置变量，使平台开发人员能够启用或禁用这些随机化选项。作为x86特定的一个例子，以下是直接从`Documentation/x86/x86_64/mm.txt`中引用的：
- en: '"Note that if CONFIG_RANDOMIZE_MEMORY is enabled, the direct mapping of all
    physical memory, vmalloc/ioremap space and virtual memory map are randomized.
    Their order is preserved but their base will be offset early at boot time."'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: “请注意，如果启用了CONFIG_RANDOMIZE_MEMORY，所有物理内存的直接映射，vmalloc/ioremap空间和虚拟内存映射都将被随机化。它们的顺序被保留，但它们的基址将在引导时提前偏移。”
- en: 'KASLR can be controlled at boot time by passing a parameter to the kernel (via
    the bootloader):'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: KASLR可以通过向内核传递参数（通过引导加载程序）在引导时进行控制：
- en: Explicitly turned *off* by passing the `nokaslr` parameter
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过传递`nokaslr`参数明确关闭
- en: Explicitly turned *on* by passing the `kaslr` parameter
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过传递`kaslr`参数明确打开
- en: So, what is the current setting on your Linux system? And can we change it?
    Yes, of course (provided we have *root* access); the next section shows you how
    to do so via a Bash script.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，您的Linux系统当前的设置是什么？我们可以更改它吗？当然可以（只要我们有*root*访问权限）；下一节将向您展示如何通过Bash脚本进行操作。
- en: Querying/setting KASLR status with a script
  id: totrans-367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用脚本查询/设置KASLR状态
- en: We provide a simple Bash script at `<book-source>/ch7/ASLR_check.sh`. It checks
    for the presence of both (user-mode) ASLR as well as KASLR, printing (color-coded!)
    status information about them. It also allows you to change the ASLR value.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在`<book-source>/ch7/ASLR_check.sh`提供了一个简单的Bash脚本。它检查（用户模式）ASLR和KASLR的存在，并打印（彩色编码！）有关它们的状态信息。它还允许您更改ASLR值。
- en: 'Let''s give it a spin on our x86_64 Ubuntu 18.04 guest. As our script is programmed
    to be color-coded, we show a screenshot of its output here:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的x86_64 Ubuntu 18.04客户端上试一试。由于我们的脚本被编程为彩色编码，我们在这里展示它的输出截图：
- en: '![](img/dba461a6-a59f-4bb7-9161-d118e6960c4b.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dba461a6-a59f-4bb7-9161-d118e6960c4b.png)'
- en: Figure 7.17 – Screenshot showing the output when our ch7/ASLR_check.sh Bash
    script runs on an x86_64 Ubuntu guest
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17 - 当我们的ch7/ASLR_check.sh Bash脚本在x86_64 Ubuntu客户端上运行时显示的输出截图
- en: 'It runs, showing you that (at least on this box) both the user mode as well
    as KASLR are indeed turned on. Not only that, we write a small "test" routine
    to see ASLR functioning. It''s very simple: it runs the following command twice:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 它运行，向您显示（至少在此框上）用户模式和KASLR确实已打开。不仅如此，我们编写了一个小的“测试”例程来查看ASLR的功能。它非常简单：运行以下命令两次：
- en: '[PRE19]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'From what you learned in an earlier section, *Interpreting the /proc/PID/maps
    output*, you can now see in Figure 7.17, that the UVAs for the heap and stack
    segments are *different in each run*, thus proving that the ASLR feature indeed
    works! For example, look at the starting heap UVA: in the first run, it''s `0x5609
    15f8 2000`, and in the second run, it''s `0x5585 2f9f 1000`.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您在早期章节中学到的内容，*解释/proc/PID/maps输出*，您现在可以在图7.17中看到，堆和栈段的UVAs在每次运行中都是*不同的*，从而证明ASLR功能确实有效！例如，看一下起始堆UVA：在第一次运行中，它是`0x5609
    15f8 2000`，在第二次运行中，它是`0x5585 2f9f 1000`。
- en: 'Next, we will perform a sample run where we pass the parameter `0` to the script,
    thus turning ASLR off; the following screenshot shows the (expected) output:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将进行一个示例运行，其中我们向脚本传递参数`0`，从而关闭ASLR；以下截图显示了（预期的）输出：
- en: '![](img/68e447d3-1b41-44ee-a652-3ac155e0d18a.png)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![](img/68e447d3-1b41-44ee-a652-3ac155e0d18a.png)'
- en: Figure 7.18 – Screenshot showing how ASLR is turned off (via our ch7/ASLR_check.sh
    script on an x86_64 Ubuntu guest)
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18 - 展示了如何通过我们的ch7/ASLR_check.sh脚本在x86_64 Ubuntu客户端上关闭ASLR的截图
- en: This time, we can see that ASLR was on by default, but we turned it off. This
    is clearly highlighted in bold font and red in the preceding screenshot. (Do remember
    to turn it on again.) Also, as expected, as it's off, the UVAs of both the heap
    and stack (respectively) remain the same in both test runs, which is insecure.
    I will leave it to you to browse through and understand the source code of the
    script.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们可以看到ASLR默认是打开的，但我们关闭了它。这在上面的截图中以粗体和红色清楚地突出显示。（请记住再次打开它。）此外，正如预期的那样，由于它已关闭，堆和栈的UVAs（分别）在两次测试运行中保持不变，这是不安全的。我将让您浏览并理解脚本的源代码。
- en: To take advantage of ASLR, applications must be compiled with the `-fPIE` and
    `-pie` GCC flags (**PIE** stands for **Position Independent Executable**).
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 要利用ASLR，应用程序必须使用`-fPIE`和`-pie`GCC标志进行编译（**PIE**代表**Position Independent Executable**）。
- en: Both ASLR and KASLR protect against some types of attack vectors, the return-to-libc, **Return-Oriented
    Programming**(ROP) ones being the typical cases. However, and unfortunately, white
    and black hat security being the cat-and-mouse game it is, defeating [K]ASLR and
    similar methodologies is something advanced exploits do quite well. Refer to this
    chapter's *Further reading* section (under the *Linux kernel security* heading)
    for more details.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: ASLR和KASLR都可以防御一些攻击向量，典型情况下是返回到libc，**Return-Oriented Programming**(ROP)。然而，不幸的是，白帽和黑帽安全是一场猫鼠游戏，[K]ASLR和类似的方法被击败是一些高级攻击确实做得很好。有关更多详细信息，请参阅本章的*进一步阅读*部分（在*Linux内核安全*标题下）。
- en: 'While on the topic of security, many useful tools exist to carry out vulnerability
    checks on your system. Check out the following:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 谈到安全性，存在许多有用的工具来对系统进行漏洞检查。查看以下内容：
- en: 'The `checksec.sh` script ([http://www.trapkit.de/tools/checksec.html](http://www.trapkit.de/tools/checksec.html))
    displays various "hardening" measures and their current status (for both individual
    files and processes): RELRO, stack canary, NX-enabled, PIE, RPATH, RUNPATH, presence
    of symbols, and compiler fortification.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`checksec.sh`脚本（[http://www.trapkit.de/tools/checksec.html](http://www.trapkit.de/tools/checksec.html)）显示各种“硬化”措施及其当前状态（对于单个文件和进程）：RELRO，堆栈canary，启用NX，PIE，RPATH，RUNPATH，符号的存在和编译器强化。'
- en: grsecurity's PaX suite.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: grsecurity的PaX套件。
- en: The `hardening-check` script (an alternative to checksec).
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`hardening-check`脚本（checksec的替代品）。'
- en: The `kconfig-hardened-check` Perl script ([https://github.com/a13xp0p0v/kconfig-hardened-check](https://github.com/a13xp0p0v/kconfig-hardened-check))
    checks (and suggests) kernel config options for security against some predefined
    checklists.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kconfig-hardened-check` Perl脚本（[https://github.com/a13xp0p0v/kconfig-hardened-check](https://github.com/a13xp0p0v/kconfig-hardened-check)）检查（并建议）内核配置选项，以防止一些预定义的检查清单中的安全问题。'
- en: 'Several others: Lynis, `linuxprivchecker.py`, memory, and so on.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他几个：Lynis，`linuxprivchecker.py`，内存等等。
- en: So, the next time you see differing kernel or user virtual addresses on multiple
    runs or sessions, you will know it's probably due to the [K]ASLR protection feature. Now,
    let's complete this chapter by moving on to an exploration of how the Linux kernel
    organizes and works with physical memory.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，下次你在多次运行或会话中看到不同的内核或用户虚拟地址时，你会知道这可能是由于[K]ASLR保护功能。现在，让我们通过继续探索Linux内核如何组织和处理物理内存来完成本章。
- en: Physical memory
  id: totrans-388
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物理内存
- en: Now that we have examined the *virtual memory *view, for both user and kernel
    VASes in some detail, let's turn to the topic of physical memory organization
    on the Linux OS.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经详细研究了*虚拟内存*视图，包括用户和内核VASes，让我们转向Linux操作系统上物理内存组织的主题。
- en: Physical RAM organization
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 物理RAM组织
- en: The Linux kernel, at boot, organizes and partitions physical RAM into a tree-like
    hierarchy consisting of nodes, zones, and page frames (page frames are physical
    pages of RAM) (see Figure 7.19 and Figure 7.20). Nodes are divided into zones,
    and zones consist of page frames. A node abstracts a physical "bank" of RAM, which
    will be associated with one or more processor (CPU) cores. At the hardware level,
    the microprocessors are connected to the RAM controller chip(s); any memory controller
    chip, and thus any RAM, can be reached from any CPU as well, across an interconnect.
    Now, obviously, being able to reach the RAM physically nearest the core on which
    a thread is allocating (kernel) memory will lead to performance enhancement. This
    very idea is leveraged by hardware and OSes that support the so-called NUMA model
    (the meaning is explained shortly).
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: Linux内核在启动时将物理RAM组织和分区为一个类似树状的层次结构，包括节点、区域和页框（页框是物理RAM页面）（参见图7.19和图7.20）。节点被划分为区域，区域由页框组成。节点抽象了一个物理的RAM“bank”，它将与一个或多个处理器（CPU）核心相关联。在硬件级别上，微处理器连接到RAM控制器芯片；任何内存控制器芯片，因此任何RAM，也可以从任何CPU访问，通过一个互连。显然，能够物理上接近线程正在分配（内核）内存的核心的RAM将会提高性能。这个想法被支持所谓的NUMA模型的硬件和操作系统所利用（这个含义很快就会解释）。
- en: Nodes
  id: totrans-392
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 节点
- en: 'Essentially, *nodes* are data structures used to denote a physical RAM module
    on the system motherboard and its associated controller chipset. Yes, we''re talking
    actual *hardware* here being abstracted via software metadata. It''s always associated
    with a physical socket (or collection of processor cores) on the system motherboard.
    Two types of hierarchies exist:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，*节点*是用于表示系统主板上的物理RAM模块及其相关控制器芯片的数据结构。是的，我们在这里谈论的是实际的*硬件*通过软件元数据进行抽象。它总是与系统主板上的物理插座（或处理器核心集合）相关联。存在两种类型的层次结构：
- en: '**Non-Uniform Memory Access (NUMA)** **systems**: Where the core on which a
    kernel allocation request occurs does matter (memory is treated *non* uniformly),
    leading to performance improvements'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非统一内存访问（NUMA）**系统：核心对内核分配请求的位置很重要（内存被*非*统一地处理），从而提高性能。'
- en: '**Uniform Memory Access (UMA)** **systems**: Where the core on which a kernel
    allocation request occurs doesn''t matter (memory is treated uniformly)'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统一内存访问（UMA）**系统：核心对内核分配请求的位置并不重要（内存被统一处理）'
- en: 'True NUMA systems are those whose hardware is multicore (two or more CPU cores,
    SMP) *and* have two or more physical "banks" of RAM each of which is associated
    with a CPU (or CPUs). In other words, NUMA systems will always have two or more
    nodes, whereas UMA systems will have exactly one node (FYI, the data structure
    that abstracts a node is called `pg_data_t` and is defined here: `include/linux/mmzone.h:pg_data_t`).'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的NUMA系统是那些硬件是多核（两个或更多CPU核心，SMP）*并且*有两个或更多物理RAM“bank”，每个与一个CPU（或多个CPU）相关联。换句话说，NUMA系统将始终具有两个或更多节点，而UMA系统将具有一个节点（FYI，抽象节点的数据结构称为`pg_data_t`，在这里定义：`include/linux/mmzone.h:pg_data_t`）。
- en: Why all this complexity, you may wonder? Well, it's – what else – all about
    performance! NUMA systems (they typically tend to be rather expensive server-class
    machines) and the OSes they run (Linux/Unix/Windows, typically) are designed in
    such a way that when a process (or thread) on a particular CPU core wants to perform
    a kernel memory allocation, the software guarantees that it does so with high
    performance by taking the required memory (RAM) from the node closest to the core
    (hence the NUMA moniker!). No such benefits accrue to UMA systems (your typical
    embedded systems, smartphones, laptops, and desktops), nor do they matter. Enterprise-class
    server systems nowadays can have hundreds of processors and terabytes, even a
    few petabytes, of RAM! These are almost always architected as NUMA systems.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想为什么会有这么复杂的结构？嗯，这就是——还有什么——都是关于性能！ NUMA系统（它们通常倾向于是相当昂贵的服务器级机器）和它们运行的操作系统（通常是Linux/Unix/Windows）都是设计成这样的方式，当一个特定CPU核心上的进程（或线程）想要执行内核内存分配时，软件会保证通过从最接近核心的节点获取所需的内存（RAM）来实现高性能（因此有了NUMA的名字！）。UMA系统（典型的嵌入式系统、智能手机、笔记本电脑和台式电脑）不会获得这样的好处，也不会有影响。现在的企业级服务器系统可以拥有数百个处理器和数TB，甚至数PB的RAM！这些几乎总是作为NUMA系统进行架构。
- en: With the way that Linux is designed, though – and this is a key point – even
    regular UMA systems are treated as NUMA by the kernel (well, pseudo-NUMA). They
    will have *exactly one node;* so that's a quick way to check whether the system
    is NUMA or UMA – if there are two or more nodes, it's a true NUMA system; only
    one, and it's a "fake NUMA" or pseudo-NUMA box. How can you check? The `numactl(8)`
    utility is one way (try doing `numactl --hardware`). There are other ways to (via
    *procfs* itself). Hang on a bit, you'll get there...
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于Linux的设计方式，这是一个关键点，即使是常规的UMA系统也被内核视为NUMA（好吧，伪NUMA）。它们将有*恰好一个节点*；所以这是一个快速检查系统是否是NUMA还是UMA的方法
    - 如果有两个或更多节点，它是一个真正的NUMA系统；只有一个，它就是一个“伪NUMA”或伪NUMA盒子。你怎么检查？`numactl(8)`实用程序是一种方法（尝试执行`numactl
    --hardware`）。还有其他方法（通过*procfs*本身）。稍等一下，你会到达那里的……
- en: 'So, a simpler way to visualize this: on a NUMA box, one or more CPU cores is
    associated with a "bank" (a hardware module) of physical RAM. Thus, a NUMA system
    is always a **Symmetric Multi Processor** (**SMP**) one.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个更简单的可视化方法是：在NUMA盒子上，一个或多个CPU核心与一块（硬件模块）物理RAM相关联。因此，NUMA系统总是一个**对称多处理器**（**SMP**）系统。
- en: 'To make this discussion practical, let''s briefly visualize the micro-architecture
    of an actual server system – one running the AMD Epyc/Ryzen/Threadripper (and
    the older Bulldozer) CPUs. It has the following:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个讨论更实际，让我们简要地想象一下实际服务器系统的微体系结构——一个运行AMD Epyc/Ryzen/Threadripper（以及旧的Bulldozer）CPU的系统。它有以下内容：
- en: A total of 32 CPU cores (as seen by the OS) within two physical sockets (P#0
    and P#1) on the motherboard. Each socket consists of a package of 8x2 CPU cores
    (8x2, as there are actually 8 physical cores each of which is hyperthreaded; the
    OS sees even the hyperthreaded cores as usable cores).
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在主板上有两个物理插槽（P#0和P#1）内的32个CPU核心（由操作系统看到）。每个插槽包含一个8x2 CPU核心的包（8x2，因为实际上每个核心都是超线程的；操作系统甚至将超线程核心视为可用核心）。
- en: A total of 32 GB of RAM split up into four physical banks of 8 GB each.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总共32GB的RAM分成四个物理内存条，每个8GB。
- en: Thus, the Linux memory management code, upon detecting this topography at boot,
    will set up *four nodes* to represent it. (We won't delve into the processor's
    various (L1/L2/L3/etc) caches here; see the *Tip* box after the following diagram
    for a way to see all of this.)
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Linux内存管理代码在引导时检测到这种拓扑结构后，将设置*四个节点*来表示它。（我们不会在这里深入讨论处理器的各种（L1/L2/L3等）缓存；在下图后的*提示*框中有一种方法可以查看所有这些。）
- en: 'The following conceptual diagram shows an approximation of the four tree-like
    hierarchies – one for each node – formed on some AMD server systems running the
    Linux OS. Figure 7.19 conceptually shows the nodes/zones/page frames per physical
    RAM bank on the system coupled to different CPU cores:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 以下概念图显示了在运行Linux OS的一些AMD服务器系统上形成的四个树状层次结构的近似情况 - 每个节点一个。图7.19在系统上显示了每个物理RAM条的节点/区域/页框：
- en: '![](img/5c7ffabc-ca83-4fc6-ba5a-7a05e9904389.png)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c7ffabc-ca83-4fc6-ba5a-7a05e9904389.png)'
- en: 'Figure 7.19 – (An approximate conceptual view of an) AMD server: physical memory
    hierarchy on Linux'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.19 - Linux上物理内存层次结构的（近似概念视图）
- en: Use the powerful `lstopo(1)` utility (and its associated `hwloc-*` – hardware
    locality – utilities) to graphically view the hardware (CPU) topology of your
    system! (On Ubuntu, install it with `sudo apt install hwloc`). FYI, the hardware
    topography graphic of the previously mentioned AMD server system, generated by
    `lstopo(1)`, can be seen here: [https://en.wikipedia.org/wiki/CPU_cache#/media/File:Hwloc.png](https://en.wikipedia.org/wiki/CPU_cache#/media/File:Hwloc.png).
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 使用强大的`lstopo(1)`实用程序（及其相关的`hwloc-*` - 硬件位置 - 实用程序）来图形化查看系统的硬件（CPU）拓扑结构！（在Ubuntu上，使用`sudo
    apt install hwloc`进行安装）。值得一提的是，由`lstopo(1)`生成的先前提到的AMD服务器系统的硬件拓扑图可以在这里看到：[https://en.wikipedia.org/wiki/CPU_cache#/media/File:Hwloc.png](https://en.wikipedia.org/wiki/CPU_cache#/media/File:Hwloc.png)。
- en: 'To reassert the key point here: for performance (here with respect to Figure
    7.19), a thread running some kernel or driver code in process context on, say,
    CPU #18 or above requests the kernel for some RAM. The kernel''s MM layer, understanding
    NUMA, will have the request serviced (as first priority) from any free RAM page
    frames in any zone on NUMA node #2 (that is, from physical RAM bank #2) as it''s
    "closest" to the processor core that the request was issued upon. Just in case
    there are no free page frames available in any zone within NUMA node #2, the kernel
    has an intelligent fallback system. It might now go across the interconnect and
    request RAM page frames from another node:zone (worry not, we cover these aspects
    in more detail in the following chapter).'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调这里的关键点：为了性能（这里是指图7.19），在某个处理器上运行一些内核或驱动程序代码的线程在进程上下文中请求内核获取一些RAM。内核的MM层，了解NUMA，将首先从NUMA节点＃2上的任何区域中的任何空闲RAM页框中服务请求（作为第一优先级），因为它“最接近”发出请求的处理器核心。以防在NUMA节点＃2中的任何区域中没有可用的空闲页框，内核有一个智能的回退系统。它现在可能跨连接并从另一个节点：区域请求RAM页框（不用担心，我们将在下一章节中更详细地介绍这些方面）。
- en: Zones
  id: totrans-409
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 区域
- en: Zones can be thought of as Linux's way of smoothing out and dealing with hardware
    quirks. These proliferate on the x86, where Linux "grew up," of course. They also
    deal with a few software difficulties (look up `ZONE_HIGHMEM` on the now mostly
    legacy 32-bit i386 architecture; we discussed this concept in an earlier section, *High
    memory on 32-bit systems*).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 区域可以被认为是Linux平滑处理和处理硬件怪癖的方式。这些在x86上大量存在，Linux当然是在那里“长大”的。它们还处理一些软件困难（在现在大部分是遗留的32位i386架构上查找`ZONE_HIGHMEM`；我们在前面的章节中讨论了这个概念，*32位系统上的高内存*）。
- en: 'Zones consist of *page frames* – physical pages of RAM. More technically, a
    range of **P****age Frame Numbers** (**PFNs**) are allocated to each zone within
    a node:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 区域由*页框*组成 - 物理RAM页。更技术性地说，每个节点内的每个区域都分配了一系列**页框号**（**PFN**）：
- en: '![](img/bf31a6e1-d55b-48cc-9f90-1dad2626c573.png)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf31a6e1-d55b-48cc-9f90-1dad2626c573.png)'
- en: Figure 7.20 – Another view of the physical memory hierarchy on Linux – nodes,
    zones, and page frames
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.20 - Linux上物理内存层次结构的另一个视图 - 节点、区域和页框
- en: 'In Figure 7.10, you can see a generic (example) Linux system with *N* nodes
    (from `0` to `N-1`), each node consisting of (say) three zones, each zone being
    made up of physical pages of RAM – *page frames*. The number (and name) of zones
    per node is dynamically determined by the kernel at boot. You can check out the
    hierarchy on a Linux system by delving under *procfs.* In the following code,
    we take a peek into a native Linux x86_64 system with 16 GB of RAM:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 在图7.10中，您可以看到一个通用（示例）Linux系统，具有*N*个节点（从`0`到`N-1`），每个节点包含（假设）三个区域，每个区域由RAM的物理页面框架组成。每个节点的区域数量（和名称）由内核在启动时动态确定。您可以通过深入*procfs*来检查Linux系统上的层次结构。在下面的代码中，我们来看一下一个具有16GB
    RAM的本机Linux x86_64系统：
- en: '[PRE20]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The leftmost column reveals that we have exactly one node – `Node 0`. This tells
    us we're actually on an *UMA system*, though of course the Linux OS will treat
    it as a (pseudo/fake) NUMA system. This single node `0` is split into three zones,
    labeled `DMA`, `DMA32`, and `Normal`, and each zone, of course, consists of page
    frames. For now, ignore the numbers on the right; we will get to their meaning
    in the following chapter.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 最左边的列显示我们只有一个节点 - `Node 0`。这告诉我们实际上我们在一个*UMA系统*上，尽管Linux操作系统会将其视为（伪/假）NUMA系统。这个单一的节点`0`分为三个区域，标记为`DMA`，`DMA32`和`Normal`，每个区域当然由页面框架组成。现在先忽略右边的数字；我们将在下一章中解释它们的含义。
- en: 'Another way to notice how Linux "fakes" a NUMA node on UMA systems is visible
    from the kernel log. We run the following command on the same native x86_64 system
    with 16 GB of RAM. For readability, I replaced the first few columns showing the
    timestamp and hostname with ellipses:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: Linux在UMA系统上“伪造”NUMA节点的另一种方法可以从内核日志中看到。我们在同一个本机x86_64系统上运行以下命令，该系统具有16GB的RAM。为了便于阅读，我用省略号替换了显示时间戳和主机名的前几列：
- en: '[PRE21]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can clearly see that, as the system is detected as not NUMA (thus, UMA),
    the kernel fakes a node. The extents of the node are the total amount of RAM on
    the system (here, `0x0-0x00000004427fffff`, which is indeed 16 GB). We can also
    see that on this particular system, the kernel instantiates three zones – `DMA`, 
    `DMA32`, and `Normal` – to organize the available physical page frames of RAM.
    This is fine and ties in with the `/proc/buddyinfo` output we saw previously.
    FYI, the data structure representing the *zone* on Linux is defined here: `include/linux/mmzone.h:struct
    zone`. We will have occasion to visit it later in the book.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到，由于系统被检测为非NUMA（因此是UMA），内核会伪造一个节点。节点的范围是系统上的总RAM量（这里是`0x0-0x00000004427fffff`，确实是16GB）。我们还可以看到在这个特定系统上，内核实例化了三个区域
    - `DMA`，`DMA32`和`Normal` - 来组织可用的物理页面框架。这与我们之前看到的`/proc/buddyinfo`输出相吻合。顺便说一下，在Linux上代表*区域*的数据结构在这里定义：`include/linux/mmzone.h:struct
    zone`。我们将在本书的后面有机会访问它。
- en: To better understand how the Linux kernel organizes RAM, let's start at the
    very beginning – boot time.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解Linux内核如何组织RAM，让我们从最开始 - 启动时间开始。
- en: Direct-mapped RAM and address translation
  id: totrans-421
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 直接映射的RAM和地址转换
- en: 'At boot, the Linux kernel "maps" all (usable) system RAM (aka *platform RAM*)
    directly into the kernel segment. So, we have the following:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动时，Linux内核将所有（可用的）系统RAM（也称为*平台RAM*）直接映射到内核段。因此，我们有以下内容：
- en: Physical page frame `0` maps to kernel virtual page `0`.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物理页面框架`0`映射到内核虚拟页面`0`。
- en: Physical page frame `1` maps to kernel virtual page `1`.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物理页面框架`1`映射到内核虚拟页面`1`。
- en: Physical page frame `2` maps to kernel virtual page `2`, and so on.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物理页面框架`2`映射到内核虚拟页面`2`，依此类推。
- en: Thus, we call this a 1:1 or direct mapping, identity-mapped RAM, or linear addresses. A
    key point is that all these kernel virtual pages are at a fixed offset from their
    physical counterparts (and, as already mentioned, these kernel addresses are referred
    to as kernel logical addresses). The fixed offset is the `PAGE_OFFSET` value (here,
    `0xc000 0000`).
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们称之为1:1或直接映射，身份映射的RAM或线性地址。一个关键点是所有这些内核虚拟页面都与它们的物理对应物有固定偏移（如前所述，这些内核地址被称为内核逻辑地址）。固定偏移是`PAGE_OFFSET`值（这里是`0xc000
    0000`）。
- en: So, think of this. On a 32-bit system with a 3:1 (GB) VM split, physical address `0x0` =
    kernel logical address `0xc000 0000` (`PAGE_OFFSET`). As already mentioned, the
    terminology *kernel logical address *is applied to kernel addresses that are at
    a fixed offset from their physical counterparts. Thus, direct-mapped RAM maps
    to kernel logical addresses. This region of direct-mapped memory is often referred
    to as the *low-memory* (or simply, **lowmem**) region within the kernel segment.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，想象一下。在一个32位系统上，3:1（GB）的虚拟内存分配，物理地址`0x0` = 内核逻辑地址`0xc000 0000`（`PAGE_OFFSET`）。如前所述，术语*内核逻辑地址*适用于与其物理对应物有固定偏移的内核地址。因此，直接映射的RAM映射到内核逻辑地址。这个直接映射内存区域通常被称为内核段中的*低内存*（或简称为**lowmem**）区域。
- en: 'We have already shown an almost identical diagram earlier, in Figure 7.10. In
    the following figure, it''s slightly modified to actually  show you how the first
    three (physical) page frames of RAM map to the first three kernel virtual pages
    (in the lowmem region of the kernel segment):'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在之前展示了一个几乎相同的图表，在图7.10中。在下图中，稍作修改，实际上显示了前三个（物理）页面框架如何映射到内核段的前三个内核虚拟页面（在内核段的低内存区域）：
- en: '![](img/318bdc85-faf9-49d4-83db-45b24c23dbe3.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![](img/318bdc85-faf9-49d4-83db-45b24c23dbe3.png)'
- en: Figure 7.21 – Direct-mapped RAM – lowmem region, on 32-bit with a 3:1 (GB) VM
    split
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.21 - 直接映射的RAM - 32位系统，3:1（GB）虚拟内存分配
- en: As an example, Figure 7.21 shows a direct mapping of platform RAM to the kernel
    segment on a 32-bit system with a 3:1 (GB) VM split. The point where physical
    RAM address `0x0` maps into the kernel is the `PAGE_OFFSET` kernel macro (in the
    preceding figure, it's kernel logical address `0xc000 0000`). Notice how Figure
    7.21 also shows the *user VAS *on the left side, ranging from `0x0` to `PAGE_OFFSET-1` (of
    size `TASK_SIZE` bytes). We have already covered details on the remainder of the
    kernel segment in the *Examining the kernel segment* section previously.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，图7.21显示了在32位系统上平台RAM直接映射到内核段的情况，VM分割比为3:1（GB）。物理RAM地址`0x0`映射到内核的位置是`PAGE_OFFSET`内核宏（在前面的图中，它是内核逻辑地址`0xc000
    0000`）。请注意，图7.21还显示了左侧的*用户VAS*，范围从`0x0`到`PAGE_OFFSET-1`（大小为`TASK_SIZE`字节）。我们已经在之前的*检查内核段*部分详细介绍了内核段的其余部分。
- en: 'Understanding this mapping of physical-to-virtual pages might well tempt you
    into reaching these seemingly logical conclusions:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 理解物理到虚拟页面的映射可能会诱使你得出这些看似合乎逻辑的结论：
- en: 'Given a KVA, to calculate the corresponding **Physical Address** (**PA**) –
    that is, to perform a KVA-to-PA calculation – simply do this:'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一个KVA，计算相应的**物理地址**（**PA**） - 也就是执行KVA到PA计算 - 只需这样做：
- en: '[PRE22]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Conversely, given a PA, to calculate the corresponding KVA – that is, to perform
    a PA-to-KVA calculation – simply do this:'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，给定一个PA，计算相应的KVA - 也就是执行PA到KVA计算 - 只需这样做：
- en: '[PRE23]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Do refer to Figure 7.21 again. The direct mapping of RAM to the kernel segment
    (starting at `PAGE_OFFSET`) certainly predicates this conclusion. So, it is correct.
    But hang on, please pay careful attention here: **these address translation calculations
    work only for direct-mapped or linear addresses** – in other words, KVAs (technically,
    the kernel logical addresses) – **within the kernel''s lowmem region, nothing
    else!** For all UVAs, and any and all KVAs *besides* the lowmem region (which
    includes module addresses, `vmalloc`/`ioremap` (MMIO) addresses, KASAN addresses,
    the (possible) highmem region addresses, DMA memory regions, and so on), it does
    *not* work!'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 再次参考图7.21。RAM直接映射到内核段（从`PAGE_OFFSET`开始）确实预示着这个结论。所以，这是正确的。但请注意，这里请仔细注意：**这些地址转换计算仅适用于直接映射或线性地址**
    - 换句话说，内核低端内存区域的KVAs（技术上来说，是内核逻辑地址） - **不适用于任何其他地方的UVAs，以及除了低端内存区域之外的任何和所有KVAs**（包括模块地址，`vmalloc`/`ioremap`（MMIO）地址，KASAN地址，（可能的）高端内存区域地址，DMA内存区域等）。
- en: 'As you will anticipate, the kernel does indeed provide APIs to perform these
    address conversions; of course, their implementation is arch-dependent. Here they
    are:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所预料的，内核确实提供了API来执行这些地址转换；当然，它们的实现是与体系结构相关的。它们如下：
- en: '| **Kernel API** | **What it does** |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: 内核API 它的作用
- en: '| `phys_addr_t virt_to_phys(volatile void *address)` | Converts the given virtual
    address to its physical counterpart (return value) |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| `phys_addr_t virt_to_phys(volatile void *address)` | 将给定的虚拟地址转换为其物理对应地址（返回值）'
- en: '| `void *phys_to_virt(phys_addr_t address)` | Converts the given physical address
    to a virtual address (return value) |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| `void *phys_to_virt(phys_addr_t address)` | 将给定的物理地址转换为虚拟地址（返回值）'
- en: 'The `virt_to_phys()` API for the x86 has a comment above it clearly advocating
    that this API (and its ilk) are **not to be used by driver authors**; for clarity
    and completeness, we have reproduced the comment in the kernel source here:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: x86的`virt_to_phys()` API上面有一条注释，明确提倡这个API（以及类似的API）**不应该被驱动程序作者使用**；为了清晰和完整，我们在这里重现了内核源代码中的注释：
- en: '[PRE24]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The preceding comment mentions the (very common) `kmalloc()` API. Worry not,
    it's covered in depth in the following two chapters. Of course, a similar comment
    to the preceding is in place for the `phys_to_virt()` API as well.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的注释提到了（非常常见的）`kmalloc()` API。不用担心，它在接下来的两章中会有详细介绍。当然，对于`phys_to_virt()` API也有类似的注释。
- en: 'So who – sparingly – uses these address conversion APIs (and the like)? The
    kernel internal *mm* code, of course! As a demo, we do actually use them in at
    least a couple of places in this book: in the following chapter, in an LKM called
    `ch8/lowlevel_mem` (well actually, its usage is within a function in our "kernel
    library" code, `klib_llkd.c`).'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 那么谁会（少量地）使用这些地址转换API（以及类似的）呢？当然是内核内部的*mm*代码！作为演示，我们在本书中至少在两个地方使用了它们：在下一章中，在一个名为`ch8/lowlevel_mem`的LKM中（实际上，它的使用是在我们的“内核库”代码`klib_llkd.c`的一个函数中）。
- en: FYI, the powerful `crash(8)` utility can indeed translate any given virtual
    address to a physical address via its `vtop` (virtual-to-physical) command (and
    vice versa, via its `ptov` command!).
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，强大的`crash(8)`实用程序确实可以通过其`vtop`（虚拟到物理）命令将任何给定的虚拟地址转换为物理地址（反之亦然，通过其`ptov`命令也可以实现相反的转换！）。
- en: 'Moving along, another key point: by mapping all physical RAM into it, do not
    get misled into thinking that the kernel is *reserving* RAM for itself. No, it
    isn''t; it''s merely *mapping* all of the available RAM, thus making it available
    for allocation to anyone who wants it – core kernel code, kernel threads, device
    drivers, or user space applications. This is part of the job of the OS; it is
    the system resource manager, after all. Of course, a certain portion of RAM will
    be taken up (allocated) – by the static kernel code, data, kernel page table,
    and so on – at boot, no doubt, but you should realize that this is quite small.
    As an example, on my guest VM with 1 GB RAM, the kernel code, data, and BSS typically
    take up a combined total of about 25 MB of RAM. All kernel memory comes to about
    100 MB, whereas user space memory usage is in the region of 550 MB! It''s almost
    always user space that is the memory hogger.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键点是：通过将所有物理RAM映射到其中，不要被误导以为内核正在*保留*RAM给自己。不，它没有；它只是*映射*了所有可用的RAM，从而使它可以分配给任何想要的人——核心内核代码、内核线程、设备驱动程序或用户空间应用程序。这是操作系统的工作之一；毕竟，它是系统资源管理器。当然，在引导时，一定会占用（分配）一定部分RAM——静态内核代码、数据、内核页表等，但您应该意识到这是非常小的。举个例子，在我的1GB
    RAM的虚拟机上，内核代码、数据和BSS通常总共占用大约25MB的RAM。所有内核内存总共约100MB，而用户空间内存使用量大约为550MB！几乎总是用户空间占用内存最多。
- en: You can try using the `smem(8)` utility with the `--system -p` option switches
    to see a summary of memory usage as percentages (also, use the `--realmem=` switch
    to pass the actual amount of RAM on the system).
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以尝试使用`smem(8)`实用程序和`--system -p`选项开关，以查看内存使用情况的百分比摘要（还可以使用`--realmem=`开关传递系统上实际的RAM数量）。
- en: 'Back to the point: we know that kernel page tables are set up early in the
    boot process. So, by the time applications start up, *the kernel has all RAM mapped
    and available*, ready for allocation! Thus, we understand that while the kernel *direct-maps *page
    frames into its VAS, user mode processes are not so lucky – they can only *indirectly
    map *page frames via the paging tables set up by the OS (at process creation – `fork(2)` –
    time) on a per-process basis. Again, it''s interesting to realize that memory
    mapping via the powerful `mmap(2)` system call can provide the illusion of "direct
    mapping" files or anonymous pages into the user VAS.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 回到重点：我们知道内核页表在引导过程中早早地设置好了。因此，当应用程序启动时，*内核已经将所有RAM映射并可用*，准备分配！因此，我们理解，虽然内核将页框*直接映射*到其虚拟地址空间，用户模式进程却没有这么幸运——它们只能通过操作系统在进程创建（`fork(2)`）时在每个进程基础上设置的分页表*间接映射*页框。再次强调，通过强大的`mmap(2)`系统调用进行内存映射可以提供将文件或匿名页面“直接映射”到用户虚拟地址空间的错觉。
- en: 'A few additional points to note:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些额外的要点需要注意：
- en: (a) For performance, kernel memory (kernel pages) can *never be swapped*, even
    if they aren't in use
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 为了性能，内核内存（内核页面）*永远不会被交换*，即使它们没有被使用。
- en: (b) Sometimes, you might think, it's quite obvious that *user space memory pages map to
    (physical) page frames (assuming the page is resident) via the paging tables set
    up by the OS on a per-process basis*. Yes, but what about kernel memory pages?
    Please be very clear on this point: *all kernel pages also map to page frames
    via the kernel "master" paging table. Kernel memory, too, is virtualized, just
    as user space memory is.* In this regard, for you, the interested reader, a QnA
    I initiated on Stack Overflow: *How exactly do kernel virtual addresses get translated
    to physical RAM?: *[http://stackoverflow.com/questions/36639607/how-exactly-do-kernel-virtual-addresses-get-translated-to-physical-ram](http://stackoverflow.com/questions/36639607/how-exactly-do-kernel-virtual-addresses-get-translated-to-physical-ram).(c)
    Several memory optimization techniques have been baked into the Linux kernel (well,
    many are configuration options); among them are **Transparent Huge Pages** (**THPs**)and,
    critical for cloud/virtualization workloads, **Kernel Samepage Merging** (**KSM**,
    aka memory de-duplication)*.* I refer you to the *Further reading *section of
    this chapter for more information.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 有时候，你可能会认为，*用户空间内存页面通过操作系统在每个进程基础上设置的分页表，映射到（物理）页框上（假设页面是常驻的）*，这是相当明显的。是的，但内核内存页面呢？请非常清楚地理解这一点：*所有内核页面也通过内核“主”分页表映射到页框上。内核内存也是虚拟化的，就像用户空间内存一样。*
    在这方面，对于您这位感兴趣的读者，我在Stack Overflow上发起了一个问答：*内核虚拟地址到物理RAM的确切转换是如何进行的？：*[http://stackoverflow.com/questions/36639607/how-exactly-do-kernel-virtual-addresses-get-translated-to-physical-ram](http://stackoverflow.com/questions/36639607/how-exactly-do-kernel-virtual-addresses-get-translated-to-physical-ram)。(c) Linux内核中已经内置了几种内存优化技术（很多是配置选项）；其中包括**透明巨大页面**（**THPs**）和对云/虚拟化工作负载至关重要的**内核同页合并**（**KSM**，也称为内存去重）*。*
    我建议您参考本章的*进一步阅读*部分以获取更多信息。
- en: Alright, with this coverage on some aspects of physical RAM management behind
    us, we complete this chapter; excellent progress!
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，通过对物理RAM管理的一些方面进行覆盖，我们完成了本章的内容；进展非常不错！
- en: Summary
  id: totrans-454
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we delved – in quite some depth – into the big topic of kernel
    memory management in a level of detail sufficient for a kernel module or device
    driver author like you; also, there''s more to come! A key piece of the puzzle
    – the VM split and how it''s achieved on various architectures running the Linux
    OS – served as a starting point. We then moved into a deep examination of both
    regions of this split: first, user space (the process VAS) and then the kernel
    VAS (or kernel segment). Here, we covered many details and tools/utilities on
    how to examine it (notably, via the quite powerful `procmap` utility). We built
    a demo kernel module that can literally generate a pretty complete memory map
    of the kernel and the calling process. User and kernel memory layout randomization
    technology ([K]ASLR) was also briefly discussed. We closed the chapter by taking
    a look at the physical organization of RAM within Linux.'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入研究了内核内存管理这一大主题，对于像您这样的内核模块或设备驱动程序作者来说，我们提供了足够详细的级别；而且还有更多内容要学习！一个关键的拼图——VM分割以及在运行Linux操作系统的各种架构上如何实现它——作为一个起点。然后我们深入研究了这个分割的两个区域：首先是用户空间（进程VAS），然后是内核VAS（或内核段）。在这里，我们涵盖了许多细节和工具/实用程序，以及如何检查它（特别是通过相当强大的`procmap`实用程序）。我们构建了一个演示内核模块，可以生成内核和调用进程的相当完整的内存映射。用户和内核内存布局随机化技术（[K]ASLR）也被简要讨论了一下。最后，我们看了一下Linux内存中RAM的物理组织。
- en: All of this information and the concepts learned within this chapter are actually
    *very useful;* not only for designing and writing better kernel/device driver
    code but very much also when you encounter system-level issues and bugs.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中学到的所有信息和概念实际上都非常有用；不仅适用于设计和编写更好的内核/设备驱动程序代码，而且在遇到系统级问题和错误时也非常有用。
- en: This chapter has been a long and indeed a critical one; great job on completing
    it! Next, in the following two chapters, you will move on to learning key and
    practical aspects of how exactly to allocate (and deallocate) kernel memory efficiently,
    along with related important concepts behind this common activity. On, on!
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章是一个漫长而且关键的章节；完成得很好！接下来，在接下来的两章中，您将继续学习如何有效地分配（和释放）内核内存的关键和实际方面，以及这一常见活动背后的重要概念。继续前进！
- en: Questions
  id: totrans-458
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'As we conclude, here is a list of questions for you to test your knowledge
    regarding this chapter''s material: [https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions).
    You will find some of the questions answered in the book''s GitHub repo: [https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn).'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这里有一些问题供您测试对本章材料的了解：[https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions)。您会发现一些问题的答案在书的GitHub存储库中：[https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn)。
- en: Further reading
  id: totrans-460
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: To help you delve deeper into the subject with useful materials, we provide
    a rather detailed list of online references and links (and at times, even books)
    in a Further reading document in this book's GitHub repository. The *Further reading*
    document is available here: [https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md](https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md).
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助您深入研究这个主题并提供有用的材料，我们在本书的GitHub存储库中提供了一个相当详细的在线参考和链接列表（有时甚至包括书籍）的Further
    reading文档。*Further reading*文档在这里可用：[https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md](https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md)。

- en: Chapter 11. Summarizing Logs with Awk
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章。使用Awk总结日志
- en: 'One of the tasks that awk is really good at is filtering data from log files.
    These log files may be many lines in length, perhaps 250,000 or more. I have worked
    with data with over a millions lines. Awk can process these lines quickly and
    effectively. As an example, we will work with a web server access log with 30,000
    lines to show how effective and well written awk code can be. As we work our way
    through the chapter, we will also see different log files and review some of the
    techniques that we can employ with the `awk` command and the awk programming language
    to help with the reporting and administration of our services. In this chapter
    we will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: awk真正擅长的任务之一是从日志文件中过滤数据。这些日志文件可能有很多行，可能有250,000行或更多。我曾处理过超过一百万行的数据。Awk可以快速有效地处理这些行。例如，我们将使用包含30,000行的Web服务器访问日志文件，以展示awk代码的有效性和良好编写。在本章中，我们将涵盖以下主题：
- en: HTTPD log file format
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HTTPD日志文件格式
- en: Displaying data from web server logs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示来自Web服务器日志的数据
- en: Summarizing HTTP access codes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总结HTTP访问代码
- en: Displaying the highest ranking client IP addresses
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示排名最高的客户端IP地址
- en: Listing browser data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列出浏览器数据
- en: Working with e-mail logs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理电子邮件日志
- en: The HTTPD log file format
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HTTPD日志文件格式
- en: When working with any a file, the first task is to become familiar with the
    file schema. In simple terms, we need to know what is represented by each field
    and what is used to delimit the fields. We will be working with the access log
    file from an Apache HTTPD web server. The location of the log file can be controlled
    from the `httpd.conf` file. The default log file location on a Debian based system
    is `/var/log/apache2/access.log`; other systems may use the `httpd` directory
    in place of `apache2`.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理任何文件时，第一项任务是熟悉文件模式。简单来说，我们需要知道每个字段代表什么，以及用于分隔字段的内容。我们将使用Apache HTTPD Web服务器的访问日志文件。日志文件的位置可以从`httpd.conf`文件中控制。基于Debian的系统上，默认的日志文件位置是`/var/log/apache2/access.log`；其他系统可能使用`apache2`目录代替`httpd`。
- en: To demonstrate the layout of the file, I have installed a brand new instance
    of Apache2 on an Ubuntu 15.10 system. Once the web server was installed, we made
    a single access from the Firefox browser to the server from the local host.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示文件的布局，我在Ubuntu 15.10系统上安装了一个全新的Apache2实例。安装完Web服务器后，我们从本地主机的Firefox浏览器进行了一次访问。
- en: 'Using the `tail` command we can display the content of the log file. Although,
    to be fair, the use of `cat` will do just as well with this file, as it will have
    just a few lines:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`tail`命令可以显示日志文件的内容。尽管公平地说，使用`cat`也可以，因为它只有几行：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of the command and the contents of the file are shown in the following
    screenshot:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 命令的输出和文件的内容如下截图所示：
- en: '![The HTTPD log file format](img/00107.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![HTTPD日志文件格式](img/00107.jpeg)'
- en: 'The output does wrap a little onto the new lines but we do get a feel of the
    layout of the log. We can also see that even though we feel that we access just
    one web page, we are in fact accessing two items: the `index.html` and the `ubuntu-logo.png`.
    We also failed to access the `favicon.ico` file. We can see that the file is space
    separated. The meaning of each of the fields is laid out in the following table:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 命令的输出会有一些换行，但我们可以感受到日志的布局。我们还可以看到，尽管我们认为只访问了一个网页，但实际上我们访问了两个项目：`index.html`和`ubuntu-logo.png`。我们还未能访问`favicon.ico`文件。我们可以看到该文件是以空格分隔的。每个字段的含义在以下表格中列出：
- en: '| Field | Purpose |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 字段 | 目的 |'
- en: '| --- | --- |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 | Client IP address. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 客户端IP地址。'
- en: '| 2 | Client identity as defined by RFC 1413 and the `identd` client. This
    is not read unless `IdentityCheck` is enabled. If it is not read the value will
    be with a hyphen. |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: 2 | RFC 1413和`identd`客户端定义的客户端身份。除非启用`IdentityCheck`，否则不会读取此内容。如果未读取，该值将带有连字符。
- en: '| 3 | The user ID of the user authentication if enabled. If authentication
    is not enabled the value will be a hyphen. |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: 3 | 如果启用了用户身份验证，则为用户身份验证的用户ID。如果未启用身份验证，则该值将为连字符。
- en: '| 4 | The date and time of the request in the format of `day/month/year:hour:minute:second
    offset`. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: 4 | 请求的日期和时间格式为`day/month/year:hour:minute:second offset`。
- en: '| 5 | The actual request and method. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: 5 | 实际请求和方法。
- en: '| 6 | The return status code, such as 200 or 404. |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: 6 | 返回状态代码，如200或404。
- en: '| 7 | File size in bytes. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: 7 | 文件大小（以字节为单位）。
- en: 'Even though these fields are defined by Apache, we have to be careful. The
    time, date, and time-zone is a single field and is defined within square braces;
    however, there are additional spaces inside the field between that data and the
    time-zone. To ensure that we print the complete time field if required, we need
    to print both `$4` and `$5`. This is shown in the following command example:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这些字段是由Apache定义的，我们也必须小心。时间、日期和时区是一个字段，并且在方括号内定义；然而，在该数据和时区之间的字段内有额外的空格。为了确保在需要时打印完整的时间字段，我们需要同时打印`$4`和`$5`。这在以下命令示例中显示：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can view the command and the output it produces in the following screenshot:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下截图中查看命令和其产生的输出：
- en: '![The HTTPD log file format](img/00108.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![HTTPD日志文件格式](img/00108.jpeg)'
- en: Displaying data from web logs
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 显示来自Web日志的数据
- en: We have already had a preview of how we can use awk to view the logs files from
    the Apache web server; however, we will now move onto our demonstration file that
    has a greater and more varied content.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经预览了如何使用awk查看Apache Web服务器的日志文件；但是，现在我们将转向我们的演示文件，其中包含更丰富和更多样化的内容。
- en: Selecting entries by date
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 按日期选择条目
- en: 'Having seen how we can display the date, we should perhaps look at how we print
    entries from just one day. To do this, we can use the match operator in `awk`.
    This is denoted by the tilde or squiggly line, if you prefer. As we only need
    the date element, there is no need for us to use both the date and time-zone field.
    The following command shows how to print entries from 10th September 2014:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 看到我们如何显示日期后，也许我们应该看看如何仅打印一天的条目。为此，我们可以在`awk`中使用匹配运算符。如果您愿意，这由波浪线表示。由于我们只需要日期元素，因此我们不需要同时使用日期和时区字段。以下命令显示了如何打印2014年9月10日的条目：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For completeness, this command and partial output is shown in the following
    screenshot:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整起见，以下是该命令和部分输出的截图：
- en: '![Selecting entries by date](img/00109.jpeg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![按日期选择条目](img/00109.jpeg)'
- en: The round brackets or parentheses embrace the range of lines that we are looking
    for and we have omitted the main block, which ensures that we print the complete
    matching lines from the range. There is nothing stopping us from further filtering
    on the fields to print from the matching lines. For example, if we want to print
    out just the client IP address that is being used to access the web server we
    can print field `1`. This is shown in the following command example.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 圆括号或括号包含我们正在寻找的行范围，我们已经省略了主块，这确保我们打印出范围内的完整匹配行。没有什么能阻止我们进一步过滤匹配行中要打印的字段。例如，如果我们只想打印正在用于访问Web服务器的客户端IP地址，我们可以打印字段`1`。这在以下命令示例中显示。
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If we want to be able to print the total number of accesses on a given date,
    we could pipe the entries through to the `wc` command. This is demonstrated in
    the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要能够打印给定日期的总访问次数，我们可以将条目通过管道传递到`wc`命令。这在以下示例中演示：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'However, if we want to use `awk` to do this for us, this will be more efficient
    than starting a new process and we can count the entries. If we use the built-in
    variable `NR,` we can print entire lines in the files not just those within the
    range. It is best to increment our own variable in the main block than matching
    the range for each line. The `END` block can be implemented to print the `count`
    variable we use. The following command line acts as an example:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们想要使用`awk`来为我们做这个，这将比启动一个新进程更有效，并且我们可以计算条目。如果我们使用内置变量`NR`，我们可以打印文件中的整行而不仅仅是范围内的行。最好在主块中递增我们自己的变量，而不是为每行匹配范围。`END`块可以被实现以打印我们使用的`count`变量。以下命令行充当示例：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Selecting entries by date](img/00110.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![按日期选择条目](img/00110.jpeg)'
- en: The output of the count from both `wc` and the internal counter will give us
    `16205` as a result from the demonstration file. We should use the variable increment
    within the main block if we want to count and nothing else.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从`wc`和内部计数器的计数输出将使我们从演示文件中得到`16205`的结果。如果我们想要计数而不做其他操作，我们应该在主块中使用变量增量。
- en: '[PRE6]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can see this in the following output:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下输出中看到这一点：
- en: '![Selecting entries by date](img/00111.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![按日期选择条目](img/00111.jpeg)'
- en: Summarizing 404 errors
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结404错误
- en: The status code of the request page is shown in field `9` of the log. The `404`
    status will represent the page not found error on the server, I am sure we have
    all seen that in our browsers at some stage. This may be indicative of a misconfigured
    link on your site or just produced by a browser searching for the icon image to
    display in tabbed browsers for the page. You can also identify potential threats
    to your site by requests looking for standard pages that may give an access to
    additional information on PHP driven sites, such as WordPress.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请求页面的状态代码显示在日志的字段`9`中。`404`状态将表示服务器上找不到页面的错误，我相信我们都在某个阶段在我们的浏览器中看到过这个。这可能表明您网站上的链接配置错误，或者只是由浏览器搜索要在选项卡式浏览器中显示的图标图像而产生的。您还可以通过寻找标准页面的请求来识别对您网站的潜在威胁，这些页面可能会提供对PHP驱动站点的其他信息的访问，例如WordPress。
- en: 'Firstly, we can solely print the status of the request:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以仅打印请求的状态：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can now extend the code a little as well as ourselves and just print the
    `404` errors:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以稍微扩展代码，也可以扩展自己，只打印`404`错误：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This is shown in the following code:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这在以下代码中显示：
- en: '![Summarizing 404 errors](img/00112.jpeg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![总结404错误](img/00112.jpeg)'
- en: 'We can extend this a little further by printing both the status code and the
    page that was being accessed. This will need us to print field `9` and field `7`.
    Simply put, this will be as shown in the following code:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步扩展，通过打印状态代码和正在访问的页面来打印。这将需要我们打印字段`9`和字段`7`。简而言之，这将如下所示：
- en: '[PRE9]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Many of these failed accessed pages will be duplicated. To summarize these
    records, we can use the command pipeline to achieve this with `sort` and `uniq`
    commands:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这些失败的访问页面中许多将是重复的。为了总结这些记录，我们可以使用`sort`和`uniq`命令的命令管道来实现这一点：
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: To use the `uniq` command, the data must be pre-sorted; hence, we use the `sort`
    command to prepare the data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`uniq`命令，数据必须经过预排序；因此，我们使用`sort`命令来准备数据。
- en: Summarizing HTTP access codes
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结HTTP访问代码
- en: 'It is time for us to leave the pure command line and start working with the
    awk control files. As always, when the complexity of the required result set increases,
    we see an increase in the complexity of the `awk` code. We will create a `status.awk`
    file in our current directory. The file should look similar to the following file:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候离开纯命令行并开始使用awk控制文件了。与以往一样，当所需结果集的复杂性增加时，我们看到`awk`代码的复杂性也在增加。我们将在当前目录中创建一个`status.awk`文件。该文件应该类似于以下文件：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'First, we will strip down the main code block and this is very simple and sparse.
    This is a simple way to count each unique occurrence of a status code. Instead
    of using a simple variable, we feed this into an array. The array in this case
    is called a record. An array is a multi-values variable and the slots in the array
    are known as keys. So we will have a collection of variables stored in the array.
    For example, we expect to see entries for `record[200]` and `record[404]`. We
    populate each key with their occurrence count. Each time we find a `404` code,
    we increment the count that is stored in the associated key:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将简化主代码块，这非常简单和稀疏。这是一种简单的方法来计算每个状态代码的唯一发生次数。我们不使用简单的变量，而是将其输入到数组中。这种情况下的数组称为记录。数组是一个多值变量，数组中的槽称为键。因此，我们将在数组中存储一组变量。例如，我们期望看到`record[200]`和`record[404]`的条目。我们用它们的发生次数填充每个键。每次我们找到`404`代码时，我们增加存储在相关键中的计数：
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the `END` block, we create the summary information using a `for` loop to
    print out each key and value from the array:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在`END`块中，我们使用`for`循环创建摘要信息，以打印数组中的每个键和值：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To run this, the associated command line will be similar to the following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行这个，相关的命令行将类似于以下内容：
- en: '[PRE14]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To view the command and output, we have included the following screenshot:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看命令和输出，我们已经包含了以下截图：
- en: '![Summarizing HTTP access codes](img/00113.jpeg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![总结HTTP访问代码](img/00113.jpeg)'
- en: 'We can take this further and focus on the `404` errors. You could, of course,
    choose any of the status codes. We can see from the results that we have `4382`
    `404` status codes. To summarize these `404` codes, we will copy the `status.awk`
    to a new file named `404.awk`. We can edit the `404.awk` adding an `if` statement
    to work only on the `404` codes. The file should be similar to the following code:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步关注`404`错误。当然，你可以选择任何状态代码。从结果中我们可以看到有`4382`个`404`状态代码。为了总结这些`404`代码，我们将`status.awk`复制到一个名为`404.awk`的新文件中。我们可以编辑`404.awk`，添加一个`if`语句，只处理`404`代码。文件应该类似于以下代码：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If we execute the code with the following command:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用以下命令执行代码：
- en: '[PRE16]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output will be similar to the following screenshot:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图：
- en: '![Summarizing HTTP access codes](img/00114.jpeg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![总结HTTP访问代码](img/00114.jpeg)'
- en: Displaying the highest ranking IP address
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 显示最高排名的IP地址
- en: 'You should now realize some powers of `awk` and how immense the language structure
    is in itself. The data we have been able to produce from the 30K line file is
    truly powerful and easily extracted. We just need to replace the field we have
    used before with `$1`. This field represents the client IP address. If we make
    use of the following code, we will be able to print each IP Address and also the
    number of times it has been used to access the web server:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该意识到`awk`的一些功能，以及语言结构本身的强大之处。我们能够从这个3万行的文件中产生的数据是非常强大且容易提取的。我们只需要用`$1`替换之前使用过的字段。这个字段代表客户端IP地址。如果我们使用以下代码，我们将能够打印每个IP地址以及它被用来访问网页服务器的次数：
- en: '[PRE17]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We want to be able to extend this to show only the highest ranking of IP address,
    the address that has been used the most to access the site. The work, again, will
    mainly be in the `END` block and will make use of a comparison against the current
    highest ranking address. The following file can be created and saved as `ip.awk`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望能够扩展这个功能，只显示IP地址中排名最高的，即访问网站最频繁的地址。工作主要在`END`块中进行，将利用与当前最高排名地址的比较。可以创建以下文件并保存为`ip.awk`：
- en: '[PRE18]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can see the output of the command in the following screenshot. Part of the
    client IP address has been obscured as it is from my public web server:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下截图中看到命令的输出。客户端IP地址的部分已被隐藏，因为它来自我的公共网页服务器：
- en: '![Displaying the highest ranking IP address](img/00115.jpeg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![显示最高排名的IP地址](img/00115.jpeg)'
- en: The functionality of the code comes from within the `END` block. On entering
    the `END` block, we run into a `for` loop. We iterate through each entry in the
    `ip` array. We use the conditional `if` statement to see if the current value
    that we are iterating through is higher than the current maximum. If it is, this
    becomes the new highest entry. When the `loop` has finished, we print the IP address
    that has the highest entry.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的功能来自`END`块内部。进入`END`块时，我们进入一个`for`循环。我们遍历`ip`数组中的每个条目。我们使用条件`if`语句来查看我们正在遍历的当前值是否高于当前最大值。如果是，这将成为新的最高条目。当`循环`结束时，我们打印具有最高条目的IP地址。
- en: Displaying the browser data
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 显示浏览器数据
- en: 'The browser that is used to access the web site is contained within the log
    file in field `12`. It may be interesting to display the list of browsers used
    to access your site. The following code will assist you in displaying the list
    of accesses by the reported browser:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 用于访问网站的浏览器包含在字段`12`的日志文件中。显示用于访问您网站的浏览器列表可能会很有趣。以下代码将帮助您显示报告的浏览器的访问列表：
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You can see how we can create little plugins to `awk` with these files and
    adjust the field and array names to suit your own liking. The output is shown
    in the following screenshot:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到我们如何可以创建`awk`的小插件，并调整字段和数组名称以适应你自己的喜好。输出如下截图所示：
- en: '![Displaying the browser data](img/00116.jpeg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![显示浏览器数据](img/00116.jpeg)'
- en: Interestingly, we see that the Mozilla 4 and 5 make up the majority of the requesting
    client. We see that Mozilla 4 is listed here **1713** times. The Mozilla/5.0 entry
    here is malformed with an extra double-quote. It appears later with 27K accesses.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，我们看到Mozilla 4和5占据了大部分请求客户端。我们看到Mozilla 4在这里列出了**1713**次。这里的Mozilla/5.0条目格式不正确，多了一个双引号。它稍后出现了27K次。
- en: Working with e-mail logs
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理电子邮件日志
- en: We have worked with logs from the Apache HTTP web server. The reality is that
    we can apply the same ideals and methodology to any log file. We will take a look
    at Postfix mail logs. The mail log holds all activity from the SMTP server and
    we can then see who has been sending e-mails to whom. The log file is usually
    located at `/var/log/mail.log`. I will access this on my Ubuntu 15.10 server that
    has a local e-mail delivery. All this means is that the STMP server is listening
    only to the localhost interface of `127.0.0.1`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用了来自Apache HTTP Web服务器的日志。事实是我们可以将相同的理念和方法应用到任何日志文件上。我们将查看Postfix邮件日志。邮件日志保存了来自SMTP服务器的所有活动，然后我们可以看到谁向谁发送了电子邮件。日志文件通常位于`/var/log/mail.log`。我将在我的Ubuntu
    15.10服务器上访问这个文件，该服务器具有本地电子邮件传递功能。这意味着STMP服务器只监听`127.0.0.1`的本地接口。
- en: The log format will change a little depending on the type of message. For example,
    `$7` will contain `from` logs on outbound message, whereas inbound messages will
    contain `to`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 日志格式将根据消息类型的不同而略有变化。例如，`$7`将包含出站消息的`from`日志，而入站消息将包含`to`。
- en: 'If we want to list all the inbound messages to the SMTP server, we can use
    the following command:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想列出所有发送到SMTP服务器的入站消息，我们可以使用以下命令：
- en: '[PRE20]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As the string `to` is very short, we can add identification to it by ensuring
    that the field begins with to using the `^`. The command and output is shown in
    the following screenshot:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由于字符串`to`非常短，我们可以通过确保字段以`^`开头来为其添加标识。命令和输出如下截图所示：
- en: '![Working with e-mail logs](img/00117.jpeg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![处理电子邮件日志](img/00117.jpeg)'
- en: It will be easy to extend the `to` or `from` searches to also include users
    names. We can see the format of the delivered or received mail. Working with the
    same template we used with the Apache logs, we can easily display the highest
    recipient or sender.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 将扩展`to`或`from`搜索以包括用户名称将会很容易。我们可以看到交付或接收邮件的格式。使用与Apache日志相同的模板，我们可以轻松显示最高的收件人或发件人。
- en: Summary
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We now have some heavy ammunition behind our text processing and we can begin
    to understand just how powerful `awk` can be. Working with real data is particularly
    useful in gauging the performance and accuracy of our searches. Having begun working
    with simple Apache entries on the newly installed Ubuntu 15.10 Apache web server,
    we soon migrated to the larger sample data from a live web server. With 30,000
    lines, this file gives us some real meat to work with and in no time we were able
    to produce credible reports. We closed up the return to the Ubuntu 15.10 server
    to analyze the Postfix SMTP logs. We can see that we can very much drag and drop
    the technology that we have previously used into the new log files.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在文本处理中有了一些重要的武器，我们可以开始理解`awk`有多么强大。使用真实数据在评估我们搜索的性能和准确性方面特别有用。在新安装的Ubuntu
    15.10 Apache Web服务器上开始使用简单的Apache条目后，我们很快就迁移到了来自实时Web服务器的更大的样本数据。有30,000行，这个文件给了我们一些真实的数据来处理，我们很快就能够生成可信的报告。我们结束了返回Ubuntu
    15.10服务器来分析Postfix SMTP日志。我们可以看到我们可以非常轻松地将之前使用过的技术拖放到新的日志文件中。
- en: Next up, we stick with `awk` and look at how we can report on the lastlog data
    and on flat XML files.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们继续使用`awk`，看看如何报告lastlog数据和平面XML文件。

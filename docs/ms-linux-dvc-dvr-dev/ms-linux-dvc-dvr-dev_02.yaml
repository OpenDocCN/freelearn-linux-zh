- en: '*Chapter 1*: Linux Kernel Concepts for Embedded Developers'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第1章*：嵌入式开发人员的Linux内核概念'
- en: As a standalone software, the Linux kernel implements a set of functions that
    help not to reinvent the wheel and ease device driver developments. The importance
    of these helpers is that it’s not a requirement to use these for code to be accepted
    upstream. This is the kernel core that drivers rely on. We’ll cover the most popular
    of these core functionalities in this book, though other ones also exist. We will
    begin by looking at the kernel locking API before discussing how to protect shared
    objects and avoid race conditions. Then, we will look at various work deferring
    mechanisms available, where you will learn what part of the code to defer in which
    execution context. Finally, you will learn how interrupts work and how to design
    interrupt handlers from within the Linux kernel.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个独立的软件，Linux内核实现了一组函数，有助于避免重复发明轮子，并简化设备驱动程序的开发。这些辅助程序的重要性在于，不需要使用这些代码才能被上游接受。这是驱动程序依赖的内核核心。我们将在本书中介绍这些核心功能中最受欢迎的功能，尽管还有其他功能存在。我们将首先查看内核锁定API，然后讨论如何保护共享对象并避免竞争条件。然后，我们将看看各种可用的工作推迟机制，您将了解在哪个执行上下文中推迟代码的部分。最后，您将学习中断的工作原理以及如何从Linux内核内部设计中断处理程序。
- en: 'This chapter will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: The kernel locking API and shared objects
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核锁定API和共享对象
- en: Work deferring mechanisms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作推迟机制
- en: Linux kernel interrupt management
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linux内核中断管理
- en: Let’s get started!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To understand and follow this chapter’s content, you will need the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解和遵循本章的内容，您将需要以下内容：
- en: Advanced computer architecture knowledge and C programming skills
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级计算机体系结构知识和C编程技能
- en: Linux kernel 4.19 sources, available at [https://github.com/torvalds/linux](https://github.com/torvalds/linux)
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linux内核4.19源代码，可在[https://github.com/torvalds/linux](https://github.com/torvalds/linux)上获得
- en: The kernel locking API and shared objects
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内核锁定API和共享对象
- en: A resource is said to be shared when it can be accessed by several contenders,
    regardless of their exclusively. When they are exclusive, access must be synchronized
    so that only the allowed contender(s) may own the resource. Such resources might
    be memory locations or peripheral devices, while the contenders might be processors,
    processes, or threads. Operating systems perform mutual exclusion by atomically
    (that is, by means of an operation that can be interrupted) modifying a variable
    that holds the current state of the resource, making this visible to all contenders
    that might access the variable at the same time. This atomicity guarantees that
    the modification will either be successful, or not successful at all. Nowadays,
    modern operating systems rely on the hardware (which should allow atomic operations)
    used for implementing synchronization, though a simple system may ensure atomicity
    by disabling interrupts (and avoiding scheduling) around the critical code section.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当可以被多个竞争者访问时，资源被称为共享资源，而不管它们的独占性。当它们是独占的时，访问必须同步，以便只有允许的竞争者可以拥有资源。这样的资源可能是内存位置或外围设备，而竞争者可能是处理器、进程或线程。操作系统通过原子地（即通过可以被中断的操作）修改持有资源当前状态的变量来执行互斥排除，使这一点对所有可能同时访问变量的竞争者可见。这种原子性保证修改要么成功，要么根本不成功。如今，现代操作系统依赖于硬件（应该允许原子操作）用于实现同步，尽管一个简单的系统可以通过在关键代码部分周围禁用中断（并避免调度）来确保原子性。
- en: 'In this section, we’ll describe the following two synchronization mechanisms:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述以下两种同步机制：
- en: '**Locks**: Used for mutual exclusion. When one contender holds the lock, no
    other contender can hold it (others are excluded). The most well-known locking
    primitives in the kernel are spinlocks and mutexes.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**锁**：用于互斥。当一个竞争者持有锁时，其他竞争者无法持有它（其他被排除）。内核中最知名的锁定原语是自旋锁和互斥锁。'
- en: '**Conditional variables**: Mostly used to sense or wait for a state change.
    These are implemented differently in the kernel, as we will see later, mainly
    in the *Waiting, sensing, and blocking in the Linux kernel* section.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**条件变量**：主要用于感知或等待状态改变。这些在内核中实现方式不同，我们稍后将看到，主要在*Linux内核中的等待、感知和阻塞*部分中。'
- en: When it comes to locking, it is up to the hardware to allow such synchronizations
    by means of atomic operations. The kernel then uses these to implement locking
    facilities. Synchronization primitives are data structures that are used for coordinating
    access to shared resources. Because only one contender can hold the lock (and
    thus access the shared resource), it might perform an arbitrary operation on the
    resource associated with the lock that would appear to be atomic to others.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到锁定时，硬件允许通过原子操作进行同步。然后内核使用这些来实现锁定设施。同步原语是用于协调对共享资源访问的数据结构。因为只有一个竞争者可以持有锁（从而访问共享资源），它可能对与锁相关的资源执行任意操作，这对其他人来说似乎是原子的。
- en: 'Apart from dealing with the exclusive ownership of a given shared resource,
    there are situations where it is better to wait for the state of the resource
    to change; for example, waiting for a list to contain at least one object (its
    state then passes from empty to not empty) or for a task to complete (a DMA transaction,
    for example). The Linux kernel does not implement conditional variables. From
    our user space, we could think of using a conditional variable for both situations,
    but to achieve the same or even better, the kernel provides the following mechanisms:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 除了处理给定共享资源的独占所有权外，还存在一些情况更好地等待资源状态改变；例如，等待列表包含至少一个对象（其状态从空变为非空）或等待任务完成（例如DMA事务）。Linux内核不实现条件变量。从用户空间，我们可以考虑在这两种情况下使用条件变量，但为了实现相同或甚至更好的效果，内核提供了以下机制：
- en: '**Wait queue**: Mainly used to wait for a state change. It’s designed to work
    in concert with locks.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**等待队列**：主要用于等待状态改变。它被设计为与锁协同工作。'
- en: '**Completion queue**: Used to wait for a given computation to complete.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完成队列**：用于等待给定计算的完成。'
- en: Both mechanisms are supported by the Linux kernel and are exposed to drivers
    thanks to a reduced set of APIs (which significantly ease their use when used
    by a developer). We will discuss these in the upcoming sections.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种机制都受到Linux内核的支持，并且由于一组较少的API（在开发人员使用时显著简化了它们的使用），它们暴露给驱动程序。我们将在接下来的部分讨论这些。
- en: Spinlocks
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自旋锁
- en: A spinlock is a hardware-based locking primitive. It depends on the capabilities
    of the hardware at hand to provide atomic operations (such as `test_and_set`,
    which, in a non-atomic implementation, would result in read, modify, and write
    operations). Spinlocks are essentially used in an atomic context where sleeping
    is not allowed or simply not needed (in interrupts, for example, or when you want
    to disable preemption), but also as an inter-CPU locking primitive.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 自旋锁是一种基于硬件的锁原语。它依赖于手头的硬件提供原子操作的能力（例如`test_and_set`，在非原子实现中，将导致读取、修改和写入操作）。自旋锁基本上用于不允许睡眠或根本不需要睡眠的原子上下文中（例如在中断中，或者当您想要禁用抢占时），但也用作CPU间的锁原语。
- en: 'It is the simplest locking primitive and also the base one. It works as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最简单的锁原语，也是基本的锁原语。它的工作方式如下：
- en: '![Figure 1.1 – Spinlock contention flow'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.1 - 自旋锁争用流'
- en: '](img/Figure_1.1_B10985.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_1.1_B10985.jpg)'
- en: Figure 1.1 – Spinlock contention flow
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 - 自旋锁争用流
- en: 'Let’s explore the diagram by looking at the following scenario:When CPUB, which
    is running task B, wants to acquire the spinlock thanks to the spinlock’s locking
    function and this spinlock is already held by another CPU (let’s say CPUA, running
    task A, which has already called this spinlock’s locking function), then CPUB
    will simply spin around a while loop, thus blocking task B until the other CPU
    releases the lock (task A calls the spinlock’s release function). This spinning
    will only happen on multi-core machines, which is why the use case described previously,
    which involves more than one CPU since it’s on a single core machine, cannot happen:
    the task either holds a spinlock and proceeds or doesn’t run until the lock is
    released. I used to say that a spinlock is a lock held by a CPU, which is the
    opposite of a mutex (we will discuss this in the next section), which is a lock
    held by a task. A spinlock operates by disabling the scheduler on the local CPU
    (that is, the CPU running the task that called the spinlock’s locking API). This
    also means that the task currently running on that CPU cannot be preempted by
    another task, except for IRQs if they are not disabled (more on this later). In
    other words, spinlocks protect resources that only one CPU can take/access at
    a time. This makes spinlocks suitable for SMP safety and for executing atomic
    tasks.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下场景来探讨图表：当运行任务B的CPUB想要通过自旋锁的锁定函数获取自旋锁，而该自旋锁已经被另一个CPU（比如运行任务A的CPUA，已经调用了该自旋锁的锁定函数）持有时，CPUB将简单地在一个while循环中旋转，从而阻塞任务B，直到另一个CPU释放锁（任务A调用自旋锁的释放函数）。这种旋转只会发生在多核机器上，这就是为什么先前描述的使用情况，涉及多个CPU，因为它是在单核机器上，是不可能发生的：任务要么持有自旋锁并继续，要么在锁被释放之前不运行。我曾经说过自旋锁是由CPU持有的锁，这与互斥锁相反（我们将在下一节讨论），互斥锁是由任务持有的锁。自旋锁通过禁用本地CPU上的调度程序（即运行调用自旋锁的任务的CPU）来运行。这也意味着当前在该CPU上运行的任务不能被另一个任务抢占，除非IRQs未被禁用（稍后会详细介绍）。换句话说，自旋锁保护只有一个CPU可以一次获取/访问的资源。这使得自旋锁适用于SMP安全和执行原子任务。
- en: Important note
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Spinlocks are not the only implementation that take advantage of hardware’s
    atomic functions. In the Linux kernel, for example, the preemption status depends
    on a per-CPU variable that, if equal to 0, means preemption is enabled. However,
    if it’s greater than 0, this means preemption is disabled (`schedule()` becomes
    inoperative). Thus, disabling preemption (`preempt_disable(`)) consists of adding
    1 to the current per-CPU variable (`preempt_count` actually), while `preempt_enable()`
    subtracts 1 (one) from the variable, checks whether the new value is 0, and calls
    `schedule()`. These addition/subtraction operations should then be atomic, and
    thus rely on the CPU being able to provide atomic addition/subtraction functions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 自旋锁并不是唯一利用硬件原子功能的实现。例如，在Linux内核中，抢占状态取决于每个CPU变量，如果等于0，则表示抢占已启用。然而，如果大于0，则表示抢占已禁用（`schedule()`变得无效）。因此，禁用抢占（`preempt_disable()`）包括向当前CPU变量（实际上是`preempt_count`）添加1，而`preempt_enable()`则从变量中减去1，并检查新值是否为0，然后调用`schedule()`。这些加法/减法操作应该是原子的，因此依赖于CPU能够提供原子加法/减法功能。
- en: 'There are two ways to create and initialize a spinlock: either statically using
    the `DEFINE_SPINLOCK` macro, which will declare and initialize the spinlock, or
    dynamically by calling `spin_lock_init()` on an uninitialized spinlock.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以创建和初始化自旋锁：一种是静态地使用`DEFINE_SPINLOCK`宏，它将声明并初始化自旋锁，另一种是通过在未初始化的自旋锁上调用`spin_lock_init()`来动态创建。
- en: 'First, we’ll introduce how to use the `DEFINE_SPINLOCK` macro. To understand
    how this works, we must look at the definition of this macro in `include/linux/spinlock_types.h`,
    which is as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍如何使用`DEFINE_SPINLOCK`宏。要理解这是如何工作的，我们必须查看`include/linux/spinlock_types.h`中此宏的定义，如下所示：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This can be used as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 可以如下使用：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After this, the spinlock will be accessible through its name, `foo_lock`. Note
    that its address would be `&foo_lock`. However, for dynamic (runtime) allocation,
    you need to embed the spinlock into a bigger structure, allocate memory for this
    structure, and then call `spin_lock_init()` on the spinlock element:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，自旋锁将通过其名称`foo_lock`可访问。请注意，其地址将是`&foo_lock`。然而，对于动态（运行时）分配，您需要将自旋锁嵌入到一个更大的结构中，为该结构分配内存，然后在自旋锁元素上调用`spin_lock_init()`：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'It is better to use `DEFINE_SPINLOCK` whenever possible. It offers compile-time
    initialization and requires less lines of code with no real drawback. At this
    stage, we can lock/unlock the spinlock using the `spin_lock()` and `spin_unlock()`
    inline functions, both of which are defined in `include/linux/spinlock.h`:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽可能使用`DEFINE_SPINLOCK`更好。它提供了编译时初始化，并且需要更少的代码行而没有真正的缺点。在这个阶段，我们可以使用`spin_lock()`和`spin_unlock()`内联函数来锁定/解锁自旋锁，这两个函数都在`include/linux/spinlock.h`中定义：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: That being said, there are some limitations to using spinlocks this way. Though
    a spinlock prevents preemption on the local CPU, it does not prevent this CPU
    from being hogged by an interrupt (thus, executing this interrupt’s handler).
    Imagine a situation where the CPU holds a “spinlock” in order to protect a given
    resource and an interrupt occurs. The CPU will stop its current task and branch
    out to this interrupt handler. So far, so good. Now, imagine that this IRQ handler
    needs to acquire this same spinlock (you’ve probably already guessed that the
    resource is shared with the interrupt handler). It will infinitely spin in place,
    trying to acquire a lock that’s already been locked by a task that it has preempted.
    This situation is known as a deadlock.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，使用自旋锁的这种方式存在一些限制。虽然自旋锁可以防止本地CPU的抢占，但它不能阻止这个CPU被中断占用（从而执行这个中断的处理程序）。想象一种情况，CPU持有一个“自旋锁”来保护一个给定的资源，然后发生了一个中断。CPU将停止当前的任务并转到这个中断处理程序。到目前为止，一切都好。现在，想象一下，这个IRQ处理程序需要获取相同的自旋锁（你可能已经猜到这个资源与中断处理程序共享）。它将无限地在原地旋转，试图获取一个已经被它抢占的任务锁。这种情况被称为死锁。
- en: 'To address this issue, the Linux kernel provides `_irq` variant functions for
    spinlocks, which, in addition to disabling/enabling the preemption, also disable/enable
    interrupts on the local CPU. These functions are `spin_lock_irq()` and `spin_unlock_irq()`,
    and they are defined as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，Linux内核为自旋锁提供了`_irq`变体函数，除了禁用/启用抢占之外，还在本地CPU上禁用/启用中断。这些函数是`spin_lock_irq()`和`spin_unlock_irq()`，定义如下：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You might think that this solution is sufficient, but it is not. The `_irq`
    variant partially solves this problem. Imagine that interrupts are already disabled
    on the processor before your code starts locking. So, when you call `spin_unlock_irq()`,
    you will not just release the lock, but also enable interrupts. However, this
    will probably happen in an erroneous manner since there is no way for `spin_unlock_irq()`
    to know which interrupts were enabled before locking and which weren’t.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为这个解决方案已经足够了，但事实并非如此。`_irq`变体只能部分解决这个问题。想象一下，在处理器上已经禁用了中断，然后你的代码开始锁定。所以，当你调用`spin_unlock_irq()`时，你不仅会释放锁，还会启用中断。然而，这可能会以错误的方式发生，因为`spin_unlock_irq()`无法知道在锁定之前哪些中断被启用，哪些没有被启用。
- en: 'The following is a short example of this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个简短的示例：
- en: Let’s say interrupts *x* and *y* were disabled before a spinlock was acquired,
    while *z* was not.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设在获取自旋锁之前，中断*x*和*y*已经被禁用，而*z*没有。
- en: '`spin_lock_irq()` will disable the interrupts (*x*, *y*, and *z* are now disabled)
    and take the lock.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spin_lock_irq()`将禁用中断（*x*、*y*和*z*现在都被禁用）并获取锁。'
- en: '`spin_unlock_irq()` will enable the interrupts. *x*, *y*, and *z* will all
    be enabled, which was not the case before the lock was acquired. This is where
    the problem arises.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`spin_unlock_irq()`将启用中断。*x*、*y*和*z*都将被启用，这在获取锁之前并不是这样。这就是问题所在。'
- en: This makes `spin_lock_irq()` unsafe when it’s called from IRQs that are off-context
    as its counterpart, `spin_unlock_irq()`, will naively enable IRQs with the risk
    of enabling those that were not enabled while `spin_lock_irq()` was invoked. It
    only makes sense to use `spin_lock_irq()`when you know that interrupts are enabled;
    that is, you are sure nothing else might have disabled interrupts on the local
    CPU.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当从关闭上下文调用时，`spin_lock_irq()`是不安全的，因为它的对应函数`spin_unlock_irq()`会天真地启用中断，有可能启用那些在调用`spin_lock_irq()`时没有启用的中断。只有在你知道中断被启用时才有意义使用`spin_lock_irq()`；也就是说，你确定没有其他东西可能在本地CPU上禁用了中断。
- en: 'Now, imagine that you save the status of your interrupts in a variable before
    acquiring the lock and restoring them to how they were while they were releasing.
    In this situation, there would be no more issues. To achieve this, the kernel
    provides `_irqsave` variant functions. These behave just like the `_irq` ones,
    while also saving and restoring the interrupts status feature. These functions
    are `spin_lock_irqsave()` and `spin_lock_irqrestore()`, and they are defined as
    follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下，在获取锁之前将中断的状态保存在一个变量中，并在释放锁时将它们恢复到获取锁时的状态。在这种情况下，就不会再有问题了。为了实现这一点，内核提供了`_irqsave`变体函数。这些函数的行为就像`_irq`函数一样，同时保存和恢复中断状态。这些函数是`spin_lock_irqsave()`和`spin_lock_irqrestore()`，定义如下：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Important note
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: '`spin_lock()` and all its variants automatically call `preempt_disable()`,
    which disables preemption on the local CPU. On the other hand, `spin_unlock()`
    and its variants call `preempt_enable()`, which try to enable (yes, try! – it
    depends on whether other spinlocks are locked, which would affect the value of
    the preemption counter) preemption, and which internally call `schedule()` if
    enabled (depending on the current value of the counter, which should be 0). `spin_unlock()`
    is then a preemption point and might reenable preemption.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`spin_lock()`及其所有变体会自动调用`preempt_disable()`，在本地CPU上禁用抢占。另一方面，`spin_unlock()`及其变体会调用`preempt_enable()`，尝试启用（是的，尝试！-
    这取决于其他自旋锁是否被锁定，这将影响抢占计数器的值）抢占，并在启用时（取决于计数器的当前值，应该为0）内部调用`schedule()`。`spin_unlock()`然后是一个抢占点，可能重新启用抢占。'
- en: Disabling interrupts versus only disabling preemption
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 禁用中断与仅禁用抢占
- en: Though disabling interrupts may prevent kernel preemption (a scheduler’s timer
    interrupts would be disabled), nothing prevents the protected section from invoking
    the scheduler (the `schedule()` function). Lots of kernel functions indirectly
    invoke the scheduler, such as those that deal with spinlocks. As a result, even
    a simple `printk()` function may invoke the scheduler since it deals with the
    spinlock that protects the kernel message buffer. The kernel disables or enables
    the scheduler (performs preemption) by increasing or decreasing a kernel-global
    and per-CPU variable (that defaults to 0, meaning “enabled”) called `preempt_count`.
    When this variable is greater than 0 (which is checked by the `schedule()` function),
    the scheduler simply returns and does nothing. Every time a spin_lock*-related
    helper gets invoked, this variable is increased by 1\. On the other hand, releasing
    a spinlock (any `spin_unlock*` family function) decreases it by 1, and whenever
    it reaches 0, the scheduler is invoked, meaning that your critical section would
    not be very atomic.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管禁用中断可以防止内核抢占（调度程序的定时器中断将被禁用），但没有什么可以阻止受保护的部分调用调度程序（`schedule()`函数）。许多内核函数间接调用调度程序，例如处理自旋锁的函数。因此，即使是一个简单的`printk()`函数也可能调用调度程序，因为它处理保护内核消息缓冲区的自旋锁。内核通过增加或减少一个称为`preempt_count`的全局和每个CPU变量（默认为0，表示“启用”）来禁用或启用调度程序（执行抢占）。当这个变量大于0时（由`schedule()`函数检查），调度程序简单地返回并不执行任何操作。每次调用与`spin_lock*`相关的帮助程序时，这个变量都会增加1。另一方面，释放自旋锁（任何`spin_unlock*`系列函数）会将其减少1，每当它达到0时，调度程序就会被调用，这意味着你的临界区不会是非常原子的。
- en: Thus, if your code does not trigger preemption itself, it can only be protected
    from preemption by disabling interrupts. That being said, code that’s locked a
    spinlock may not sleep as there would be no way to wake it up (remember, timer
    interrupts and schedulers are disabled on the local CPU).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你的代码本身不触发抢占，它只能通过禁用中断来防止抢占。也就是说，锁定自旋锁的代码可能不会休眠，因为没有办法唤醒它（记住，定时器中断和调度程序在本地CPU上被禁用）。
- en: Now that we are familiar with the spinlock and its subtilities, let’s look at
    the mutex, which is our second locking primitive.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经熟悉了自旋锁及其细微差别，让我们来看看互斥锁，这是我们的第二个锁原语。
- en: Mutexes
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 互斥锁
- en: The mutex is the other locking primitive we will discuss in this chapter. It
    behaves just like the spinlock, with the only difference being that your code
    can sleep. If you try to lock a mutex that is already held by another task, your
    task will find itself suspended, and it will only be woken when the mutex is released.
    There’s no spinning this time, which means that the CPU can process something
    else while your task is waiting. As I mentioned previously, *a spinlock is a lock
    held by a CPU, while a mutex is a lock held by a task*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 互斥锁是本章讨论的另一种锁原语。它的行为就像自旋锁一样，唯一的区别是你的代码可以休眠。如果你尝试锁定一个已经被另一个任务持有的互斥锁，你的任务将被挂起，只有在互斥锁被释放时才会被唤醒。这次没有自旋，这意味着CPU可以在你的任务等待时处理其他事情。正如我之前提到的，*自旋锁是由CPU持有的锁，而互斥锁是由任务持有的锁*。
- en: 'A mutex is a simple data structure that embeds a wait queue (to put contenders
    to sleep), while a spinlock protects access to this wait queue. The following
    is what `struct mutex` looks like:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 互斥锁是一个简单的数据结构，嵌入了一个等待队列（用于让竞争者休眠），而自旋锁则保护对这个等待队列的访问。以下是`struct mutex`的样子：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the preceding code, the elements that are only used in debugging mode have
    been removed for the sake of readability. However, as you can see, mutexes are
    built on top of spinlocks. `owner` represents the process that actually owns (hold)
    the lock. `wait_list` is the list in which the mutex’s contenders are put to sleep.
    `wait_lock` is the spinlock that protects `wait_list` while contenders are inserted
    and are put to sleep. This helps keep `wait_list` coherent on SMP systems.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，为了可读性，仅在调试模式下使用的元素已被删除。但是，正如你所看到的，互斥锁是建立在自旋锁之上的。`owner`表示实际拥有（持有）锁的进程。`wait_list`是互斥锁的竞争者被放置在其中休眠的列表。`wait_lock`是保护`wait_list`的自旋锁，当竞争者被插入并休眠时，它有助于保持`wait_list`在SMP系统上的一致性。
- en: 'The mutex APIs can be found in the `include/linux/mutex.h` header file. Prior
    to acquiring and releasing a mutex, it must be initialized. As for other kernel
    core data structures, there may be a static initialization, as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 互斥锁的API可以在`include/linux/mutex.h`头文件中找到。在获取和释放互斥锁之前，必须对其进行初始化。与其他内核核心数据结构一样，可能存在静态初始化，如下所示：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following is the definition of the `DEFINE_MUTEX()` macro:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`DEFINE_MUTEX()`宏的定义：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The second approach the kernel offers is dynamic initialization. This can be
    done by making a call to the low-level `__mutex_init()` function, which is actually
    wrapped by a much more user-friendly macro known as `mutex_init()`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 内核提供的第二种方法是动态初始化。这可以通过调用底层的`__mutex_init()`函数来实现，实际上这个函数被一个更加用户友好的宏`mutex_init()`所包装：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Acquiring (also known as locking) a mutex is as simple calling one of the following
    three functions:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 获取（也称为锁定）互斥锁就像调用以下三个函数之一那样简单：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If the mutex is free (unlocked), your task will immediately acquire it without
    going to sleep. Otherwise, your task will be put to sleep in a manner that depends
    on the locking function you use. With `mutex_lock()`, your task will be put in
    an uninterruptible sleep (`TASK_UNINTERRUPTIBLE`) while you wait for the mutex
    to be released (in case it is held by another task). `mutex_lock_interruptible()`
    will put your task in an interruptible sleep, in which the sleep can be interrupted
    by any signal. `mutex_lock_killable()` will allow your task’s sleep to be interrupted,
    but only by signals that actually kill the task. Both functions return zero if
    the lock has been acquired successfully. Moreover, interruptible variants return
    `-EINTR` when the locking attempt is interrupted by a signal.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果互斥锁是空闲的（未锁定），您的任务将立即获取它而不会进入睡眠状态。否则，您的任务将以取决于您使用的锁定函数的方式进入睡眠。使用`mutex_lock()`时，您的任务将进入不可中断的睡眠状态（`TASK_UNINTERRUPTIBLE`），直到等待互斥锁被释放（如果它被另一个任务持有）。`mutex_lock_interruptible()`将使您的任务进入可中断的睡眠状态，其中睡眠可以被任何信号中断。`mutex_lock_killable()`将允许您的任务的睡眠被中断，但只能被实际杀死任务的信号中断。这两个函数在成功获取锁时返回零。此外，可中断的变体在锁定尝试被信号中断时返回`-EINTR`。
- en: 'Whatever locking function is used, the mutex owner (and only the owner) should
    release the mutex using `mutex_unlock()`, which is defined as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 无论使用哪种锁定函数，互斥锁所有者（仅所有者）应使用`mutex_unlock()`释放互斥锁，其定义如下：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If you wish to check the status of your mutex, you can use `mutex_is_locked()`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望检查互斥锁的状态，可以使用`mutex_is_locked()`：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This function simply checks whether the mutex owner is `NULL` and returns true
    if it is, or false otherwise.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数只是检查互斥锁所有者是否为`NULL`，如果是，则返回true，否则返回false。
- en: Important note
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: It is only recommended to use `mutex_lock()`when you can guarantee that the
    mutex will not be held for a long time. Typically, you should use the interruptible
    variant instead.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在可以保证互斥锁不会长时间持有时才建议使用`mutex_lock()`。通常情况下，应该使用可中断的变体。
- en: 'There are specific rules when using mutexes. The most important are enumerated
    in the kernel’s mutex API header file, `include/linux/mutex.h`. The following
    is an excerpt from it:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用互斥锁时有特定的规则。最重要的规则在内核的互斥锁API头文件`include/linux/mutex.h`中列出。以下是其中的一部分摘录：
- en: '[PRE13]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The full version can be found in the same file.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 完整版本可以在同一文件中找到。
- en: Now, let’s look at some cases where we can avoid putting the mutex to sleep
    while it is being held. This is known as the try-lock method.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一些情况，我们可以避免在持有锁时将互斥锁置于睡眠状态。这被称为尝试锁定方法。
- en: The try-lock method
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 尝试锁定方法
- en: There are cases where we may need to acquire the lock if it is not already held
    by another elsewhere. Such methods try to acquire the lock and immediately (without
    spinning if we are using a spinlock, nor sleeping if we are using a mutex) return
    a status value. This tells us whether the lock has been successfully locked. They
    can be used if we do not need to access the data that’s being protected by the
    lock when some other thread is holding the lock.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们可能需要获取锁，如果它不是由其他地方持有的话。这样的方法尝试获取锁并立即返回一个状态值（如果我们使用自旋锁，则不会自旋，如果我们使用互斥锁，则不会休眠）。这告诉我们锁是否已成功锁定。如果我们不需要在其他线程持有锁时访问被保护的数据，可以使用它们。
- en: 'Both the spinlock and mutex APIs provide a try-lock method. They are called
    `spin_trylock()` and `mutex_trylock()`, respectively. Both methods return 0 on
    a failure (the lock is already locked) or 1 on a success (lock acquired). Thus,
    it makes sense to use these functions along with an statement:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 自旋锁和互斥锁API都提供了尝试锁定方法。它们分别称为`spin_trylock()`和`mutex_trylock()`。这两种方法在失败时（锁已被锁定）返回0，在成功时（锁已获取）返回1。因此，使用这些函数与语句是有意义的：
- en: '[PRE14]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`spin_trylock()` actually targets spinlocks. It will lock the spinlock if it
    is not already locked in the same way that the `spin_lock()` method is. However,
    it immediately returns `0` without spinning if the spinlock is already locked:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`spin_trylock()`实际上是针对自旋锁的。如果自旋锁尚未被锁定，它将锁定自旋锁，方式与`spin_lock()`方法相同。但是，如果自旋锁已经被锁定，它将立即返回`0`而不会自旋：'
- en: '[PRE15]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'On the other hand, `mutex_trylock()` targets mutexes. It will lock the mutex
    if it is not already locked in the same way that the `mutex_lock()` method is.
    However, it immediately returns `0` without sleeping if the mutex is already locked.
    The following is an example of this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`mutex_trylock()`是针对互斥锁的。如果互斥锁尚未被锁定，它将以与`mutex_lock()`方法相同的方式锁定互斥锁。但是，如果互斥锁已经被锁定，它将立即返回`0`而不会休眠。以下是一个示例：
- en: '[PRE16]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the preceding code, the try-lock is being used along with the `if` statement
    so that the driver can adapt its behavior.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，使用了尝试锁定以及`if`语句，以便驱动程序可以调整其行为。
- en: Waiting, sensing, and blocking in the Linux kernel
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Linux内核中等待、感知和阻塞
- en: This section could have been named *kernel sleeping mechanism* as the mechanisms
    we will deal with involve putting the processes involved to sleep. A device driver,
    during its life cycle, can initiate completely separate tasks, some of which depend
    on the completion of others. The Linux kernel addresses such dependencies with
    `struct completion` items. On the other hand, it may be necessary to wait for
    a particular condition to become true or the state of an object to change. This
    time, the kernel provides work queues to address this situation.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分本来可以被命名为*内核睡眠机制*，因为我们将处理的机制涉及将涉及的进程置于睡眠状态。设备驱动程序在其生命周期中可以启动完全独立的任务，其中一些任务依赖于其他任务的完成。Linux内核使用`struct
    completion`项来解决这种依赖关系。另一方面，可能需要等待特定条件成为真或对象状态发生变化。这时，内核提供工作队列来解决这种情况。
- en: Waiting for completion or a state change
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 等待完成或状态改变
- en: 'You may not necessarily be waiting exclusively for a resource, but for the
    state of a given object (shared or not) to change or for a task to complete. In
    kernel programming practices, it is common to initiate an activity outside the
    current thread, and then wait for that activity to complete. Completion is a good
    alternative to `sleep()` when you’re waiting for a buffer to be used, for example.
    It is suitable for sensing data, as is the case with DMA transfers. Working with
    completions requires including the `<linux/completion.h>` header. Its structure
    looks as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能不一定是在专门等待资源，而是等待给定对象（共享或非共享）的状态改变或任务完成。在内核编程实践中，通常会在当前线程之外启动一个活动，然后等待该活动完成。当您等待缓冲区被使用时，完成是
    `sleep()` 的一个很好的替代方案，例如。它适用于传感数据，就像 DMA 传输一样。使用完成需要包括 `<linux/completion.h>` 头文件。其结构如下：
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You can create instances of the struct completion structure either statically
    using the static `DECLARE_COMPLETION(my_comp)` function or dynamically by wrapping
    the completion structure into a dynamic (allocated on the heap, which will be
    alive for the lifetime of the function/driver) data structure and invoking `init_completion(&dynamic_object->my_comp)`.
    When the device driver performs some work (a DMA transaction, for example) and
    others (threads, for example) need to be notified of their completion, the waiter
    has to call `wait_for_completion()` on the previously initialized struct completion
    object in order to be notified of this:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用静态的 `DECLARE_COMPLETION(my_comp)` 函数静态地创建结构完成的实例，也可以通过将完成结构包装到动态数据结构中（在堆上分配，将在函数/驱动程序的生命周期内保持活动状态）并调用
    `init_completion(&dynamic_object->my_comp)` 来动态创建。当设备驱动程序执行一些工作（例如 DMA 事务）并且其他人（例如线程）需要被通知它们已经完成时，等待者必须在先前初始化的结构完成对象上调用
    `wait_for_completion()` 以便得到通知：
- en: '[PRE18]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'When the other part of the code has decided that the work has been completed
    (the transaction has been completed, in the case of DMA), it can wake up anyone
    (the code that needs to access the DMA buffer) who is waiting by either calling
    `complete()`, which will only wake one waiting process, or `complete_all()`, which
    will wake everyone waiting for this to complete:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当代码的另一部分已经决定工作已经完成（在 DMA 的情况下，事务已经完成），它可以通过调用 `complete()` 来唤醒任何等待的人（需要访问 DMA
    缓冲区的代码），这将只唤醒一个等待的进程，或者调用 `complete_all()`，这将唤醒所有等待完成的人：
- en: '[PRE19]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'A typical usage scenario is as follows (this excerpt has been taken from the
    kernel documentation):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的使用场景如下（此摘录摘自内核文档）：
- en: '[PRE20]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The order in which `wait_for_completion()` and `complete()` are called does
    not matter. As semaphores, the completions API is designed so that they will work
    properly, even if `complete()` is called before `wait_for_completion()`. In such
    a case, the waiter will simply continue immediately once all the dependencies
    have been satisfied.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`wait_for_completion()` 和 `complete()` 被调用的顺序并不重要。与信号量一样，完成 API 被设计成，即使在 `complete()`
    被调用之前调用 `wait_for_completion()`，它们也能正常工作。在这种情况下，一旦所有依赖关系得到满足，等待者将立即继续执行。'
- en: Note that `wait_for_completion()` will invoke `spin_lock_irq()` and `spin_unlock_irq()`,
    which, according to the *Spinlocks* section, are not recommended to be used from
    within an interrupt handler or with disabled IRQs. This is because it would result
    in spurious interrupts being enabled, which are hard to detect. Additionally,
    by default, `wait_for_completion()` marks the task as uninterruptible (`TASK_UNINTERRUPTIBLE`),
    making it unresponsive to any external signal (even kill). This may block for
    a long time, depending on the nature of the activity it’s waiting for.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`wait_for_completion()` 将调用 `spin_lock_irq()` 和 `spin_unlock_irq()`，根据 *自旋锁*
    部分的说明，不建议在中断处理程序内部或禁用 IRQs 时使用它们。这是因为它会导致启用难以检测的虚假中断。另外，默认情况下，`wait_for_completion()`
    将任务标记为不可中断 (`TASK_UNINTERRUPTIBLE`)，使其对任何外部信号（甚至 kill 信号）都不响应。这可能会根据它等待的活动的性质而阻塞很长时间。
- en: 'You may need the *wait* not to be done in an uninterruptible state, or at least
    you may need the *wait* being able to be interrupted either by any signal or only
    by signals that kill the process. The kernel provides the following APIs:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能需要 *等待* 不是在不可中断状态下完成，或者至少您可能需要 *等待* 能够被任何信号中断，或者只能被杀死进程的信号中断。内核提供了以下 API：
- en: '`wait_for_completion_interruptible()`'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wait_for_completion_interruptible()`'
- en: '`wait_for_completion_interruptible_timeout()`'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wait_for_completion_interruptible_timeout()`'
- en: '`wait_for_completion_killable()`'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wait_for_completion_killable()`'
- en: '`wait_for_completion_killable_timeout()`'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`wait_for_completion_killable_timeout()`'
- en: '`_killable` variants will mark the task as `TASK_KILLABLE`, thus only making
    it responsive to signals that actually kill it, while `_interruptible` variants
    mark the task as `TASK_INTERRUPTIBLE`, allowing it to be interrupted by any signal.
    `_timeout` variants will, at most, wait for the specified timeout:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`_killable` 变体将任务标记为 `TASK_KILLABLE`，因此只会对真正杀死它的信号做出响应，而 `_interruptible` 变体将任务标记为
    `TASK_INTERRUPTIBLE`，允许它被任何信号中断。 `_timeout` 变体将最多等待指定的超时时间：'
- en: '[PRE21]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Since `wait_for_completion*()` may sleep, it can only be used in this process
    context. Because the `interruptible`, `killable`, or `timeout` variant may return
    before the underlying job has run until completion, their return values should
    be checked carefully so that you can adopt the right behavior. The killable and
    interruptible variants return `-ERESTARTSYS` if they’re interrupted and `0` if
    they’ve been completed. On the other hand, the timeout variants will return `-ERESTARTSYS`
    if they’re interrupted, `0` if they’ve timed out, and the number of jiffies (at
    least 1) left until timeout if they’ve completed before timeout. Please refer
    to `kernel/sched/completion.c` in the kernel source for more on this, as well
    as more functions that will not be covered in this book.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`wait_for_completion*()`可能会休眠，因此只能在此进程上下文中使用。因为`interruptible`、`killable`或`timeout`变体可能在底层作业完成之前返回，所以应该仔细检查它们的返回值，以便采取正确的行为。可杀死和可中断的变体如果被中断则返回`-ERESTARTSYS`，如果已完成则返回`0`。另一方面，超时变体将在被中断时返回`-ERESTARTSYS`，在超时时返回`0`，在超时之前完成则返回剩余的jiffies数（至少为1）。有关更多信息，请参阅内核源代码中的`kernel/sched/completion.c`，以及本书中未涵盖的更多函数。
- en: On the other hand, `complete()` and `complete_all()` never sleep and internally
    call `spin_lock_irqsave()`/`spin_unlock_irqrestore()`, making completion signaling,
    from an IRQ context, completely safe.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`complete()`和`complete_all()`永远不会休眠，并且在内部调用`spin_lock_irqsave()`/`spin_unlock_irqrestore()`，使得从IRQ上下文中进行完成信号完全安全。
- en: Linux kernel wait queues
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Linux内核等待队列
- en: 'Wait queues are high-level mechanisms that are used to process block I/O, wait
    for particular conditions to be true, wait for a given event to occur, or to sense
    data or resource availability. To understand how they work, let’s have a look
    at the structure in `include/linux/wait.h`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 等待队列是用于处理块I/O、等待特定条件成立、等待特定事件发生或感知数据或资源可用性的高级机制。要了解它们的工作原理，让我们来看看`include/linux/wait.h`中的结构：
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'A `wait queue` is nothing but a list (in which processes are put to sleep so
    that they can be awaken if some conditions are met) where there’s a spinlock to
    protect access to this list. You can use a `wait queue` when more than one process
    wants to sleep and you’re waiting for one or more events to occur so that it can
    be woke up. The head member is the list of processes waiting for the event(s).
    Each process that wants to sleep while waiting for the event to occur puts itself
    in this list before going to sleep. When a process is in the list, it is called
    a `wait queue entry`. When the event occurs, one or more processes on the list
    are woken up and moved off the list. We can declare and initialize a `wait queue`
    in two ways. First, we can declare and initialize it statically using `DECLARE_WAIT_QUEUE_HEAD`,
    as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`等待队列`只是一个列表（其中进程被放入休眠，以便在满足某些条件时唤醒）的名称，其中有一个自旋锁来保护对该列表的访问。当多个进程希望休眠并且正在等待一个或多个事件发生时，可以使用`等待队列`。头成员是等待事件的进程列表。希望在等待事件发生时休眠的每个进程都会在进入休眠之前将自己放入此列表中。当进程在列表中时，它被称为`等待队列条目`。当事件发生时，列表中的一个或多个进程将被唤醒并移出列表。我们可以以两种方式声明和初始化`等待队列`。首先，我们可以使用`DECLARE_WAIT_QUEUE_HEAD`静态声明和初始化它，如下所示：'
- en: '[PRE23]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can also do this dynamically using `init_waitqueue_head()`:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用`init_waitqueue_head()`来动态执行此操作：
- en: '[PRE24]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Any process that wants to sleep while waiting for `my_event` to occur can invoke
    either `wait_event_interruptible()` or `wait_event()`. Most of the time, the event
    is just the fact that a resource has become available. Thus, it only makes sense
    for a process to go to sleep after the availability of that resource has been
    checked. To make things easy for you, these functions both take an expression
    in place of the second argument so that the process is only put to sleep if the
    expression evaluates to false:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 任何希望在等待`my_event`发生时休眠的进程都可以调用`wait_event_interruptible()`或`wait_event()`。大多数情况下，事件只是资源已经可用的事实。因此，只有在检查资源的可用性之后，进程才会进入休眠状态。为了方便您，这些函数都接受一个表达式作为第二个参数，因此只有在表达式评估为假时，进程才会进入休眠状态：
- en: '[PRE25]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`wait_event()` and `wait_event_interruptible()` simply evaluate the condition
    when it’s called. If the condition is false, the process is put into either a
    `TASK_UNINTERRUPTIBLE` or a `TASK_INTERRUPTIBLE` (for the `_interruptible` variant)
    state and removed from the running queue.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`wait_event()`和`wait_event_interruptible()`在调用时简单地评估条件。如果条件为假，则进程将被放入`TASK_UNINTERRUPTIBLE`或`TASK_INTERRUPTIBLE`（对于`_interruptible`变体）状态，并从运行队列中移除。'
- en: 'There may be cases where you need the condition to not only be true, but to
    time out after waiting a certain amount of time. You can address such cases using
    `wait_event_timeout()`, whose prototype is as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 可能存在这样的情况，您不仅需要条件为真，而且还需要在等待一定时间后超时。您可以使用`wait_event_timeout()`来处理这种情况，其原型如下：
- en: '[PRE26]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This function has two behaviors, depending on the timeout having elapsed or
    not:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数具有两种行为，取决于超时是否已经过去：
- en: '`timeout` has elapsed: The function returns 0 if the condition is evaluated
    to false or 1 if it is evaluated to true.'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`timeout`已经过去：如果条件评估为假，则函数返回0，如果评估为真，则返回1。'
- en: '`timeout` has not elapsed yet: The function returns the remaining time (in
    jiffies – must at least be 1) if the condition is evaluated to true.'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`timeout`尚未过去：如果条件评估为真，则函数返回剩余时间（以jiffies为单位，必须至少为1）。'
- en: 'The time unit for the timeout is `jiffies`. So that you don’t have to bother
    with seconds to `jiffies` conversion, you should use the `msecs_to_jiffies()`
    and `usecs_to_jiffies()` helpers, which convert milliseconds or microseconds into
    jiffies, respectively:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 超时的时间单位是`jiffies`。因此，您不必担心将秒转换为`jiffies`，您应该使用`msecs_to_jiffies()`和`usecs_to_jiffies()`辅助函数，分别将毫秒或微秒转换为`jiffies`：
- en: '[PRE27]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'After a change has been made to any variable that could mangle the result of
    the wait condition, you must call the appropriate `wake_up*` family function.
    That being said, in order to wake up a process sleeping on a wait queue, you should
    call either `wake_up()`, `wake_up_all()`, `wake_up_interruptible()`, or `wake_up_interruptible_all()`.
    Whenever you call any of these functions, the condition is reevaluated. If the
    condition is true at this time, then a process (or all the processes for the `_all()`
    variant) in `wait queue` will be awakened, and its (their) state will be set to
    `TASK_RUNNING`; otherwise (the condition is false), nothing will happen:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在更改可能破坏等待条件结果的任何变量后，必须调用适当的`wake_up*`系列函数。也就是说，为了唤醒在等待队列上休眠的进程，您应该调用`wake_up()`、`wake_up_all()`、`wake_up_interruptible()`或`wake_up_interruptible_all()`中的任何一个。每当调用这些函数时，条件都会被重新评估。如果此时条件为真，则等待队列中的进程（或`_all()`变体的所有进程）将被唤醒，并且其状态将被设置为`TASK_RUNNING`；否则（条件为假），什么也不会发生。
- en: '[PRE28]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Since they can be interrupted by signals, you should check the return values
    of `_interruptible` variants. A non-zero value means your sleep has been interrupted
    by some sort of signal, so the driver should return `ERESTARTSYS`:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们可以被信号中断，您应该检查`_interruptible`变体的返回值。非零值意味着您的睡眠已被某种信号中断，因此驱动程序应返回`ERESTARTSYS`。
- en: '[PRE29]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In the preceding example, the current process (actually, this is `insmod`)
    will be put to sleep in the wait queue for 5 seconds and woken up by the work
    handler. The output of `dmesg` is as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，当前进程（实际上，这是`insmod`）将在等待队列中休眠5秒，并由工作处理程序唤醒。`dmesg`的输出如下：
- en: '[PRE30]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You may have noticed that I did not check the return value of `wait_event_interruptible()`.
    Sometimes (if not most of the time), this can lead to serious issues. The following
    is a true story: I’ve had to intervene in a company to fix a bug where killing
    (or sending a signal to) a user space task was making their kernel module crash
    the system (panic and reboot – of course, the system was configured so that it
    rebooted on panic). The reason this happened was because there was a thread in
    this user process that did an `ioctl()` on the `char` device exposed by their
    kernel module. This resulted in a call to `wait_event_interruptible()` in the
    kernel on a given flag, which meant there was some data that needed to be processed
    in the kernel (the `select()` system call could not be used).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，我没有检查`wait_event_interruptible()`的返回值。有时（如果不是大多数时候），这可能会导致严重问题。以下是一个真实的故事：我曾经不得不介入一家公司来修复一个bug，即杀死（或向）用户空间任务发送信号会使他们的内核模块使系统崩溃（恐慌和重启-当然，系统被配置为在恐慌时重新启动）。发生这种情况的原因是因为这个用户进程中有一个线程在其内核模块公开的`char`设备上执行`ioctl()`。这导致内核中对给定标志的`wait_event_interruptible()`的调用，这意味着内核中需要处理一些数据（不能使用`select()`系统调用）。
- en: So, what was their mistake? The signal that was sent to the process was making
    `wait_event_interruptible()` return without the flag being set (which meant data
    was still not available), and its code was not checking its return value, nor
    rechecking the flag or performing a sanity check on the data that was supposed
    to be available. The data was being accessed as if the flag had been set and it
    actually dereferenced an invalid pointer.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，他们的错误是什么？发送给进程的信号使`wait_event_interruptible()`返回而没有设置标志（这意味着数据仍然不可用），它的代码没有检查其返回值，也没有重新检查标志或对应该可用的数据进行合理性检查。数据被访问，就好像标志已经被设置，并且实际上对一个无效的指针进行了解引用。
- en: 'The solution could have been as simple as using the following code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案可能只需使用以下代码：
- en: '[PRE31]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: However, for some reason (historical to their design), we had to make it uninterruptible,
    which resulted in us using `wait_event()`. However, note that this function puts
    the process into an exclusive wait (an uninterruptible sleep), which means it
    can’t be interrupted by signals. It should only be used for critical tasks. Interruptible
    functions are recommended in most situations.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于某种原因（与他们的设计有关），我们不得不使其不可中断，这导致我们使用了`wait_event()`。但是，请注意，此函数将进程置于独占等待状态（不可中断睡眠），这意味着它不能被信号中断。它应该只用于关键任务。在大多数情况下建议使用可中断函数。
- en: Now that we are familiar with the kernel locking APIs, we will look at various
    work deferring mechanisms, all of which are heavily used when writing Linux device
    drivers.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们熟悉了内核锁定API，我们将看一下各种工作推迟机制，这些机制在编写Linux设备驱动程序时被广泛使用。
- en: Work deferring mechanisms
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作推迟机制
- en: 'Work deferring is a mechanism the Linux kernel offers. It allows you to defer
    work/a task until the system’s workload allows it to run smoothly or after a given
    time has lapsed. Depending on the type of work, the deferred task can run either
    in a process context or in an atomic context. It is common to using work deferring
    to complement the interrupt handler in order to compensate for some of its limitations,
    some of which are as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 工作推迟是Linux内核提供的一种机制。它允许您推迟工作/任务，直到系统的工作负载允许它平稳运行或经过一定的时间。根据工作的类型，延迟任务可以在进程上下文或原子上下文中运行。通常使用工作推迟来补充中断处理程序，以弥补其中一些限制，其中一些如下：
- en: The interrupt handler must be as fast as possible, meaning that only a critical
    task should be performed in the handler so that the rest can be deferred later
    when the system is less busy.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中断处理程序必须尽可能快，这意味着处理程序中只能执行关键任务，以便其余任务在系统不太忙时稍后推迟。
- en: In the interrupt context, we cannot use blocking calls. The sleeping task should
    be scheduled in the process context.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在中断上下文中，我们不能使用阻塞调用。睡眠任务应该在进程上下文中被调度。
- en: 'The deferring work mechanism allows us to perform the minimum possible work
    in the interrupt handler (the so-called *top-half*, which runs in an interrupt
    context) and schedule an asynchronous action (the so-called *bottom-half*, which
    may – but not always – run in a user context) from the interrupt handler so that
    it can be run at a later time and execute the rest of the operations. Nowadays,
    the concept of bottom-half is mostly assimilated to deferred work running in a
    process context, since it was common to schedule work that might sleep (unlike
    rare work running in an interrupt context, which cannot happen). Linux now has
    three different implementations of this: **softIRQs**, **tasklets**, and **work
    queues**. Let’s take a look at these:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟工作机制允许我们在中断处理程序中执行尽可能少的工作（所谓的*top-half*），并安排一个异步操作（所谓的*bottom-half*），以便稍后可以运行并执行其余的操作。现在，底半部分的概念大多被吸收到在进程上下文中运行的延迟工作中，因为常见的是安排可能休眠的工作（与在中断上下文中运行的罕见工作不同，后者不会发生）。Linux现在有三种不同的实现：**softIRQs**，**tasklets**和**work
    queues**。让我们来看看这些：
- en: '**SoftIRQs**: These are executed in an atomic context.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SoftIRQs**：这些在原子上下文中执行。'
- en: '**Tasklets**: These are also executed in an atomic context.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tasklets**：这些也在原子上下文中执行。'
- en: '**Work queues**: These run in a process context.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Work queues**：这些在进程上下文中运行。'
- en: We will learn about each of them in detail in the next few sections.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的几节中详细了解每一个。
- en: SoftIRQs
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SoftIRQs
- en: As the name suggests, `kernel/softirq.c` in the kernel source tree, and any
    drivers that wish to use this API need to include `<linux/interrupt.h>`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，`kernel/softirq.c`在内核源树中，任何希望使用此API的驱动程序都需要包含`<linux/interrupt.h>`。
- en: 'Note that you cannot dynamically register nor destroy softIRQs. They are statically
    allocated at compile time. Moreover, the usage of softIRQs is restricted to statically
    compiled kernel code; they cannot be used with dynamically loadable modules. SoftIRQs
    are represented by `struct softirq_action` structures defined in `<linux/interrupt.h>`,
    as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您不能动态注册或销毁softIRQ。它们在编译时静态分配。此外，softIRQ的使用受到静态编译内核代码的限制；它们不能与动态可加载模块一起使用。SoftIRQ由`<linux/interrupt.h>`中定义的`struct
    softirq_action`结构表示，如下所示：
- en: '[PRE32]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This structure embeds a pointer to the function to be run when the `softirq`
    action is raised. Thus, the prototype of your softIRQ handler should look as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 此结构嵌入了指向`softirq`动作被触发时要运行的函数的指针。因此，您的softIRQ处理程序的原型应如下所示：
- en: '[PRE33]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Running a softIRQ handler results in this action function being executed. It
    only has one parameter: a pointer to the corresponding `softirq_action` structure.
    You can register the softIRQ handler at runtime by means of the `open_softirq()`
    function:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 运行softIRQ处理程序会导致执行此动作函数。它只有一个参数：指向相应的`softirq_action`结构的指针。您可以通过`open_softirq()`函数在运行时注册softIRQ处理程序：
- en: '[PRE34]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '`nr` represents the softIRQ’s index, which is also considered as the softIRQ’s
    priority (where `0` is the highest). `action` is a pointer to the softIRQ’s handler.
    Any possible indexes are enumerated in the following `enum`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`nr`表示softIRQ的索引，也被视为softIRQ的优先级（其中`0`最高）。`action`是指向softIRQ处理程序的指针。任何可能的索引都在以下`enum`中列举：'
- en: '[PRE35]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'SoftIRQs with lower indexes (highest priority) run before those with higher
    indexes (lowest priority). The names of all the available softIRQs in the kernel
    are listed in the following array:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 具有较低索引（最高优先级）的SoftIRQ在具有较高索引（最低优先级）的SoftIRQ之前运行。内核中所有可用的softIRQ的名称都列在以下数组中：
- en: '[PRE36]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'It is easy to check the output of the `/proc/softirqs` virtual file, as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 可以轻松检查`/proc/softirqs`虚拟文件的输出，如下所示：
- en: '[PRE37]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'A `NR_SOFTIRQS` entry array of `struct softirq_action` is declared in `kernel/softirq.c`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在`kernel/softirq.c`中声明了一个`struct softirq_action`的`NR_SOFTIRQS`条目数组：
- en: '[PRE38]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Each entry in this array may contain one and only one softIRQ. As a consequence
    of this, there can be a maximum of `NR_SOFTIRQS` (10 in v4.19, which is the last
    version at the time of writing this) for registered softIRQs:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 该数组中的每个条目可能只包含一个softIRQ。因此，最多可以有`NR_SOFTIRQS`（在撰写本文时的最后版本为v4.19，为10）个已注册的softIRQ：
- en: '[PRE39]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'A concrete example of this is the network subsystem, which registers softIRQs
    that it needs (in `net/core/dev.c`) as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个具体的例子是网络子系统，它在`net/core/dev.c`中注册所需的softIRQ，如下所示：
- en: '[PRE40]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Before a registered softIRQ gets a chance to run, it should be activated/scheduled.
    To do this, you must call `raise_softirq()` or `raise_softirq_irqoff()` (if interrupts
    are already off):'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在注册的softIRQ有机会运行之前，它应该被激活/安排。要做到这一点，您必须调用`raise_softirq()`或`raise_softirq_irqoff()`（如果中断已关闭）：
- en: '[PRE41]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The first function simply sets the appropriate bit in the per-CPU softIRQ bitmap
    (the `__softirq_pending` field in the `struct irq_cpustat_t` data structure, which
    is allocated per-CPU in `kernel/softirq.c`), as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个函数只是在每个CPU的softIRQ位图中设置适当的位（`kernel/softirq.c`中为每个CPU分配的`struct irq_cpustat_t`数据结构中的`__softirq_pending`字段），如下所示：
- en: '[PRE42]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This allows it to run when the flag is checked. This function has been described
    here for study purposes and should not be used directly.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许在检查标志时运行。此函数已在此处描述供学习目的，并且不应直接使用。
- en: '`raise_softirq_irqoff` needs be called with interrupts disabled. First, it
    internally calls `__raise_softirq_irqoff()`, as described previously, to activate
    the softIRQ. Then, it checks whether it has been called from within an interrupt
    (either hard or soft) context by means of the `in_interrupt()` macro (which simply
    returns the value of `current_thread_info( )->preempt_count`, where 0 means preemption
    is enabled. This states that we are not in an interrupt context. A value greater
    than 0 means we are in an interrupt context). If `in_interrupt() > 0`, this does
    nothing as we are in an interrupt context. This is because softIRQ flags are checked
    on the exit path of any I/O IRQ handler (`asm_do_IRQ()` for ARM or `do_IRQ()`
    for x86 platforms, which makes a call to `irq_exit()`). Here, softIRQs run in
    an interrupt context. However, if `in_interrupt() == 0`, then `wakeup_softirqd()`
    gets invoked. This is responsible for waking the local CPU `ksoftirqd` thread
    up (it schedules it) to ensure the softIRQ runs soon but in a process context
    this time.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`raise_softirq_irqoff` 需要在中断被禁用时调用。首先，它内部调用 `__raise_softirq_irqoff()`，如前所述，来激活
    softIRQ。然后，它通过 `in_interrupt()` 宏检查是否从中断（硬件或软件）上下文中调用（该宏简单地返回 `current_thread_info(
    )->preempt_count` 的值，其中 0 表示启用了抢占。这表示我们不在中断上下文中。大于 0 的值表示我们在中断上下文中）。如果 `in_interrupt()
    > 0`，则不执行任何操作，因为我们在中断上下文中。这是因为 softIRQ 标志在任何 I/O IRQ 处理程序的退出路径上被检查（对于 ARM 是 `asm_do_IRQ()`，对于
    x86 平台是 `do_IRQ()`，它调用 `irq_exit()`）。在这里，softIRQ 在中断上下文中运行。但是，如果 `in_interrupt()
    == 0`，那么会调用 `wakeup_softirqd()`。这负责唤醒本地 CPU 的 `ksoftirqd` 线程（它调度它）以确保 softIRQ
    很快运行，但这次是在进程上下文中。'
- en: '`raise_softirq` first calls `local_irq_save()` (which disables interrupts on
    the local processor after saving its current interrupt flags). It then calls `raise_softirq_irqoff()`,
    as described previously, to schedule the softIRQ on the local CPU (remember, this
    function must be invoked with IRQs disabled on the local CPU). Finally, it calls
    `local_irq_restore()`to restore the previously saved interrupt flags.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`raise_softirq` 首先调用 `local_irq_save()`（在保存当前中断标志后禁用本地处理器上的中断）。然后调用 `raise_softirq_irqoff()`，如前所述，在本地
    CPU 上调度 softIRQ（请记住，必须在本地 CPU 上禁用 IRQ 时调用此函数）。最后，它调用 `local_irq_restore()` 来恢复先前保存的中断标志。'
- en: 'There are a few things to remember about softIRQs:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 softIRQ，有一些需要记住的事情：
- en: A softIRQ can never preempt another softIRQ. Only hardware interrupts can. SoftIRQs
    are executed at a high priority with scheduler preemption disabled, but with IRQs
    enabled. This makes softIRQs suitable for the most time-critical and important
    deferred processing on the system.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: softIRQ 永远不会抢占另一个 softIRQ。只有硬件中断可以。SoftIRQ 以高优先级执行，禁用调度程序抢占，但启用 IRQ。这使得 softIRQ
    适用于系统上最关键和重要的延迟处理。
- en: While a handler runs on a CPU, other softIRQs on this CPU are disabled. SoftIRQs
    can run concurrently, however. While a softIRQ is running, another softIRQ (even
    the same one) can run on another processor. This is one of the main advantages
    of softIRQs over hardIRQs, and is the reason why they are used in the networking
    subsystem, which may require heavy CPU power.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当处理程序在 CPU 上运行时，该 CPU 上的其他 softIRQ 被禁用。但是 softIRQ 可以并发运行。在 softIRQ 运行时，另一个 softIRQ（甚至是相同的
    softIRQ）可以在另一个处理器上运行。这是 softIRQ 相对于 hardIRQ 的主要优势之一，也是它们被用于可能需要大量 CPU 力量的网络子系统的原因。
- en: For locking between softIRQs (or even the same softIRQ as it may be running
    on a different CPU), you should use `spin_lock()` and `spin_unlock()`.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 softIRQ 之间的锁定（甚至是同一个 softIRQ，因为它可能在不同的 CPU 上运行），应该使用 `spin_lock()` 和 `spin_unlock()`。
- en: SoftIRQs are mostly scheduled in the return paths of hardware interrupt handlers.
    `ksoftirqd``CONFIG_SMP` enabled). See `timer_tick()`, `update_process_times()`,
    and `run_local_timers()` for more.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: softIRQ 主要在硬件中断处理程序的返回路径上调度。`ksoftirqd``CONFIG_SMP` 已启用。更多信息请参见 `timer_tick()`、`update_process_times()`
    和 `run_local_timers()`。
- en: --By making a call to the `local_bh_enable()` function (mostly invoked by the
    network subsystem for handling packet receiving/transmitting softIRQs).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '- 通过调用 `local_bh_enable()` 函数（主要由网络子系统调用，用于处理软IRQ 的数据包接收/发送）。'
- en: --On the exit path of any I/O IRQ handler (see `do_IRQ`, which makes a call
    to `irq_exit()`, which in turn invokes `invoke_softirq()`).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在任何 I/O IRQ 处理程序的退出路径上（参见 `do_IRQ`，它调用 `irq_exit()`，后者又调用 `invoke_softirq()`）。'
- en: --When the local `ksoftirqd` is given the CPU (that is, it’s been awakened).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '- 当本地的 `ksoftirqd` 获得 CPU 时（也就是说，它被唤醒）。'
- en: 'The actual kernel function responsible for walking through the softIRQ’s pending
    bitmap and running them is `__do_softirq()`, which is defined in `kernel/softirq.c`.
    This function is always invoked with interrupts disabled on the local CPU. It
    performs the following tasks:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 负责遍历 softIRQ 挂起位图并运行它们的实际内核函数是 `__do_softirq()`，它在 `kernel/softirq.c` 中定义。该函数始终在本地
    CPU 上禁用中断时调用。它执行以下任务：
- en: Once invoked, the function saves the current per-CPU pending softIRQ’s bitmap
    in a so-called pending variable and locally disables softIRQs by means of `__local_bh_disable_ip`.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦调用，该函数将当前每个 CPU 挂起的 softIRQ 位图保存在一个所谓的挂起变量中，并通过 `__local_bh_disable_ip` 本地禁用
    softIRQ。
- en: It then resets the current per-CPU pending bitmask (which has already been saved)
    and then reenables interrupts (softIRQs run with interrupts enabled).
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后重置当前每个 CPU 挂起位掩码（已保存），然后重新启用中断（softIRQ 在启用中断时运行）。
- en: After this, it enters a `while` loop, checking for pending softIRQs in the saved
    bitmap. If there is no softIRQ pending, nothing happens. Otherwise, it will execute
    the handlers of each pending softIRQ, taking care to increment their executions'
    statistics.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，它进入一个 `while` 循环，检查保存的位图中是否有挂起的 softIRQ。如果没有 softIRQ 挂起，则什么也不会发生。否则，它将执行每个挂起
    softIRQ 的处理程序，并注意增加它们的执行统计。
- en: After all the pending handlers have been executed (we are out of the `while`
    loop), `__do_softirq()` once again reads the per-CPU pending bitmask (required
    to disable IRQs and save them into the same pending variable) in order to check
    if any softIRQs were scheduled when it was in the `while` loop. If there are any
    pending softIRQs, the whole process will restart (based on a `goto` loop), starting
    from *step 2*. This helps with handling, for example, softIRQs that have rescheduled
    themselves.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在执行完所有挂起的处理程序之后（我们已经退出了`while`循环），`__do_softirq()`再次读取每个CPU的挂起位掩码（需要禁用IRQ并将它们保存到相同的挂起变量中），以检查在`while`循环中是否安排了任何softIRQs。如果有任何挂起的softIRQs，整个过程将重新启动（基于`goto`循环），从*步骤2*开始。这有助于处理例如重新安排自己的softIRQs。
- en: 'However, `__do_softirq()` will not repeat if one of the following conditions
    occurs:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果发生以下条件之一，`__do_softirq()`将不会重复：
- en: It has already repeated up to `MAX_SOFTIRQ_RESTART` times, which is set to `10`
    in `kernel/softirq.c`. This is actually the limit for the softIRQ processing loop,
    not the upper bound of the previously described `while` loop.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它已经重复了`MAX_SOFTIRQ_RESTART`次，该次数在`kernel/softirq.c`中设置为`10`。这实际上是softIRQ处理循环的限制，而不是先前描述的`while`循环的上限。
- en: It has hogged the CPU more than `MAX_SOFTIRQ_TIME`, which is set to 2 ms (`msecs_to_jiffies(2)`)
    in `kernel/softirq.c`, since this prevents the scheduler from being enabled.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它已经占用了CPU超过`MAX_SOFTIRQ_TIME`，在`kernel/softirq.c`中设置为2毫秒（`msecs_to_jiffies(2)`），因为这会阻止调度程序被启用。
- en: If one of the two situations occurs, `__do_softirq()` will break its loop and
    call `wakeup_softirqd()`to wake the local `ksoftirqd` thread, which will later
    execute the pending softIRQs in the process context. Since `do_softirq` is called
    at many points in the kernel, it is likely that another invocation of `__do_softirqs`
    will handle pending softIRQs before `ksoftirqd` has the chance to run.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如果发生两种情况中的一种，`__do_softirq()`将中断其循环并调用`wakeup_softirqd()`来唤醒本地的`ksoftirqd`线程，后者将在进程上下文中执行挂起的softIRQs。由于`do_softirq`在内核中的许多点被调用，很可能在`ksoftirqd`有机会运行之前，另一个`__do_softirqs`的调用将处理挂起的softIRQs。
- en: Note that softIRQs do not always run in an atomic context, but in this case,
    this is quite specific. The next section explains how and why softIRQs may be
    executed in a process context.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，softIRQs并不总是在原子上下文中运行，但在这种情况下，这是非常特定的。下一节将解释softIRQs可能在进程上下文中执行的方式和原因。
- en: A word on ksoftirqd
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于ksoftirqd
- en: 'A `ksoftirqd` is a per-CPU kernel thread that’s raised in order to handle unserved
    software interrupts. It is spawned early on in the kernel boot process, as stated
    in `kernel/softirq.c`:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`ksoftirqd`是一个每个CPU的内核线程，用于处理未处理的软件中断。它在内核引导过程中早期生成，如`kernel/softirq.c`中所述：'
- en: '[PRE43]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: After running the `top` command, you will be able to see some `ksoftirqd/n`
    entries, where *n* is the logical CPU index of the CPU running the `ksoftirqd`
    thread. Since the `ksoftirqds` run in a process context, they are equal to classic
    processes/threads, and so are their competing claims for the CPU. `ksoftirqd`
    hogging CPUs for a long time may indicate a system under heavy load.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 运行`top`命令后，您将能够看到一些`ksoftirqd/n`条目，其中*n*是运行`ksoftirqd`线程的CPU的逻辑CPU索引。由于`ksoftirqds`在进程上下文中运行，它们等同于经典的进程/线程，因此它们对CPU的竞争要求也是一样的。`ksoftirqd`长时间占用CPU可能表明系统负载很重。
- en: Now that we have finished looking at our first work deferring mechanism in the
    Linux kernel, we’ll discuss tasklets, which are an alternative (from an atomic
    context point of view) to softIRQs, though the former are built using the latter.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了对Linux内核中第一个工作延迟机制的讨论，我们将讨论tasklets，它们是一种替代（从原子上下文的角度来看）softIRQs的机制，尽管前者是使用后者构建的。
- en: Tasklets
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务
- en: Tasklets are *bottom halves* built on top of the `HI_SOFTIRQ` and `TASKLET_SOFTIRQ`
    softIRQs, with the only difference being that `HI_SOFTIRQ`-based tasklets run
    prior to the `TASKLET_SOFTIRQ`-based ones. This simply means tasklets are softIRQs,
    so they follow the same rules. *Unlike softIRQs however, two of the same tasklets
    never run concurrently*. The tasklet API is quite basic and intuitive.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Tasklets是基于`HI_SOFTIRQ`和`TASKLET_SOFTIRQ` softIRQ构建的*底半部*，唯一的区别是基于`HI_SOFTIRQ`的tasklets在基于`TASKLET_SOFTIRQ`的tasklets之前运行。这意味着tasklets是softIRQs，因此它们遵循相同的规则。*但是，与softIRQs不同，两个相同的tasklets永远不会同时运行*。tasklet
    API非常基本和直观。
- en: 'Tasklets are represented by the `struct tasklet_struct` structure, which is
    defined in `<linux/interrupt.h>`. Each instance of this structure represents a
    unique tasklet:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是由`struct tasklet_struct`结构表示，该结构在`<linux/interrupt.h>`中定义。该结构的每个实例表示一个唯一的任务：
- en: '[PRE44]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The `func` member is the handler of the tasklet that will be executed by the
    underlying softIRQ. It is the equivalent of what `action` is to a softIRQ, with
    the same prototype and the same argument meaning. `data` will be passed as its
    sole argument.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`func`成员是tasklet的处理程序，将由底层softIRQ执行。它相当于softIRQ的`action`，具有相同的原型和相同的参数含义。`data`将作为其唯一参数传递。'
- en: 'You can use the `tasklet_init()` function to dynamically allocate and initialize
    tasklets at run-ime. For the static method, you can use the `DECLARE_TASKLET`
    macro. The option you choose will depend on your need (or requirement) to have
    a direct or indirect reference to the tasklet. Using `tasklet_init()` would require
    embedding the tasklet structure into a bigger and dynamically allocated object.
    An initialized tasklet can be scheduled by default – you could say it is enabled.
    `DECLARE_TASKLET_DISABLED` is an alternative to declaring default-disabled tasklets,
    and this will require the `tasklet_enable()` function to be invoked to make the
    tasklet schedulable. Tasklets are scheduled (similar to raising a softIRQ) via
    the `tasklet_schedule()` and `tasklet_hi_schedule()` functions. You can use `tasklet_disable()`
    to disable a tasklet. This function disables the tasklet and only returns when
    the tasklet has terminated its execution (assuming it was running). After this,
    the tasklet can still be scheduled, but it will not run on the CPU until it is
    enabled again. The asynchronous variant known as `tasklet_disable_nosync()` can
    be used too and returns immediately, even if termination has not occurred. Moreover,
    a tasklet that has been disabled several times should be enabled exactly the same
    number of times (this is allowed thanks to its `count` field):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`tasklet_init()`函数在运行时动态分配和初始化tasklet。对于静态方法，您可以使用`DECLARE_TASKLET`宏。您选择的选项将取决于您是否需要直接或间接引用tasklet。使用`tasklet_init()`需要将tasklet结构嵌入到一个更大的动态分配对象中。初始化的tasklet可以默认调度-你可以说它是启用的。`DECLARE_TASKLET_DISABLED`是声明默认禁用tasklet的替代方法，这将需要调用`tasklet_enable()`函数使tasklet可调度。tasklet通过`tasklet_schedule()`和`tasklet_hi_schedule()`函数进行调度（类似于触发softIRQ）。您可以使用`tasklet_disable()`来禁用tasklet。此函数禁用tasklet，并且只有在tasklet终止执行后才会返回（假设它正在运行）。之后，tasklet仍然可以被调度，但在再次启用之前，它不会在CPU上运行。也可以使用异步变体`tasklet_disable_nosync()`，即使终止尚未发生，也会立即返回。此外，已多次禁用的tasklet应该被启用相同次数（这要归功于它的`count`字段）：
- en: '[PRE45]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The kernel maintains normal priority and high priority tasklets in two different
    queues. Queues are actually singly linked lists, and each CPU has its own queue
    pair (low and high priority). Each processor has its own pair. `tasklet_schedule()`
    adds the tasklet to the normal priority list, thereby scheduling the associated
    softIRQ with a `TASKLET_SOFTIRQ` flag. With `tasklet_hi_schedule()`, the tasklet
    is added to the high priority list, thereby scheduling the associated softIRQ
    with a `HI_SOFTIRQ` flag. Once the tasklet has been scheduled, its `TASKLET_STATE_SCHED`
    flag is set, and the tasklet is added to a queue. At the time of execution, the
    `TASKLET_STATE_RUN` flag is set and the `TASKLET_STATE_SCHED` state is removed,
    thus allowing the tasklet to be rescheduled during its execution, either by the
    tasklet itself or from within an interrupt handler.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 内核在两个不同的队列中维护正常优先级和高优先级的tasklet。队列实际上是单链表，每个CPU都有自己的队列对（低优先级和高优先级）。每个处理器都有自己的对。`tasklet_schedule()`将tasklet添加到正常优先级列表中，从而使用`TASKLET_SOFTIRQ`标志调度相关的softIRQ。使用`tasklet_hi_schedule()`，tasklet将被添加到高优先级列表中，从而使用`HI_SOFTIRQ`标志调度相关的softIRQ。一旦tasklet被调度，它的`TASKLET_STATE_SCHED`标志就会被设置，并且tasklet会被添加到队列中。在执行时，`TASKLET_STATE_RUN`标志被设置，`TASKLET_STATE_SCHED`状态被移除，从而允许tasklet在执行期间被重新调度，无论是由tasklet本身还是在中断处理程序中。
- en: 'High-priority tasklets are meant to be used for soft interrupt handlers with
    low latency requirements. Calling `tasklet_schedule()` on a tasklet that’s already
    been scheduled, but whose execution has not started, will do nothing, resulting
    in the tasklet being executed only once. A tasklet can reschedule itself, which
    means you can safely call `tasklet_schedule()` in a tasklet. High-priority tasklets
    are always executed before normal ones and should be used carefully; otherwise,
    you may increase system latency.Stopping a tasklet is as simple as calling `tasklet_kill()`,
    which will prevent the tasklet from running again or waiting for it to complete
    before killing it if the tasklet is currently scheduled to run. If the tasklet
    reschedules itself, you should prevent the tasklet from rescheduling itself prior
    to calling this function:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 高优先级tasklet用于具有低延迟要求的软中断处理程序。对已经被调度但尚未开始执行的tasklet调用`tasklet_schedule()`将不会产生任何效果，导致tasklet只执行一次。tasklet可以重新调度自己，这意味着你可以在tasklet中安全地调用`tasklet_schedule()`。高优先级tasklet总是在正常tasklet之前执行，应该谨慎使用；否则，可能会增加系统延迟。停止tasklet就像调用`tasklet_kill()`一样简单，这将阻止tasklet再次运行，或者在杀死tasklet之前等待它完成，如果tasklet当前已被调度运行。如果tasklet重新调度自己，你应该在调用此函数之前阻止tasklet再次调度自己：
- en: '[PRE46]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'That being said, let’s take a look at the following example of tasklet code
    usage:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，让我们来看一下以下tasklet代码使用示例：
- en: '[PRE47]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In the preceding code, we statically declared our `my_tasklet` tasklet and the
    function that’s supposed to be invoked when this tasklet is scheduled, along with
    the data that will be given as an argument to this function.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们静态声明了我们的`my_tasklet` tasklet以及当调度此tasklet时应该调用的函数，以及将作为参数传递给此函数的数据。
- en: Important note
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Because the same tasklet never runs concurrently, the locking case between a
    tasklet and itself doesn’t need to be addressed. However, any data that’s shared
    between two tasklets should be protected with `spin_lock()` and `spin_unlock()`.
    Remember, tasklets are implemented on top of softIRQs.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 因为相同的tasklet永远不会同时运行，所以tasklet和自身之间的锁定情况不需要解决。然而，任何在两个tasklet之间共享的数据都应该用`spin_lock()`和`spin_unlock()`来保护。记住，tasklet是在softIRQ的基础上实现的。
- en: Workqueues
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作队列
- en: In the previous section, we dealt with tasklets, which are atomically deferred
    mechanisms. Apart from atomic mechanisms, there are cases where we may want to
    sleep in the deferred task. Workqueues allow this.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们处理了tasklet，它们是原子延迟机制。除了原子机制，还有一些情况下我们可能希望在延迟任务中进行睡眠。工作队列允许这样做。
- en: 'A workqueue is an asynchronous work deferring mechanism that is widely used
    across kernels, allowing them to run a dedicated function asynchronously in a
    process execution context. This makes them suitable for long-running and lengthy
    tasks or work that needs to sleep, thus improving the user experience. At the
    core of the workqueue subsystem, there are two data structures that can explain
    the concept behind this mechanism:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 工作队列是一种异步工作推迟机制，在内核中被广泛使用，允许它们在进程执行上下文中异步运行专用函数。这使它们适用于长时间运行和耗时的任务，或者需要休眠的工作，从而提高用户体验。在工作队列子系统的核心，有两个数据结构可以解释这种机制背后的概念：
- en: 'The work to be deferred (that is, the work item) is represented in the kernel
    by instances of `struct work_struct`, which indicates the handler function to
    be run. Typically, this structure is the first element of a user’s structure of
    the work definition. If you need a delay before the work can be run after it has
    been submitted to the workqueue, the kernel provides `struct delayed_work` instead.
    A work item is a basic structure that holds a pointer to the function that is
    to be executed asynchronously. To summarize, we can enumerate two types of work
    item structures:'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要延迟的工作（即工作项）在内核中由 `struct work_struct` 的实例表示，它指示要运行的处理函数。通常，这个结构是用户结构的工作定义的第一个元素。如果需要在提交给工作队列后延迟运行工作，内核提供了
    `struct delayed_work`。工作项是一个基本结构，它保存了要异步执行的函数的指针。总之，我们可以列举两种工作项结构：
- en: --The `struct work_struct` structure, which schedules a task to be run at a
    later time (as soon as possible when the system allows it).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: --`struct work_struct` 结构，安排一个任务在稍后的时间运行（尽快在系统允许的情况下）。
- en: --The `struct delayed_work` structure, which schedules a task to be run after
    at least a given time interval.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: --`struct delayed_work` 结构，安排一个任务在至少给定的时间间隔之后运行。
- en: The workqueue itself, which is represented by a `struct workqueue_struct`. This
    is the structure that work is placed on. It is a queue of work items.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作队列本身由 `struct workqueue_struct` 表示。这是工作被放置的结构。它是一个工作项队列。
- en: 'Apart from these data structures, there are two generic terms you should be
    familiar with:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些数据结构之外，还有两个通用术语你应该熟悉：
- en: '**Worker threads**, which are dedicated threads that execute the functions
    off the queue, one by one, one after the other.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工作线程**，这些是专用线程，按顺序执行队列中的函数。'
- en: '**Workerpools** are a collection of worker threads (a thread pool) that are
    used to manage worker threads.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Workerpools** 是一组用于管理工作线程的工作线程（线程池）。'
- en: 'The first step in using work queues consists of creating a work item, represented
    by `struct work_struct` or `struct delayed_work` for the delayed variant, that’s
    defined in `linux/workqueue.h`. The kernel provides either the `DECLARE_WORK`
    macro for statically declaring and initializing a work structure, or the `INIT_WORK`
    macro for doing the same by dynamically. If you need delayed work, you can use
    the `INIT_DELAYED_WORK` macro for dynamic allocation and initialization, or `DECLARE_DELAYED_WORK`
    for the static option:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 使用工作队列的第一步是创建一个工作项，由 `linux/workqueue.h` 中定义的 `struct work_struct` 或延迟变体的 `struct
    delayed_work` 表示。内核提供了 `DECLARE_WORK` 宏用于静态声明和初始化工作结构，或者 `INIT_WORK` 宏用于动态执行相同的操作。如果需要延迟工作，可以使用
    `INIT_DELAYED_WORK` 宏进行动态分配和初始化，或者使用 `DECLARE_DELAYED_WORK` 进行静态选项：
- en: '[PRE48]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The following code shows what our work item structure looks like:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了我们的工作项结构是什么样子的：
- en: '[PRE49]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The `func` field, which is of the `work_func_t` type, tells us a bit more about
    the header of a `work` function:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`func` 字段是 `work_func_t` 类型，告诉我们有关 `work` 函数头的一些信息：'
- en: '[PRE50]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '`work` is an input parameter that corresponds to the work structure associated
    with your work. If you’ve submitted a delayed work, this would correspond to the
    `delayed_work.work` field. Here, it will be necessary to use the `to_delayed_work()`
    function to get the underlying delayed work structure:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`work` 是一个输入参数，对应于与你的工作相关的工作结构。如果你提交了一个延迟的工作，这将对应于 `delayed_work.work` 字段。在这里，需要使用
    `to_delayed_work()` 函数来获取基础的延迟工作结构：'
- en: '[PRE51]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Workqueues let your driver create a kernel thread, called a worker thread,
    to handle deferred work. A new workqueue can be created with these functions:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 工作队列允许你的驱动程序创建一个内核线程，称为工作线程，来处理延迟的工作。可以使用以下函数创建一个新的工作队列：
- en: '[PRE52]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '`create_workqueue()` creates a dedicated thread (a worker) per CPU on the system,
    which is probably not a good idea. On an 8-core system, this will result in 8
    kernel threads being created to run work that’s been submitted to your workqueue.
    In most cases, a single system-wide kernel thread should be enough. In this case,
    you should use `create_singlethread_workqueue()` instead, which creates, as its
    name suggests, a single threaded workqueue; that is, with one worker thread system-wide.
    Either normal or delayed work can be enqueued on the same queue. To schedule works
    on your created workqueue, you can use either `queue_work()` or `queue_delayed_work()`,
    depending on the nature of the work:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`create_workqueue()` 在系统的每个 CPU 上创建一个专用线程（工作线程），这可能不是一个好主意。在一个 8 核系统上，这将导致创建
    8 个内核线程来运行提交到你的工作队列的工作。在大多数情况下，一个全局的内核线程应该足够了。在这种情况下，你应该使用 `create_singlethread_workqueue()`，它创建一个单线程工作队列；也就是说，在整个系统中只有一个工作线程。可以在同一个队列上排定普通或延迟工作。要在创建的工作队列上安排工作，可以使用
    `queue_work()` 或 `queue_delayed_work()`，具体取决于工作的性质：'
- en: '[PRE53]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'These functions return false if the work was already on a queue and true otherwise.
    `queue_dalayed_work()` can be used to plan (delayed) work for execution with a
    given delay. The time unit for the delay is jiffies. If you don’t want to bother
    with seconds-to-jiffies conversion, you can use the `msecs_to_jiffies()` and `usecs_to_jiffies()`
    helpers, which convert milliseconds or microseconds into jiffies, respectively:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数如果工作已经在队列中则返回false，否则返回true。`queue_dalayed_work（）`可用于计划（延迟）工作以在给定延迟后执行。延迟的时间单位是jiffies。如果您不想麻烦秒到jiffies的转换，可以使用`msecs_to_jiffies（）`和`usecs_to_jiffies（）`辅助函数，分别将毫秒或微秒转换为jiffies：
- en: '[PRE54]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The following example uses 200 ms as a delay:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例使用200毫秒作为延迟：
- en: '[PRE55]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Submitted work items can be canceled by calling either `cancel_delayed_work()`,
    `cancel_delayed_work_sync()`, or `cancel_work_sync()`:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 提交的工作项可以通过调用`cancel_delayed_work（）`、`cancel_delayed_work_sync（）`或`cancel_work_sync（）`来取消：
- en: '[PRE56]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The following describes what these functions do:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 以下描述了这些函数的作用：
- en: '`cancel_work_sync()` synchronously cancels the given workqueue entry. In other
    words, it cancels `work` and waits for its execution to finish. The kernel guarantees
    that work won’t be pending or executing on any CPU when it’s return from this
    function, even if the work migrates to another workqueue or requeues itself. It
    returns `true` if `work` was pending, or `false` otherwise.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cancel_work_sync（）`同步取消给定的工作队列条目。换句话说，它取消`work`并等待其执行完成。内核保证当从此函数返回时，工作不会挂起或在任何CPU上执行，即使工作迁移到另一个工作队列或重新排队。如果`work`挂起，则返回`true`，否则返回`false`。'
- en: '`cancel_delayed_work()` asynchronously cancels a pending workqueue entry (a
    delayed one). It returns `true` (a non-zero value) if `dwork` was pending and
    canceled and `false` if it wasn’t pending, probably because it is actually running,
    and thus might still be running after `cancel_delayed_work()`. To ensure the work
    really ran to its end, you may want to use `flush_workqueue()`, which flushes
    every work item in the given queue, or `cancel_delayed_work_sync()`, which is
    the synchronous version of `cancel_delayed_work()`.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cancel_delayed_work（）`异步取消挂起的工作队列条目（延迟的）。如果`dwork`挂起并取消，则返回`true`（非零值），如果没有挂起，则返回`false`，可能是因为它实际上正在运行，因此在`cancel_delayed_work（）`之后可能仍在运行。为了确保工作确实运行到结束，您可能希望使用`flush_workqueue（）`，它会刷新给定队列中的每个工作项，或者使用`cancel_delayed_work_sync（）`，它是`cancel_delayed_work（）`的同步版本。'
- en: 'To wait for all the work items to finish, you can call `flush_workqueue()`.
    When you are done with a workqueue, you should destroy it with `destroy_workqueue()`.
    Both these options can be seen in the following code:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 要等待所有工作项完成，可以调用`flush_workqueue（）`。完成工作队列后，应使用`destroy_workqueue（）`销毁它。这两个选项可以在以下代码中看到：
- en: '[PRE57]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: While you’re waiting for any pending work to execute, the `_sync` variant functions
    sleep, which means they can only be called from a process context.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 当您等待任何挂起的工作执行时，`_sync`变体函数会休眠，这意味着它们只能从进程上下文中调用。
- en: The kernel shared queue
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内核共享队列
- en: In most situations, your code does not necessarily need to have the performance
    of its own dedicated set of threads, and because `create_workqueue()` creates
    one worker thread for each CPU, it may be a bad idea to use it on very large multi-CPU
    systems. In this situation, you may want to use the kernel shared queue, which
    has its own set of kernel threads preallocated (early during boot, via the `workqueue_init_early()`
    function) for running works.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，您的代码不一定需要拥有自己专用的线程集的性能，因为`create_workqueue（）`为每个CPU创建一个工作线程，所以在非常大的多CPU系统上使用它可能是一个坏主意。在这种情况下，您可能希望使用内核共享队列，它具有预先分配的一组内核线程（在引导期间通过`workqueue_init_early（）`函数提前分配）来运行工作。
- en: 'This global kernel workqueue is the so-called `system_wq`, and is defined in
    `kernel/workqueue.c`. There is one instance per CPU, with each backed by a dedicated
    thread named `events/n`, where `n` is the processor number that the thread is
    bound to. You can queue work to the system’s default workqueue using one of the
    following functions:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这个全局内核工作队列被称为`system_wq`，在`kernel/workqueue.c`中定义。每个CPU都有一个实例，每个实例都由一个名为`events/n`的专用线程支持，其中`n`是线程绑定的处理器编号。您可以使用以下函数之一将工作排队到系统的默认工作队列：
- en: '[PRE58]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '`schedule_work()` immediately schedules the work that will be executed as soon
    as possible after the worker thread on the current processor wakes up. With `schedule_delayed_work()`,
    the work will be put in the queue in the future, after the delay timer has ticked.
    The `_on` variants are used to schedule the work on a specific CPU (this does
    not need to be the current one). Each of these function queues work on the system’s
    shared workqueue, `system_wq`, which is defined in `kernel/workqueue.c`:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '`schedule_work（）`立即安排工作，该工作将在当前处理器上的工作线程唤醒后尽快执行。使用`schedule_delayed_work（）`，工作将在延迟计时器滴答后的未来放入队列中。`_on`变体用于在特定CPU上安排工作（这不需要是当前CPU）。这些函数中的每一个都在系统的共享工作队列`system_wq`上排队工作，该队列在`kernel/workqueue.c`中定义：'
- en: '[PRE59]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'To flush the kernel-global workqueue – that is, to ensure the given batch of
    work is completed – you can use `flush_scheduled_work()`:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 要刷新内核全局工作队列-也就是确保给定的工作批次已完成-可以使用`flush_scheduled_work（）`：
- en: '[PRE60]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '`flush_scheduled_work()` is a wrapper that calls `flush_workqueue()` on `system_wq`.
    Note that there may be work in `system_wq` that you have not submitted and have
    no control over. Due to this, flushing this workqueue entirely is overkill. It
    is recommended to use `cancel_delayed_work_sync()` or `cancel_work_sync()` instead.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`flush_scheduled_work（）`是一个包装器，它在`system_wq`上调用`flush_workqueue（）`。请注意，`system_wq`中可能有您尚未提交且无法控制的工作。因此，完全刷新此工作队列是过度的。建议改用`cancel_delayed_work_sync（）`或`cancel_work_sync（）`。'
- en: Tip
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Unless you have a strong reason to create a dedicated thread, the default (kernel-global)
    thread is preferred.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 除非您有充分的理由创建专用线程，否则首选默认（内核全局）线程。
- en: Workqueues – a new generation
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作队列-新一代
- en: 'The original (now legacy) workqueue implementation used two kinds of workqueues:
    those with a **single thread system-wide**, and those with a **thread per-CPU**.
    However, due to the increasing number of CPUs, this led to some limitations:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 原始（现在是传统的）工作队列实现使用了两种工作队列：一种是**系统范围内单个线程**，另一种是**每个CPU一个线程**。然而，由于CPU数量的增加，这导致了一些限制：
- en: On very large systems, the kernel could run out of process IDs (defaulted to
    32k) just at boot, before the init was started.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在非常大的系统上，内核可能在启动时耗尽进程ID（默认为32k），甚至在init启动之前。
- en: Multi-threaded workqueues provided poor concurrency management as their threads
    competed for the CPU with other threads on the system. Since there were more CPU
    contenders, this introduced some overhead; that is, more context switches than
    necessary.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多线程工作队列提供了较差的并发管理，因为它们的线程与系统上的其他线程竞争CPU。由于有更多的CPU竞争者，这引入了一些开销；即比必要的更多的上下文切换。
- en: The consumption of much more resources than what was really needed.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消耗了比实际需要的更多资源。
- en: Moreover, subsystems that needed a dynamic or fine-grained level of concurrency
    had to implement their own thread pools. As a result of this, a new workqueue
    API has been designed and the legacy workqueue API (`create_workqueue()`, `create_singlethread_workqueue()`,
    and `create_freezable_workqueue()`) has been scheduled to be removed. However,
    these are actually wrappers around the new ones – the so-called concurrency-managed
    workqueues. This is done using per-CPU worker pools that are shared by all the
    workqueues in order to automatically provide a dynamic and flexible level of concurrency,
    thus abstracting such details for API users.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，需要动态或细粒度并发级别的子系统必须实现自己的线程池。因此，设计了一个新的工作队列API，并计划删除传统的工作队列API（`create_workqueue()`、`create_singlethread_workqueue()`和`create_freezable_workqueue()`）。然而，这些实际上是新API的包装器，即所谓的并发管理的工作队列。这是通过每个CPU的工作线程池来实现的，所有工作队列共享这些线程池，以自动提供动态和灵活的并发级别，从而为API用户抽象出这些细节。
- en: Concurrency-managed workqueues
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 并发管理的工作队列
- en: 'The concurrency-managed workqueue is an upgrade of the workqueue API. Using
    this new API implies that you must choose between two macros to create the workqueue:
    `alloc_workqueue()` and `alloc_ordered_workqueue()`. These macros both allocate
    a workqueue and return a pointer to it on success, and NULL on failure. The returned
    workqueue can be freed using the `destroy_workqueue()` function:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 并发管理的工作队列是工作队列API的升级。使用这个新API意味着你必须在`alloc_workqueue()`和`alloc_ordered_workqueue()`之间选择一个宏来创建工作队列。这些宏在成功时都分配一个工作队列并返回指针，失败时返回NULL。返回的工作队列可以使用`destroy_workqueue()`函数释放。
- en: '[PRE61]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '`fmt` is the `printf` format for the name of the workqueue, while `args...`
    are arguments for `fmt`. `destroy_workqueue()` is to be called on the workqueue
    once you are done with it. All work that’s currently pending will be completed
    first, before the kernel destroys the workqueue. `alloc_workqueue()` creates a
    workqueue based on `max_active`, which defines the concurrency level by limiting
    the number of work (tasks) that can be executing (workers in a runnable sate)
    simultaneously from this workqueue on any given CPU. For example, a `max_active`
    of 5 would mean that, at most, five work items on this workqueue can be executing
    at the same time per CPU. On the other hand, `alloc_ordered_workqueue()` creates
    a workqueue that processes each work item one by one in the queued order (that
    is, FIFO order).'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`fmt`是工作队列名称的`printf`格式，而`args...`是`fmt`的参数。`destroy_workqueue()`在完成工作队列后调用。内核在销毁工作队列之前会先完成所有当前挂起的工作。`alloc_workqueue()`基于`max_active`创建一个工作队列，通过限制在任何给定CPU上同时执行（处于可运行状态的工作线程）的工作（任务）数量来定义并发级别。例如，`max_active`为5意味着在同一时间内每个CPU上最多可以执行五个工作队列的工作项。另一方面，`alloc_ordered_workqueue()`创建一个按队列顺序依次处理每个工作项的工作队列（即FIFO顺序）。'
- en: '`flags` controls how and when work items are queued, assigned execution resources,
    scheduled, and executed. Various flags are used in this new API. Let’s take a
    look at some of them:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '`flags`控制工作项如何排队、分配执行资源、调度和执行。在这个新API中使用了各种标志。让我们来看看其中一些：'
- en: '`WQ_UNBOUND`: Legacy workqueues had a worker thread per CPU and were designed
    to run tasks on the CPU where they were submitted. The kernel scheduler had no
    choice but to always schedule a worker on the CPU that it was defined on. With
    this approach, even a single workqueue could prevent a CPU from idling and being
    turned off, which leads to increased power consumption or poor scheduling policies.
    `WQ_UNBOUND` turns off this behavior. Work is not bound to a CPU anymore, hence
    the name unbound workqueues. There is no more locality, and the scheduler can
    reschedule the worker on any CPU as it sees fit. The scheduler has the last word
    now and can balance CPU load, especially for long and sometimes CPU-intensive
    work.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WQ_UNBOUND`：传统的工作队列每个CPU都有一个工作线程，并且设计为在提交任务的CPU上运行任务。内核调度程序只能在定义的CPU上调度工作线程。采用这种方法，即使是一个工作队列也可能阻止CPU空闲并关闭，从而导致增加功耗或者调度策略不佳。`WQ_UNBOUND`关闭了这种行为。工作不再绑定到CPU，因此被称为无绑定工作队列。不再有局部性，调度程序可以根据需要在任何CPU上重新调度工作线程。调度程序现在有最后的决定权，可以平衡CPU负载，特别是对于长时间且有时CPU密集的工作。'
- en: '`WQ_MEM_RECLAIM`: This flag is to be set for workqueues that need to guarantee
    forward progress during a memory reclaim path (when free memory is running dangerously
    low; here, the system is under memory pressure. In this case, `GFP_KERNEL` allocations
    may block and deadlock the entire workqueue). The workqueue is then guaranteed
    to have a ready-to-use worker thread, a so-called rescuer thread reserved for
    it, regardless of memory pressure, so that it can progress. One rescuer thread
    is allocated for each workqueue that has this flag set.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WQ_MEM_RECLAIM`：设置此标志用于需要在内存回收路径中保证前进进度的工作队列（当空闲内存严重不足时；在这种情况下，系统处于内存压力下。在这种情况下，`GFP_KERNEL`分配可能会阻塞并死锁整个工作队列）。然后，工作队列将保证有一个可用的工作线程，一个所谓的拯救者线程为其保留，无论内存压力如何，以便它可以继续前进。为每个设置了此标志的工作队列分配一个拯救者线程。'
- en: 'Let’s consider a situation where we have three work items (*w1*, *w2*, and
    *w3*) in our workqueue, *W*. *w1* does some work and then waits for *w3* to complete
    (let’s say it depends on the computation result of *w3*). Afterward, *w2* (which
    is independent of the others) does some `kmalloc()` allocation (`GFP_KERNEL`).
    Now, it seems like there is not enough memory. While *w2* is blocked, it still
    occupies the workqueue of *W*. This results in *w3* not being able to run, despite
    the fact that there is no dependency between *w2* and *w3*. Since there is not
    enough memory available, there is no way to allocate a new thread to run *w3*.
    A pre-allocated thread would definitely solve this problem, not by magically allocating
    the memory for *w2*, but by running *w3* so that *w1* can continue its job, and
    so on. *w2* will continue its progression as soon as possible, when there is enough
    available memory to allocate. This pre-allocated thread is the so-called rescuer
    thread. You must set this `WQ_MEM_RECLAIM` flag if you think the workqueue might
    be used in the memory reclaim path. This flag replaces the old `WQ_RESCUER` flag
    as of the following commit: [https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=493008a8e475771a2126e0ce95a73e35b371d277](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=493008a8e475771a2126e0ce95a73e35b371d277).'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑这样一种情况：我们的工作队列*W*中有三个工作项（*w1*、*w2*和*w3*）。*w1*完成一些工作，然后等待*w3*完成（假设它依赖于*w3*的计算结果）。之后，*w2*（与其他工作项无关）执行一些`kmalloc()`分配（`GFP_KERNEL`）。现在，似乎没有足够的内存。虽然*w2*被阻塞，但它仍然占据了*W*的工作队列。这导致*w3*无法运行，尽管*w2*和*w3*之间没有依赖关系。由于没有足够的内存可用，无法分配新线程来运行*w3*。预先分配的线程肯定会解决这个问题，不是通过为*w2*魔法般地分配内存，而是通过运行*w3*，以便*w1*可以继续其工作，依此类推。当有足够的可用内存时，*w2*将尽快继续其进展。这个预先分配的线程就是所谓的拯救者线程。如果您认为工作队列可能在内存回收路径中使用，则必须设置`WQ_MEM_RECLAIM`标志。此标志取代了旧的`WQ_RESCUER`标志，如以下提交所示：[https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=493008a8e475771a2126e0ce95a73e35b371d277](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=493008a8e475771a2126e0ce95a73e35b371d277)。
- en: '`WQ_FREEZABLE`: This flag is used for power management purposes. A workqueue
    with this flag set will be frozen when the system is suspended or hibernates.
    On the freezing path, all current work(s) of the worker(s) will be processed.
    When the freeze is complete, no new work items will be executed until the system
    is unfrozen. Filesystem-related workqueue(s) may use this flag to ensure that
    modifications that are made to files are pushed to disk or create the hibernation
    image on the freezing path and that no modifications are made on-disk after the
    hibernation image has been created. In this situation, non-freezable items may
    do things differently that could lead to filesystem corruption. As an example,
    all of the XFS internal workqueues have this flag set (see `fs/xfs/xfs_super.c`)
    to ensure no further changes are made on disk once the freezer infrastructure
    freezes the kernel threads and creates the hibernation image. You should not set
    this flag if your workqueue can run tasks as part of the hibernation/suspend/resume
    process of the system. More information on this topic can be found in `Documentation/power/freezing-of-tasks.txt`,
    as well as by taking a look at the kernel’s internal `freeze_workqueues_begin()`
    and `thaw_workqueues()` functions.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WQ_FREEZABLE`：此标志用于电源管理目的。设置了此标志的工作队列将在系统挂起或休眠时被冻结。在冻结路径上，将处理工作队列中所有当前工作的工作者。冻结完成后，直到系统解冻，将不会执行新的工作项。文件系统相关的工作队列可能使用此标志来确保对文件的修改被推送到磁盘或在冻结路径上创建休眠镜像，并且在创建休眠镜像后不对磁盘进行修改。在这种情况下，非可冻结项目可能会以不同的方式执行操作，这可能导致文件系统损坏。例如，XFS的所有内部工作队列都设置了此标志（请参阅`fs/xfs/xfs_super.c`），以确保在冻结器基础结构冻结内核线程并创建休眠镜像后不会对磁盘进行进一步更改。如果您的工作队列可以作为系统的休眠/挂起/恢复过程的一部分运行任务，则不应设置此标志。有关此主题的更多信息，请参阅`Documentation/power/freezing-of-tasks.txt`，以及查看内核的内部`freeze_workqueues_begin()`和`thaw_workqueues()`函数。'
- en: '`WQ_HIGHPRI`: Tasks that have this flag set run immediately and do not wait
    for the CPU to become available. This flag is used for workqueues that queue work
    items that require high priority for execution. Such workqueues have worker threads
    with a high priority level (a lower `nice` value).'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WQ_HIGHPRI`：设置了此标志的任务将立即运行，并且不会等待CPU可用。此标志用于排队需要高优先级执行的工作项的工作队列。这样的工作队列具有具有较高优先级级别的工作者线程（较低的`nice`值）。'
- en: In the early days of the CMWQ, high-priority work items were just queued at
    the head of a global normal priority worklist so that they could immediately run.
    Nowadays, there is no interaction between normal priority and high-priority workqueues
    as each has its own worklist and its own worker pool. The work items of a high-priority
    workqueue are queued to the high-priority worker pool of the target CPU. Tasks
    in this workqueue should not block much. Use this flag if you do not want your
    work item competing for CPU with normal or lower-priority tasks. Crypto and Block
    subsystems use this, for example.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在CMWQ的早期，高优先级的工作项只是排队在全局正常优先级工作列表的开头，以便它们可以立即运行。如今，正常优先级和高优先级工作队列之间没有交互，因为每个工作队列都有自己的工作列表和自己的工作池。高优先级工作队列的工作项被排队到目标CPU的高优先级工作池。这个工作队列中的任务不应该阻塞太多。例如，加密和块子系统使用这个标志。
- en: '`WQ_CPU_INTENSIVE`: Work items that are part of a CPU-intensive workqueue may
    burn a lot of CPU cycles and will not participate in the workqueue’s concurrency
    management. Instead, their execution is regulated by the system scheduler, just
    like any other task. This makes this flag useful for bound work items that may
    hog CPU cycles. Though their execution is regulated by the system scheduler, the
    start of their execution is still regulated by concurrency management, and runnable
    non-CPU-intensive work items can delay the execution of CPU-intensive work items.
    Actually, the crypto and dm-crypt subsystems use such workqueues. To prevent such
    tasks from delaying the execution of other non-CPU-intensive work items, they
    will not be taken into account when the workqueue code determines whether the
    CPU is available.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WQ_CPU_INTENSIVE`：作为CPU密集型工作队列的一部分的工作项可能会消耗大量CPU周期，并且不会参与工作队列的并发管理。相反，它们的执行由系统调度程序调节，就像任何其他任务一样。这使得这个标志对可能占用CPU周期的绑定工作项非常有用。尽管它们的执行由系统调度程序调节，但它们的执行开始仍然由并发管理调节，并且可运行的非CPU密集型工作项可能会延迟CPU密集型工作项的执行。实际上，加密和dm-crypt子系统使用这样的工作队列。为了防止这样的任务延迟执行其他非CPU密集型工作项，当工作队列代码确定CPU是否可用时，它们将不被考虑在内。'
- en: 'In order to be compliant with the old workqueue API, the following mappings
    are made to keep this API compatible with the original one:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 为了符合旧的工作队列API，进行以下映射以保持此API与原始API兼容：
- en: '`create_workqueue(name)` is mapped to `alloc_workqueue(name,WQ_MEM_RECLAIM,
    1)`.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create_workqueue(name)`映射到`alloc_workqueue(name,WQ_MEM_RECLAIM, 1)`。'
- en: '`create_singlethread_workqueue(name)` is mapped to `alloc_ordered_workqueue(name,
    WQ_MEM_RECLAIM)`.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create_singlethread_workqueue(name)`映射到`alloc_ordered_workqueue(name, WQ_MEM_RECLAIM)`。'
- en: '`create_freezable_workqueue(name)` is mapped to `alloc_workqueue(name,WQ_FREEZABLE
    | WQ_UNBOUND|WQ_MEM_RECLAIM, 1)`.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`create_freezable_workqueue(name)`映射到`alloc_workqueue(name,WQ_FREEZABLE | WQ_UNBOUND|WQ_MEM_RECLAIM,
    1)`。'
- en: 'To summarize, `alloc_ordered_workqueue()` actually replaces `create_freezable_workqueue()`
    and `create_singlethread_workqueue()` (as per the following commit: [https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=81dcaf6516d8](https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=81dcaf6516d8)).
    Workqueues allocated with `alloc_ordered_workqueue()` are unbound and have `max_active`
    set to `1`.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，`alloc_ordered_workqueue()`实际上取代了`create_freezable_workqueue()`和`create_singlethread_workqueue()`（根据以下提交：[https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=81dcaf6516d8](https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=81dcaf6516d8)）。使用`alloc_ordered_workqueue()`分配的工作队列是无绑定的，并且`max_active`设置为`1`。
- en: When it comes to scheduled items in a workqueue, the work items that have been
    queued to a specific CPU using `queue_work_on()` will execute on that CPU. Work
    items that have been queued via `queue_work()` will prefer the queueing CPU, though
    this locality is not guaranteed.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在工作队列中的计划项方面，使用`queue_work_on()`将被排队到特定CPU的工作项将在该CPU上执行。通过`queue_work()`排队的工作项将优先使用排队CPU，尽管这种局部性不能保证。
- en: Important Note
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Note that `schedule_work()` is a wrapper that calls `queue_work()` on the system
    workqueue (`system_wq`), while `schedule_work_on()` is a wrapper around `queue_work_on()`.
    Also, keep in mind that `system_wq = alloc_workqueue(“events”, 0, 0);`. Take a
    look at the `workqueue_init_early()` function in `kernel/workqueue.c` in the kernel
    sources to see how other system-wide workqueues are created.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`schedule_work()`是一个包装器，它在系统工作队列（`system_wq`）上调用`queue_work()`，而`schedule_work_on()`是`queue_work_on()`的包装器。另外，请记住`system_wq
    = alloc_workqueue(“events”, 0, 0);`。查看内核源代码中的`kernel/workqueue.c`中的`workqueue_init_early()`函数，以了解如何创建其他系统范围的工作队列。
- en: Memory reclaim is a Linux kernel mechanism on the memory allocation path. This
    consists of allocating memory after throwing the current content of that memory
    somewhere else.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 内存回收是Linux内核在内存分配路径上的一种机制。这包括在将当前内存内容抛弃到其他地方后分配内存。
- en: With that, we have finished looking at workqueues and the concurrency-managed
    ones in particular. Next, we’ll introduce Linux kernel interrupt management, which
    is where most of the previous mechanisms will be solicited.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经完成了对工作队列和特别是并发管理的工作队列的研究。接下来，我们将介绍Linux内核中断管理，这是以前大部分机制将被征求意见的地方。
- en: Linux kernel interrupt management
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Linux内核中断管理
- en: Apart from servicing processes and user requests, another job of the Linux kernel
    is managing and speaking with hardware. This is either from the CPU to the device
    or from the device to the CPU. This is achieved by means of interrupts. An interrupt
    is a signal that’s sent to the processor by an external hardware device requesting
    immediate attention. Prior to an interrupt being visible to the CPU, this interrupt
    should be enabled by the interrupt controller, which is a device on its own, and
    whose main job consists of routing interrupts to CPUs.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 除了为进程和用户请求提供服务之外，Linux内核的另一个工作是管理和与硬件通信。这要么是从CPU到设备，要么是从设备到CPU。这是通过中断实现的。中断是由外部硬件设备发送给处理器的请求立即处理的信号。在中断对CPU可见之前，这个中断应该由中断控制器启用，中断控制器是一个独立的设备，其主要工作是将中断路由到CPU。
- en: 'An interrupt may have five states:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 中断可能有五种状态：
- en: '**Active**: An interrupt that has been acknowledged by a **processing element**
    (**PE**) and is being handled. While being handled, another assertion of the same
    interrupt is not presented as an interrupt to a processing element, until the
    initial interrupt is no longer active.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**活动**：已被处理器核心（PE）确认并正在处理的中断。在处理过程中，同一中断的另一个断言不会作为中断呈现给处理器核心，直到初始中断不再活动为止。'
- en: '**Pending (asserted)**: An interrupt that is recognized as asserted in hardware,
    or generated by software, and is waiting to be handled by the target PE. It is
    a common behavior for most hardware devices not to generate other interrupts until
    their “interrupt pending” bit has been cleared. A disabled interrupt can’t be
    pending as it is never asserted, and it is immediately dropped by the interrupt
    controller.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**挂起（已断言）**：在硬件中被识别为已断言的中断，或者由软件生成，并且正在等待目标PE处理。对于大多数硬件设备来说，直到其“中断挂起”位被清除之前，它们不会生成其他中断。已禁用的中断不能处于挂起状态，因为它从未被断言，并且会立即被中断控制器丢弃。'
- en: '**Active and pending**: An interrupt that is active from one assertion of the
    interrupt and is pending from a subsequent assertion.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**活动且挂起**：从中断的一个断言处于活动状态，并从随后的断言处于挂起状态。'
- en: '**Inactive**: An interrupt that is not active or pending. Deactivation clears
    the active state of the interrupt, and thereby allows the interrupt, when it is
    pending, to be taken again.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非活动**：不活动或未挂起的中断。停用会清除中断的活动状态，从而允许挂起时再次被接收。'
- en: '**Disabled/Deactivated**: This is unknown to the CPU and not even seen by the
    interrupt controller. This will never be asserted. Disabled interrupts are lost.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**已禁用/已停用**：对CPU来说是未知的，甚至中断控制器也看不到。这将永远不会被断言。已禁用的中断会丢失。'
- en: Important note
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: There are interrupt controllers where disabling an interrupt means masking that
    interrupt, or vice versa. In the remainder of this book, we will consider disabling
    to be the same as masking, though this is not always true.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 有中断控制器，禁用中断意味着屏蔽该中断，反之亦然。在本书的其余部分中，我们将考虑禁用与屏蔽相同，尽管这并不总是正确的。
- en: Upon reset, the processor disables all the interrupts until they are enabled
    again by the initialization code (this is the job of the Linux kernel in our case).
    The interrupts are enabled/disabled by setting/clearing the bits in the processor
    status/control registers. Upon an interrupt assertion (an interrupt occurred),
    the processor will check whether the interrupts are masked or not and will do
    nothing if they are masked. Once unmasked, the processor will pick one pending
    interrupt, if any (the order does not matter since it will do this for each pending
    interrupt until they are all serviced), and will execute a specially purposed
    function called the `kernel irq` core code) at a special location called the vector
    table. Right before the processor starts executing this ISR, it does some context
    saving (including the unmasked status of interrupts) and then masks the interrupts
    on the local CPU (interrupts can be asserted and will be serviced once unmasked).
    Once the ISR is running, we can say that the interrupt is being serviced.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 复位时，处理器会禁用所有中断，直到它们再次由初始化代码启用（在我们的情况下是Linux内核的工作）。通过设置/清除处理器状态/控制寄存器中的位来启用/禁用中断。在中断断言（发生中断）时，处理器将检查中断是否被屏蔽，如果被屏蔽则不会执行任何操作。一旦取消屏蔽，处理器将选择一个挂起的中断（如果有的话，顺序无关紧要，因为它将为每个挂起的中断执行此操作，直到它们全部被服务），并将在称为向量表的特殊位置执行一个特殊用途的函数（内核中断核心代码）。在处理器开始执行此ISR之前，它会进行一些上下文保存（包括中断的未屏蔽状态），然后在本地CPU上屏蔽中断（中断可以被断言，并且一旦取消屏蔽就会被服务）。一旦ISR运行，我们可以说中断正在被服务。
- en: 'The following is the complete IRQ handling flow on ARM Linux. This happens
    when an interrupt occurs and the interrupts are enabled in the PSR:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是ARM Linux上完整的IRQ处理流程。当中断发生并且PSR中启用了中断时，以下情况会发生：
- en: The ARM core will disable further interrupts occurring on the local CPU.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ARM核心将禁用在本地CPU上发生的进一步中断。
- en: The ARM core will then put the **Current Program Status Register** (**CPSR**)
    in the **Saved Program Status Register** (**SPSR**), put the current **Program
    Counter** (**PC**) in the **Link Register** (**LR**), and then switch to IRQ mode.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，ARM核心将把当前程序状态寄存器（CPSR）放入保存的程序状态寄存器（SPSR），将当前程序计数器（PC）放入链接寄存器（LR），然后切换到IRQ模式。
- en: Finally, the ARM processor will refer to the vector table and jumps to the exception
    handler. In our case, it jumps to the exception handler of IRQ, which in the Linux
    kernel corresponds to the `vector_stub` macro defined in `arch/arm/kernel/entry-armv.S`.
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，ARM处理器将引用向量表并跳转到异常处理程序。在我们的情况下，它跳转到IRQ的异常处理程序，在Linux内核中对应于`arch/arm/kernel/entry-armv.S`中定义的`vector_stub`宏。
- en: 'These three steps are done by the ARM processor itself. Now, the kernel jumps
    into action:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个步骤由ARM处理器本身完成。现在，内核开始行动：
- en: The `vector_stub` macro checks from what processor mode we used to get here
    – either kernel mode or user mode – and determines the macro to call accordingly;
    either `__irq_user` or `__irq_svc`.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`vector_stub`宏检查我们用什么处理器模式到达这里 - 内核模式或用户模式 - 并相应地确定要调用的宏；要么是`__irq_user`，要么是`__irq_svc`。'
- en: '`__irq_svc()` will save the registers (from `r0` to `r12`) on the kernel stack
    and then call the `irq_handler()` macro, which either calls `handle_arch_irq()`
    (present in `arch/arm/include/asm/entry-macro-multi.S`) if `CONFIG_MULTI_IRQ_HANDLER`
    is defined, or `arch_irq_handler_default()` otherwise, with `handle_arch_irq`
    being a global pointer to the function that’s set in `arch/arm/kernel/setup.c`
    (from within the `setup_arch()` function).'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`__irq_svc()`将在内核堆栈上保存寄存器（从`r0`到`r12`），然后调用`irq_handler()`宏，如果定义了`CONFIG_MULTI_IRQ_HANDLER`，则调用`handle_arch_irq()`（存在于`arch/arm/include/asm/entry-macro-multi.S`中），否则调用`arch_irq_handler_default()`，其中`handle_arch_irq`是在`arch/arm/kernel/setup.c`中设置的指向函数的全局指针（在`setup_arch()`函数内）。'
- en: Now, we need to identify the hardware-IRQ number, which is what `asm_do_IRQ()`
    does. It then calls `handle_IRQ()` on that hardware-IRQ, which in turn calls `__handle_domain_irq()`,
    which will translate the hardware-irq into its corresponding Linux IRQ number
    (`irq = irq_find_mapping(domain, hwirq)`) and call `generic_handle_irq()` on the
    decoded Linux IRQ (`generic_handle_irq(irq)`).
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要确定硬件IRQ编号，这就是`asm_do_IRQ()`所做的。然后它调用该硬件IRQ上的`handle_IRQ()`，然后调用`__handle_domain_irq()`，它将硬件irq转换为相应的Linux
    IRQ编号（`irq = irq_find_mapping(domain, hwirq)`）并在解码的Linux IRQ上调用`generic_handle_irq()`（`generic_handle_irq(irq)`）。
- en: '`generic_handle_irq()` will look for the IRQ descriptor structure (Linux’s
    view of an interrupt) that corresponds to the decoded Linux IRQ (`struct irq_desc
    *desc = irq_to_desc(irq)`) and calling `generic_handle_irq_desc()` on this descriptor),
    which will result in `desc->handle_irq(desc)`. `desc->handle_irq` corresponding
    to the high-level IRQ handler that was set using `irq_set_chip_and_handler()`
    during the mapping of this IRQ.'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`generic_handle_irq()`将寻找与解码的Linux IRQ对应的IRQ描述符结构（Linux对中断的视图）（`struct irq_desc
    *desc = irq_to_desc(irq)`）并在此描述符上调用`generic_handle_irq_desc()`，这将导致`desc->handle_irq(desc)`。`desc->handle_irq`对应于在此IRQ映射期间使用`irq_set_chip_and_handler()`设置的高级IRQ处理程序。'
- en: '`desc->handle_irq()` may result in a call to `handle_level_irq()`, `handle_simple_irq()`,
    `handle_edge_irq()`, and so on.'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`desc->handle_irq()`可能会导致调用`handle_level_irq()`、`handle_simple_irq()`、`handle_edge_irq()`等。'
- en: The high-level IRQ handler calls our ISR.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高级IRQ处理程序调用我们的ISR。
- en: Once the ISR has been completed, `irq_svc` will return and restore the processor
    state by restoring registers (r0-r12), the PC, and the CSPR.
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成ISR后，`irq_svc`将返回并通过恢复寄存器（r0-r12）、PC和CSPR来恢复处理器状态。
- en: Important note
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'Going back to *step 1*, during an interrupt, the ARM core disables further
    IRQs on the local CPU. It is worth mentioning that in the earlier Linux kernel
    days, there were two families of interrupt handlers: those running with interrupts
    disabled (that is, with the old `IRQF_DISABLED` flag set) and those running with
    interrupts enabled: they were then interruptible. The former were called **fast
    handlers**, while the latter were called **slow handlers**. For the latter, interrupts
    were actually reenabled by the kernel prior to invoking the handler.Since the
    interrupt context has a really small stack size compared to the process stack,
    it makes no sense that we may run into a stack overflow if we are in an interrupt
    context (running a given IRQ handler) while other interrupts keep occurring, even
    the one being serviced. This is confirmed by the commit at [https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=e58aa3d2d0cc](https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=e58aa3d2d0cc),
    which deprecated the fact of running interrupt handlers with IRQs enabled. As
    of this patch, IRQs remain disabled (left untouched after ARM core disabled them
    on the local CPU) during the execution of an IRQ handler. Additionally, the aforementioned
    flags have been entirely removed by the commit at [https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=d8bf368d0631](https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=d8bf368d0631),
    since Linux v4.1.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 回到*步骤1*，在中断期间，ARM核心会禁用本地CPU上的进一步IRQ。值得一提的是，在早期的Linux内核中，有两类中断处理程序：一类是在禁用中断的情况下运行（即设置了旧的`IRQF_DISABLED`标志），另一类是在启用中断的情况下运行：它们是可中断的。前者被称为**快速处理程序**，而后者被称为**慢处理程序**。对于后者，在调用处理程序之前，内核实际上重新启用了中断。由于中断上下文的堆栈大小与进程堆栈相比非常小，所以如果我们在中断上下文中运行给定的IRQ处理程序，同时其他中断继续发生，甚至正在服务的中断也会发生堆栈溢出是没有意义的。这得到了提交的确认[https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=e58aa3d2d0cc](https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=e58aa3d2d0cc)，该提交废弃了在启用IRQ的情况下运行中断处理程序的事实。从这个补丁开始，在执行IRQ处理程序期间，IRQ保持禁用（在ARM核心在本地CPU上禁用它们后保持不变）。此外，上述标志已经被提交[https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=d8bf368d0631](https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=d8bf368d0631)完全删除，自Linux
    v4.1以来。
- en: Designing an interrupt handler
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计中断处理程序
- en: 'Now that we’re familiar with the concept of bottom halves and deferring mechanisms,
    the time for us to implement interrupt handlers has come. In this section, we’ll
    take care of some specifics. Nowadays, the fact that interrupt handlers run with
    interrupts disabled (on the local CPU) means that we need to respect certain constraints
    in the ISR design:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们熟悉了底半部和延迟机制的概念，我们需要实现中断处理程序。在本节中，我们将处理一些具体的问题。如今，中断处理程序在本地CPU上禁用中断的事实意味着我们需要在ISR设计中遵守一定的约束：
- en: '**Execution time:** Since IRQ handlers run with interrupts disabled on the
    local CPU, the code must be as short and as small as possible, as well as fast
    enough to ensure the previously disabled CPU-local interrupts are reenabled quickly
    in so that other IRQs are not missed. Time-consuming IRQ handlers may considerably
    alter the real-time properties of the system and slow it down.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**执行时间：**由于IRQ处理程序在本地CPU上禁用中断运行，代码必须尽可能短小和快速，以确保先前禁用的CPU本地中断能够快速重新启用，以便不会错过其他IRQ。耗时的IRQ处理程序可能会显著改变系统的实时特性并减慢系统速度。'
- en: '**Execution context**: Since interrupt handlers are executed in an atomic context,
    sleeping (or any other mechanism that may sleep, such as mutexes, copying data
    from kernel to user space or vice versa, and so on) is forbidden. Any part of
    the code that requires or involves sleeping must be deferred into another, safer
    context (that is, a process context).'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**执行上下文：**由于中断处理程序在原子上下文中执行，因此禁止睡眠（或任何可能导致睡眠的机制，例如互斥、从内核到用户空间或反之的数据复制等）。任何需要或涉及睡眠的代码部分必须延迟到另一个更安全的上下文中（即进程上下文）。'
- en: 'An IRQ handler needs to be given two arguments: the interrupt line to install
    the handler for, and a unique device identifier of the peripheral (mostly used
    as a context data structure; that is, the pointer to the per-device or private
    structure of the associated hardware device):'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: IRQ 处理程序需要给出两个参数：要为其安装处理程序的中断线，以及外围设备的唯一标识符（主要用作上下文数据结构；也就是关联硬件设备的每个设备或私有结构的指针）：
- en: '[PRE62]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The device driver that wants to enable a given interrupt and register an ISR
    for it should call `request_irq()`, which is declared in `<linux/interrupt.h>`.
    This must be included in the driver code:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 想要启用给定中断并为其注册 ISR 的设备驱动程序应调用 `<linux/interrupt.h>` 中声明的 `request_irq()`。这必须包含在驱动程序代码中。
- en: '[PRE63]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'While the aforementioned API would require the caller to free the IRQ when
    it is no longer needed (that is, on driver detach), you can use the device managed
    variant, `devm_request_irq()`, which contains internal logic that allows it to
    take care of releasing the IRQ line automatically. It has the following prototype:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然上述 API 在不再需要时需要调用者释放 IRQ（即在驱动程序分离时），但您可以使用设备管理的变体 `devm_request_irq()`，它包含内部逻辑，可以自动处理释放
    IRQ 线。它具有以下原型：
- en: '[PRE64]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Except for the extra `dev` parameter (which is the device that requires the
    interrupt), both `devm_request_irq()` and `request_irq()` expect the following
    arguments:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 除了额外的 `dev` 参数（即需要中断的设备），`devm_request_irq()` 和 `request_irq()` 都期望以下参数：
- en: '`irq`, which is the interrupt line (that is, the interrupt number of the issuing
    device). Prior to validating the request, the kernel will make sure the requested
    interrupt is valid and that it is not already assigned to another device, unless
    both devices request that this IRQ line needs to be shared (with the help of flags).'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`irq`，即中断线（即发出设备的中断号）。在验证请求之前，内核将确保请求的中断是有效的，并且尚未分配给另一个设备，除非两个设备都要求共享此 IRQ
    线（借助 flags 的帮助）。'
- en: '`handler`, which is a function pointer to the interrupt handler.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`handler`，是指向中断处理程序的函数指针。'
- en: '`flags`, which represents the interrupt flags.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flags`，表示中断标志。'
- en: '`name`, an ASCII string representing the name of the device generating or claiming
    this interrupt.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`，一个表示生成或声明此中断的设备名称的 ASCII 字符串。'
- en: '`dev` should be unique to each registered handler. This cannot be `NULL` for
    shared IRQs since it is used to identify the device via the kernel IRQ core. The
    most common way of using it is to provide a pointer to the device structure or
    a pointer to any per-device (that’s potentially useful to the handler) data structure.
    This is because when an interrupt occurs, both the interrupt line (`irq`) and
    this parameter will be passed to the registered handler, which can use this data
    as context data for further processing.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dev` 应该对每个注册的处理程序是唯一的。对于共享的 IRQ，这不能是 `NULL`，因为它用于通过内核 IRQ 核心识别设备。最常用的方法是提供指向设备结构或指向任何每个设备（对处理程序有潜在用处）数据结构的指针。这是因为当发生中断时，中断线
    (`irq`) 和此参数将传递给注册的处理程序，该处理程序可以将此数据用作上下文数据以进行进一步处理。'
- en: '`flags` mangle the state or behavior of the IRQ line or its handler by means
    of the following masks, which can be OR’ed to form the final desired bit mask
    according to your needs:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '`flags` 通过以下掩码来篡改 IRQ 线或其处理程序的状态或行为，这些掩码可以进行 OR 运算，以形成最终所需的位掩码：'
- en: '[PRE65]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Note that flags can also be zero. Let’s take a look at some important flags.
    I’ll leave the rest for you to explore in `include/linux/interrupt.h`:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，flags 也可以为零。让我们来看看一些重要的 flags。我会留下其余的供您在 `include/linux/interrupt.h` 中探索：
- en: '`IRQF_TRIGGER_HIGH` and `IRQF_TRIGGER_LOW` flags are to be used for level-sensitive
    interrupts. The former is for interrupts triggered at high level and the latter
    is for the low-level triggered interrupts. Level-sensitive interrupts are triggered
    as long as the physical interrupt signal is high. If the interrupt source is not
    cleared by the end of its interrupt handler in the kernel, the operating system
    will repeatedly call that kernel interrupt handler, which may lead platform to
    hang. In other words, when the handler services the interrupt and returns, if
    the IRQ line is still asserted, the CPU will signal the interrupt again immediately.
    To prevent such a situation, the interrupt must be acknowledged (that is, cleared
    or de-asserted) by the kernel interrupt handler immediately when it is received.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IRQF_TRIGGER_HIGH` 和 `IRQF_TRIGGER_LOW` 标志用于电平敏感中断。前者用于高电平触发的中断，后者用于低电平触发的中断。只要物理中断信号保持高电平，电平敏感中断就会被触发。如果中断源在内核中的中断处理程序结束时未被清除，操作系统将重复调用该内核中断处理程序，这可能导致平台挂起。换句话说，当处理程序服务中断并返回时，如果
    IRQ 线仍处于断言状态，CPU 将立即再次发出中断。为了防止这种情况发生，中断必须在内核中断处理程序接收到时立即被确认（即清除或取消断言）。'
- en: However, those flags are safe with regard to interrupt sharing because if several
    devices pull the line active, an interrupt will be signaled (assuming the IRQ
    is enabled or as soon as it becomes so) until all drivers have serviced their
    devices. The only drawback is that it may lead to lockup if a driver fails to
    clear its interrupt source.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些 flags 对于中断共享是安全的，因为如果多个设备拉低了该线路，中断将被触发（假设 IRQ 已启用，或者一旦启用）。直到所有驱动程序都为其设备提供服务。唯一的缺点是，如果驱动程序未清除其中断源，可能会导致死机。
- en: '`IRQF_TRIGGER_RISING` and `IRQF_TRIGGER_FALLING` concern edge-triggered interrupts,
    rising and falling edges respectively. Such interrupts are signaled when the line
    changes from inactive to active state, but only once. To get a new request the
    line must go back to inactive and then to active again. Most of the time, no special
    action is required in software in order to acknowledge this type of interrupt.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IRQF_TRIGGER_RISING` 和 `IRQF_TRIGGER_FALLING` 关注边沿触发中断，分别是上升沿和下降沿。当线路从非活动状态变为活动状态时，会发出此类中断信号，但只发出一次。要获得新的请求，线路必须再次变为非活动状态，然后再次变为活动状态。大多数情况下，软件不需要采取特殊措施来确认此类中断。'
- en: 'When using edge-triggered interrupts however, interrupts may be lost, especially
    in the context of a shared interrupt line: if one device pulls the line active
    for too long a time, when another device pulls the line active, no edge will be
    generated, the second request will not be seen by the processor and then will
    be ignored. With a shared edge-triggered interrupts, if a hardware does not de-assert
    the IRQ line, no other interrupt will be notified for either shared device.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当使用边沿触发中断时，中断可能会丢失，特别是在共享中断线路的情况下：如果一个设备将线路保持活动状态的时间太长，当另一个设备拉高线路时，不会生成边沿，处理器将看不到第二个请求，然后将被忽略。对于共享的边沿触发中断，如果硬件不取消IRQ线路，那么不会通知任何其他共享设备的中断。
- en: Important note
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: As a quick reminder, you can just remember that level triggered interrupts signal
    a state, while edge triggered ones signal an event.Moreover, when requesting an
    interrupt without specifying an `IRQF_TRIGGER` flag, the setting should be assumed
    to be *as already configured*, which may be as per machine or firmware initialization.
    In such cases, you can refer to the device tree (if specified in there) for example
    to see what this *assumed configuration* is.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个快速提醒，你可以记住，电平触发中断信号状态，而边沿触发中断信号事件。此外，当请求中断而没有指定`IRQF_TRIGGER`标志时，应该假定设置为*已经配置*，这可能是根据机器或固件初始化。在这种情况下，你可以参考设备树（如果在其中指定）来查看这个*假定配置*是什么。
- en: '`IRQF_SHARED`: This allows the interrupt line to be shared among several devices.
    However, each device driver that needs to share the given interrupt line must
    set this flag; otherwise, the registration will fail.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IRQF_SHARED`：这允许中断线路在多个设备之间共享。然而，需要共享给定中断线路的每个设备驱动程序都必须设置此标志；否则，注册将失败。'
- en: '`IRQF_NOBALANCING`: This excludes the interrupt from *IRQ balancing*, which
    is a mechanism that consists of distributing/relocating interrupts across CPUs,
    with the goal of increasing performance. This prevents the CPU affinity of this
    IRQ from being changed. This flag can be used to provide a flexible setup for
    *clocksources* in order to prevent the event from being misattributed to the wrong
    core. This misattribution may result in the IRQ being disabled because if the
    CPU handling the interrupt is not the one that triggered it, the handler will
    return `IRQ_NONE`. This flag is only meaningful on multicore systems.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IRQF_NOBALANCING`：这将中断排除在*中断平衡*之外，这是一个机制，用于在CPU之间分配/重新定位中断，以提高性能。这可以防止更改此IRQ的CPU亲和性。此标志可用于为*时钟源*提供灵活的设置，以防止事件被错误地归因于错误的核心。这种错误的归因可能导致中断被禁用，因为如果处理中断的CPU不是触发它的CPU，则处理程序将返回`IRQ_NONE`。这个标志只在多核系统上有意义。'
- en: '`IRQF_IRQPOLL`: This flag allows the *irqpoll* mechanism to be used, which
    fixes interrupt problems. This means that this handler should be added to the
    list of known interrupt handlers that can be looked for when a given interrupt
    is not handled.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IRQF_IRQPOLL`：此标志允许使用*irqpoll*机制，用于修复中断问题。这意味着当给定中断未被处理时，应该将此处理程序添加到已知中断处理程序列表中。'
- en: '`IRQF_ONESHOT`: Normally, the actual interrupt line being serviced is enabled
    after its hard-IRQ handler completes, whether it awakes a threaded handler or
    not. This flag keeps the interrupt line disabled after the hard-IRQ handler completes.
    This flag must be set on threaded interrupts (we will discuss this later) for
    which the interrupt line must remain disabled until the threaded handler has completed.
    After this, it will be enabled.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IRQF_ONESHOT`：通常，在硬中断处理程序完成后，正在服务的实际中断线路将被启用，无论它是否唤醒了线程处理程序。此标志在硬件中断上保持中断线路在硬中断处理程序完成后禁用。对于必须保持中断线路禁用直到线程处理程序完成的线程化中断（稍后我们将讨论这一点），必须设置此标志。之后，它将被启用。'
- en: '`IRQF_NO_SUSPEND`: This does not disable the IRQ during system hibernation/suspension.
    This means that the interrupt is able to save the system from a suspended state.
    Such IRQs may be timer interrupts, which may trigger and need to be handled while
    the system is suspended. The whole IRQ line is affected by this flag in that if
    the IRQ is shared, every registered handler for this shared line will be executed,
    not just the one who installed this flag. You should avoid using `IRQF_NO_SUSPEND`
    and `IRQF_SHARED` at the same time as much as possible.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IRQF_NO_SUSPEND`：这不会在系统休眠/挂起期间禁用中断。这意味着中断能够从挂起状态保存系统。这样的中断可能是定时器中断，可能在系统挂起时触发并需要处理。此标志影响整个中断线，如果中断是共享的，那么对于这个共享线路的每个注册处理程序都将被执行，而不仅仅是安装了此标志的处理程序。尽量避免同时使用`IRQF_NO_SUSPEND`和`IRQF_SHARED`。'
- en: '`IRQF_FORCE_RESUME`: This enables the IRQ in the system resume path, even if
    `IRQF_NO_SUSPEND` is set.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IRQF_FORCE_RESUME`：这使得在系统恢复路径中启用中断，即使设置了`IRQF_NO_SUSPEND`。'
- en: '`IRQF_NO_THREAD`: This prevents the interrupt handler from being threaded.
    This flag overrides the `threadirqs` kernel (used on RT kernels, such as when
    applying the `PREEMPT_RT` patch) command-line option, which forces every interrupt
    to be threaded. This flag was introduced to address the non-threadability of some
    interrupts (for example, timers, which cannot be threaded even when all the interrupt
    handlers are forced to be threaded).'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IRQF_NO_THREAD`：这可以防止中断处理程序被线程化。此标志覆盖了`threadirqs`内核（在RT内核上使用，例如在应用`PREEMPT_RT`补丁时）命令行选项，该选项强制每个中断都被线程化。此标志是为了解决某些中断（例如定时器）无法线程化的问题而引入的。'
- en: '`IRQF_TIMER`: This marks this handler as being specific to the system timer
    interrupts. It helps not to disable the timer IRQ during system suspend to ensure
    that it resumes normally and does not thread them when full preemption (see `PREEMPT_RT`)
    is enabled. It is just an alias for `IRQF_NO_SUSPEND | IRQF_NO_THREAD`.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IRQF_TIMER`：这将此处理程序标记为特定于系统定时器中断。它有助于在系统挂起期间不禁用定时器中断，以确保它正常恢复并在启用完全抢占（参见`PREEMPT_RT`）时不会线程化。它只是`IRQF_NO_SUSPEND
    | IRQF_NO_THREAD`的别名。'
- en: '`IRQF_EARLY_RESUME`: This resumes IRQ early at the resume time of system core
    (syscore) operations instead of at device resume time. Go to [https://lkml.org/lkml/2013/11/20/89](https://lkml.org/lkml/2013/11/20/89)
    to see the commit introducing its support.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IRQF_EARLY_RESUME`：这在系统核心（syscore）操作的恢复时间而不是设备恢复时间提前恢复IRQ。转到[https://lkml.org/lkml/2013/11/20/89](https://lkml.org/lkml/2013/11/20/89)查看引入其支持的提交。'
- en: 'We must also consider the return type, `irqreturn_t`, of interrupt handlers
    since they may involve further actions once the handler is returned:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须考虑中断处理程序的返回类型`irqreturn_t`，因为它们可能在处理程序返回后涉及进一步的操作：
- en: '`IRQ_NONE`: On a shared interrupt line, once the interrupt occurs, the kernel
    irqcore successively walks through the handlers that have been registered for
    this line and executes them in the order they have been registered. The driver
    then has the responsibility of checking whether it is their device that issued
    the interrupt. If the interrupt does not come from its device, it must return
    `IRQ_NONE` in order to instruct the kernel to call the next registered interrupt
    handler. This return value is mostly used on shared interrupt lines since it informs
    the kernel that the interrupt does not come from our device. However, if `__report_bad_irq()`
    function in the kernel source tree.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IRQ_NONE`：在共享中断线上，一旦中断发生，内核irqcore会依次遍历为此线注册的处理程序，并按照它们注册的顺序执行它们。然后，驱动程序有责任检查是否是他们的设备发出了中断。如果中断不是来自其设备，则必须返回`IRQ_NONE`，以指示内核调用下一个注册的中断处理程序。此返回值在共享中断线上经常使用，因为它通知内核中断不是来自我们的设备。但是，如果`__report_bad_irq()`函数在内核源代码树中。'
- en: '`IRQ_HANDLED`: This value should be returned if the interrupt has been handled
    successfully. On a threaded IRQ, this value acknowledges the interrupt without
    waking the thread handler up.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IRQ_HANDLED`：如果中断已成功处理，应返回此值。在线程化的IRQ上，此值会确认中断，而不会唤醒线程处理程序。'
- en: '`IRQ_WAKE_THREAD`: On a thread IRQ handler, this value must be returned the
    by hard-IRQ handler in order to wake the handler thread. In this case, `IRQ_HANDLED`
    must only be returned by the threaded handler that was previously registered with
    `request_threaded_irq()`. We will discuss this later in the *Threaded IRQ handlers*
    section of this chapter.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`IRQ_WAKE_THREAD`：在线程IRQ处理程序上，硬中断处理程序必须返回此值，以唤醒处理程序线程。在这种情况下，`IRQ_HANDLED`只能由先前使用`request_threaded_irq()`注册的线程处理程序返回。我们将在本章的*线程化IRQ处理程序*部分讨论这一点。'
- en: Important note
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You must be very careful when reenabling interrupts in the handler. Actually,
    you must never reenable IRQs from within your IRQ handler as this would involve
    allowing “interrupts reentrancy”. In this case, it is your responsibility to address
    this.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理程序中重新启用中断时必须非常小心。实际上，您绝不能从IRQ处理程序中重新启用IRQ，因为这将涉及“中断重入”的允许。在这种情况下，您有责任解决这个问题。
- en: 'In the unloading path of your driver (or once you think you do not need the
    IRQ line anymore during your driver runtime life cycle, which is quite rare),
    you must release your IRQ resource by unregistering your interrupt handler and
    potentially disabling the interrupt line. The `free_irq()` interface does this
    for you:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在驱动程序的卸载路径（或者在驱动程序运行时生命周期中不再需要IRQ线路时，这种情况非常罕见），您必须通过注销中断处理程序并可能禁用中断线路来释放IRQ资源。`free_irq()`接口为您执行此操作：
- en: '[PRE66]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'That being said, if an IRQ allocated with `devm_request_irq()` needs to be
    freed separately, `devm_free_irq()` must be used. It has the following prototype:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，如果使用`devm_request_irq()`分配的IRQ需要单独释放，则必须使用`devm_free_irq()`。它具有以下原型：
- en: '[PRE67]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: This function has an extra `dev` argument, which is the device to free the IRQ
    for. This is usually the same as the one that the IRQ has been registered for.
    Except for `dev`, this function takes the same arguments and performs the same
    function as `free_irq()`. However, instead of `free_irq()`, it should be used
    to manually free IRQs that have been allocated with `devm_request_irq()`.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数有一个额外的`dev`参数，即要释放IRQ的设备。这通常与已注册IRQ的设备相同。除了`dev`之外，此函数接受相同的参数并执行与`free_irq()`相同的功能。但是，它应该用于手动释放使用`devm_request_irq()`分配的IRQ，而不是`free_irq()`。
- en: Both `devm_request_irq()` and `free_irq()` remove the handler (identified by
    `dev_id` when it comes to shared interrupts) and disable the line. If the interrupt
    line is shared, the handler is simply removed from the list of handlers for this
    IRQ, and the interrupt line is disabled in the future when the last handler is
    removed. Moreover, if possible, your code must ensure the interrupt is really
    disabled on the card it drives before calling this function, since omitting this
    may leads to spurious IRQs.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '`devm_request_irq()`和`free_irq()`都会删除处理程序（在共享中断时由`dev_id`标识）并禁用该线路。如果中断线路是共享的，处理程序将从此IRQ的处理程序列表中删除，并且在删除最后一个处理程序时将来禁用中断线路。此外，如果可能，您的代码必须确保在调用此函数之前真正在驱动的卡上禁用中断，因为忽略这一点可能导致虚假的中断。'
- en: 'There are few things that are worth mentioning here about interrupts that you
    should never forget:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些关于中断的事情是值得一提的，你永远不应该忘记的：
- en: Since interrupt handlers in Linux run with IRQs disabled on the local CPU and
    the current line is masked in all other cores, they don’t need to be reentrant,
    since the same interrupt will never be received until the current handler has
    completed. However, all other interrupts (on other cores) remain enabled (or should
    we say untouched), so other interrupts keep being serviced, even though the current
    line is always disabled, as well as further interrupts on the local CPU. Consequently,
    the same interrupt handler is never invoked concurrently to service a nested interrupt.
    This greatly simplifies writing your interrupt handler.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于Linux中的中断处理程序在本地CPU上禁用IRQ，并且当前行在所有其他核心上都被屏蔽，因此它们不需要是可重入的，因为在当前处理程序完成之前，同一个中断永远不会被接收。然而，所有其他中断（在其他核心上）保持启用（或者我们应该说是未触及），因此其他中断仍在被服务，即使当前行总是被禁用，以及本地CPU上的进一步中断。因此，相同的中断处理程序永远不会同时被调用来服务嵌套中断。这极大地简化了编写中断处理程序。
- en: Critical regions that need to run with interrupts disabled should be limited
    as much as possible. To remember this, tell yourselves that your interrupt handler
    has interrupted other code and needs to give CPU back.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要在禁用中断的情况下运行的关键区域应尽量限制。要记住这一点，告诉自己，你的中断处理程序已经中断了其他代码，需要把CPU让出来。
- en: Interrupt handlers cannot block as they do not run in a process context.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中断处理程序不能阻塞，因为它们不在进程上下文中运行。
- en: They may not transfer data to/from user space since this may block.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们不能在用户空间传输数据，因为这可能会导致阻塞。
- en: They may not sleep or rely on code that may lead to sleep, such as invoking
    `wait_event()`, memory allocation with anything other than `GFP_ATOMIC`, or using
    a mutex/semaphore. The threaded handler can handle this.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们不能休眠，也不能依赖可能导致休眠的代码，比如调用`wait_event()`，使用除`GFP_ATOMIC`之外的内存分配，或者使用互斥锁/信号量。线程处理程序可以处理这个问题。
- en: They may not trigger nor call `schedule()`.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们不能触发也不能调用`schedule()`。
- en: Only one interrupt on a given line can be pending (its interrupt flag bits get
    set when its interrupt condition occurs, regardless of the state of its corresponding
    enabled bit or the global enabled bit). Any further interrupt of this line is
    lost. For example, if you are processing an RX interrupt while five more packets
    are received at the same time, you should not expect five times more interrupts
    to appear sequentially. You’ll only be notified once. If the processor doesn’t
    service the ISR first, there’s no way to check how many RX interrupts will occur
    later. This means that if the device generates another interrupt before the handler
    function returns `IRQ_HANDLED`, the interrupt controller will be notified of the
    pending interrupt flag and the handler will get called again (only once), so you
    may miss some interrupts if you are not fast enough. Multiple interrupts will
    happen while you are still handling the first one.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在给定线上只能有一个待处理的中断（当其中断条件发生时，其中断标志位被设置，而不管其对应的启用位或全局启用位的状态如何）。这条线的任何进一步中断都会丢失。例如，如果在处理RX中断时同时接收到了五个数据包，你不应该期望会依次出现五次更多的中断。你只会被通知一次。如果处理器在服务ISR之前不服务，就没有办法检查以后会发生多少个RX中断。这意味着如果设备在处理函数返回`IRQ_HANDLED`之前生成另一个中断，中断控制器将被通知有待处理的中断标志，并且处理程序将再次被调用（只有一次），所以如果你不够快，可能会错过一些中断。在你处理第一个中断时，可能会发生多个中断。
- en: Important note
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If an interrupt occurs while it is disabled (or masked), it will not be processed
    at all (masked in the flow handler), but will be recognized as asserted and will
    remain pending so that it will be processed when enabled (or unmasked).
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在禁用（或屏蔽）中发生中断，它将根本不会被处理（在流处理程序中被屏蔽），但将被识别为断言，并保持挂起状态，以便在启用（或取消屏蔽）时进行处理。
- en: The interrupt context has its own (fixed and quite low) stack size. Therefore,
    it totally makes sense to disable IRQs while running an ISR as reentrancy could
    cause stack overflow if too many preemptions happen.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 中断上下文有自己的（固定且相当低的）堆栈大小。因此，在运行ISR时完全禁用IRQ是有意义的，因为可重入性可能会导致堆栈溢出，如果发生太多的抢占。
- en: The concept of non-reentrancy for an interrupt means that if an interrupt is
    already in an active state, it cannot enter it again until the active status is
    cleared.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 中断的不可重入性概念意味着，如果中断已经处于活动状态，它不能再次进入，直到活动状态被清除。
- en: The concept of top and bottom halves
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 顶半部和底半部的概念
- en: External devices send interrupt requests to the CPU either to signal a particular
    event or to request a service. As stated in the previous section, bad interrupt
    management may considerably increase system latency and decrease its real-time
    quality. We also stated that interrupt processing – that is, the hard-IRQ handler
    – must be very fast, not only to keep the system responsive, but also so that
    it doesn’t miss other interrupt events.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 外部设备向CPU发送中断请求，要么是为了通知特定事件，要么是为了请求服务。如前一节所述，糟糕的中断管理可能会显著增加系统的延迟，并降低其实时性。我们还指出，中断处理
    - 即硬中断处理程序 - 必须非常快，不仅要保持系统的响应性，还要确保不会错过其他中断事件。
- en: 'Take a look at the following diagram:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下下面的图表：
- en: '![Figure 1.2 – Interrupt splitting flow'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '![图1.2 - 中断分流流程'
- en: '](img/Figure_1.2_B10985.jpg)'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_1.2_B10985.jpg)'
- en: Figure 1.2 – Interrupt splitting flow
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 - 中断分流流程
- en: The basic idea is that you split the interrupt handler into two parts. The first
    part is a function) that will run in a so-called hard-IRQ context, with interrupts
    disabled, and perform the minimum required work (such as doing some quick sanity
    checks, time-sensitive tasks, read/write hardware registers, and processing this
    data and acknowledging the interrupt on the device that raised it). This first
    part is the so-called top-half on Linux systems. The top-half then schedules a
    (sometimes threaded) handler, which then runs a so-called bottom-half function,
    with interrupts re-enabled. This is the second part of the interrupt. The bottom-half
    may then perform time-consuming tasks (such as buffer processing) – tasks that
    may sleep, depending on the deferring mechanism.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想是将中断处理程序分成两部分。第一部分是一个函数，将在所谓的硬中断上下文中运行，禁用中断，并执行最少的必要工作（例如进行一些快速的健全性检查，时间敏感的任务，读/写硬件寄存器，并处理此数据并在引发中断的设备上确认中断）。这第一部分在Linux系统中被称为顶半部。顶半部然后调度一个（有时是线程化的）处理程序，然后运行所谓的下半部函数，重新启用中断。这是中断的第二部分。下半部然后可以执行耗时的任务（例如缓冲区处理）-
    根据推迟机制可能会休眠的任务。
- en: This splitting would considerably increase the system’s responsiveness as the
    time spent with IRQs disabled is reduced to its minimum. When the bottom halves
    are run in kernel threads, they compete for the CPU with other processes on the
    runqueue. Moreover, they may have their real-time properties set. The top half
    is actually the handler that’s registered using `request_irq()`. When using `request_threaded_irq()`,
    as we will see in the next section, the top half is the first handler that’s given
    to the function.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分割将显著提高系统的响应性，因为禁用IRQ的时间减少到最低。当下半部在内核线程中运行时，它们将与运行队列上的其他进程竞争CPU。此外，它们可能已经设置了它们的实时属性。实际上，顶半部是使用`request_irq()`注册的处理程序。当使用`request_threaded_irq()`时，如我们将在下一节中看到的，顶半部是给定给该函数的第一个处理程序。
- en: As we described previously, a bottom half represents any task (or work) that’s
    scheduled from within an interrupt handler. Bottom halves are designed using a
    work-deferring mechanism, which we have seen previously. Depending on which one
    you choose, it may run in a (software) interrupt context or in a process context.
    This includes *SoftIRQs*, *tasklets, workqueues*, and *threaded IRQs*.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所描述的，下半部代表从中断处理程序内部调度的任何任务（或工作）。下半部是使用工作推迟机制设计的，我们之前已经看到了。根据您选择的是哪一个，它可能在（软件）中断上下文中运行，也可能在进程上下文中运行。这包括*SoftIRQs*，*tasklets*，*workqueues*和*threaded
    IRQs*。
- en: Important note
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Tasklets and SoftIRQs do not actually fit into the so-called “thread interrupts”
    mechanism since they run in their own special contexts.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: Tasklets和SoftIRQs实际上并不适合所谓的“线程中断”机制，因为它们在自己的特殊上下文中运行。
- en: Since softIRQ handlers run at a high priority with scheduler preemption disabled,
    they do not relinquish the CPU to processes/threads until they complete, so care
    must be taken while using them for bottom-half delegation. Nowadays, since the
    quantum that’s allocated for a particular process may vary, there is no strict
    rule regarding how long the softIRQ handler should take to complete so that it
    doesn’t slow the system down as the kernel would not be able to give CPU time
    to other processes. I would say that this should be no longer than a half of jiffy.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 由于软中断处理程序以高优先级运行，并且禁用了调度程序抢占，它们在完成之前不会将CPU让给进程/线程，因此在将它们用于下半部委托时必须小心。如今，由于为特定进程分配的时间片可能会有所不同，因此没有严格的规定软中断处理程序应该花多长时间才能完成，以免减慢系统速度，因为内核将无法为其他进程分配CPU时间。我认为这个时间不应该超过半个节拍。
- en: 'The hard-IRQ handler (the top half) has to be as fast as possible, and most
    of time, it should just be reading and writing in I/O memory. Any other computation
    should be deferred to the bottom half, whose main goal is to perform any time-consuming
    and minimal interrupt-related work that’s not performed by the top half. There
    are no clear guidelines on repartitioning work between the top and bottom halves.
    The following is some advice:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 硬中断处理程序（顶半部）必须尽可能快，大部分时间应该只是在I/O内存中读写。任何其他计算都应该延迟到下半部，其主要目标是执行任何耗时且与中断相关的最小工作，这些工作不是由顶半部执行的。关于在顶半部和下半部之间重新分配工作没有明确的指导方针。以下是一些建议：
- en: Hardware-related work and time-sensitive work should be performed in the top
    half.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与硬件相关的工作和时间敏感的工作应该在上半部分执行。
- en: If the work doesn’t need to be interrupted, perform it in the top half.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果工作不需要中断，就在上半部分执行。
- en: From my point of view, everything else can be deferred – that is, performed
    in the bottom half – so that it runs with interrupts enabled and when the system
    is less busy.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从我的角度来看，其他所有工作都可以推迟 - 也就是说，在下半部执行 - 以便在系统不太忙时启用中断。
- en: If the hard-IRQ handler is fast enough to process and acknowledge interrupts
    consistently within a few microseconds, then there is absolutely no need to use
    bottom-half delegations at all.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果硬中断处理程序足够快，可以在几微秒内一致地处理和确认中断，那么根本没有必要使用下半部委托。
- en: Next, we will look at threaded IRQ handlers.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一下线程中断请求处理程序。
- en: Threaded IRQ handlers
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线程中断请求处理程序
- en: Threaded interrupt handlers were introduced to reduce the time spent in the
    interrupt handler and deferring the rest of the work (that is, processing) out
    to kernel threads. So, the top half (hard-IRQ handler) would consist of quick
    sanity checks such as ensuring that the interrupt comes from its device and waking
    the bottom half accordingly. A threaded interrupt handler runs in its own thread,
    either in the thread of their parent (if they have one) or in a separate kernel
    thread. Moreover, the dedicated kernel thread can have its real-time priority
    set, though it runs at normal real-time priority (that is, `MAX_USER_RT_PRIO/2`
    as shown in the `setup_irq_thread()` function in `kernel/irq/manage.c`).
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 线程化中断处理程序的引入是为了减少中断处理程序中所花费的时间，并将其余的工作（即处理）推迟到内核线程中。因此，顶半部分（硬中断处理程序）将包括快速的健全性检查，例如确保中断来自其设备，并相应地唤醒底半部分。线程化中断处理程序在自己的线程中运行，可以在其父线程中运行（如果有的话），也可以在一个单独的内核线程中运行。此外，专用内核线程可以设置其实时优先级，尽管它以正常的实时优先级运行（即`MAX_USER_RT_PRIO/2`，如`kernel/irq/manage.c`中的`setup_irq_thread()`函数所示）。
- en: 'The general rule behind threaded interrupts is simple: keep the hard-IRQ handler
    as minimal as possible and defer as much work to the kernel thread as possible
    (preferably all work). You should use `request_threaded_irq()` (defined in `kernel/irq/manage.c`)
    if you want to request a threaded interrupt handler:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 线程化中断背后的一般规则很简单：尽可能将硬中断处理程序保持最小，并尽可能将更多工作推迟到内核线程（最好是所有工作）。如果要请求线程化中断处理程序，应该使用`request_threaded_irq()`（定义在`kernel/irq/manage.c`中）：
- en: '[PRE68]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'This function accepts two special parameters `handler` and `thread_fn`. The
    other parameters are the same as they are for `request_irq()`:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数接受两个特殊参数`handler`和`thread_fn`。其他参数与`request_irq()`的参数相同：
- en: '`handler` immediately runs when the interrupt occurs in the interrupt context,
    and acts as a hard-IRQ handler. Its job usually consists of reading the interrupt
    cause (in the device’s status register) to determine whether or how to handle
    the interrupt (this is frequent on MMIO devices). If the interrupt does not come
    from its device, this function should return `IRQ_NONE`. This return value usually
    only makes sense on shared interrupt lines. In the other case, if this hard-IRQ
    handler can finish interrupt processing fast enough (this is not a universal rule,
    but let’s say no longer than half a jiffy – that is, no longer than 500 µs if
    `CONFIG_HZ`, which defines the value of a jiffy, is set to 1,000) for a set of
    interrupt causes, it should return `IRQ_HANDLED` after processing in order to
    acknowledge the interrupts. Interrupt processing that does not fall into this
    time lapse should be deferred to the threaded IRQ handler. In this case, the hard-IRQ
    handler should return `IRQ_WAKE_T HREAD` in order to awake the threaded handler.
    Returning `IRQ_WAKE_THREAD` only makes sense when the `thread_fn` handler is also
    registered.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`handler`在中断上下文中立即运行，充当硬中断处理程序。它的工作通常包括读取中断原因（在设备的状态寄存器中）以确定如何处理中断（这在MMIO设备上很常见）。如果中断不是来自其设备，此函数应返回`IRQ_NONE`。这个返回值通常只在共享中断线上有意义。在另一种情况下，如果这个硬中断处理程序可以在足够快的时间内完成中断处理（这不是一个普遍的规则，但假设不超过半个jiffy，也就是不超过500微秒，如果`CONFIG_HZ`，定义了jiffy的值，设置为1,000），它应该在处理后返回`IRQ_HANDLED`以确认中断。不在这个时间范围内的中断处理应该推迟到线程化的中断处理程序。在这种情况下，硬中断处理程序应该返回`IRQ_WAKE_THREAD`以唤醒线程处理程序。只有在`thread_fn`处理程序也注册时，返回`IRQ_WAKE_THREAD`才有意义。'
- en: '`thread_fn` is the threaded handler that’s added to the scheduler runqueue
    when the hard-IRQ handler function returns `IRQ_WAKE_THREAD`. If `thread_fn` is
    `NULL` while `handler` is set and it returns `IRQ_WAKE_THREAD`, nothing happens
    at the return path of the hard-IRQ handler except for a simple warning message
    being shown. Have a look at the `__irq_wake_thread()` function in the kernel sources
    for more information. As `thread_fn` competes for the CPU with other processes
    on the runqueue, it may be executed immediately or later in the future when the
    system has less load. This function should return `IRQ_HANDLED` when it has completed
    the interrupt handling process successfully. At this stage, the associated kthread
    will be taken off the runqueue and put in a blocked state until it’s woken up
    again by the hard-IRQ function.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`thread_fn`是当硬中断处理程序返回`IRQ_WAKE_THREAD`时添加到调度程序运行队列中的线程处理程序。如果`thread_fn`为`NULL`，而`handler`被设置并且返回`IRQ_WAKE_THREAD`，则在硬中断处理程序的返回路径上除了显示一个简单的警告消息外，什么也不会发生。查看内核源代码中的`__irq_wake_thread()`函数以获取更多信息。由于`thread_fn`与运行队列上的其他进程竞争CPU，它可能会立即执行，也可能在将来系统负载较轻时执行。当它成功完成中断处理过程时，此函数应返回`IRQ_HANDLED`。在这个阶段，相关的kthread将被从运行队列中取出，并处于阻塞状态，直到再次被硬中断函数唤醒。'
- en: 'A default hard-IRQ handler will be installed by the kernel if `handler` is
    `NULL` and `thread_fn != NULL`. This is the default primary handler. It is an
    almost empty handler that simply returns `IRQ_WAKE_THREAD` in order to wake up
    the associated kernel thread that will execute the `thread_fn` handler. This makes
    it possible to move the execution of interrupt handlers entirely to the process
    context, thus preventing buggy drivers (buggy IRQ handlers) from breaking the
    whole system and reducing interrupt latency. A dedicated handler’s kthreads will
    be visible in `ps ax`:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`handler`为`NULL`且`thread_fn != NULL`，内核将安装默认的硬中断处理程序。这是默认的主处理程序。它是一个几乎空的处理程序，只是返回`IRQ_WAKE_THREAD`以唤醒相关的内核线程，该线程将执行`thread_fn`处理程序。这样可以完全将中断处理程序的执行移动到进程上下文，从而防止有错误的驱动程序（错误的中断处理程序）破坏整个系统并减少中断延迟。专用处理程序的kthread将在`ps
    ax`中可见。
- en: '[PRE69]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Important note
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Nowadays, `request_irq()` is just a wrapper around `request_threaded_irq()`,
    with the `thread_fn` parameter set to `NULL`.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`request_irq()`只是`request_threaded_irq()`的一个包装，`thread_fn`参数设置为`NULL`。
- en: Note that the interrupt is acknowledged at the interrupt controller level when
    you return from the hard-IRQ handler (whatever the return value is), thus allowing
    you to take other interrupts into account. In such a situation, if the interrupt
    hasn’t been acknowledged at the device level, the interrupt will fire again and
    again, resulting in stack overflows (or being stuck in the hard-IRQ handler forever)
    for level-triggered interrupts since the issuing device still has the interrupt
    line asserted. Before threaded IRQs were a thing, when you needed to run the bottom-half
    in a thread, you would instruct the top half to disable the IRQ at the device
    level, prior to waking the thread up. This way, even if the controller is ready
    to accept another interrupt, it is not raised again by the device.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当您从硬中断处理程序返回时（无论返回值是什么），中断在中断控制器级别被确认，从而允许您考虑其他中断。在这种情况下，如果中断在设备级别没有被确认，中断将一次又一次地触发，导致堆栈溢出（或者永远停留在硬中断处理程序中）对于级联触发的中断，因为发出中断的设备仍然保持中断线被断言。在线程中断出现之前，当您需要在线程中运行底半部时，您将指示顶半部在唤醒线程之前在设备级别禁用中断。这样，即使控制器准备接受另一个中断，设备也不会再次引发中断。
- en: 'The `IRQF_ONESHOT` flag resolves this problem. It must be set when it comes
    to use a threaded interrupt (at the `request_threaded_irq()` call); otherwise,
    the request will fail with the following error:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '`IRQF_ONESHOT`标志解决了这个问题。在使用线程中断时（在`request_threaded_irq()`调用时），必须设置它；否则，请求将失败，并显示以下错误：'
- en: '[PRE70]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: For more information on this, please have a look at the `__setup_irq()` function
    in the kernel source tree.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 有关更多信息，请查看内核源树中的`__setup_irq()`函数。
- en: 'The following is an excerpt from the message that introduced the `IRQF_ONESHOT`
    flag and explains what it does (the entire message can be found at [http://lkml.iu.edu/hypermail/linux/kernel/0908.1/02114.html](http://ebay.co.uk)):'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是介绍`IRQF_ONESHOT`标志并解释其作用的消息摘录（完整消息可在[http://lkml.iu.edu/hypermail/linux/kernel/0908.1/02114.html](http://ebay.co.uk)找到）：
- en: “It allows drivers to request that the interrupt is not unmasked (at the controller
    level) after the hard interrupt context handler has been executed and the thread
    has been woken. The interrupt line is unmasked after the thread handler function
    has been executed.”
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: “它允许驱动程序请求在硬中断上下文处理程序执行并唤醒线程后不解除中断（在控制器级别）。线程处理程序函数执行后，中断线将被解除屏蔽。”
- en: Important note
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: If you omit the `IRQF_ONESHOT` flag, you’ll have to provide a hard-IRQ handler
    (in which you should disable the interrupt line); otherwise, the request will
    fail.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 如果省略`IRQF_ONESHOT`标志，您将需要提供一个硬中断处理程序（在其中应禁用中断线）；否则，请求将失败。
- en: 'An example of a thread-only IRQ is as follows:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 线程专用中断的示例如下：
- en: '[PRE71]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: In the preceding example, our device sits on an I2C bus. Thus, accessing the
    available data may cause it to sleep, so this should not be performed in the hard-IRQ
    handler. This is why our handler parameter is `NULL`.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们的设备位于I2C总线上。因此，访问可用数据可能会导致其休眠，因此不应在硬中断处理程序中执行。这就是为什么我们的处理程序参数是`NULL`。
- en: Tip
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: If the IRQ line where you need threaded ISR handling to be shared among several
    devices (for example, some SoCs share the same interrupt among their internal
    ADCs and the touchscreen module), you must implement the hard-IRQ handler, which
    should check whether the interrupt has been raised by your device or not. If the
    interrupt does come from your device, you should disable the interrupt at the
    device level and return `IRQ_WAKE_THREAD` to wake the threaded handler. The interrupt
    should be enabled back at the device level in the return path of the threaded
    handler. If the interrupt does not come from your device, you should return `IRQ_NONE`
    directly from the hard-IRQ handler.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要在线程ISR处理之间共享的IRQ线（例如，某些SoC在其内部ADC和触摸屏模块之间共享相同的中断），则必须实现硬中断处理程序，该处理程序应检查中断是否由您的设备引发。如果中断确实来自您的设备，则应在设备级别禁用中断并返回`IRQ_WAKE_THREAD`以唤醒线程处理程序。在线程处理程序的返回路径中应重新启用设备级别的中断。如果中断不来自您的设备，则应直接从硬中断处理程序返回`IRQ_NONE`。
- en: Moreover, if one driver has set either the `IRQF_SHARED` or `IRQF_ONESHOT` flag
    on the line, every other driver sharing the line must set the same flags. The
    `/proc/interrupts` file lists the IRQs and their processing per CPU, the IRQ name
    that was given during the requesting step, and a comma-separated list of drivers
    that registered an ISR for that interrupt.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果一个驱动程序在该线上设置了`IRQF_SHARED`或`IRQF_ONESHOT`标志，那么共享该线的每个其他驱动程序都必须设置相同的标志。`/proc/interrupts`文件列出了每个CPU的IRQ及其处理，请求步骤中给出的IRQ名称以及为该中断注册ISR的驱动程序的逗号分隔列表。
- en: Threaded IRQs are the best choice for interrupt processing as they can hog too
    many CPU cycles (exceeding a jiffy in most cases), such as bulk data processing.
    Threading IRQs allow the priority and CPU affinity of their associated thread
    to be managed individually. Since this concept comes from the real-time kernel
    tree (from *Thomas Gleixner*), it fulfills many requirements of a real-time system,
    such as allowing a fine-grained priority model to be used and reducing interrupt
    latency in the kernel.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 线程中断是中断处理的最佳选择，因为它们可能占用太多CPU周期（在大多数情况下超过一个节拍），例如大量数据处理。线程中断允许管理其关联线程的优先级和CPU亲和性。由于这个概念来自实时内核树（来自*Thomas
    Gleixner*），它满足了实时系统的许多要求，例如允许使用细粒度优先级模型并减少内核中断延迟。
- en: Take a look at `/proc/irq/IRQ_NUMBER/smp_affinity`, which can be used to get
    or set the corresponding `IRQ_NUMBER` affinity. This file returns and accepts
    a bitmask that represents which processors can handle ISRs that have been registered
    for this IRQ. This way, you can, for example, decide to set the affinity of a
    hard-IRQ to one CPU while setting the affinity of the threaded handler to another
    CPU.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 查看`/proc/irq/IRQ_NUMBER/smp_affinity`，它可用于获取或设置相应的`IRQ_NUMBER`亲和性。该文件返回并接受一个位掩码，表示可以处理已为此IRQ注册的ISR的处理器。这样，您可以决定将硬中断的亲和性设置为一个CPU，同时将线程处理程序的亲和性设置为另一个CPU。
- en: Requesting a context IRQ
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 请求上下文IRQ
- en: A driver requesting an IRQ must know the nature of the interrupt in advance
    and decide whether its handler can run in the hard-IRQ context in order to call
    `request_irq()` or `request_threaded_irq()` accordingly.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 请求IRQ的驱动程序必须预先了解中断的性质，并决定其处理程序是否可以在硬中断上下文中运行，以便相应地调用`request_irq()`或`request_threaded_irq()`。
- en: 'There is a problem when it comes to request IRQ lines provided by discrete
    and non-MMIO-based interrupt controllers, such as I2C/SPI gpio-expanders. Since
    accessing those buses may cause them to sleep, it would be a disaster to run the
    handler of such slow controllers in a hard-IRQ context. Since the driver does
    not contain any information about the nature of the interrupt line/controller,
    the IRQ core provides the `request_any_context_irq()` API. This function determines
    whether the interrupt controller/line can sleep and calls the appropriate requesting
    function:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到由离散和非基于MMIO的中断控制器提供的请求IRQ线时，例如I2C/SPI gpio扩展器，存在一个问题。由于访问这些总线可能导致它们进入睡眠状态，因此在硬中断上下文中运行这些慢速控制器的处理程序将是灾难性的。由于驱动程序不包含有关中断线/控制器性质的任何信息，因此IRQ核心提供了`request_any_context_irq()`API。此函数确定中断控制器/线是否可以休眠，并调用适当的请求函数：
- en: '[PRE72]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '`request_any_context_irq()` and `request_irq()` have the same interface but
    different semantics. Depending on the underlying context (the hardware platform),
    `request_any_context_irq()` selects either a hardIRQ handling method using `request_irq()`
    or a threaded handling method using `request_threaded_irq()`. It returns a negative
    error value on failure, while on success, it returns either `IRQC_IS_HARDIRQ`
    (meaning hardI-RQ handling is used) or `IRQC_IS_NESTED` (meaning the threaded
    version is used). With this function, the behavior of the interrupt handler is
    decided at runtime. For more information, take a look at the comment introducing
    it in the kernel by following this link: [https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=ae731f8d0785](https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=ae731f8d0785).'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '`request_any_context_irq()`和`request_irq()`具有相同的接口，但具有不同的语义。根据底层上下文（硬件平台），`request_any_context_irq()`选择使用`request_irq()`的硬中断处理方法，或使用`request_threaded_irq()`的线程处理方法。它在失败时返回负错误值，而在成功时返回`IRQC_IS_HARDIRQ`（表示使用了硬中断处理）或`IRQC_IS_NESTED`（表示使用了线程版本）。使用此函数，中断处理程序的行为在运行时决定。有关更多信息，请查看内核中介绍它的注释，通过以下链接进行查看：[https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=ae731f8d0785](https://git.kernel.org/pub/scm/linux/kernel/git/next/linux-next.git/commit/?id=ae731f8d0785)。'
- en: The advantage of using `request_any_context_irq()` is that you don’t need to
    care about what can be done in the IRQ handler. This is because the context in
    which the handler will run depends on the interrupt controller that provides the
    IRQ line. For example, for a gpio-IRQ-based device driver, if the gpio belongs
    to a controller that seats on an I2C or SPI bus (in which case gpio access may
    sleep), the handler will be threaded. Otherwise (the gpio access may not sleep
    and is memory mapped as it belongs to the SoC), the handler will run in the hardIRQ
    context.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`request_any_context_irq()`的优点是您不需要关心在IRQ处理程序中可以做什么。这是因为处理程序将运行的上下文取决于提供IRQ线的中断控制器。例如，对于基于gpio-IRQ的设备驱动程序，如果gpio属于坐落在I2C或SPI总线上的控制器（在这种情况下，gpio访问可能会休眠），处理程序将是线程化的。否则（gpio访问可能不会休眠，并且作为SoC的一部分是内存映射的），处理程序将在硬中断上下文中运行。
- en: 'In the following example, the device expects an IRQ line mapped to a gpio.
    The driver cannot assume that the given gpio line will be memory mapped since
    it’s coming from the SoC. It may come from a discrete I2C or SPI gpio controller
    as well. A good practice would be to use `request_any_context_irq()` here:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，设备期望一个IRQ线映射到一个gpio。驱动程序不能假设给定的gpio线将被内存映射，因为它来自SoC。它也可能来自离散的I2C或SPI
    gpio控制器。在这里使用`request_any_context_irq()`是一个好的做法：
- en: '[PRE73]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The preceding code is simple enough but is quite safe thanks to `request_any_context_irq()`,
    which prevents us from mistaking the type of the underlying gpio.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码足够简单，但由于`request_any_context_irq()`的存在，它非常安全，这可以防止我们错误地将底层gpio的类型弄错。
- en: Using a workqueue to defer a bottom-half
  id: totrans-425
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用工作队列来推迟底半部
- en: Since we have already discussed the workqueue API, we will provide an example
    of how to use it here. This example is not error-free and has not been tested.
    It is just a demonstration that highlights the concept of bottom-half deferring
    by means of a workqueue.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经讨论了工作队列API，我们将在这里提供一个使用它的示例。这个示例并不是没有错误的，也没有经过测试。这只是一个演示，突出了通过工作队列推迟底半部的概念。
- en: 'Let’s start by defining the data structure that will hold the elements we need
    for further development:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先定义数据结构，该数据结构将保存我们需要进行进一步开发的元素：
- en: '[PRE74]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'In the preceding data structure, our work structure is represented by the `my_work`
    element. We aren’t using the pointer here because we will need to use the `container_of()`
    macro to grab a pointer to the initial data structure. Next, we can define the
    method that will be invoked in the worker thread:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的数据结构中，我们的工作结构由`my_work`元素表示。我们在这里不使用指针，因为我们需要使用`container_of()`宏来获取指向初始数据结构的指针。接下来，我们可以定义将在工作线程中调用的方法：
- en: '[PRE75]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'In the preceding code, we start data processing when enough data has been buffered.
    Now, we can provide our IRQ handler, which is responsible for scheduling our work,
    as follows:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，当缓冲了足够的数据时开始数据处理。现在，我们可以提供我们的IRQ处理程序，负责调度我们的工作，如下：
- en: '[PRE76]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'The comments in the IRQ handler code are meaningful enough. `schedule_work()`
    is the function that schedules our work. Finally, we can write our `probe` method,
    which will request our IRQ and register the previous handler:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: IRQ处理程序代码中的注释是有意义的。`schedule_work()`是调度我们的工作的函数。最后，我们可以编写我们的`probe`方法，该方法将请求我们的IRQ并注册先前的处理程序：
- en: '[PRE77]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: The structure of the preceding probe method shows without a doubt that we are
    facing a platform device driver. Generic IRQ and workqueue APIs have been used
    here to initialize our workqueue and register our handler.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的`probe`方法的结构无疑表明我们面对的是一个平台设备驱动程序。通用的IRQ和工作队列API在这里被用来初始化我们的工作队列并注册我们的处理程序。
- en: Locking from within an interrupt handler
  id: totrans-436
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在中断处理程序内部进行锁定
- en: 'If a resource is shared between two or more use contexts (kthread, work, threaded
    IRQ, and so on) and only with a threaded bottom-half (that is, they’re never accessed
    by the hard-IRQ), then mutex locking is the way to go, as shown in the following
    example:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个资源在两个或更多用户上下文（kthread、work、线程化IRQ等）之间共享，并且只与线程化底半部（即，它们永远不会被硬中断访问）共享，则互斥锁定是正确的方式，如下例所示：
- en: '[PRE78]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: In the preceding code, both the user task (kthread, work, and so on) and the
    threaded bottom half must hold the mutex before accessing the resource.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，用户任务（kthread、work等）和线程化底半部在访问资源之前必须持有互斥锁。
- en: 'The preceding case is the simplest one to exemplify. The following are some
    rules that will help you lock between hard-IRQ contexts and others:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的情况是最简单的例子。以下是一些规则，可以帮助你在硬中断上下文和其他上下文之间进行锁定：
- en: '*If a resource is shared between a user context and a hard interrupt handler*,
    you will want to use the spinlock variant, which disables interrupts; that is,
    the simple `_irq` or `_irqsave`/`_irq_restore` variants. This ensures that the
    user context is never preempted by this IRQ when it’s accessing the resource.
    This can be seen in the following example:'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如果资源在用户上下文和硬中断处理程序之间共享*，则应该使用禁用中断的spinlock变体，即简单的`_irq`或`_irqsave`/`_irq_restore`变体。这可以确保用户上下文在访问资源时不会被此中断抢占。这可以在以下示例中看到：'
- en: '[PRE79]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: In the preceding code, the hard-IRQ handler doesn’t need to hold the spinlock
    as it can never be preempted. Only the user context must be held. There is a case
    where protection may not be necessary between the hard-IRQ and its threaded counterpart;
    that is, when the `IRQF_ONESHOT` flag is set while requesting the IRQ line. This
    flag keeps the interrupt disabled after the hard-IRQ handler has finished. With
    this flag set, the IRQ line remains disabled until the threaded handler has been
    run until its completion. This way, the hard-IRQ handler and its threaded counterpart
    will never compete and a lock for a resource shared between the two might not
    be necessary.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，硬中断处理程序不需要持有自旋锁，因为它永远不会被抢占。只有用户上下文必须持有。在硬中断和其线程化对应物之间可能不需要保护的情况下；也就是说，在请求IRQ线时设置了`IRQF_ONESHOT`标志。此标志在硬中断处理程序完成后保持中断禁用。设置了此标志后，IRQ线保持禁用，直到线程化处理程序运行完成。这样，硬中断处理程序和其线程化对应物将永远不会竞争，并且可能不需要为两者之间共享的资源加锁。
- en: 'When the resource is shared between user context and softIRQ, there are two
    things you need to guard against: the fact the user context can be interrupted
    by the softIRQ (remember, softIRQs run on the return path of hard-IRQ handlers)
    and the fact that the critical region can be entered from another CPU (remember,
    the same softIRQ may run concurrently on another CPU). In this case, you should
    use spinlock API variants that will disable softIRQs; that is, `spin_lock_bh()`
    and `spin_unlock_bh()`. The `_bh` prefix means the bottom half. Because those
    APIs have not been discussed in detail in this chapter, you can use the `_irq`
    or even `_irqsave` variants, which disable hardware interrupts as well.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当资源在用户上下文和软中断之间共享时，有两件事需要防范：用户上下文可能会被软中断打断（记住，软中断在硬中断处理程序的返回路径上运行），以及临界区可能会从另一个CPU进入（记住，相同的软中断可能在另一个CPU上并发运行）。在这种情况下，应该使用禁用软中断的spinlock
    API变体，即`spin_lock_bh()`和`spin_unlock_bh()`。`_bh`前缀表示底半部。因为这些API在本章中没有详细讨论，可以使用`_irq`甚至`_irqsave`变体，这些变体也会禁用硬件中断。
- en: The same applies to tasklets (because tasklets are built on top of softIRQs),
    with the only difference that a tasklet never runs concurrently (it never runs
    on more than one CPU at once); a tasklet is exclusive by design.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相同的规则适用于任务队列（因为任务队列是建立在软中断之上），唯一的区别是任务队列永远不会并发运行（它永远不会同时在多个CPU上运行）；任务队列是通过设计独占的。
- en: 'There are two things to guard against when it comes to locking between hard
    IRQ and softIRQ: the softIRQ can be interrupted by the hard-IRQ and the critical
    region can be entered (`1` for either by another hard-IRQ if designed in this
    way, `2` by the same softIRQ, or `3` by another softIRQ) from another CPU. Because
    the softIRQ can never run when the hard-IRQ handler is running, hard-IRQ handlers
    only need to use the `spin_lock()` and `spin_unlock()` APIs, which prevent concurrent
    access by other hard handlers on another CPU. However, softIRQ needs to use the
    locking API that actually disables interrupts – that is, the `_irq()` or `irqsave()`
    variants – with a preference for the latter.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在处理硬中断和软中断之间进行锁定时，有两件事需要防范：软中断可能会被硬中断打断，临界区可能会被另一个CPU（1）硬中断（如果设计成这样），（2）相同的软中断，或者（3）另一个软中断进入。因为当硬中断处理程序运行时，软中断永远不会运行，硬中断处理程序只需要使用`spin_lock()`和`spin_unlock()`API，这可以防止其他CPU上的硬中断处理程序的并发访问。然而，软中断需要使用实际禁用中断的锁定API，即`_irq()`或`irqsave()`变体，最好使用后者。
- en: Because softIRQs may run concurrently, *locking may be necessary between two
    different softIRQs, or even between a softIRQ and itself* (running on another
    CPU). In this case, `spinlock()`/`spin_unlock()` should be used. There’s no need
    to disable hardware interrupts.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为softIRQs可能会并发运行，*在两个不同的softIRQs之间，甚至在softIRQ和自身之间*（在另一个CPU上运行）可能需要锁定。在这种情况下，应该使用`spinlock()`/`spin_unlock()`。不需要禁用硬件中断。
- en: At this point, we are done looking at interrupt locking, which means we have
    come to the end of this chapter.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们已经完成了对中断锁定的查看，这意味着我们已经到达了本章的结尾。
- en: Summary
  id: totrans-449
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter introduced some core kernel functionalities that will be used in
    the next few chapters of this book. The concepts we covered concerned bit manipulation
    to Linux kernel interrupt design and implementation, through locking helpers and
    work deferring mechanisms. By now, you should be able to decide whether you should
    split your interrupt handler into two parts or not, as well as know what locking
    primitive suits your needs.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了一些核心内核功能，这些功能将在本书的接下来的几章中使用。我们涵盖的概念涉及位操作到Linux内核中断设计和实现，通过锁定辅助程序和工作推迟机制。到目前为止，您应该能够决定是否应该将中断处理程序分成两部分，以及了解哪种锁定原语适合您的需求。
- en: In the next chapter, we’ll cover Linux kernel managed resources, which is an
    interface that’s used to offload allocated resource management to the kernel core.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将涵盖Linux内核管理的资源，这是一个用于将分配的资源管理卸载到内核核心的接口。

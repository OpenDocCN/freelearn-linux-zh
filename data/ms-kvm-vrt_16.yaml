- en: '*Chapter 12*: Scaling Out KVM with OpenStack'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第12章*：使用OpenStack扩展KVM'
- en: 'Being able to virtualize a machine is a big thing, but sometimes, just virtualization
    is not enough. The problem is how to give individual users tools so that they
    can virtualize whatever they need, when they need it. If we combine that user-centric
    approach with virtualization, we are going to end up with a system that needs
    to be able to do two things: it should be able to connect to KVM as a virtualization
    mechanism (and not only KVM) and enable users to get their virtual machines running
    and automatically configured in a self-provisioning environment that''s available
    through a web browser. OpenStack adds one more thing to this since it is completely
    free and based entirely on open source technologies. Provisioning such a system
    is a big problem due to its complexity, and in this chapter, we are going to show
    you – or to be more precise, point you – in the right direction regarding whether
    you need a system like this.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 能够虚拟化一台机器是一件大事，但有时，仅仅虚拟化是不够的。问题在于如何给予个人用户工具，使他们可以在需要时虚拟化他们需要的任何东西。如果我们将以用户为中心的方法与虚拟化相结合，我们将得到一个需要能够连接到KVM作为虚拟化机制（不仅仅是KVM）并使用户能够在自助环境中通过Web浏览器获取其虚拟机并自动配置的系统。OpenStack增加了一件事情，因为它完全免费并且完全基于开源技术。由于其复杂性，配置这样的系统是一个大问题，在本章中，我们将向您展示
    - 或者更准确地说，指向您 - 关于是否需要这样的系统的正确方向。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introduction to OpenStack
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍OpenStack
- en: Software-defined networking
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软件定义网络
- en: OpenStack components
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenStack组件
- en: Additional OpenStack use cases
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 额外的OpenStack用例
- en: Provisioning the OpenStack environment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置OpenStack环境
- en: Integrating OpenStack with Ansible
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将OpenStack与Ansible集成
- en: Let's get started!
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Introduction to OpenStack
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍OpenStack
- en: In its own words, **OpenStack** is a cloud operating system that is used to
    control a large number of different resources in order to provide all the essential
    services for **Infrastructure-as-a-Service** (**IaaS**) and **orchestration**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 根据其自己的说法，**OpenStack**是一个云操作系统，用于控制大量不同的资源，以提供**基础设施即服务**（**IaaS**）和**编排**的所有基本服务。
- en: But what does this mean? OpenStack is designed to completely control all the
    resources that are in the data center, and to provide both central management
    and direct control over anything that can be used to deploy both its own and third-party
    services. Basically, for every service that we mention in this book, there is
    a place in the whole OpenStack landscape where that service is or can be used.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但这意味着什么？OpenStack旨在完全控制数据中心中的所有资源，并提供对可以用于部署其自身和第三方服务的任何资源的集中管理和直接控制。基本上，对于我们在本书中提到的每项服务，在整个OpenStack景观中都有一个可以使用该服务的地方。
- en: OpenStack itself consists of several different interconnected services or service
    parts, each with its own set of functionalities, and each with its own API that
    enables full control of the service. In this part of this book, we will try to
    explain what different parts of OpenStack do, how they interconnect, what services
    they provide, and how to use those services to our advantage.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack本身由几个不同的相互连接的服务或服务部分组成，每个都有自己的功能集，每个都有自己的API，可以完全控制该服务。在本书的这一部分，我们将尝试解释OpenStack的不同部分的功能，它们如何相互连接，它们提供的服务以及如何利用这些服务来获得优势。
- en: The reason OpenStack exists is because there was the need for an open source
    cloud computing platform that would enable creating public and private clouds
    that are independent of any commercial cloud platform. All parts of OpenStack
    are open source and were released under the Apache License 2.0\. The software
    was created by a large, mixed group of individuals and large cloud providers.
    Interestingly, the first major release was the result of NASA (a US government
    agency) and Rackspace Technology (a large US hosting company) joining their internal
    storage and computing infrastructure solutions. These releases were later designated
    with the names Nova and Swift, and we will cover them in more detail later.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack存在的原因是因为需要一个开源的云计算平台，可以创建独立于任何商业云平台的公共和私有云。OpenStack的所有部分都是开源的，并且根据Apache许可证2.0发布。该软件是由大型混合个人和大型云提供商创建的。有趣的是，第一个主要版本的发布是NASA（美国政府机构）和Rackspace
    Technology（美国大型托管公司）合并其内部存储和计算基础设施解决方案的结果。这些发布后来被指定为Nova和Swift，并且我们将在后面更详细地介绍它们。
- en: The first thing you will notice about OpenStack is its services since there
    is no single *OpenStack* service but an actual stack of services. The name *OpenStack*
    comes directly from this concept because it correctly identifies OpenStack as
    an open source component that acts as services that are, in turn, grouped into
    functional sets.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你将注意到关于OpenStack的第一件事是它的服务，因为没有单一的*OpenStack*服务，而是一整套服务。名称*OpenStack*直接来自这个概念，因为它正确地将OpenStack识别为一个作为服务的开源组件，这些服务又被分组成功能集。
- en: Once we understand that we are talking about autonomous services, we also need
    to understand that services in OpenStack are grouped by their function, and that
    some functions have more than one specialized service under them. We will try
    to cover as much as possible about different services in this chapter, but there
    are simply too many of them to even mention all of them here. All the documentation
    and all the whitepapers can be found at [http://openstack.org](http://openstack.org),
    and we strongly suggest that you consult it for anything not mentioned here, and
    even for things that we mention but that could have changed by the time you read
    this.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们了解到我们正在谈论自主服务，我们还需要了解OpenStack中的服务是按其功能分组的，并且某些功能下有不止一个专门的服务。我们将尽量在本章中涵盖尽可能多的不同服务，但是其中有太多的服务，甚至无法在这里提及所有。所有文档和白皮书都可以在[http://openstack.org](http://openstack.org)找到，我们强烈建议您查阅其中任何未在此处提及的内容，甚至对我们提到但在您阅读时可能已经发生变化的内容也要查阅。
- en: The last thing we need to clarify is the naming – every service in OpenStack
    has its project name and is referred to by that name in the documentation. This
    might, at first glance, look confusing since some of the names are completely
    unrelated to the specific function a particular service has in the whole project,
    but using names instead of official designators for a function is far easier once
    you start using OpenStack. Take, for example, Swift. Swift's full name is *OpenStack
    Object Store*, but this is rarely mentioned in the documentation or its implementation.
    The same goes for other services or *projects* under OpenStack, such as Nova,
    Ironic, Neutron, Keystone, and over 20 other different services.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要澄清的最后一件事是命名——OpenStack中的每个服务都有其项目名称，并且在文档中以该名称来引用。乍一看，这可能看起来令人困惑，因为其中一些名称与特定服务在整个项目中的具体功能完全无关，但一旦你开始使用OpenStack，使用名称而不是官方指示符来表示功能会更容易。例如，Swift。Swift的全名是*OpenStack对象存储*，但在文档或其实施中很少提到。其他服务或在OpenStack下的*项目*，如Nova、Ironic、Neutron、Keystone以及其他20多个不同的服务也是如此。
- en: If you step away from OpenStack for a second, then you need to consider what
    cloud services are all about. The cloud is all about scaling – in terms of compute
    resources, storage, network, APIs – whatever. But, as always in life, as you scale
    things, you're going to run into problems. And these problems have their own *names*
    and *solutions*. So, let's discuss these problems for a minute.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你暂时离开OpenStack，那么你需要考虑云服务的本质。云服务的本质就是扩展——无论是计算资源、存储、网络、API等。但是，就像生活中的一切一样，随着事物的扩展，你会遇到问题。这些问题有它们自己的*名称*和*解决方案*。所以，让我们讨论一下这些问题。
- en: 'The basic problems for cloud provider scalability can be divided into three
    groups of problems that need to be solved at scale:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商可扩展性的基本问题可以分为三组需要大规模解决的问题：
- en: '**Compute problems** (Compute = CPU + memory power): These problems are pretty
    straightforward to solve – if you need more CPU and memory power, you buy more
    servers, which, by design, means more CPU and memory. If you need a quality of
    service/**service-level agreement** (**SLA**) type of concept, we can introduce
    a concept such as compute resource pools so that we can slice the compute *pie*
    according to our needs and divide those resources between our clients. It doesn''t
    matter whether our client is just a private person or a company buying into cloud
    services. In cloud technologies, we call our clients *tenants*.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算问题**（计算=CPU+内存能力）：这些问题解决起来相当简单——如果你需要更多的CPU和内存能力，你就购买更多的服务器，这意味着更多的CPU和内存。如果你需要服务质量/**服务级别协议**（**SLA**）类型的概念，我们可以引入计算资源池的概念，这样我们可以根据需要切割计算*饼*并在我们的客户之间分配这些资源。无论我们的客户是私人还是购买云服务的公司，这都无关紧要。在云技术中，我们称我们的客户为*租户*。'
- en: '**Storage problems**: As you scale your cloud environments, things become really
    messy in terms of storage capacity, management, monitoring and – especially –
    performance. The performance side of that problem has a couple of most commonly
    used variables – read and write throughput and read and write IOPS. When you grow
    your environment from 100 hosts to 1,000 hosts or more, performance bottlenecks
    are going to become a major issue that will be difficult to tackle without proper
    concepts. So, the storage problem can be solved by adding additional storage devices
    and capacity, but it''s much more involved than the compute problem as it needs
    much more configuration and money. Remember, every virtual machine has a statistical
    influence on other virtual machines'' performance, and the more virtual machines
    you have, the greater this entropy is. This is the most difficult process to manage
    in storage infrastructure.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储问题**：随着云环境的扩展，存储容量、管理、监控以及性能方面变得非常混乱。性能问题有一些最常用的变量——读写吞吐量和读写IOPS。当你将环境从100个主机扩展到1,000个或更多时，性能瓶颈将成为一个难以解决的主要问题。因此，存储问题可以通过增加额外的存储设备和容量来解决，但它比计算问题更复杂，需要更多的配置和资金。记住，每个虚拟机对其他虚拟机的性能有统计上的影响，虚拟机越多，这种熵就越大。这是存储基础设施中最难管理的过程。'
- en: '`A` can''t communicate with the network traffic of tenant `B`. At the same
    time, you still need to offer a capability where you can have multiple networks
    (usually implemented via VLANs in non-cloud infrastructures) per tenant and routing
    between these networks, if that''s what the tenant needs.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`A`无法与租户`B`的网络流量通信。与此同时，你仍然需要提供一个能够让租户拥有多个网络（通常在非云基础设施中通过VLAN实现）并在这些网络之间进行路由的能力，如果这是租户需要的话。'
- en: This network problem is a scalability problem based on technology, as the technology
    behind VLAN was standardized years before the number of VLANs could become a scalability
    problem.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络问题是一个基于技术的可扩展性问题，因为VLAN背后的技术在VLAN数量成为可扩展性问题之前已经标准化了多年。
- en: Let's continue our journey through OpenStack by explaining the most fundamental
    subject of cloud environments, which is scaling cloud networking via **software-defined
    networking** (**SDN**). The reason for this is really simple – without SDN concepts,
    the cloud wouldn't really be scalable enough for customers to be happy, and that
    would be a complete showstopper. So, buckle up your seatbelts and let's do an
    SDN primer.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续通过解释云环境的最基本主题来了解OpenStack，即通过**软件定义网络**（**SDN**）扩展云网络。这样做的原因非常简单——没有SDN概念，云对于客户来说就不够可扩展，这将是一个完全的停滞。所以，系好你的安全带，让我们进行SDN入门。
- en: Software-defined networking
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 软件定义网络
- en: One of the straightforward stories about the cloud – at least on the face of
    it – should have been the story about cloud networking. In order to understand
    how simple this story should've been, we only need to look at one number, and
    that number is the `VLAN 1`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 云的一个直接的故事——至少表面上是这样——应该是关于云网络的故事。为了理解这个故事应该有多简单，我们只需要看一个数字，那个数字就是`VLAN 1`。
- en: So, basically, we're left with 4,093 separate logical networks in a real-life
    scenario, which is probably more than enough for the internal infrastructure of
    any given company. However, this is nowhere near enough for public cloud providers.
    The same problem applies to public cloud providers that use hybrid-cloud types
    of services to – for example – extend their compute power to the cloud.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，基本上，在现实场景中，我们剩下了4,093个单独的逻辑网络，这可能已经足够用于任何公司的内部基础设施。然而，对于公共云提供商来说，这远远不够。同样的问题也适用于使用混合云类型服务的公共云提供商，例如将他们的计算能力扩展到云端。
- en: So, let's focus on this network problem for a bit. Realistically, if we look
    at this problem from the cloud user perspective, data privacy is of utmost importance
    to us. If we look at this problem from the cloud provider perspective, then we
    want our network isolation problem to be a non-issue for our tenants. This is
    what cloud services are all about at a more basic level – no matter what the background
    complexity in terms of technology is, users have to be able to access all of the
    necessary services in as user-friendly a way as possible. Let's explain this by
    using an example.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们稍微关注一下这个网络问题。从云用户的角度来看，数据隐私对我们来说至关重要。如果从云提供商的角度来看这个问题，那么我们希望我们的网络隔离问题对我们的租户来说不成问题。这就是云服务在更基本层面上的全部意义——无论技术背景的复杂性如何，用户都必须能够以尽可能用户友好的方式访问所有必要的服务。让我们通过一个例子来解释这一点。
- en: What happens if we have 5,000 different clients (tenants) in our public cloud
    environment? What happens if every tenant needs to have five or more logical networks?
    We quickly realize that we have a big problem as cloud environments need to be
    separated, isolated, and fenced. They need to be separated from one another at
    a network level for security and privacy reasons. However, they also need to be
    routable, if a tenant needs that kind of service. On top of that, we need the
    ability to scale so that situations in which we need more than 5,000 or 50,000
    isolated networks don't bother us. And, going back to our previous point – roughly
    4,000 VLANs just isn't going to cut it.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的公共云环境中有5,000个不同的客户（租户）会发生什么？如果每个租户都需要拥有五个或更多的逻辑网络会发生什么？我们很快意识到我们有一个大问题，因为云环境需要被分隔、隔离和围栏起来。出于安全和隐私原因，它们需要在网络层面相互分隔。然而，如果租户需要那种服务，它们也需要可路由。除此之外，我们需要能够扩展，以便在需要超过5,000或50,000个隔离网络的情况下不会困扰我们。再次回到我们之前的观点——大约4,000个VLAN根本不够用。
- en: There's a reason why we said that this should have been a straightforward story.
    The engineers among us see these situations in black and white – we focus on a
    problem and try to come to a solution. And the solution seems rather simple –
    we need to extend the 12-bit VLAN ID field so that we can have more available
    logical networks. How difficult can that be?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之所以说这应该是一个简单的故事，是有原因的。我们中的工程师们将这些情况看得很黑白分明——我们专注于问题并试图找到解决方案。解决方案似乎相当简单——我们需要扩展12位VLAN
    ID字段，以便我们可以拥有更多可用的逻辑网络。这有多难呢？
- en: As it turns out, very difficult. If history teaches us anything, it's that various
    different interests, companies, and technologies compete for years for that *top
    dog* status in anything in terms of IT technology. Just think of the good old
    days of DVD+R, DVD-R, DVD+RW, DVD-RW, DVD-RAM, and so on. To simplify things a
    bit, the same thing happened here when the initial standards for cloud networking
    were introduced. We usually call these network technologies cloud overlay network
    technologies. These technologies are the basis for SDN, the principle that describes
    the way cloud networking works at a global, centralized management level. There
    are multiple standards on the market to solve this problem – VXLAN, GRE, STT,
    NVGRE, NVO3, and more.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，这是非常困难的。如果历史教会我们任何东西，那就是各种不同的利益、公司和技术会在IT技术方面争夺多年的*顶级*地位。只需想想DVD+R、DVD-R、DVD+RW、DVD-RW、DVD-RAM等旧日的情形。简化一下，当云网络的初始标准被引入时，同样的情况也发生了。我们通常将这些网络技术称为云覆盖网络技术。这些技术是SDN的基础，它描述了云网络在全球、集中管理层面上的工作方式。市场上有多种标准来解决这个问题——VXLAN、GRE、STT、NVGRE、NVO3等等。
- en: Realistically, there's no need to break them all down one by one. We are going
    to take a simpler route – we're going to describe one of them that's the most
    valuable for us in the context of today (**VXLAN**) and then move on to something
    that's considered to be a *unified* standard of tomorrow (**GENEVE**).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，没有必要一一解释它们。我们将采取更简单的方式——我们将描述其中一个在今天环境中最有价值的标准（**VXLAN**），然后转向被认为是明天*统一*标准的东西（**GENEVE**）。
- en: First, let's define what an overlay network is. When we're talking about overlay
    networks, we're talking about networks that are built on top of another network
    in the same infrastructure. The idea behind an overlay network is simple – we
    need to disentangle the physical part of the network from the logical part of
    the network. If we want to do that in absolute terms (configure everything without
    spending massive amounts of time in the CLI to configure physical switches, routers,
    and so on), we can do that as well. If we don't want to do it that way and we
    still want to work directly with our physical network environment, we need to
    add a layer of programmability to the overall scheme. Then, if we want to, we
    can interact with our physical devices and push network configuration to them
    for a more top-to-bottom approach. If we do things this way, we'll need a bit
    more support from our hardware devices in terms of capability and compatibility.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义什么是叠加网络。当我们谈论叠加网络时，我们指的是建立在同一基础设施中另一个网络之上的网络。叠加网络背后的想法很简单 - 我们需要将网络的物理部分与逻辑部分分开。如果我们想要以绝对方式进行配置（在CLI中配置物理交换机、路由器等，而不需要花费大量时间），我们也可以这样做。如果我们不想以这种方式做，而且仍然希望直接与我们的物理网络环境一起工作，我们需要在整体方案中添加一层可编程性。然后，如果我们愿意，我们可以与我们的物理设备进行交互，并向它们推送网络配置，以实现更自上而下的方法。如果我们以这种方式做事情，我们将需要更多来自我们的硬件设备的支持，以满足能力和兼容性方面的要求。
- en: Now that we've described what network overlay is, let's talk about VXLAN, one
    of the most prominent overlay network standards. It also serves as a basis for
    developing some other network overlay standards (such as GENEVE), so – as you
    might imagine – it's very important to understand how it works.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经描述了什么是网络叠加，让我们谈谈VXLAN，这是最重要的叠加网络标准之一。它还作为开发其他网络叠加标准（如GENEVE）的基础，因此 - 您可能会想象到
    - 了解其工作原理非常重要。
- en: Understanding VXLAN
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解VXLAN
- en: Let's start with the confusing part. VXLAN (IETF RFC 7348) is an extensible
    overlay network standard that enables us to aggregate and tunnel multiple Layer
    2 networks across Layer 3 networks. How does it do that? By encapsulating a Layer
    2 packet inside a Layer 3 packet. In terms of transport protocol, it uses UDP,
    by default on port `4789` (more about that in just a bit). In terms of special
    requests for VXLAN implementation – as long as your physical network supports
    MTU 1600, you can implement VXLAN as a cloud overlay solution easily. Almost all
    the switches you can buy (except for the cheap home switches, but we're talking
    about enterprises here) support jumbo frames, which means that we can use MTU
    9000 and be done with it.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从令人困惑的部分开始。VXLAN（IETF RFC 7348）是一种可扩展的叠加网络标准，使我们能够在Layer 3网络中聚合和隧道多个Layer
    2网络。它是如何做到的呢？通过在Layer 3数据包内封装Layer 2数据包。在传输协议方面，默认情况下使用UDP，在端口`4789`上（稍后会详细介绍）。在VXLAN实现的特殊请求方面
    - 只要您的物理网络支持MTU 1600，您就可以轻松实现VXLAN作为云叠加解决方案。您可以购买的几乎所有交换机（除了廉价的家用交换机，但我们在这里谈论的是企业）都支持巨型帧，这意味着我们可以使用MTU
    9000并完成。
- en: 'From the standpoint of encapsulation, let''s see what it looks like:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从封装的角度来看，让我们看看它是什么样子的：
- en: '![Figure 12.1 – VXLAN frame encapsulation'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.1 – VXLAN帧封装'
- en: '](img/B14834_12_01.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_01.jpg)'
- en: Figure 12.1 – VXLAN frame encapsulation
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 – VXLAN帧封装
- en: In more simplistic terms, VXLANs use tunneling between two VXLAN endpoints (called
    VTEPs; that is, VXLAN tunneling endpoints) that check **VXLAN network identifiers**
    (**VNIs**) so that they can decide which packets go where.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 更简单地说，VXLAN使用两个VXLAN端点（称为VTEP；即VXLAN隧道端点）之间的隧道，检查**VXLAN网络标识符**（**VNIs**），以便它们可以决定数据包的去向。
- en: If this seems complicated, then don't worry – we can simplify this. From the
    perspective of VXLAN, a VNI is the same thing as a VLAN ID is to VLAN. It's a
    unique network identifier. The difference is just the size – the VNI field has
    24 bits, compared to VLAN's 12\. That means that we have 2^24 VNIs compared to
    VLAN's 2^12\. So, VXLANs – in terms of network isolation – are VLANs squared.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这看起来很复杂，那就不用担心 - 我们可以简化这个过程。从VXLAN的角度来看，VNI与VLAN ID对VLAN的作用是一样的。它是一个唯一的网络标识符。区别只是大小
    - VNI字段有24位，而VLAN有12位。这意味着我们有2^24个VNI，而VLAN有2^12个。因此，就网络隔离而言，VXLAN是VLAN的平方。
- en: Why does VXLAN use UDP?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么VXLAN使用UDP？
- en: When designing overlay networks, what you usually want to do is reduce latency
    as much as possible. Also, you don't want to introduce any kind of overhead. When
    you consider these two basic design principles and couple that with the fact that
    VXLAN tunnels Layer 2 traffic inside Layer 3 (whatever the traffic is – unicast,
    multicast, broadcast), that literally means we should use UDP. There's no way
    around the fact that TCP's two methods – three-way handshakes and retransmissions
    – would get in the way of these basic design principles. In the simplest of terms,
    TCP would be too complicated for VXLAN as it would mean too much overhead and
    latency at scale.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计叠加网络时，通常希望尽量减少延迟。此外，您不希望引入任何形式的开销。当您考虑这两个基本设计原则，并将其与VXLAN隧道Layer 2流量封装在Layer
    3内（无论流量是单播、组播还是广播）的事实相结合时，这实际上意味着我们应该使用UDP。无论如何，TCP的两种方法 - 三次握手和重传 - 都会妨碍这些基本设计原则。简而言之，TCP对于VXLAN来说太复杂了，因为这意味着在规模上会产生太多的开销和延迟。
- en: 'In terms of VTEPs, just imagine them as two interfaces (implemented in software
    or hardware) that can encapsulate and decapsulate traffic based on VNIs. From
    a technology standpoint, VTEPs map various tenant''s virtual machines and devices
    to VXLAN segments (VXLAN-backed isolated networks), perform package inspection,
    and encapsulate/decapsulate network traffic based on VNIs. Let''s describe this
    communication with the help of the following diagram:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 就VTEP而言，只需将它们想象成两个接口（以软件或硬件实现），可以根据VNI封装和解封流量。从技术角度来看，VTEP将各种租户的虚拟机和设备映射到VXLAN段（由VXLAN支持的隔离网络），执行包检查，并根据VNI封装/解封网络流量。让我们借助以下图表描述这种通信：
- en: '![](img/B14834_12_02.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14834_12_02.jpg)'
- en: Figure 12.2 – VTEPs in unicast mode
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2-单播模式下的VTEP
- en: In our open source-based cloud infrastructure, we're going to implement cloud
    overlay networks by using OpenStack Neutron or Open vSwitch, a free, open source
    distributed switch that supports almost all network protocols that you could possibly
    think of, including the already mentioned VXLAN, STT, GENEVE, and GRE overlay
    networks.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们基于开源的云基础设施中，我们将使用OpenStack Neutron或Open vSwitch来实现云覆盖网络，后者是一个免费的开源分布式交换机，支持几乎所有你能想到的网络协议，包括前面提到的VXLAN、STT、GENEVE和GRE覆盖网络。
- en: Also, there's a kind of gentleman's agreement in place in cloud networking regarding
    not using VXLANs from `1-4999` in most use cases. The reason for this is simple
    – because we still want to have our VLANs with their reserved range of `0-4095`
    in a way that is simple and not error-prone. In other words, by design, we leave
    network IDs `0-4095` for VLANs and start VXLANs with VNI 5000 so that it's really
    easy to differentiate between the two. Not using 5,000 VXLAN-backed networks out
    of 16.7 million VXLAN-backed networks isn't that much of a sacrifice for good
    engineering practices.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在云网络中有一种绅士协议，即在大多数情况下不使用VXLANs从`1-4999`。这样做的原因很简单-因为我们仍然希望以简单且不易出错的方式保留VLAN的保留范围为`0-4095`。换句话说，按设计，我们将网络ID
    `0-4095` 留给VLAN，并以VNI 5000开始VXLAN，这样很容易区分两者。在1670万个VXLAN支持的网络中，不使用5000个VXLAN支持的网络并不是为了良好的工程实践而做出的太大牺牲。
- en: 'The simplicity, scalability, and extensibility of VXLAN also means more really
    useful usage models, such as the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: VXLAN的简单性、可扩展性和可扩展性也意味着更多真正有用的使用模型，例如以下内容：
- en: '**Stretching Layer 2 across sites**: This is one of the most common problems
    regarding cloud networking, as we will describe shortly.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在站点之间拉伸第2层**：这是关于云网络的最常见问题之一，我们将很快描述。'
- en: '**Layer 2 bridging**: Bridging a VLAN to a cloud overlay network (such as VXLAN)
    is *very* useful when onboarding our users to our cloud services as they can then
    just connect to our cloud network directly. Also, this usage model is heavily
    used when we want to physically insert a hardware device (for example, a physical
    database server or a physical appliance) into a VXLAN. If we didn''t have Layer
    2 bridging, imagine all the pain that we would have. All our customers running
    the Oracle Database Appliance would have no way to connect their physical servers
    to our cloud-based infrastructure.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第2层桥接**：将VLAN桥接到云覆盖网络（如VXLAN）在我们将用户引入我们的云服务时非常有用，因为他们可以直接连接到我们的云网络。此外，当我们想要将硬件设备（例如物理数据库服务器或物理设备）插入VXLAN时，这种使用模型也被广泛使用。如果没有第2层桥接，想象一下我们将会有多么痛苦。我们所有运行Oracle数据库设备的客户将无法将他们的物理服务器连接到我们基于云的基础设施。'
- en: '**Various offloading technologies**: These include load balancing, antivirus,
    vulnerability and antimalware scanning, firewall, IDS, IPS integration, and so
    on. All of these technologies enable us to have useful, secure environments with
    simple management concepts.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**各种卸载技术**：包括负载平衡、防病毒、漏洞和恶意软件扫描、防火墙、IDS、IPS集成等。所有这些技术使我们能够拥有简单管理概念的有用、安全的环境。'
- en: We mentioned that stretching Layer 2 across sites is a fundamental problem,
    so it's obvious that we need to discuss it. We'll do that next. Without a solution
    to this problem, you'd have very little chance of creating multiple data center
    cloud infrastructures efficiently.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到在站点之间拉伸第2层是一个基本问题，因此很明显我们需要讨论它。我们将在下一节讨论。如果没有解决这个问题的方法，你几乎没有机会有效地创建多个数据中心的云基础设施。
- en: Stretching Layer 2 across sites
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在站点之间拉伸第2层
- en: One of the most common sets of problems that cloud providers face is how to
    stretch their environment across sites or continents. In the past, when we didn't
    have concepts such as VXLAN, we were forced to use some kind of Layer 2 VPN or
    MPLS-based technologies. These types of services are really expensive, and sometimes,
    our service providers aren't exactly happy with our *give me MPLS* or *give me
    Layer 2 access* requests. They would be even less happy if we mentioned the word
    *multicast* in the same sentence, and this was a set of technical criteria that
    was *often* used in the past. So, having the capability to deliver Layer 2 over
    Layer 3 fundamentally changes that conversation. Basically, if you have the capability
    to create a Layer 3-based VPN between sites (which you can almost always do),
    you don't have to be bothered with that discussion at all. Also, that significantly
    reduces the price of these types of infrastructure connections.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 云提供商面临的最常见一组问题之一是如何在站点或大陆之间扩展其环境。在过去，当我们没有诸如VXLAN之类的概念时，我们被迫使用某种第2层VPN或基于MPLS的技术。这些类型的服务非常昂贵，有时，我们的服务提供商并不完全满意我们的*给我MPLS*或*给我第2层访问*的要求。如果我们在同一句中提到*组播*这个词，他们会更不高兴，这在过去是一组经常使用的技术标准。因此，具备通过第3层传输第2层的能力从根本上改变了这种对话。基本上，如果你有能力在站点之间创建基于第3层的VPN（你几乎总是可以做到的），你就不必再讨论这个问题。此外，这显著降低了这些类型基础设施连接的价格。
- en: 'Consider the following multicast-based example:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下基于组播的示例：
- en: '![Figure 12.3 – Extending VXLAN segments across sites in multicast mode'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.3-在组播模式下跨站扩展VXLAN段'
- en: '](img/B14834_12_03.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_03.jpg)'
- en: Figure 12.3 – Extending VXLAN segments across sites in multicast mode
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3-在组播模式下跨站扩展VXLAN段
- en: Let's say that the left-hand side of this diagram is the first site and that
    the right-hand side of this diagram is the second site. From the perspective of
    `VM1`, it doesn't really matter that `VM4` is in some other remote site as its
    segment (VXLAN 5001) *spans* across those sites. How? As long as the underlying
    hosts can communicate with each other over the VXLAN transport network (usually
    via the management network as well), the VTEPs from the first site can *talk*
    to the VTEPs from the second site. This means that virtual machines that are backed
    by VXLAN segments in one site can talk to the same VXLAN segments in the other
    site by using the aforementioned Layer 2-to-Layer 3 encapsulation. This is a really
    simple and elegant way to solve a complex and costly problem.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设这个图表的左侧是第一个站点，右侧是第二个站点。从`VM1`的角度来看，`VM4`在其他远程站点并不重要，因为它的段（VXLAN 5001）*跨越*这些站点。怎么做？只要底层主机可以通过VXLAN传输网络相互通信（通常也通过管理网络），第一个站点的VTEP可以与第二个站点的VTEP进行*通信*。这意味着在一个站点由VXLAN段支持的虚拟机可以通过上述的第2层到第3层封装与另一个站点中相同的VXLAN段进行通信。这是解决一个复杂且昂贵问题的一个非常简单而优雅的方法。
- en: We mentioned that VXLAN, as a technology, served as a basis for developing some
    other standards, with the most important being GENEVE. As most manufacturers work
    toward GENEVE compatibility, VXLAN will slowly but surely disappear. Let's discuss
    what the purpose of the GENEVE protocol is and how it aims to become *the standard*
    for cloud overlay networking.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到，作为一种技术，VXLAN作为开发其他标准的基础，其中最重要的是GENEVE。随着大多数制造商朝着GENEVE兼容性迈进，VXLAN将慢慢消失。让我们讨论一下GENEVE协议的目的，以及它如何成为云覆盖网络的*标准*。
- en: Understanding GENEVE
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解GENEVE
- en: The basic problem that we touched upon earlier is the fact that history kind
    of repeated itself in cloud overlay networks, as it did many times before. Different
    standards, different firmwares, and different manufacturers supporting one standard
    over another, where all of the standards are incredibly similar but still not
    compatible with each other. That's why VMware, Microsoft, Red Hat, and Intel proposed
    GENEVE, a new cloud overlay standard that only defines the encapsulation data
    format, without interfering with the control planes of these technologies, which
    are fundamentally different. For example, VXLAN uses a 24-bit field width for
    VNI, while STT uses 64-bit. So, the GENEVE standard proposes no fixed field size
    as you can't possibly know what the future brings. Also, taking a look at the
    existing user base, we can still happily use our VXLANs as we don't believe that
    they will be influenced by future GENEVE deployments.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到的基本问题是，云覆盖网络中的历史重演了很多次。不同的标准，不同的固件，不同的制造商支持一种标准而不是另一种，所有这些标准都非常相似，但仍然不兼容。这就是为什么VMware、微软、红帽和英特尔提出了GENEVE，这是一个新的云覆盖标准，只定义封装数据格式，而不干涉这些技术的控制平面，这些技术在根本上是不同的。例如，VXLAN使用24位字段宽度的VNI，而STT使用64位。因此，GENEVE标准不提出固定的字段大小，因为你不可能知道未来会发生什么。此外，从现有用户群的角度来看，我们仍然可以愉快地使用我们的VXLAN，因为我们不认为它们会受到未来GENEVE部署的影响。
- en: 'Let''s see what the GENEVE header looks like:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看GENEVE标头是什么样的：
- en: '![Figure 12.4 – GENEVE cloud overlay network header'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.4-GENEVE云覆盖网络标头'
- en: '](img/B14834_12_04.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_04.jpg)'
- en: Figure 12.4 – GENEVE cloud overlay network header
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4-GENEVE云覆盖网络标头
- en: The authors of GENEVE learned from some other standards (BGP, IS-IS, and LLDP)
    and decided that the key to doing things right is extensibility. This is why it
    was embraced by the Linux community in Open vSwitch and VMware in NSX-T. VXLAN
    is supported as the network overlay technology for **Hyper-V Network Virtualization**
    (**HNV**) since Windows Server 2016 as well. Overall, GENEVE and VXLAN seem to
    be two technologies that are surely here to stay – and both are supported nicely
    from the perspective of OpenStack.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: GENEVE的作者从其他一些标准（BGP、IS-IS和LLDP）中学到了一些东西，并认为做正确的事情的关键是可扩展性。这就是为什么它被Linux社区在Open
    vSwitch和VMware在NSX-T中采用。自Windows Server 2016以来，VXLAN也被支持为**Hyper-V网络虚拟化**（**HNV**）的网络覆盖技术。总的来说，GENEVE和VXLAN似乎是两种肯定会留下来的技术-从OpenStack的角度来看，两者都得到了很好的支持。
- en: Now that we've covered the most basic problem regarding the cloud – cloud networking
    – we can go back and discuss OpenStack. Specifically, our next subject is related
    to OpenStack components – from Nova through to Glance and then to Swift, and others.
    So, let's get started.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经解决了云的最基本问题-云网络-我们可以回过头来讨论OpenStack。具体来说，我们下一个主题与OpenStack组件有关-从Nova到Glance，然后到Swift，以及其他组件。所以，让我们开始吧。
- en: OpenStack components
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OpenStack组件
- en: 'When OpenStack was first formed as a project, it was designed from two different
    services:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当OpenStack最初作为一个项目形成时，它是从两种不同的服务设计的：
- en: A computing service that was designed to manage and run virtual machines themselves
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个计算服务，旨在管理和运行虚拟机本身
- en: A storage service that was designed for large-scale object storage
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个旨在进行大规模对象存储的存储服务
- en: These services are now called OpenStack Compute or *Nova*, and OpenStack Object
    Store or *Swift*. These services were later joined by *Glance* or the OpenStack
    Image service, which was designed to simplify working with disk images. Also,
    after our SDN primer, we need to discuss OpenStack Neutron, the **Network-as-a-Service**
    (**NaaS**) component of OpenStack.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '这些服务现在被称为OpenStack Compute或*Nova*，以及OpenStack Object Store或*Swift*。这些服务后来又加入了*Glance*或OpenStack镜像服务，旨在简化与磁盘映像的工作。此外，在我们的SDN入门之后，我们需要讨论OpenStack
    Neutron，OpenStack的**网络即服务**（**NaaS**）组件。 '
- en: 'The following diagram shows the components of OpenStack:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了OpenStack的组件：
- en: '![Figure 12.5 – Conceptual architecture of OpenStack (source: https://docs.openstack.org/)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.5-OpenStack的概念架构（来源：https://docs.openstack.org/）'
- en: '](img/B14834_12_05.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_05.jpg)'
- en: 'Figure 12.5 – Conceptual architecture of OpenStack (source: https://docs.openstack.org/)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5-OpenStack的概念架构（来源：https://docs.openstack.org/）
- en: We'll go through these in no particular order and will include additional services
    that are important. Let's start with **Swift**.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照没有特定顺序进行介绍，并包括其他重要的服务。让我们从**Swift**开始。
- en: Swift
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Swift
- en: 'The first service we need to talk about is Swift. For that purpose, we are
    going to grab the project''s own definition from the OpenStack official documentation
    and parse it to try and explain what services are fulfilled by this project, [and
    what is it used for. The Swift webs](https://docs.openstack.org/swift/latest/)ite
    ([https://docs.openstack.org/swift/latest/](https://docs.openstack.org/swift/latest/))
    states the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要谈论的第一个服务是Swift。为此，我们将从OpenStack官方文档中获取项目的定义，并解析它，以尝试解释这个项目实现了哪些服务，[以及它的用途。Swift网站](https://docs.openstack.org/swift/latest/)
    ([https://docs.openstack.org/swift/latest/](https://docs.openstack.org/swift/latest/))中陈述了以下内容：
- en: '*"Swift is a highly available, distributed, eventually consistent object/blob
    store. Organizations can use Swift to store lots of data efficiently, safely,
    and cheaply. It''s built for scale and optimized for durability, availability,
    and concurrency across the entire dataset. Swift is ideal for storing unstructured
    data that can grow without bounds."*'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*"Swift是一个高度可用的、分布式的、最终一致的对象/大块存储。组织可以使用Swift高效、安全、廉价地存储大量数据。它专为规模而构建，并针对整个数据集的耐用性、可用性和并发性进行了优化。Swift非常适合存储可以无限增长的非结构化数据。"*'
- en: Having read that, we need to point out quite a few things that may be completely
    new to you. First and foremost, we are talking about storing data in a particular
    way that is not common in computing unless you have used unstructured data stores.
    Unstructured does not mean that this way of storing data is lacking structure;
    in this context, it means that we are the ones that are defining the structure
    of the data, but the service itself does not care about our structure, instead
    relying on the concept of objects to store our data. One result of this is something
    that may also sound unusual at first, and that is that the data we store in Swift
    is not directly accessible through any filesystem, or any other way we are used
    to manipulating files through our machines. Instead, we are manipulating data
    as objects and we must use the API that is provided as part of Swift to get the
    data objects. Our data is stored in *blobs*, or objects, that the system itself
    just labels and stores to take care of availability and access speed. We are supposed
    to know what the internal structure of our data is and how to parse it. On the
    other hand, because of this approach, Swift can be amazingly fast with any amount
    of data and scales horizontally in a way that is almost impossible to achieve
    using normal, classic databases.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 读完这段话后，我们需要指出一些可能对你来说完全新的事情。首先，我们谈论的是以一种特定的方式存储数据，这在计算中并不常见，除非你使用过非结构化数据存储。非结构化并不意味着这种存储数据的方式缺乏结构；在这个上下文中，它意味着我们定义数据的结构，但服务本身不关心我们的结构，而是依赖于对象的概念来存储我们的数据。这种方式的一个结果是，我们存储在Swift中的数据不能直接通过任何文件系统或我们习惯通过机器操作文件的其他方式直接访问。相反，我们是以对象的方式操作数据，必须使用Swift提供的API来获取数据对象。我们的数据存储在*大块*或对象中，系统本身只是标记和存储以确保可用性和访问速度。我们应该知道我们的数据的内部结构以及如何解析它。另一方面，由于这种方法，Swift可以以惊人的速度处理任意数量的数据，并且以一种几乎不可能使用普通的经典数据库实现的方式进行水平扩展。
- en: 'Another thing worth mentioning is that this service offers highly available,
    distributed, and *eventually consistent* storage. This means that, first and foremost,
    the priority is for the data to be distributed and highly available, which are
    two things that are important in the cloud. Consistency comes after that but is
    eventually achieved. Once you come to use this service, you will understand what
    that means. In almost all usual scenarios where data is read and rarely written,
    it is nothing to even think about, but there are some cases where this can change
    the way we need to think about the way we go about delivering the service. The
    documentation states the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，这项服务提供了高度可用、分布式和*最终一致*的存储。这意味着，首先，优先级是数据分布和高可用性，这两点在云中非常重要。一致性在此之后，但最终会实现。一旦你开始使用这项服务，你就会明白这意味着什么。在几乎所有通常的情况下，数据被读取而很少被写入，这根本不值得考虑，但也有一些情况下，这可能改变我们需要思考如何提供服务的方式。文档中陈述了以下内容：
- en: '*"Because each replica in Object Storage functions independently and clients
    generally require only a simple majority of nodes to respond to consider an operation
    successful, transient failures such as network partitions can quickly cause replicas
    to diverge. These differences are eventually reconciled by asynchronous, peer-to-peer
    replicator processes. The replicator processes traverse their local filesystems
    and concurrently perform operations in a manner that balances load across physical
    disks."*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*"因为对象存储中的每个副本都是独立运行的，客户端通常只需要大多数节点简单响应即可认为操作成功，瞬时故障如网络分区可能会迅速导致副本发散。这些差异最终由异步的点对点复制进程协调一致。复制进程遍历其本地文件系统，并以一种平衡负载的方式同时执行操作。"*'
- en: We can roughly translate this. Let's say that you have a three-node Swift cluster.
    In such a scenario, a Swift object will become available to clients after the
    `PUT` operation has been confirmed to have been completed on at least two nodes.
    So, if your goal is to create a low-latency, synchronous storage replication with
    Swift, there are other solutions available for that.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以粗略地翻译一下。假设你有一个三节点的Swift集群。在这种情况下，Swift对象在`PUT`操作至少在两个节点上确认已完成后，才会对客户端可用。因此，如果你的目标是创建一个低延迟、同步的Swift存储复制，那么还有其他解决方案可供选择。
- en: Having put aside all the abstract promises regarding what Swift offers, let's
    go into more details. High availability and distribution are the direct result
    of using a concept of *zones* and having multiple copies of the same data written
    onto multiple storage servers. Zones are nothing but a simple way of logically
    dividing the storage resources we have at our disposal and deciding on what kind
    of isolation we are ready to provide, as well as what kind of redundancy we need.
    We can group servers by the server itself, by the rack, by sets of servers across
    a Datacenter, in groups across different Datacenters, and in any combination of
    those. Everything really depends on the amount of available resources and the
    data redundancy and availability we need and want, as well as, of course, the
    cost that will accompany our configuration.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在搁置了有关Swift提供的所有抽象承诺之后，让我们进一步详细讨论一下。高可用性和分布是使用“区域”概念以及将相同数据写入多个存储服务器的直接结果。区域只是一种简单的逻辑划分我们可用的存储资源的方式，并决定我们愿意提供的隔离类型以及我们需要的冗余类型。我们可以按照服务器本身、机架、数据中心内的服务器集、跨不同数据中心的组以及任何这些的组合来对服务器进行分组。一切都取决于可用资源的数量以及我们需要和想要的数据冗余和可用性，当然还有伴随我们配置的成本。
- en: Based on the resources we have, we are supposed to configure our storage system
    in terms of how many copies it will hold and how many zones we are prepared to
    use. A copy of a particular data object in Swift is referred to as a *replica*,
    and currently, the best practices call for at least three replicas across no less
    than five zones.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们拥有的资源，我们应该根据它将我们的存储系统配置为它将保存多少副本以及我们准备使用多少区域。Swift中特定数据对象的副本被称为*副本*，目前，最佳实践要求至少在不少于五个区域中拥有至少三个副本。
- en: A zone can be a server or a set of servers, and if we configure everything correctly,
    losing any one zone should have no impact on the availability or distribution
    of data. Since a zone can be as small as a server and as big as any number of
    data centers, the way we structure our zones has a huge impact on the way the
    system reacts to any failures and changes. The same goes for replicas. In the
    recommended scenario, configuration has a smaller number of replicas than the
    number of zones, so only some of the zones will hold some of these replicas. This
    means the system must balance the way data is written in order to evenly distribute
    both the data and the load, including both the writing and the reading load for
    the data. At the same time, the way we structure the zones will have an enormous
    impact on the cost – redundancy has a real cost in terms of server and storage
    hardware, and multiplying replicas and zones creates additional demands in regard
    to how much storage and computing power we need to allocate for our OpenStack
    installation. Being able to do this correctly is the biggest problem that a Datacenter
    architect has to solve.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 区域可以是服务器或一组服务器，如果我们正确配置了一切，失去任何一个区域对数据的可用性或分布都不会产生影响。由于区域可以小到一个服务器，大到任意数量的数据中心，我们构建区域的方式对系统对任何故障和变化的反应有巨大影响。副本也是如此。在推荐的方案中，配置的副本数量比区域数量少，因此只有一些区域将持有这些副本。这意味着系统必须平衡数据的写入方式，以均匀分布数据和负载，包括数据的写入和读取负载。同时，我们构建区域的方式将对成本产生巨大影响-冗余在服务器和存储硬件方面具有实际成本，而增加副本和区域会对我们OpenStack安装需要分配多少存储和计算能力提出额外要求。能够正确做到这一点是数据中心架构师必须解决的最大问题。
- en: Now, we need to go back to the concept of eventual consistency. Eventual consistency
    in this context means that data is going to be written to the Swift store and
    that objects are going to get updated, but the system will not be able to do a
    completely simultaneous write of all the data into all the copies (replicas) of
    the data across all zones. Swift will try to reconcile the differences as soon
    as possible and will be aware of these changes, so it serves new versions of the
    objects to whoever tries to read them. Scenarios where data is inconsistent due
    to a failure of some part of the system exist, but they are to be considered abnormal
    states of the system and need to be repaired rather than the system being designed
    to ignore them.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要回到最终一致性的概念。在这个背景下，最终一致性意味着数据将被写入Swift存储，并且对象将被更新，但系统将无法完全同时将所有数据写入所有副本中。Swift将尽快协调差异，并意识到这些变化，因此为尝试读取它们的任何人提供对象的新版本。由于系统的某些部分失败而导致数据不一致的情况存在，但它们应被视为系统的异常状态，并需要修复，而不是系统被设计为忽略它们。
- en: Swift daemons
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Swift守护程序
- en: 'Next, we need to talk about the way Swift is designed in regard to its architecture.
    Data is managed through three separate logical daemons:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要讨论Swift在架构方面的设计。数据通过三个独立的逻辑守护程序进行管理：
- en: '**Swift-account** is used to manage a SQL database that contains all the accounts
    defined with the object storage service. Its main task is to read and write the
    data that all the other services need, primarily in order to validate and find
    appropriate authentication and other data.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Swift-account**用于管理包含所有定义的对象存储服务帐户的SQL数据库。它的主要任务是读取和写入所有其他服务需要的数据，主要是为了验证和查找适当的身份验证和其他数据。'
- en: '**Swift-container** is another database process, but it is used strictly to
    map data into containers, a logical structure similar to AWS *buckets*. This can
    include any number of objects that are grouped together.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Swift-container**是另一个数据库进程，但它严格用于将数据映射到容器中，这是一种类似于AWS *buckets*的逻辑结构。这可以包括任意数量的对象，它们被分组在一起。'
- en: '**Swift-object** manages mapping to actual objects, and it keeps track of the
    location and availability of the objects themselves.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Swift-object**管理到实际对象的映射，并跟踪对象本身的位置和可用性。'
- en: 'All these daemons are just in charge of data and make sure that everything
    is both mapped and replicated correctly. Data is used by another layer in the
    architecture: the presentation layer.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些守护程序都负责数据，并确保一切都被正确映射和复制。数据被架构的另一层使用：表示层。
- en: When a user wants to use any data object, it first needs to authenticate via
    a token that can be either externally provided or created by an authentication
    system inside Swift. After that, the main process that orchestrates data retrieval
    is Swift-proxy, which handles communication with three daemons that deal with
    the data. Provided that the user presented a valid token, it gets the data object
    delivered to the user request.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户想要使用任何数据对象时，首先需要通过一个令牌进行身份验证，这个令牌可以是外部提供的，也可以是由Swift内部的身份验证系统创建的。之后，编排数据检索的主要过程是Swift-proxy，它处理与处理数据的三个守护程序的通信。只要用户提供了有效的令牌，就可以将数据对象交付给用户请求。
- en: This is just the briefest of overviews regarding how Swift works. In order to
    understand this, you need to not only read the documentation but also use some
    kind of system that will perform low-level object retrieval and storage into and
    out of Swift.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是关于Swift工作原理的最简要的概述。为了理解这一点，你不仅需要阅读文档，还需要使用某种系统来执行低级对象的检索和存储到Swift中。
- en: Cloud services can't be scaled or used efficiently if we don't have orchestration
    services, which is why we need to discuss the next service on our list – **Nova**.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有编排服务，云服务就无法扩展或高效使用，这就是为什么我们需要讨论我们列表上的下一个服务 - **Nova**。
- en: Nova
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Nova
- en: 'Another important service or project is Nova – an orchestration service that
    is used for providing both provisioning and management for computing instances
    at a large scale. What it basically does is allow us to use an API structure to
    directly allocate, create, reconfigure, and delete or *destroy* virtual servers.
    The following is a diagram of a logical Nova service structure:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的服务或项目是Nova - 一个编排服务，用于在大规模上提供计算实例的供应和管理。它的基本作用是允许我们使用API结构直接分配、创建、重新配置和删除或*销毁*虚拟服务器。以下是Nova服务结构的逻辑图：
- en: '![Figure 12.6 – Logical structure of the Nova service (openstack.org)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.6 - Nova服务的逻辑结构（openstack.org）'
- en: '](img/B14834_12_06.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_06.jpg)'
- en: Figure 12.6 – Logical structure of the Nova service (openstack.org)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6 - Nova服务的逻辑结构（openstack.org）
- en: Most of Nova is a very complex distributed system written almost entirely in
    Python that consists of a number of working scripts that do the orchestration
    part and a gateway service that receives and carries through API calls. The API
    is also based on Python; it's a **Web Server Gateway Interface** (**WSGI**)-compatible
    application that handles calls. WSGI, in turn, is a standard that defines how
    a web application and a server should exchange data and commands. This means that,
    in theory, any system capable of using the WSGI standard can also establish communication
    with this service.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分Nova是一个非常复杂的分布式系统，几乎完全由Python编写，包括一些执行编排部分的工作脚本和一个接收和执行API调用的网关服务。API也是基于Python的；它是一个**Web服务器网关接口**（**WSGI**）兼容的应用程序，用于处理调用。而WSGI则是定义Web应用程序和服务器应该如何交换数据和命令的标准。这意味着理论上，任何能够使用WSGI标准的系统也可以与这个服务建立通信。
- en: Aside from this multifaceted orchestration solution, there are two more services
    that are at the heart of Nova – the database and messaging queue. Neither of these
    is Python-based. We'll talk about messaging and databases first.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这个多方面的编排解决方案，还有另外两个服务是Nova的核心 - 数据库和消息队列。这两者都不是基于Python的。我们将首先讨论消息传递和数据库。
- en: Almost all distributed systems must rely on queues to be able to perform their
    tasks. Messages need to be forwarded to a central place that will enable all daemons
    to do their tasks, and using the right messaging and queueing system is crucial
    for system speed and reliance. Nova currently uses RabbitMQ, a highly scalable
    and available system by itself. Using a production-ready system like this means
    that not only are there tools to debug the system itself, but there are a lot
    of reporting tools available for directly querying the messaging queue.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有分布式系统都必须依赖队列来执行它们的任务。消息需要被转发到一个中央位置，这将使所有守护程序能够执行它们的任务，使用正确的消息传递和排队系统对于系统的速度和可靠性至关重要。Nova目前使用RabbitMQ，这是一个高度可扩展和可用的系统。使用这样一个生产就绪的系统意味着不仅有工具来调试系统本身，还有很多报告工具可用于直接查询消息队列。
- en: The main purpose of using a messaging queue is to completely decouple any clients
    from servers, and to provide asynchronous communication between different clients.
    There is a lot to be said on how the actual messaging works, but for this chapter,
    we will just refer you to the official documentation at [https://docs.openstack.org/nova/latest/](https://docs.openstack.org/nova/latest/),
    since we are not talking about a couple of functions on a server but an entirely
    independent software stack.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用消息队列的主要目的是完全解耦任何客户端和服务器，并在不同客户端之间提供异步通信。关于实际消息传递的工作有很多要说的，但在本章中，我们将只是引用官方文档[https://docs.openstack.org/nova/latest/](https://docs.openstack.org/nova/latest/)，因为我们不是在谈论服务器上的一些功能，而是一个完全独立的软件堆栈。
- en: The database is in charge of holding all the state data for the tasks currently
    being performed, as well as enabling the API to return information about the current
    state of different parts of Nova.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库负责保存当前正在执行的任务的所有状态数据，并使API能够返回有关Nova不同部分当前状态的信息。
- en: 'All in all, the system consists of the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，系统包括以下内容：
- en: '`nova-api` is actually referring to this daemon, sometimes calling it just
    *API*, *controller*, or *cloud controller*. We need to explain a little bit more
    about Nova in order to understand that calling nova-api a controller is wrong,
    but since there exists a class inside a daemon named `CloudController`, a lot
    of users confuse this daemon for the whole distributed system.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nova-api`实际上是指这个守护程序，有时称其为API、控制器或云控制器。我们需要更详细地解释一下Nova，以便理解将nova-api称为控制器是错误的，但由于守护程序中存在一个名为CloudController的类，许多用户将这个守护程序误认为是整个分布式系统。'
- en: nova-api is a powerful system since it can, by itself, process and sort out
    some API calls, getting the data from the database and working out what needs
    to be done. In a more common case, nova-api will just initiate a task and forward
    it in the form of messages to other daemons inside Nova.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: nova-api是一个强大的系统，因为它可以自行处理和整理一些API调用，从数据库获取数据并找出需要做什么。在更常见的情况下，nova-api将只是启动一个任务，并以消息的形式转发给Nova内的其他守护程序。
- en: Another important daemon is the **scheduler**. Its main function is to go through
    the queue and determine when and where a particular request should run. This sounds
    simple enough, but given the possible complexity of the system, this *where and
    when* can lead to extreme gains or losses in performance. In order to solve this,
    we can choose how the scheduler makes decisions regarding choosing the right place
    to perform requests. Users can choose either to write their own request or to
    use one of the predetermined ones.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个重要的守护程序是调度程序。它的主要功能是浏览队列，并确定特定请求应该在何时何地运行。这听起来足够简单，但鉴于系统可能的复杂性，这个“何时何地”可能导致性能的极端增益或损失。为了解决这个问题，我们可以选择调度程序在选择执行请求的正确位置时如何做出决策。用户可以选择编写自己的请求，也可以使用预定的请求。
- en: 'If we are choosing the ones provided by Nova, we have three choices:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们选择Nova提供的存储卷，我们有三种选择：
- en: a) **Simple scheduler** determines where the request will be run based on the
    load on the hosts – it will monitor all the hosts and try to allocate the one
    that has the least load in a particular slice of time.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: a) 简单调度程序根据主机的负载确定请求将在哪里运行 - 它将监视所有主机，并尝试分配在特定时间片内负载最小的主机。
- en: b) **Chance** is the default way of scheduling. As its name suggests, it's the
    simplest algorithm – a host is randomly chosen from the list and given the request.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: b) **Chance**是默认的调度方式。顾名思义，这是最简单的算法 - 从列表中随机选择一个主机并给出请求。
- en: c) **Zone scheduling** will also randomly choose a host but will do so from
    within a zone.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: c) 区域调度也会随机选择主机，但是会在区域内进行选择。
- en: 'Now, we will look at *workers*, daemons that actually perform requests. There
    are three of these – network, volume, and compute:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将看一下*workers*，实际执行请求的守护程序。这些守护程序有三个 - 网络、存储和计算。
- en: '**nova-network** is in charge of the network. It will perform whatever is given
    to it from the queue that is related to anything on the network and will create
    interfaces and rules as needed. It is also in charge of IP address allocation;
    it will allocate both fixed and dynamically assigned addresses and take care of
    both the external and internal networks. Instances usually use one or more fixed
    IPs to enable management and connectivity, and these are usually local addresses.
    There are also floating addresses to enable connecting from the outside. This
    service has been obsolete since the OpenStack Newton release from 2016, although
    you can still use it in some legacy configurations.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**nova-network**负责网络。它将执行与网络相关的队列中的任何任务，并根据需要创建接口和规则。它还负责IP地址分配；它将分配固定和动态分配的地址，并处理外部和内部网络。实例通常使用一个或多个固定IP来实现管理和连接，这些通常是本地地址。还有浮动地址用于从外部进行连接。自2016年OpenStack
    Newton版本发布以来，这项服务已经过时，尽管在一些传统配置中仍然可以使用。'
- en: '**nova-volume** handles storage volumes or, to be more precise, all the ways
    data storage can be connected to any instance. This includes standards such as
    iSCSI and AoE, which are targeted at encapsulating known common protocols, and
    providers such as Sheepdog, LeftHand, and RBD, which cover connections to open
    source and closed source storage systems such as CEPH or HP LeftHand.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: nova-volume处理存储卷，或者更准确地说，处理数据存储与任何实例连接的所有方式。这包括诸如iSCSI和AoE之类的标准，这些标准旨在封装已知的常见协议，以及诸如Sheepdog、LeftHand和RBD之类的提供者，这些提供者涵盖了与开源和闭源存储系统（如CEPH或HP
    LeftHand）的连接。
- en: '`nova-compute` must adapt itself to using different virtualization technologies
    and to completely different platforms. It also needs to be able to dynamically
    allocate and free resources. Primarily, it uses libvirt for its VM management,
    directly supporting KVM for creating and deleting new instances. This is the reason
    this chapter exists, since nova-compute using libvirt to start KVM machines is
    by far the most common way of configuring OpenStack, but support for different
    technologies extends a lot further. The libvirt interface also supports Xen, QEMU,
    LXC, and **user mode Linux** (**UML**), and through different APIs, nova-compute
    can support Citrix, XCP, VMware ESX/ESXi vSphere, and Microsoft Hyper-V. This
    enables Nova to control all the currently used enterprise virtualization solutions
    from one central API.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nova-compute`必须适应不同的虚拟化技术和完全不同的平台。它还需要能够动态分配和释放资源。主要使用libvirt进行VM管理，直接支持KVM创建和删除新实例。这就是本章存在的原因，因为nova-compute使用libvirt启动KVM机器是配置OpenStack的最常见方式，但对不同技术的支持范围更广。libvirt接口还支持Xen、QEMU、LXC和用户模式Linux（UML），通过不同的API，nova-compute可以支持Citrix、XCP、VMware
    ESX/ESXi vSphere和Microsoft Hyper-V。这使得Nova能够从一个中央API控制所有当前使用的企业虚拟化解决方案。'
- en: As a side note, **nova-conductor** is there to process requests that require
    any conversion regarding objects, resizing, and database/proxy access.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个旁注，nova-conductor用于处理需要对对象、调整大小和数据库/代理访问进行任何转换的请求。
- en: The next service on our list is **Glance** – a service that is very important
    for virtual machine deployment as we want to do this from images. Let's discuss
    Glance now.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列表中的下一个服务是**Glance** - 这是对虚拟机部署非常重要的服务，因为我们希望从图像中进行部署。现在让我们讨论一下Glance。
- en: Glance
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Glance
- en: At first, having a separate service for cloud disk image management makes little
    sense, but when scaling any infrastructure, image management will become a problem
    that needs an API to be solved. Glance basically has this dual identity – it can
    be used to directly manipulate VM images and store them inside blobs of data,
    but at the same time it can be used to completely automatically orchestrate a
    lot of tasks when dealing with a huge number of images.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，为云磁盘图像管理单独设置一个服务似乎没有多大意义，但是在扩展任何基础架构时，图像管理将成为需要API解决的问题。Glance基本上具有这种双重身份-它可以用于直接操作VM图像并将它们存储在数据块中，但同时也可以用于在处理大量图像时完全自动地编排许多任务。
- en: Glance is relatively simple in terms of its internal structure as it consists
    of an image information database, an image store that uses Swift (or a similar
    service), and an API that glues everything together. Database is sometimes called
    Registry, and it basically gives information about a given image. Images themselves
    can be stored on different types of stores, either from Swift (as blobs) on HTTP
    servers or on a filesystem (such as NFS).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Glance在内部结构方面相对简单，因为它包括图像信息数据库，使用Swift（或类似服务）的图像存储以及将所有内容粘合在一起的API。数据库有时被称为注册表，它基本上提供了有关给定图像的信息。图像本身可以存储在不同类型的存储器上，可以是来自Swift（作为blob）的HTTP服务器上或文件系统（如NFS）上。
- en: Glance is completely nonspecific about the type of image store it uses, so NFS
    is perfectly okay and makes implementing OpenStack a little bit easier, but when
    scaling OpenStack, both Swift and Amazon S3 can be used.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Glance对其使用的图像存储类型完全不加限制，因此NFS是完全可以的，并且使得实施OpenStack变得更加容易，但是在扩展OpenStack时，可以使用Swift和Amazon
    S3。
- en: When thinking about the place in the big OpenStack puzzle that Glance belongs
    to, we could describe it as being the service that Nova uses to find and instantiate
    images. Glance itself uses Swift (or any other storage) to store images. Since
    we are dealing with multiple architectures, we need a lot of different supported
    file formats for images, and Glance does not disappoint. Every disk format that
    is supported by different virtualization engines is supported by Glance. This
    includes both unstructured formats such as `raw` and structured formats such as
    VHD, VMDK, `qcow2`, VDI ISO, and AMI. OVF – as an example of an image container
    – is also supported.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑Glance在大型OpenStack拼图中所属的位置时，我们可以将其描述为Nova用来查找和实例化图像的服务。Glance本身使用Swift（或任何其他存储）来存储图像。由于我们处理多种架构，因此需要支持图像的许多不同文件格式，而Glance并不令人失望。每种受不同虚拟化引擎支持的磁盘格式都受到Glance的支持。这包括无结构格式，如`raw`和结构化格式，如VHD、VMDK、`qcow2`、VDI
    ISO和AMI。例如，作为图像容器的OVF也受到支持。
- en: Glance probably has the simplest API of them all, enabling it to be used even
    from the command line using curl to query the server and JSON as the format of
    the messages.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Glance可能拥有所有API中最简单的API，使其甚至可以使用curl从命令行查询服务器，并使用JSON作为消息的格式。
- en: 'We''ll finish this section with a small note directly from the Nova documentation:
    it explicitly states that everything in OpenStack is designed to be horizontally
    scalable but that, at any time, there should be significantly more computing nodes
    than any other type. This actually makes a lot of sense – computing nodes are
    the ones in charge of actually accepting and working on requests. The amount of
    storage nodes you''ll need will depend on your usage scenario, and Glance''s will
    inevitably depend on the capabilities and resources available to Swift.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以一条小提示直接从Nova文档中结束本节：它明确指出OpenStack中的一切都设计为水平可扩展，但是在任何时候，计算节点的数量应该明显多于任何其他类型。这实际上是有道理的-计算节点负责接受并处理请求。您将需要的存储节点数量将取决于您的使用场景，而Glance的数量将不可避免地取决于Swift可用的功能和资源。
- en: The next service in line is **Horizon** – a *human-readable* GUI dashboard of
    OpenStack where we *consume* a lot of OpenStack visual information.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 排队中的下一个服务是**Horizon** - 一个OpenStack的*人类可读*GUI仪表板，我们在那里*消耗*大量的OpenStack可视信息。
- en: Horizon
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 地平线
- en: Having explained the core services that enable OpenStack to do what it does
    the way it does in some detail, we need to address the user interaction. In almost
    every paragraph in this chapter, we refer to APIs and scripting interfaces as
    a way to communicate and orchestrate OpenStack. While this is completely true
    and is the usual way of managing large-scale deployments, OpenStack also has a
    pretty useful interface that is available as a web service in a browser. The name
    of this project is Horizon, and its sole purpose is to provide a user with a way
    of interacting with all the services from one place, called the dashboard. Users
    can also reconfigure most, if not all, the things in the OpenStack installation,
    including security, networking, access rights, users, containers, volumes, and
    everything else that exists in the OpenStack installation.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在相当详细地解释了使OpenStack能够以某种方式做到它所做的核心服务之后，我们需要解决用户交互的问题。在本章的几乎每一段中，我们都提到API和脚本接口作为与OpenStack通信和编排的方式。虽然这完全是真实的，并且是管理大规模部署的常规方式，但OpenStack还具有一个非常有用的界面，可以作为浏览器中的Web服务使用。这个项目的名称是Horizon，它的唯一目的是为用户提供一种与所有服务进行交互的方式，称为仪表板。用户还可以重新配置OpenStack安装中的大多数（如果不是全部）内容，包括安全性、网络、访问权限、用户、容器、卷以及OpenStack安装中存在的其他所有内容。
- en: Horizon also supports plugins and *pluggable panels*. There is an active plugin
    marketplace for Horizon that aims at extending its functionality even further
    than it already has. If that's still not enough for your particular scenario,
    you can create your own plugins in Angular and get them to run in Horizon.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Horizon还支持插件和*可插拔面板*。Horizon有一个活跃的插件市场，旨在扩展其功能，甚至比它已经具有的功能更进一步。如果这对您的特定情况仍然不够，您可以使用Angular创建自己的插件，并让它们在Horizon中运行。
- en: Pluggable panels are also a nice idea – without changing any defaults, a user
    or a group of users can change the way the dashboard looks and get more (or less)
    information presented to them. All of this requires a little bit of coding; changes
    are made in the config files, but the main thing is that the Horizon system itself
    supports such a customization model. You can find out more about the interface
    itself and the functions that are available to the user when we cover installing
    OpenStack and creating OpenStack instances in the *Provisioning the OpenStack
    environment* section.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 可插拔面板也是一个不错的主意 - 在不改变任何默认设置的情况下，用户或一组用户可以改变仪表板的外观，并获得更多（或更少）呈现给他们的信息。所有这些都需要一点编码；更改是在配置文件中进行的，但主要的是Horizon系统本身支持这样的定制模型。在我们讨论安装OpenStack和创建OpenStack实例的*配置OpenStack环境*部分时，您可以了解更多关于界面本身和用户可用的功能。
- en: As you are aware, networks don't really work all that well without name resolution,
    which is why OpenStack has a service called **Designate**. We'll briefly discuss
    Designate next.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所知，没有名称解析，网络实际上无法正常工作，这就是为什么OpenStack有一个名为**Designate**的服务。我们接下来会简要讨论Designate。
- en: Designate
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指定
- en: Every system that uses any kind of network must have at least some kind of name
    resolution service in the form of a local or remote DNS or a similar mechanism.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 任何使用任何类型网络的系统都必须至少有某种形式的名称解析服务，如本地或远程DNS或类似的机制。
- en: Designate is a service that tries to integrate the *DNSaaS* concept in OpenStack
    in one place. When connected to Nova and Neutron, it will try to keep up-to-date
    records in regards to all the hosts and infrastructure details.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Designate是一项服务，试图在OpenStack中整合*DNSaaS*概念。当连接到Nova和Neutron时，它将尝试保持有关所有主机和基础设施详细信息的最新记录。
- en: Another very important aspect of the cloud is how we manage identities. For
    that specific purpose, OpenStack has a service called **Keystone**. We'll discuss
    what it does next.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 云的另一个非常重要的方面是我们如何管理身份。为此，OpenStack有一个名为**Keystone**的服务。我们接下来会讨论它的作用。
- en: Keystone
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Keystone
- en: Identity management is a big thing in cloud computing, simply because when deploying
    a large-scale infrastructure, not only do you need a way to scale your resources,
    but you also need a way to scale user management. A simple list of users that
    can access a resource is not an option anymore, mainly because we are not talking
    about simple users anymore. Instead, we are talking about domains containing thousands
    of users separated by groups and by roles – we are talking about multiple ways
    of logging in and providing authentication and authorization. Of course, this
    also can span multiple standards for authentication, as well as multiple specialized
    systems.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 身份管理在云计算中非常重要，因为在部署大规模基础设施时，不仅需要一种方式来扩展资源，还需要一种方式来扩展用户管理。简单的用户访问资源的列表不再是一个选择，主要是因为我们不再谈论简单的用户。相反，我们谈论包含成千上万用户的域，由组和角色分隔
    - 我们谈论多种登录和提供身份验证和授权的方式。当然，这也可能涉及多种身份验证标准，以及多个专门的系统。
- en: For these reasons, user management is a separate project/service in OpenStack
    named Keystone.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 出于这些原因，用户管理是OpenStack中名为Keystone的一个独立项目/服务。
- en: Keystone supports simple user management and the creation of users, groups,
    and roles, but it also supports LDAP, Oauth, OpenID Connect, SAML, and SQL database
    authentication and has its own API that can support every possible scenario for
    user management. Keystone is in a world by itself, and in this book, we will treat
    it as a simple user provider. However, it can be much more and can require a lot
    of configuration, depending on the case. The good thing is that, once installed,
    you will rarely need to think about this part of OpenStack.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Keystone支持简单的用户管理和用户、组和角色的创建，但它也支持LDAP、Oauth、OpenID Connect、SAML和SQL数据库身份验证，并且有自己的API，可以支持用户管理的各种场景。Keystone是一个独立的世界，在本书中，我们将把它视为一个简单的用户提供者。然而，它可以是更多，可能需要根据情况进行大量配置。好消息是，一旦安装好，你很少需要考虑OpenStack的这一部分。
- en: The next service on our list is **Neutron**, the API/backend for (cloud) networking
    in OpenStack.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们列表上的下一个服务是**Neutron**，这是OpenStack中（云）网络的API/后端。
- en: Neutron
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Neutron
- en: "OpenStack Neutron is an API-based service that aims to provide a simple and\
    \ extensible cloud network concept as a development of what used to be called\
    \ a *Quantum* service \Lin older releases of OpenStack. Before this service, networking\
    \ was managed by nova-network, which, as we mentioned, is a solution that's obsolete,\
    \ with Neutron being the reason for this. Neutron integrates with some of the\
    \ services that we've already discussed – Nova, Horizon, and Keystone. As a standalone\
    \ concept, we can deploy Neutron to a separate server, which will then give us\
    \ the ability to use the Neutron API. This is reminiscent of what VMware does\
    \ in NSX with the NSX Controller concept."
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack Neutron是一个基于API的服务，旨在提供一个简单且可扩展的云网络概念，作为OpenStack旧版本中称为*Quantum*服务的发展。在这项服务之前，网络是由nova-network管理的，正如我们提到的，这是一个已经过时的解决方案，而Neutron正是这一变化的原因。Neutron与我们已经讨论过的一些服务集成
    - Nova、Horizon和Keystone。作为一个独立的概念，我们可以部署Neutron到一个单独的服务器，然后就可以使用Neutron API。这让人想起了VMware在NSX中使用NSX
    Controller概念的做法。
- en: 'When we deploy neutron-server, a web-based service that hosts the API connects
    to the Neutron plugin in the background so that we can introduce networking changes
    to our Neutron-managed cloud network. In terms of architecture, it has the following
    services:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们部署neutron-server时，一个托管API的基于Web的服务会与Neutron插件后台连接，以便我们可以对我们的Neutron管理的云网络进行网络更改。在架构方面，它有以下服务：
- en: Database for persistent storage
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于持久存储的数据库
- en: neutron-server
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: neutron-server
- en: External agents (plugins) and drivers
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部代理（插件）和驱动程序
- en: 'In terms of plugins, it has a *lot* of them, but here''s a short list:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在插件方面，它有*很多*，但这里是一个简短的列表：
- en: Open vSwitch
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Open vSwitch
- en: Cisco UCS/Nexus
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cisco UCS/Nexus
- en: The Brocade Neutron plugin
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brocade Neutron插件
- en: IBM SDN-VE
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IBM SDN-VE
- en: VMware NSX
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VMware NSX
- en: Juniper OpenContrail
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Juniper OpenContrail
- en: Linux bridging
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linux bridging
- en: ML2
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ML2
- en: Many others
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有很多其他的
- en: Most of these plugin names are logical, so you won't have any problems understanding
    what they do. But we'd like to mention one of these plugins specifically, which
    is the **Modular Layer 2** (**ML2**) plugin.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些插件名称都是逻辑的，所以你不会有任何问题理解它们的作用。但我们想特别提到其中一个插件，即**Modular Layer 2**（**ML2**）插件。
- en: By using the ML2 plugin, OpenStack Neutron can connect to various Layer 2 backends
    – VLAN, GRE, VXLAN, and so on. It also enables Neutron to go away from the Open
    vSwitch and Linux bridge plugins as its basic plugins (which are now obsolete).
    These plugins are considered to be too monolithic for Neutron's modular architecture,
    and ML2 has replaced them completely since the release of Havana (2013). ML2 today
    has many vendor-based plugins for integration. As shown by the preceding list,
    Arista, Cisco, Avaya, HP, IBM, Mellanox, and VMware all have ML2-based plugins
    for OpenStack.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用ML2插件，OpenStack Neutron可以连接到各种第2层后端 - VLAN、GRE、VXLAN等。它还使Neutron摆脱了Open
    vSwitch和Linux桥插件作为其基本插件（现在已经过时）。这些插件被认为对于Neutron的模块化架构来说过于庞大，自2013年Havana发布以来，ML2已完全取代了它们。今天，ML2有许多基于供应商的插件用于集成。正如前面的列表所示，Arista、Cisco、Avaya、HP、IBM、Mellanox和VMware都有基于ML2的OpenStack插件。
- en: 'In terms of network categories, Neutron supports two:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 关于网络类别，Neutron支持两种：
- en: '**Provider networks**: Created by an OpenStack administrator, these are used
    for external connections on a physical level, which are usually backed by flat
    (untagged) or VLAN (802.1q tagged) concepts. These networks are shared since tenants
    use them to access their private infrastructure in hybrid cloud models or to access
    the internet. Also, these networks describe the way underlay and overlay networks
    interact, as well as their mappings.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供者网络**：由OpenStack管理员创建，用于物理级别的外部连接，通常由平面（未标记）或VLAN（802.1q标记）概念支持。这些网络是共享的，因为租户使用它们来访问他们的私有基础设施在混合云模型中或访问互联网。此外，这些网络描述了底层和覆盖网络的交互方式，以及它们的映射关系。'
- en: '**Tenant networks**, **self-service networks**, **project networks**: These
    networks are created by users/tenants and their administrators so that they can
    connect their virtual resources and networks in whatever shape or form they need.
    These networks are isolated and usually backed by a network overlay such as GRE
    or VXLAN, as that''s the whole purpose of tenant networks.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**租户网络**，**自助服务网络**，**项目网络**：这些网络是由用户/租户及其管理员创建的，以便他们可以连接他们需要的任何形状或形式的虚拟资源和网络。这些网络是隔离的，通常由GRE或VXLAN等网络覆盖层支持，因为这就是租户网络的整个目的。'
- en: Tenant networks usually use some kind of SNAT mechanism to access external networks,
    and this service is usually implemented via virtual routers. The same concept
    is used in other cloud technologies such as VMware NSX-v and NSX-t, as well as
    Microsoft Hyper-V SDN technologies backed by Network Controller.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 租户网络通常使用某种SNAT机制来访问外部网络，这项服务通常通过虚拟路由器实现。同样的概念也适用于其他云技术，如VMware NSX-v和NSX-t，以及由网络控制器支持的Microsoft
    Hyper-V SDN技术。
- en: 'In terms of network types, Neutron supports multiple types:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络类型方面，Neutron支持多种类型：
- en: '**Local**: Allows us to communicate within the same host.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Local**：允许我们在同一主机内进行通信。'
- en: '**Flat**: An untagged virtual network.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Flat**：未标记的虚拟网络。'
- en: '**VLAN**: An 802.1Q VLAN tagged virtual network.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VLAN**：一个802.1Q VLAN标记的虚拟网络。'
- en: '**GRE**, VXLAN, GENEVE: Depending on the network overlay technologies, we select
    these network backends.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GRE**，VXLAN，GENEVE：根据网络覆盖技术，我们选择这些网络后端。'
- en: Now that we've covered OpenStack's usage models, ideas, and services, let's
    discuss additional ways in which OpenStack can be used. As you might imagine,
    OpenStack – being what it is – is highly capable of being used in many non-standard
    scenarios. We'll discuss these non-obvious scenarios next.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了OpenStack的使用模型、思想和服务，让我们讨论一下OpenStack可以被用于的其他方式。正如你所想象的，OpenStack -
    作为它所是的东西 - 非常适合在许多非标准场景中使用。接下来我们将讨论这些非明显的场景。
- en: Additional OpenStack use cases
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他OpenStack使用案例
- en: OpenStack has a lot of really detailed documentation available at [https://docs.openstack.org](https://docs.openstack.org).
    One of the more useful topics is the architecture and design examples, which both
    explain the usage scenarios and the ideas behind how a particular scenario can
    be solved using the OpenStack infrastructure. We are going to talk a lot about
    two different edge cases when we deploy our test OpenStack, but some things need
    to be said about configuring and running an OpenStack installation.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack在[https://docs.openstack.org](https://docs.openstack.org)上有很多非常详细的文档。其中一个更有用的主题是架构和设计示例，它们都解释了使用场景和使用OpenStack基础设施解决特定场景的思想。当我们部署我们的测试OpenStack时，我们将讨论两种不同的边缘情况，但有些事情需要说一下关于配置和运行OpenStack安装。
- en: OpenStack is a complex system that encompasses not only computing and storage
    but also a lot of networking and supporting infrastructure. You will first notice
    that when you realize that even the documentation is neatly divided into an administration,
    architecture, operations, security, and virtual machine image guide. Each of these
    subjects is practically a topic for a single book, and a lot of things that guides
    cover are part experience, part best practice advice, and part assumptions based
    on best guesses.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack是一个复杂的系统，不仅涵盖计算和存储，还涉及大量的网络和支持基础设施。当你意识到即使文档也被整齐地分成了管理、架构、运维、安全和虚拟机镜像指南时，你会首先注意到这一点。每个主题实际上都可以成为一本书的主题，指南涵盖的许多内容既是经验，又是最佳实践建议，也是基于最佳猜测的假设的一部分。
- en: There are a couple of things that are more or less common to all these use cases.
    First, when designing a cloud, you must try and get all the information about
    possible loads and your clients as soon as possible, even before a first server
    is booted. This will enable you to plan not only how many servers you need, but
    their location, the ratio of computing to storage nodes, the network topology,
    energy requirements, and all the other things that need to be thought through
    in order to create a working solution.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些用例大致上有一些共同之处。首先，在设计云时，你必须尽早获取关于可能的负载和客户的所有信息，甚至在第一台服务器启动之前。这将使你能够计划不仅需要多少服务器，还有它们的位置、计算与存储节点的比例、网络拓扑、能源需求以及所有其他需要深思熟虑的事情，以创建一个可行的解决方案。
- en: 'When deploying OpenStack, we are talking about a large-scale enterprise solution
    that is usually deployed for one of three reasons:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署OpenStack时，我们通常是出于以下三个原因之一部署大规模企业解决方案：
- en: '*Testing and learning*: Maybe we need to learn how to configure a new installation,
    or we need to test a new computing node before we even go near production systems.
    For that reason, we need a small OpenStack environment, perhaps a single server
    that we can expand if there is a need for that. In practice, this system should
    be able to support probably a single user with a couple of instances. Those instances
    will usually not be the focus of your attention; they are going to be there just
    to enable you to explore all the other functionalities of the system. Deploying
    such a system is usually done the way we described in this chapter – using a readymade
    script that installs and configures everything so that we can focus on the part
    we are actually working on.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*测试和学习*：也许我们需要学习如何配置新的安装，或者在接近生产系统之前需要测试新的计算节点。因此，我们需要一个小型的OpenStack环境，也许是一个单独的服务器，如果有需要的话可以扩展。在实践中，这个系统应该能够支持一个用户和几个实例。这些实例通常不会成为你关注的焦点；它们只是为了让你能够探索系统的所有其他功能而存在。部署这样的系统通常是使用本章描述的方式完成的——使用一个现成的脚本来安装和配置一切，这样我们就可以专注于我们实际正在处理的部分。'
- en: '*We have a need for a staging or pre-production environment*: Usually, this
    means that we need to either support the production team so they have a safe environment
    to work in, or we are trying to keep a separate test environment for storing and
    running instances before they are pushed into production.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要一个暂存或预生产环境：通常，这意味着我们需要支持生产团队，让他们有一个安全的工作环境，或者我们正在尝试保留一个单独的测试环境，用于存储和运行实例，然后再推入生产环境。
- en: Having such an environment is definitively recommended, even if you haven't
    had it yet, since it enables you and your team to experiment without fear of breaking
    the production environment. The downside is that this installation requires an
    environment that has to have some resources available for the users and their
    instances. This means we are not going to be able to get away with using a single
    server. Instead, we will have to create a cloud that will be, at least in some
    parts, as powerful as the production environment. Deploying such an installation
    is basically the same as production deployment since once it comes online, this
    environment will, from your perspective, be just another system in production.
    Even if we are calling it pre-production or test, if the system goes down, your
    users will inevitably call and complain. This is the same as what happens with
    the production environment; you will have to plan downtime, schedule upgrades,
    and try to keep it running as best as you can.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这样的环境是明确建议的，即使你还没有拥有它，因为它使你和你的团队能够在不担心破坏生产环境的情况下进行实验。缺点是，这种安装需要一个环境，必须为用户和他们的实例提供一些资源。这意味着我们不能只用一台服务器。相反，我们将不得不创建一个云，至少在某些部分，它要和生产环境一样强大。部署这样的安装基本上与生产部署相同，因为一旦它上线，从你的角度来看，这个环境将只是生产中的另一个系统。即使我们称其为预生产或测试，如果系统崩溃，你的用户必然会打电话抱怨。这与生产环境发生的情况相同；你必须计划停机时间，安排升级，并尽力使其尽可能地运行良好。
- en: '*For production*: This one is demanding in another way – maintenance. When
    creating an actual production cloud environment, you will need to design it well,
    and then carefully monitor the system to be able to respond to problems. Clouds
    are a flexible thing from the user''s perspective since they offer scaling and
    easy configuration, but being a cloud administrator means that you need to enable
    these configuration changes by having spare resources ready. At the same time,
    you need to pay attention to your equipment, servers, storage, networking, and
    everything else to be able to spot problems before the users see them. Has a switch
    failed over? Are the computing nodes all running correctly? Have the disks degraded
    in performance due to a failure? Each of these things, in a carefully configured
    system, will have minimal to no impact on the users, but if we are not proactive
    in our approach, compounding errors can quickly bring the system down.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用于生产*：这种方式要求另一种方式——维护。在创建实际的生产云环境时，您需要设计良好，然后仔细监视系统以便能够应对问题。从用户的角度来看，云是一种灵活的东西，因为它们提供了扩展和简单的配置，但作为云管理员意味着您需要通过准备好备用资源来启用这些配置更改。同时，您需要注意您的设备、服务器、存储、网络和其他一切，以便能够在用户看到问题之前发现问题。交换机是否故障转移？计算节点是否都正常运行？由于故障而导致磁盘性能下降了吗？在一个精心配置的系统中，这些事情中的每一件都对用户几乎没有或没有影响，但如果我们在处理问题时不主动，复合错误很快就会导致系统崩溃。'
- en: Having distinguished between a single server and a full install in two different
    scenarios, we are going to go through both. The single server will be done manually
    using scripts, while the multi-server will be done using Ansible playbooks.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在两种不同的场景中区分了单个服务器和完整安装之后，我们将两者都进行一遍。单个服务器将使用脚本手动完成，而多服务器将使用Ansible playbooks完成。
- en: Now that we've covered OpenStack in quite a bit of detail, it's time to start
    using it. Let's start with some small things (a small environment to test) in
    order to provision a regular OpenStack environment for production, and then discuss
    integrating OpenStack with Ansible. We'll revisit OpenStack in the next chapter,
    when we start discussing scaling out KVM to Amazon AWS.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经相当详细地介绍了OpenStack，是时候开始使用它了。让我们从一些小事情开始（测试一个小环境），以便为生产环境提供一个常规的OpenStack环境，然后讨论如何将OpenStack与Ansible集成。在下一章中，当我们开始讨论将KVM扩展到Amazon
    AWS时，我们将重新讨论OpenStack。
- en: Creating a Packstack demo environment for OpenStack
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为OpenStack创建一个Packstack演示环境
- en: 'If you just need a **Proof of Concept** (**POC**), there''s a very easy way
    to install OpenStack. We are going to use **Packstack** as it''s the simplest
    way to do this. By using Packstack installation on CentOS 7, you''ll be able to
    configure OpenStack in 15 minutes or so. It all starts with a simple sequence
    of commands:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只需要一个**概念验证**（**POC**），有一种非常简单的方法可以安装OpenStack。我们将使用**Packstack**，因为这是最简单的方法。通过在CentOS
    7上使用Packstack安装，您将能够在大约15分钟内配置OpenStack。一切都始于一系列简单的命令：
- en: '[PRE0]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As the process goes through its various phases, you''ll see various messages,
    such as the following, which are quite nice as you get to see what''s happening
    in real time with a decent verbosity level:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 随着过程经历各种阶段，您将看到各种消息，例如以下消息，这些消息非常好，因为您可以实时查看发生的事情，具有相当不错的详细程度：
- en: '![Figure 12.7 – Appreciating Packstack''s installation verbosity'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.7 – 欣赏Packstack的安装详细程度'
- en: '](img/B14834_12_07.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_07.jpg)'
- en: Figure 12.7 – Appreciating Packstack's installation verbosity
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7 – 欣赏Packstack的安装详细程度
- en: 'After the installation is finished, you will get a report screen that looks
    similar to this:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，您将获得一个类似于此的报告屏幕：
- en: '![Figure 12.8 – Successful Packstack installation'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.8 – 成功的Packstack安装'
- en: '](img/B14834_12_08.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_08.jpg)'
- en: Figure 12.8 – Successful Packstack installation
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.8 – 成功的Packstack安装
- en: 'The installer has finished successfully, and it gives us a warning about `NetworkManager`
    and a kernel update, which means we need to restart our system. After the restart
    and checking the `/root/keystonerc_admin` file for our username and password,
    Packstack is alive and kicking and we can log in by using the URL mentioned in
    the previous screen''s output (`http://IP_or_hostname_where_PackStack_is_deployed/dashboard`):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 安装程序已成功完成，并且它向我们发出了关于`NetworkManager`和内核更新的警告，这意味着我们需要重新启动系统。重新启动并检查`/root/keystonerc_admin`文件以获取我们的用户名和密码后，Packstack已经准备就绪，我们可以通过使用上一个屏幕输出中提到的URL（`http://IP_or_hostname_where_PackStack_is_deployed/dashboard`）进行登录：
- en: '![Figure 12.9 – Packstack UI'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.9 – Packstack UI'
- en: '](img/B14834_12_09.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_09.jpg)'
- en: Figure 12.9 – Packstack UI
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.9 – Packstack UI
- en: There's a bit of additional configuration that needs to be done, as noted in
    the Packstack documentation at [https://wiki.openstack.org/wiki/Packstack](https://wiki.openstack.org/wiki/Packstack).
    If you're going to use an external network, you need a static IP address without
    `NetworkManager`, and you probably want to either configure `firewalld` or stop
    it altogether. Other than that, you can start using this as your demo environment.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些额外的配置需要完成，如Packstack文档中所述[https://wiki.openstack.org/wiki/Packstack](https://wiki.openstack.org/wiki/Packstack)。如果您要使用外部网络，您需要一个没有`NetworkManager`的静态IP地址，并且您可能要配置`firewalld`或完全停止它。除此之外，您可以开始将其用作演示环境。
- en: Provisioning the OpenStack environment
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为OpenStack环境进行配置
- en: 'One of the tasks that is going to be the simplest and, at the same time, the
    hardest when you need to create your first OpenStack configuration is going to
    be provisioning. There are basically two ways you can go with this: one is to
    install services one at a time in a carefully prepared hardware configuration,
    while the other is to just use a *single server install* guide from the OpenStack
    site and create a single machine that will serve as your test bed. In this chapter,
    everything we do is created in such an instance, but before we learn how to install
    the system, we need to understand the differences.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建第一个OpenStack配置时，最简单且最困难的任务之一将是配置。基本上有两种方法可以选择：一种是在精心准备的硬件配置中逐个安装服务，而另一种是只需使用OpenStack网站上的*单服务器安装*指南，创建一台作为测试平台的单台机器。在本章中，我们所做的一切都是在这样的实例中创建的，但在学习如何安装系统之前，我们需要了解其中的区别。
- en: OpenStack is a cloud operating system, and its main idea is to enable us to
    use multiple servers and other devices to create a coherent, easily configured
    cloud that can be managed from a central point, either through an API or through
    a web server. The size and type of the OpenStack deployment can be from one server
    running everything, to thousands of servers and storage units integrated across
    several Datacenters. OpenStack does not have a problem with large-scale deployment;
    the only real limiting factor is usually the cost and other requirements for the
    environment we are trying to create.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack是一个云操作系统，其主要思想是使我们能够使用多台服务器和其他设备创建一个一致、易于配置的云，可以通过API或Web服务器从中心点进行管理。OpenStack的部署规模和类型可以从运行所有内容的单个服务器到跨多个数据中心集成的数千台服务器和存储单元。OpenStack在大规模部署方面没有问题；通常唯一的限制因素是我们尝试创建的环境的成本和其他要求。
- en: We mentioned scalability a few times, and this is where OpenStack shines in
    both ways. The amazing thing is that not only does it scale up easily but that
    it also scales down. An installation that will work perfectly fine for a single
    user can be done on a single machine – even on a single VM inside a single machine
    – so you will be able to have your own cloud within a virtual environment on your
    laptop. This is great for testing things but nothing else.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们多次提到了可扩展性，这正是OpenStack在两方面的闪光之处。令人惊奇的是，它不仅可以轻松扩展，而且还可以缩小规模。一个适用于单个用户的安装可以在单台机器上完成
    - 甚至在单台机器内的单个虚拟机上完成 - 因此您将能够在笔记本电脑的虚拟环境中拥有自己的云。这对于测试很好，但其他用途不大。
- en: Having a bare-metal install that will follow the guidelines and recommended
    configuration requirements for particular roles and services is the only way to
    go forward when creating a working, scalable cloud, and obviously this is the
    way to go if you need to create a production environment. Having said that, between
    a single machine and a thousand server installs, there are a lot of ways that
    your infrastructure can be shaped and redesigned to support your particular use
    case scenario.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建工作、可扩展的云时，遵循特定角色和服务的指南和推荐配置要求是前进的唯一途径，显然这也是在需要创建生产环境时的最佳选择。话虽如此，在单台机器和一千台服务器的安装之间，有很多方式可以塑造和重新设计您的基础架构，以支持您特定的用例场景。
- en: Let's first quickly go through an installation inside another VM, a task that
    can be accomplished in under 10 minutes on a faster host machine. For our platform,
    we decided on installing Ubuntu 18.04.3 LTS in order to be able to keep the host
    system to a minimum. The entire guide for Ubuntu regarding what we are trying
    to do is available at [https://docs.openstack.org/devstack/latest/guides/single-machine.html](https://docs.openstack.org/devstack/latest/guides/single-machine.html).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先快速地在另一个虚拟机内进行安装，这是可以在更快的主机上在10分钟内完成的任务。对于我们的平台，我们决定安装Ubuntu 18.04.3 LTS，以便将主机系统保持到最小。关于我们尝试做的事情，Ubuntu的整个指南都可以在[https://docs.openstack.org/devstack/latest/guides/single-machine.html](https://docs.openstack.org/devstack/latest/guides/single-machine.html)上找到。
- en: One thing that we must point out is that the OpenStack site has a guide for
    a number of different install scenarios, both on virtual and bare-metal hardware,
    and they are all extremely easy to follow, simply because the documentation is
    straight to the point. There's also a simple install script that takes care of
    everything once a few steps are done manually by you.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须指出的一件事是，OpenStack网站提供了一些不同安装方案的指南，无论是在虚拟还是裸机硬件上，它们都非常容易遵循，因为文档直截了当。一旦您手动完成了一些步骤，还有一个简单的安装脚本可以处理一切。
- en: 'Be careful with hardware requirements. There are some good sources available
    to cover this subject. Start here: [https://docs.openstack.org/newton/install-guide-rdo/overview.html#figure-hwreqs](https://docs.openstack.org/newton/install-guide-rdo/overview.html#figure-hwreqs).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件要求要小心。有一些很好的资源可供参考。从这里开始：[https://docs.openstack.org/newton/install-guide-rdo/overview.html#figure-hwreqs](https://docs.openstack.org/newton/install-guide-rdo/overview.html#figure-hwreqs)。
- en: Installing OpenStack step by step
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逐步安装OpenStack
- en: The first thing we need to do is create a user that is going to install the
    entire system. This user needs to have `sudo` privileges since a lot of things
    require system-wide permissions.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是创建一个将安装整个系统的用户。由于许多事情都需要系统范围的权限，这个用户需要具有`sudo`权限。
- en: 'Create a user either as root or through `sudo`:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 以root用户或通过`sudo`创建用户：
- en: '[PRE1]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The next thing we need to do is allow this user to use `sudo`:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要做的是允许这个用户使用`sudo`：
- en: '[PRE2]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We also need to install `git` and switch to our newly created user:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要安装`git`并切换到我们新创建的用户：
- en: '![Figure 12.10 – Installing git, the first step in deploying OpenStack'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.10 - 安装git，部署OpenStack的第一步'
- en: '](img/B14834_12_10.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_10.jpg)'
- en: Figure 12.10 – Installing git, the first step in deploying OpenStack
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.10 - 安装git，部署OpenStack的第一步
- en: 'Now for the fun part. We are going to clone (copy the latest version of) `devstack`,
    the installation script that will provide everything we need to be able to run
    and use OpenStack on this machine:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是有趣的部分。我们将克隆（复制最新版本的）`devstack`，这是安装脚本，将为我们提供在此机器上运行和使用OpenStack所需的一切：
- en: '![Figure 12.11 – Cloning devstack by using git'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.11 - 使用git克隆devstack'
- en: '](img/B14834_12_11.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_11.jpg)'
- en: Figure 12.11 – Cloning devstack by using git
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.11 - 使用git克隆devstack
- en: 'A little bit of configuration is now needed. Inside the `samples` directory,
    in the directory we just cloned, there is a file called `local.conf`. Use it to
    configure all the things the installer needs. Networking is one thing that has
    to be configured manually – not just the local network, which is the one that
    connects you to the rest of the internet, but also the internal network address
    space, which is going to get used for everything OpenStack needs to do between
    instances. Different passwords for different services also need to be set. All
    of this can be read in the sample file. Directions on how to exactly configure
    this are both on the web at the address we gave you earlier, and inside the file
    itself:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在需要一点配置。在我们刚刚克隆的目录中的`samples`目录中，有一个名为`local.conf`的文件。使用它来配置安装程序需要的所有内容。网络是必须手动配置的一件事
    - 不仅是本地网络，它是将您连接到互联网的网络，还有内部网络地址空间，它将用于OpenStack实例之间需要执行的所有操作。还需要设置不同服务的不同密码。所有这些都可以在示例文件中阅读到。关于如何精确配置这些的说明既可以在我们之前给出的网址上找到，也可以在文件本身中找到：
- en: '![Figure 12.12 – Installer configuration'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.12 - 安装程序配置'
- en: '](img/B14834_12_12.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_12.jpg)'
- en: Figure 12.12 – Installer configuration
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.12 - 安装程序配置
- en: 'There will be some issues with this installation process, and as a result,
    installation might break twice because of the following reasons:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 安装过程中可能会出现一些问题，因此由于以下原因，安装可能会出现两次中断：
- en: Ownership of `/opt/stack/.cache` is `root:root`, instead of `stack:stack`. Please
    correct this ownership before running the installer;
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/opt/stack/.cache`的所有权是`root:root`，而不是`stack:stack`。在运行安装程序之前，请更正此所有权；'
- en: An installer problem (a known one), as it fails to install a component and then
    fails. Solution is rather simple - there's a line that needs to be changed in
    a file in inc directory, called python. At time of writing, line 192 of that file
    needs to be changed from `$cmd_pip $upgrade \` to `$cmd_pip $upgrade --ignore-installed
    \`
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装程序问题（已知问题），因为它无法安装一个组件然后失败。解决方案相当简单 - 需要更改inc目录中名为python的文件中的一行。在撰写本文时，该文件的第192行需要从`$cmd_pip
    $upgrade \`更改为`$cmd_pip $upgrade --ignore-installed \`
- en: 'In the end, after we collected all the data and modified the file, we settled
    on this configuration:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在收集了所有数据并修改了文件之后，我们确定了这个配置：
- en: '![Figure 12.13 – Example configuration'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.13 - 示例配置'
- en: '](img/B14834_12_13.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_13.jpg)'
- en: Figure 12.13 – Example configuration
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.13 - 示例配置
- en: 'Most of these parameters are understandable, but let''s cover two of them first:
    `FLOATING_RANGE` and `FIXED_RANGE`. The `FLOATING_RANGE` parameter tells our OpenStack
    installation which network scope will be used for *private* networks. On the other
    hand, `FIXED_RANGE` is the network scope that will be used by OpenStack-provisioned
    virtual machines. Basically, virtual machines provisioned in OpenStack environments
    will be given internal addresses from `FIXED_RANGE`. If a virtual machine needs
    to be available from the outside world as well, we will assign a network address
    from `FLOATING_RANGE`. Be careful with `FIXED_RANGE` as it shouldn''t match an
    existing network range in your environment.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数中大多数是可以理解的，但让我们先讨论其中的两个：`FLOATING_RANGE`和`FIXED_RANGE`。`FLOATING_RANGE`参数告诉我们的OpenStack安装将使用哪个网络范围用于*私有*网络。另一方面，`FIXED_RANGE`是OpenStack配置的虚拟机将使用的网络范围。基本上，在OpenStack环境中配置的虚拟机将从`FIXED_RANGE`获得内部地址。如果虚拟机也需要从外部世界可用，我们将从`FLOATING_RANGE`分配一个网络地址。请注意`FIXED_RANGE`，因为它不应该与您的环境中现有的网络范围匹配。
- en: One thing we changed from what is given in the guide is that we reduced the
    number of replicas in the Swift installation to one. This gives us no redundancy,
    but reduces the space used for storage and speeds things up a little. Do not do
    this in the production environment.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从指南中所给出的内容中做出的一个改变是将Swift安装中的副本数减少到一个。这样做会使我们失去冗余，但会减少存储空间并加快速度。在生产环境中不要这样做。
- en: Depending on your configuration, you may also need to set the `HOST_IP` address
    variable in the file. Here, set it to your current IP address.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的配置，您可能还需要在文件中设置`HOST_IP`地址变量。在这里，将其设置为您当前的IP地址。
- en: Then, run `./stack.sh`.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 然后运行`./stack.sh`。
- en: 'Once you''ve run the script, a really verbose installation should start and
    dump a lot of lines on your screen. Wait for it to finish – it is going to take
    a while and download a lot of files from the internet. At the end, it is going
    to give you an installation summary that looks something like this:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本后，一个非常冗长的安装过程将开始，并在屏幕上输出大量行。等待它完成 - 这将需要一段时间并从互联网下载大量文件。最后，它将给出一个安装摘要，看起来像这样：
- en: '![Figure 12.14 – Installation summary'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.14 - 安装摘要'
- en: '](img/B14834_12_14.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_14.jpg)'
- en: Figure 12.14 – Installation summary
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.14 - 安装摘要
- en: 'Once this is done, if everything is okay, you should have a complete running
    version of OpenStack on your local machine. In order to verify that, connect to
    your machine using a web browser; a welcome screen should appear:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，如果一切正常，您应该在本地机器上拥有一个完整运行的OpenStack版本。为了验证这一点，使用Web浏览器连接到您的机器；应该出现一个欢迎界面：
- en: '![Figure 12.15 – OpenStack login screen'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.15 - OpenStack登录界面'
- en: '](img/B14834_12_15.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_15.jpg)'
- en: Figure 12.15 – OpenStack login screen
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.15 - OpenStack登录界面
- en: After logging in with the credentials that are written on your machine, after
    the installation (the default administrator name is `admin` and the password is
    the one you set in `local.conf` when installing the service), you are going to
    be welcomed by a screen showing you the stats for your cloud. The screen you are
    looking at is actually a Horizon dashboard and is the main screen that provides
    you with all you need to know about your cloud at a glance.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 使用安装后您机器上写的凭据登录（默认管理员名称是`admin`，密码是在安装服务时在`local.conf`中设置的密码），您将会看到一个显示云统计信息的屏幕。您看到的屏幕实际上是一个Horizon仪表板，是提供您一目了然的有关云的所有信息的主要屏幕。
- en: OpenStack administration
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenStack管理
- en: 'Looking at the top-left corner of Horizon, we can see that there are three
    distinct sections that are configured by default. The first one – **Project**
    – covers everything about our default instance and its performance. This is where
    you can create new instances, manage images, and work on server groups. Our cloud
    is just a core installation, so we only have one server and two defined zones,
    which means that we have no server groups installed:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 查看Horizon左上角，我们可以看到默认配置的三个不同部分。第一个 - **项目** - 包括有关默认实例及其性能的所有内容。在这里，您可以创建新实例、管理图像，并处理服务器组。我们的云只是一个核心安装，所以我们只有一个服务器和两个定义的区域，这意味着我们没有安装服务器组：
- en: '![Figure 12.16 – Basic Horizon dashboard'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.16 – 基本的Horizon仪表板'
- en: '](img/B14834_12_16.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_16.jpg)'
- en: Figure 12.16 – Basic Horizon dashboard
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.16 – 基本的Horizon仪表板
- en: 'First, let''s create a quick instance to show how this is done:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个快速实例来展示如何完成这个过程：
- en: '![Figure 12.17 – Creating an instance'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.17 – 创建实例'
- en: '](img/B14834_12_17.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_17.jpg)'
- en: Figure 12.17 – Creating an instance
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.17 – 创建实例
- en: 'Follow these steps to create an instance:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤创建一个实例：
- en: Go to **Launch Instance** in the far-right part of the screen. A window will
    open that will enable you to give OpenStack all the information it needs to create
    a new VM instance:![Figure 12.18 – Launch Instance wizard
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到屏幕的最右侧的**启动实例**。将打开一个窗口，让您可以为OpenStack提供创建新VM实例所需的所有信息：![图12.18 – 启动实例向导
- en: '](img/B14834_12_18.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_18.jpg)'
- en: Figure 12.18 – Launch Instance wizard
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.18 – 启动实例向导
- en: On the next screen, you need to supply the system with the image source. We
    already mentioned glances – these images are taken from the Glance store and can
    be either an image snapshot, a ready-made volume, or a volume snapshot. We can
    also create a persistent image if we want to. One thing that you'll notice is
    that there are two differences when comparing this process to almost any other
    deployment. The first is that we are using a ready-made image by default as one
    was provided for us. Another big thing is the ability to create a new persistent
    volume to store our data in, or to have it deleted when we are done with the image,
    or have it not be created at all.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个屏幕上，您需要提供图像来源。我们已经提到glances - 这些图像来自Glance存储，并且可以是图像快照、现成的卷，或卷快照。如果需要的话，我们也可以创建一个持久图像。您会注意到，与几乎任何其他部署过程相比，有两个不同之处。首先，我们默认使用现成的图像，因为已经为我们提供了一个。另一个重要的事情是能够创建一个新的持久卷来存储我们的数据，或者在完成图像后将其删除，或者根本不创建它。
- en: 'Choose the one image you have allocated in the public repository; it should
    be called something similar to the one shown in the following screenshot. CirrOS
    is a test image provided with OpenStack. It''s a minimal Linux distribution that
    is designed to be as small as possible and enable easy testing of the whole cloud
    infrastructure but to be as unobtrusive as possible. CirrOS is basically an OS
    placeholder. Of course, we need to click on the **Launch Instance** button to
    go to the next step:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 选择您在公共存储库中分配的一个图像；它应该与以下截图中显示的类似。CirrOS是OpenStack提供的一个测试图像。它是一个被设计为尽可能小并且能够轻松测试整个云基础架构的最小Linux发行版，但尽可能不显眼。CirrOS基本上是一个操作系统占位符。当然，我们需要点击**启动实例**按钮进入下一步：
- en: '![Figure 12.19 – Selecting an instance source'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.19 – 选择实例来源'
- en: '](img/B14834_12_19.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_19.jpg)'
- en: Figure 12.19 – Selecting an instance source
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.19 – 选择实例来源
- en: The next important part of creating a new image is choosing a flavor. This is
    another one of those peculiarly named things in OpenStack. A flavor is a combination
    of certain resources that basically creates a computing, memory, and storage template
    for new instances. We can choose from instances that have as little as 64 MB of
    RAM and 1 vCPU and go as far as our infrastructure can provide:![Figure 12.20
    – Selecting an instance flavor
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建新镜像的下一个重要部分是选择规格。这是OpenStack中另一个奇怪命名的东西。规格是某些资源的组合，基本上为新实例创建了一个计算、内存和存储模板。我们可以选择从具有最少64MB
    RAM和1 vCPU的实例开始，一直到我们的基础设施可以提供的程度：![图12.20 – 选择实例规格
- en: '](img/B14834_12_20.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_20.jpg)'
- en: Figure 12.20 – Selecting an instance flavor
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.20 – 选择实例规格
- en: In this particular example, we are going to choose `cirros256`, a flavor that
    is basically designed to provide our test system with as few resources as is feasible.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的示例中，我们将选择`cirros256`，这是一种基本上设计为为我们的测试系统提供尽可能少的资源的规格。
- en: 'The last thing we actually need to choose is the network connectivity. We need
    to set all the adapters our instance will be able to use while running. Since
    this is a simple test, we are going to use both adapters we have, both the internal
    and external one. They are called `public` and `shared`:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们实际上需要选择的最后一件事是网络连接。我们需要设置实例在运行时可以使用的所有适配器。由于这是一个简单的测试，我们将使用我们拥有的两个适配器，即内部和外部适配器。它们被称为`public`和`shared`：
- en: '![Figure 12.21 – Instance network configuration'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.21 – 实例网络配置'
- en: '](img/B14834_12_21.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_21.jpg)'
- en: Figure 12.21 – Instance network configuration
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.21 – 实例网络配置
- en: Now, we can launch our instance and it will be able to boot. Once you click
    on the **Launch Instance** button, it is going to take probably under a minute
    to create a new instance. The screen showing its current progress and instance
    status will auto update while the instance is being deployed.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以启动我们的实例，它将能够引导。一旦您点击**启动实例**按钮，创建新实例可能需要不到一分钟。在实例部署过程中，显示其当前进度和实例状态的屏幕将自动更新。
- en: 'Once this is done, our instance will be ready:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，我们的实例将准备就绪：
- en: '![Figure 12.22 – The instance is ready'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.22 - 实例已准备好'
- en: '](img/B14834_12_22.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_22.jpg)'
- en: Figure 12.22 – The instance is ready
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.22 - 实例已准备好
- en: 'We''ll quickly create another instance, and then create a snapshot so that
    we can show you how image management works. If you click on the **Create snapshot**
    button on the right-hand side of the instance list, Horizon will create a snapshot
    and immediately put you in the interface meant for image administration:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将快速创建另一个实例，然后创建一个快照，以便向您展示镜像管理的工作原理。如果您点击实例列表右侧的**创建快照**按钮，Horizon将创建一个快照，并立即将您放入用于镜像管理的界面：
- en: '![Figure 12.23 – Images'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.23 - 图像'
- en: '](img/B14834_12_23.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_23.jpg)'
- en: Figure 12.23 – Images
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.23 - 图像
- en: 'Now, we have two different snapshots: one that is the start image and another
    that is an actual snapshot of the image that is running. So far, everything has
    been simple. What about creating an instance out of a snapshot? It''s just a click
    away! What you need to do is just click on the **Launch Instance** button on the
    right and go through the wizard for creating new instances.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有两个不同的快照：一个是启动镜像，另一个是正在运行的镜像的实际快照。到目前为止，一切都很简单。那么从快照创建实例呢？只需点击一下！您需要做的就是点击右侧的**启动实例**按钮，然后按照创建新实例的向导进行操作。
- en: 'The end result of our short example of instance creation should be something
    like this:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们短暂示例的实例创建的最终结果应该是这样的：
- en: '![Figure 12.24 – New instance creation finished'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.24 - 新实例创建完成'
- en: '](img/B14834_12_24.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_24.jpg)'
- en: Figure 12.24 – New instance creation finished
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.24 - 新实例创建完成
- en: 'What we can see is all the information we need on our instances, what their
    IP addresses are, their flavor (which translates into what amount of resources
    are allocated for a particular instance), the availability zone that the image
    is running in, and information on the current instance state. The next thing we
    are going to check out is the **Volumes** tab on the left. When we created our
    instances, we told OpenStack to create one permanent volume for the first instance.
    If we now click on **Volumes**, we should see the volume under a numeric name:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到有关我们实例的所有信息，它们的IP地址是什么，它们的规格（这转化为为特定实例分配了多少资源），镜像正在运行的可用区域，以及当前实例状态的信息。我们接下来要检查的是左侧的**卷**选项卡。当我们创建实例时，我们告诉OpenStack为第一个实例创建一个永久卷。如果我们现在点击**卷**，我们应该会看到卷在一个数字名称下：
- en: '![Figure 12.25 – Volumes'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.25 - 卷'
- en: '](img/B14834_12_25.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_25.jpg)'
- en: Figure 12.25 – Volumes
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.25 - 卷
- en: From this screen, we can now snapshot the volume, reattach it to a different
    instance, and even upload it as an image to the repository.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个屏幕上，我们现在可以对卷进行快照，重新连接到不同的实例，甚至将其上传为镜像到存储库。
- en: The third tab on the left-hand side, named **Network**, contains even more information
    about our currently configured setup.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧的第三个选项卡名为**网络**，包含有关当前配置设置的更多信息。
- en: 'If we click on the **Network Topology** tab, we will get the whole network
    topology of our currently running network, shown in a simple graphical display.
    We can choose from **Topology** and **Graph**, both of which basically represent
    the same thing:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们点击**网络拓扑**选项卡，我们将得到当前运行网络的整个网络拓扑，显示在简单的图形显示中。我们可以选择**拓扑**和**图形**，两者基本上代表相同的东西：
- en: '![Figure 12.26 – Network topology'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.26 - 网络拓扑'
- en: '](img/B14834_12_26.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_26.jpg)'
- en: Figure 12.26 – Network topology
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.26 - 网络拓扑
- en: If we need to create another network or change anything in the network matrix,
    we can do so here. We consider this to be really administrator-friendly, on top
    of being documentation-friendly. Both of these points make our next topic – day-to-day
    administration – much easier.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要创建另一个网络或更改网络矩阵中的任何内容，我们可以在这里进行。我们认为这真的很适合管理员使用，而且也很适合文档使用。这两点使我们下一个主题
    - 日常管理 - 变得更加容易。
- en: Day-to-day administration
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 日常管理
- en: 'We are more or less finished with the most important options that are in any
    way connected to the administration of our day-to-day tasks in the **Project**
    Datacenter. If we click on the tab named **Admin**, we will notice that the menu
    structure we''ve opened looks a lot like the one under **Project**. This is because,
    now, we are looking at administration tasks that have something to do with the
    infrastructure of the cloud, not the infrastructure of our particular logical
    Datacenter, but the same building blocks exist in both of these. However, if we
    – for example – open **Compute**, a completely different set of options exist:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们或多或少已经完成了与**项目**数据中心日常任务管理有关的最重要选项。如果我们点击名为**管理**的选项卡，我们会注意到我们打开的菜单结构看起来很像**项目**下的菜单结构。这是因为现在我们正在查看与云基础设施有关的管理任务，而不是与我们特定逻辑数据中心的基础设施有关，但这两者都存在相同的构建模块。然而，如果我们
    - 例如 - 打开**计算**，会出现完全不同的选项集：
- en: '![Figure 12.27 – Different available configuration options'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.27 - 不同的可用配置选项'
- en: '](img/B14834_12_27.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_27.jpg)'
- en: Figure 12.27 – Different available configuration options
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.27 - 不同的可用配置选项
- en: This part of the interface is used to completely administer parts that form
    our infrastructure and those that define different things we can use while working
    in our *Datacenter*. When logged in as a user, we can add and remove virtual machines,
    configure networks, and use resources, but to put resources online, add new hypervisors,
    define flavors, and do these kinds of tasks that completely change the infrastructure,
    we need to be assigned the administrative role. Some of the functions overlap,
    such as both the administrative part of the interface and the user-specific part,
    which have control over instances. However, the administrative part has all these
    functions, and users can have their set of commands tweaked so that they are,
    for instance, unable to delete or create new instances.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 界面的这一部分用于完全管理构成我们基础设施的部分，以及定义我们在*数据中心*工作时可以使用的不同事物。当以用户身份登录时，我们可以添加和删除虚拟机，配置网络，并使用资源，但要上线资源，添加新的hypervisors，定义规格，以及执行这些完全改变基础设施的任务，我们需要被分配管理角色。一些功能重叠，例如界面的管理部分和用户特定部分，它们都可以控制实例。然而，管理部分具有所有这些功能，用户可以调整其一组命令，例如无法删除或创建新实例。
- en: 'The adminsitrative view enables us to monitor our nodes on a more direct level,
    not only through the services they provide, but also through raw data about a
    particular host and the resources utilized on it:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 管理视图使我们能够更直接地监视我们的节点，不仅通过它们提供的服务，还可以通过关于特定主机和其上使用的资源的原始数据：
- en: '![Figure 12.28 – Available hypervisors in our Datacenter'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.28 - 数据中心中可用的hypervisors'
- en: '](img/B14834_12_28.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_28.jpg)'
- en: Figure 12.28 – Available hypervisors in our Datacenter
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.28 - 数据中心中可用的hypervisors
- en: Our Datacenter has only one hypervisor, but we can see the amount of resources
    physically available on it, and the share of those resources the current setup
    is using at this particular moment.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据中心只有一个hypervisor，但我们可以看到其上物理可用资源的数量，以及当前设置在这一特定时刻使用的这些资源的份额。
- en: Flavors are also one important part of the whole of OpenStack. We already mentioned
    them as a predefined sets of resource presets that form a platform that the instance
    is going to run on. Our test setup has a few of them defined, but we can delete
    the ones that are shipped in this setup and create new ones tailored to our needs.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 规格也是OpenStack整体的重要组成部分。我们已经提到它们是预定义的资源预设集，形成实例将在其上运行的平台。我们的测试设置已经定义了一些规格，但我们可以删除此设置中提供的规格，并创建符合我们需求的新规格。
- en: Since the point of the cloud is to optimize resource management, flavors play
    a big part in this concept. Creating flavors is not an easy task in terms of planning
    and design. First and foremost, it requires deep knowledge of what is possible
    on a given hardware platform, how much and what computing resources even exist,
    and how to utilize it to the full extent possible. So, it is essential that we
    plan and design things properly. The other thing is that we actually need to understand
    what kind of load we are preparing for. Is it memory-intensive? Do we have many
    small services that require a lot of nodes with a simple configuration? Are we
    going to need a lot of computing power and/or a lot of storage? The answers to
    those questions are something that will not only enable us to create what our
    clients want, but also create flavors that will have users utilizing our infrastructure
    in full.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 由于云的目的是优化资源管理，规格在这一概念中起着重要作用。在规划和设计方面，创建规格并不是一项容易的任务。首先，它需要深入了解在给定的硬件平台上可能发生的事情，甚至存在多少和什么样的计算资源，以及如何充分利用它。因此，我们需要妥善规划和设计。另一件事是，我们实际上需要了解我们正在为何种负载做准备。它是内存密集型的吗？我们是否有许多需要简单配置的节点的小服务？我们是否需要大量的计算能力和/或大量的存储？这些问题的答案不仅能让我们创建客户想要的东西，还能创建让用户充分利用我们基础设施的规格。
- en: 'The basic idea is to create flavors that will give individual users just enough
    resources to get their job done in a satisfactory way. This is not obvious in
    a deployment that has 10 instances, but once we run into thousands, a flavor that
    always leaves 10 percent of the storage unused is quickly going to eat into our
    resources and limit our ability to serve more users. Striking this balance between
    what we have and what we give users to use in a particular way is probably the
    hardest task in planning and designing our environments:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想是创建规格，为个别用户提供足够的资源，以满足其工作需求。在拥有10个实例的部署中，这并不明显，但一旦我们遇到成千上万个实例，一个总是留下10％存储空间未使用的规格将迅速消耗我们的资源，并限制我们为更多用户提供服务的能力。在规划和设计我们的环境中，找到我们拥有的资源和我们以特定方式提供给用户使用之间的平衡可能是最困难的任务：
- en: '![Figure 12.29 – Create Flavor wizard'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.29 - 创建规格向导'
- en: '](img/B14834_12_29.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_29.jpg)'
- en: Figure 12.29 – Create Flavor wizard
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.29 - 创建规格向导
- en: 'Creating a flavor is a simple task. We need to do the following:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 创建规格是一项简单的任务。我们需要做以下事情：
- en: Give it a name; an ID will be assigned automatically.
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给它一个名称；ID将自动分配。
- en: Set the number of vCPUs and RAM for our flavor.
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为我们的规格设置vCPU和RAM的数量。
- en: Select the size of a base disk, and an ephemeral disk that doesn't get included
    in any of the snapshots and gets deleted when a virtual machine is terminated.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择基本磁盘的大小，以及一个临时磁盘，不包括在任何快照中，并在虚拟机终止时被删除。
- en: Select the amount of swap space.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择交换空间的大小。
- en: Select the RX/TX factor so that we can create a bit of QoS on the network level.
    Some flavors will need to have more network traffic priority than others.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择RX/TX因子，以便我们可以在网络级别创建一些QoS。一些规格将需要比其他规格更高的网络流量优先级。
- en: OpenStack allows a particular project to have more than one flavor, and for
    a single flavor to belong to different projects. Now that we've learned that,
    let's work with our user identities and assign them some objects.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack允许一个项目拥有多个规格，并且一个规格属于不同的项目。现在我们已经了解了这一点，让我们与用户身份一起工作，并为他们分配一些对象。
- en: Identity management
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 身份管理
- en: 'The last tab on the left-hand side is **Identity**, which is responsible for
    handling users, roles, and projects. This is where we are going to configure not
    only our usernames, but the user roles, groups, and projects a particular user
    can use:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 左侧最后一个选项卡是**身份**，负责处理用户、角色和项目。在这里，我们不仅要配置我们的用户名，还要配置用户角色、组和用户可以使用的项目：
- en: '![Figure 12.30 – Users, Groups, and Roles management'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.30 - 用户、组和角色管理'
- en: '](img/B14834_12_30.jpg)'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_30.jpg)'
- en: Figure 12.30 – Users, Groups, and Roles management
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.30 - 用户、组和角色管理
- en: 'We are not going to go too much into how users are managed and installed, but
    just cover the basics of user management. As always, the original documentation
    on the OpenStack site is the place to go to learn more. Make sure that you check
    out this link: [https://docs.openstack.org/keystone/pike/admin/cli-manage-projects-users-and-roles.html](https://docs.openstack.org/keystone/pike/admin/cli-manage-projects-users-and-roles.html).'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入讨论用户是如何管理和安装的，只是涵盖用户管理的基础知识。与往常一样，OpenStack网站上的原始文档是学习更多知识的好地方。确保您查看此链接：[https://docs.openstack.org/keystone/pike/admin/cli-manage-projects-users-and-roles.html](https://docs.openstack.org/keystone/pike/admin/cli-manage-projects-users-and-roles.html)。
- en: In short, once you create a project, you need to define which users are going
    to be able to see and work on a particular project. In order to ease administration,
    users can also be part of groups, and you can then assign whole groups to a project.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，一旦您创建了一个项目，您需要定义哪些用户能够查看和处理特定项目。为了简化管理，用户也可以成为组的一部分，然后您可以将整个组分配给一个项目。
- en: 'The point of this structure is to enable the administrator to limit the users
    not only to what they can administer, but also to how many of the available resources
    are available for a particular project. Let''s use an example for this. If we
    go to **Projects** and edit an existing project (or create a new one), we''ll
    see a tab called **Quota** in the configuration menu, which looks like this:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构的重点是使管理员不仅限制用户可以管理的内容，还限制特定项目可用的资源数量。让我们举个例子。如果我们转到**项目**并编辑一个现有项目（或创建一个新项目），我们将在配置菜单中看到一个名为**配额**的选项卡，它看起来像这样：
- en: '![Figure 12.31 – Quotas on the default project'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.31 - 默认项目的配额'
- en: '](img/B14834_12_31.jpg)'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_12_31.jpg)'
- en: Figure 12.31 – Quotas on the default project
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.31 - 默认项目的配额
- en: Once you create a project, you can assign all the resources in the form of quotas.
    This assignment limits the maximum available resources for a particular group
    of instances. The user has no overview of the whole system; they can only *see*
    and utilize resources available through the project. If a user is part of multiple
    projects, they can create, delete, and manage instances based on their role in
    the project, and the resources available to them are specific to a project.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您创建了一个项目，您可以将所有资源分配为配额的形式。这种分配限制了特定实例组的最大可用资源。用户无法查看整个系统；他们只能*看到*和利用通过项目可用的资源。如果用户是多个项目的一部分，他们可以根据其在项目中的角色创建、删除和管理实例，并且他们可用的资源是特定于项目的。
- en: We'll discuss OpenStack/Ansible integration next, as well as some potential
    use cases for these two concepts to work together. Keep in mind that the larger
    the OpenStack environment is, the more use cases we will find for them.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论OpenStack/Ansible集成，以及这两个概念共同工作的一些潜在用例。请记住，OpenStack环境越大，我们将为它们找到的用例就越多。
- en: Integrating OpenStack with Ansible
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将OpenStack与Ansible集成
- en: Dealing with any large-scale application is not easy, and not having the right
    tool can make it impossible. OpenStack provides a lot of ways for us to directly
    orchestrate and manage a huge horizontal deployment, but sometimes, this is not
    enough. Luckily, in our arsenal of tools, we have another one – **Ansible**. In
    [*Chapter 11*](B14834_11_Final_ASB_ePub.xhtml#_idTextAnchor191), *Ansible for
    Orchestration and Automation*, we covered some other, smaller ways to use Ansible
    to deploy and configure individual machines, so we are not going to go back to
    that. Instead, we are going to focus on things that Ansible is good for in the
    OpenStack environment.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 处理任何大规模应用程序都不容易，如果没有正确的工具，可能会变得不可能。OpenStack为我们提供了许多直接编排和管理大规模水平部署的方式，但有时这还不够。幸运的是，在我们的工具库中，我们还有另一个工具
    - **Ansible**。在[*第11章*](B14834_11_Final_ASB_ePub.xhtml#_idTextAnchor191)，*用于编排和自动化的Ansible*中，我们介绍了一些其他使用Ansible部署和配置单个机器的较小方式，因此我们不会回到那里。相反，我们将专注于Ansible在OpenStack环境中的优势。
- en: One thing that we must make clear, though, is that using Ansible in an OpenStack
    environment can be based on two very distinct scenarios. One is using Ansible
    to handle deployed instances, in a way that would pretty much look the same across
    all the other cloud or bare-metal deployments. You, as an administrator of a large
    number of instances, create a management node that is nothing more than a Python-enabled
    server with added Ansible packages and playbooks. After that, you sort out the
    inventory for your deployment and are ready to manage your instances. This scenario
    is not what this part of this chapter is about.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 有一件事我们必须搞清楚，那就是在OpenStack环境中使用Ansible可以基于两种非常不同的场景。一种是使用Ansible来处理部署的实例，这在所有其他云或裸金属部署中看起来几乎是一样的。作为大量实例的管理员，您创建一个管理节点，它只是一个启用Python的服务器，添加了Ansible软件包和playbooks。之后，您整理部署清单，准备好管理您的实例。这种情况不是本章的重点。
- en: What we are talking about here is using Ansible to manage the cloud itself.
    This means we are not deploying instances inside the OpenStack cloud; we are deploying
    compute and storage nodes for OpenStack itself.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里讨论的是使用Ansible来管理云本身。这意味着我们不是在OpenStack云内部部署实例；我们是为OpenStack本身部署计算和存储节点。
- en: 'The environment we are talking about is sometimes referred to as **OpenStack-Ansible**
    (**OSA**) and is common enough to have its own deployment guide, located at the
    following URL: [https://docs.openstack.org/project-deploy-guide/openstack-ansible/latest/](https://docs.openstack.org/project-deploy-guide/openstack-ansible/latest/).'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所说的环境有时被称为**OpenStack-Ansible**（**OSA**），并且足够常见，以至于有自己的部署指南，位于以下URL：[https://docs.openstack.org/project-deploy-guide/openstack-ansible/latest/](https://docs.openstack.org/project-deploy-guide/openstack-ansible/latest/)。
- en: The requirements for a minimal installation in OpenStack-Ansible are considerably
    greater than those in a single VM or on a single machine. The reason for this
    is not just that the system needs all the resources; it's the tools that need
    to be used and the philosophy behind it all.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenStack-Ansible中，最小安装的要求要比单个VM或单台机器上的要求大得多。这不仅是因为系统需要所有资源；还有需要使用的工具和背后的哲学。
- en: 'Let''s quickly go through what Ansible means in terms of OpenStack:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速了解Ansible在OpenStack方面的含义：
- en: Once configured, it enables the quick deployment of any kind of resource, be
    it storage or computing.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦配置完成，它就能快速部署任何类型的资源，无论是存储还是计算。
- en: It makes sure you are not forgetting to configure something in the process.
    When deploying a single server, you will have to make sure that everything works
    and that errors in configuration are easy to spot, but when deploying multiple
    nodes, errors can creep in and degrade the performance of part of the system without
    anyone noticing. The normal deployment practice to avoid this is an installation
    checklist, but Ansible is a much better solution than that.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它确保您在过程中没有忘记配置某些内容。在部署单个服务器时，您必须确保一切正常，并且配置错误易于发现，但在部署多个节点时，错误可能会潜入并降低系统的性能，而没有人注意到。避免这种情况的正常部署实践是安装清单，但Ansible比那更好。
- en: More streamlined configuration changes. Sometimes, we need to apply a confi[guration
    change across the whole system or some part](https://docs.openstack.org/openstack-ansible/latest/)
    of it. This can be frustrating if not scripted.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更简化的配置更改。有时，我们需要对整个系统或其中的一部分应用配置更改。如果没有脚本化，这可能会很令人沮丧。
- en: So, having said all that, let's quickly go through [https://docs.openstack.org/openstack-ansible/latest/](https://docs.openstack.org/openstack-ansible/latest/)
    and see what the official documentation says about how to deploy and use Ansible
    and OpenStack.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，说了这么多，让我们快速浏览一下[https://docs.openstack.org/openstack-ansible/latest/](https://docs.openstack.org/openstack-ansible/latest/)，看看官方文档对如何部署和使用Ansible和OpenStack的建议。
- en: What exactly does OpenStack offer to the administrator in regard to Ansible?
    The simplest answer is playbooks and roles.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack在Ansible方面为管理员提供了什么？最简单的答案是playbooks和roles。
- en: 'To use Ansible to deploy OpenStack, you basically need to create a deployment
    host and then use Ansible to deploy the whole OpenStack system. The whole workflow
    goes something like this:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Ansible部署OpenStack，基本上需要创建一个部署主机，然后使用Ansible部署整个OpenStack系统。整个工作流程大致如下：
- en: Prepare the deployment host
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备部署主机
- en: Prepare the target hosts
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备目标主机
- en: Configure Ansible for deployment
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为部署配置Ansible
- en: Run playbooks and let them install everything
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行playbooks并让它们安装所有内容
- en: Check whether OpenStack is correctly installed
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查OpenStack是否正确安装
- en: 'When we are talking about deployment and target hosts, we need to make a clear
    distinction: the deployment host is a single entity holding Ansible, scripts,
    playbooks, roles, and all the supporting bits. The target hosts are the actual
    servers that are going to be part of the OpenStack cloud.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论部署和目标主机时，我们需要明确区分：部署主机是一个单一实体，包含Ansible、脚本、playbooks、角色和所有支持的部分。目标主机是实际将成为OpenStack云一部分的服务器。
- en: 'The requirements for installation are straightforward:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 安装的要求很简单：
- en: The operating system should be a minimal installation of Debian, Ubuntu CentOS,
    or openSUSE (experimental) with the latest kernel and full updates applied.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作系统应该是Debian、Ubuntu CentOS或openSUSE（实验性）的最小安装，具有最新的内核和完整的更新。
- en: Systems should also run Python 2.7, have SSH access enabled with public key
    authentication, and have NTP time sync enabled. This covers the deployment host.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统还应该运行Python 2.7，启用SSH访问并进行公钥身份验证，并启用NTP时间同步。这涵盖了部署主机。
- en: There are also usual recommendations for different types of nodes. Computing
    nodes must support hardware-assisted virtualization, but that's an obvious requirement.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型的节点也有通常的建议。计算节点必须支持硬件辅助虚拟化，但这是一个明显的要求。
- en: There is a requirement that should go without saying, and that is to use multicore
    processors, with as many cores as possible, to enable some services to run much
    faster.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有一个应该不言而喻的要求，那就是使用多核处理器，尽可能多的核心，以加快某些服务的运行速度。
- en: Disk requirements are really up to you. OpenStack suggests using fast disks
    if possible, recommending SSD drives in a RAID, and large pools of disks available
    for block storage.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 磁盘要求真的取决于你。OpenStack建议尽可能使用快速磁盘，建议在RAID中使用SSD驱动器，并为块存储提供大型磁盘池。
- en: Infrastructure nodes have requirements that are different than the other types
    of nodes since they are running a few databases that grow over time and need at
    least 100 GB of space. The infrastructure also runs its services as containers,
    so it will consume resources in a particular way that will be different than the
    other compute nodes.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础设施节点有不同于其他类型节点的要求，因为它们运行一些随时间增长并且需要至少100 GB空间的数据库。基础设施还以容器形式运行其服务，因此它将以与其他计算节点不同的方式消耗资源。
- en: The deployment guide also suggests running a logging host since all the services
    create logs. The recommended disk space is at least 50 GB for logs, but in production,
    this will quickly grow in orders of magnitude.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 部署指南还建议运行日志主机，因为所有服务都会创建日志。推荐的磁盘空间至少为50 GB，但在生产环境中，这将迅速增长数量级。
- en: OpenStack needs a fast, stable network to work with. Since everything in OpenStack
    will depend on the network, every possible solution that will speed up network
    access is recommended, including using 10G and bonded interfaces. Installing a
    deployment server is the first step in the overall process, which is why we'll
    do that next.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: OpenStack需要一个快速、稳定的网络才能正常工作。由于OpenStack中的所有内容都依赖于网络，建议使用任何可能加快网络访问速度的解决方案，包括使用10G和绑定接口。安装部署服务器是整个过程的第一步，因此我们将在下一步进行。
- en: Installing an Ansible deployment server
  id: totrans-364
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Ansible部署服务器
- en: Our deployment server needs to be up to date with all the upgrades and have
    Python, `git`, `ntp`, `sudo`, and `ssh` support installed. After you've installed
    the required packages, you need to configure the `ssh` keys to be able to log
    into the target hosts. This is an Ansible requirement and is also a best practice
    that leverages security and ease of access.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的部署服务器需要及时更新所有升级，并安装Python、`git`、`ntp`、`sudo`和`ssh`支持。安装所需的软件包后，您需要配置`ssh`密钥以便能够登录到目标主机。这是Ansible的要求，也是一种利用安全性和便利性的最佳实践。
- en: The network is simple – our deployment host must have connectivity to all the
    other hosts. The deployment host should also be installed on the L2 of the network,
    which is designed for container management.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 网络很简单-我们的部署主机必须与所有其他主机保持连接。部署主机还应安装在网络的L2上，该网络专为容器管理而设计。
- en: 'Then, the repository should be cloned:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，应该克隆存储库：
- en: '[PRE3]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, an Ansible bootstrap script needs to be run:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，需要运行一个Ansible引导脚本：
- en: '[PRE4]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This concludes preparing the Ansible deployment server. Now, we need to prepare
    the target computers we are going to use for OpenStack. Target computers are currently
    supported on Ubuntu Server (18.04) LTS, CentOS 7, and openSUSE 42.x (at the time
    of writing, there still isn''t CentOS 8 support). You can use any of these systems.
    For each of them, there is a helpful guide that will get you up and running quickly:
    [https://docs.openstack.org/project-deploy-guide/openstack-ansible/latest/deploymenthost.html](https://docs.openstack.org/project-deploy-guide/openstack-ansible/latest/deploymenthost.html).
    We''ll just explain the general steps to ease you into installing it, but in all
    truth, just copy and paste the commands that have been published for your operating
    system from [https://www.openstack.org/](https://www.openstack.org/).'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了准备Ansible部署服务器的工作。现在，我们需要准备要用于OpenStack的目标计算机。目标计算机目前支持Ubuntu Server（18.04）LTS、CentOS
    7和openSUSE 42.x（在撰写本文时，仍然没有CentOS 8支持）。您可以使用这些系统中的任何一个。对于每个系统，都有一个有用的指南，可以帮助您快速上手：[https://docs.openstack.org/project-deploy-guide/openstack-ansible/latest/deploymenthost.html](https://docs.openstack.org/project-deploy-guide/openstack-ansible/latest/deploymenthost.html)。我们将解释一般步骤以便您轻松安装，但实际上，只需从[https://www.openstack.org/](https://www.openstack.org/)复制并粘贴已发布的命令即可。
- en: No matter which system you decide to run on, you have to be completely up to
    date with system updates. After that, install the `linux-image-extra` package
    (if it exists for your kernel) and install the `bridge-utils`, `debootstrap`,
    `ifenslave`, `lsof`, `lvm2`, `chrony`, `openssh-server`, `sudo`, `tcpdump`, `vlan`,
    and Python packages. Also, enable bonding and VLAN interfacing. All these things
    may or may not be available for your system, so if something is already installed
    or configured, just skip over it.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您决定在哪个系统上运行，都必须完全更新系统更新。之后，安装`linux-image-extra`软件包（如果适用于您的内核），并安装`bridge-utils`、`debootstrap`、`ifenslave`、`lsof`、`lvm2`、`chrony`、`openssh-server`、`sudo`、`tcpdump`、`vlan`和Python软件包。还要启用绑定和VLAN接口。所有这些东西可能适用于您的系统，也可能不适用，因此如果某些内容已安装或配置，只需跳过即可。
- en: Configure the NTP time sync in `chrony.conf` to synchronize time across the
    whole deployment. You can use any time source you like, but for the system to
    work, time must be in sync.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在`chrony.conf`中配置NTP时间同步，以在整个部署中同步时间。您可以使用任何时间源，但为了系统正常工作，时间必须同步。
- en: Now, configure the `ssh` keys. Ansible is going to deploy using `ssh` and key-based
    authentication. Just copy the public keys from the appropriate user on your deployment
    machine to `/root/.ssh/authorized_keys`. Test this setup by simply logging in
    from the deployment host to the target machine. If everything is okay, you should
    be able to log in without any password or any other prompt. Also, note that the
    root user on the deployment host is the default user for managing everything and
    that they have to have their `ssh` keys generated in advance since they are used
    not only on the target hosts but also in all the containers for different services
    running across the system. These keys must exist when you start to configure the
    system.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，配置`ssh`密钥。Ansible将使用`ssh`和基于密钥的身份验证进行部署。只需将部署机器上适当用户的公钥复制到`/root/.ssh/authorized_keys`。通过从部署主机登录到目标机器来测试此设置。如果一切正常，您应该能够无需任何密码或其他提示登录。还要注意，部署主机上的root用户是管理所有内容的默认用户，并且他们必须提前生成他们的`ssh`密钥，因为它们不仅用于目标主机，还用于系统中运行的所有不同服务的所有容器。在开始配置系统时，这些密钥必须存在。
- en: For storage nodes, please note that LVM volumes will be created on the local
    disks, thus overwriting any existing configuration. Network configuration is going
    to be done automatically; you just need to ensure that Ansible is able to connect
    to the target machines.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 对于存储节点，请注意LVM卷将在本地磁盘上创建，从而覆盖任何现有配置。网络配置将自动完成；您只需确保Ansible能够连接到目标机器即可。
- en: The next step is configuring our Ansible inventory so that we can use it. Let's
    do that now.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是配置我们的Ansible清单，以便我们可以使用它。让我们现在就做。
- en: Configuring the Ansible inventory
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置Ansible清单
- en: 'Before we can run the Ansible playbooks, we need to finish configuring the
    Ansible inventory so that it points the system to the hosts it should install
    on. We are going to quote the verbatim, available at [https://docs.openstack.org/project-deploy-guide/openstack-ansible/queens/configure.html](https://docs.openstack.org/project-deploy-guide/openstack-ansible/queens/configure.html):'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们运行Ansible playbooks之前，我们需要完成配置Ansible清单，以便将系统指向应该安装的主机。我们将引用[https://docs.openstack.org/project-deploy-guide/openstack-ansible/queens/configure.html](https://docs.openstack.org/project-deploy-guide/openstack-ansible/queens/configure.html)上提供的原文：
- en: 1\. Copy the contents of the /opt/openstack-ansible/etc/openstack_deploy directory
    to
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 将/opt/openstack-ansible/etc/openstack_deploy目录的内容复制到
- en: the /etc/openstack_deploy directory.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: /etc/openstack_deploy目录。
- en: 2\. Change to the /etc/openstack_deploy directory.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 切换到/etc/openstack_deploy目录。
- en: 3\. Copy the openstack_user_config.yml.example file to
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 将openstack_user_config.yml.example文件复制到
- en: /etc/openstack_deploy/openstack_user_config.yml.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: /etc/openstack_deploy/openstack_user_config.yml。
- en: 4\. Review the openstack_user_config.yml file and make changes to the deployment
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 回顾openstack_user_config.yml文件并对部署进行更改
- en: of your OpenStack environment.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 你的OpenStack环境。
- en: Once inside the configuration file, review all the options. `Openstack_user_config.yml`
    defines which hosts run which services and nodes. Before committing to installing,
    please review the documentation mentioned in the previous paragraph.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 进入配置文件后，审查所有选项。`Openstack_user_config.yml`定义了哪些主机运行哪些服务和节点。在承诺安装之前，请查看前一段提到的文档。
- en: 'One thing that stands out on the web is `install_method`. You can choose either
    source or distro. Each has its pros and cons:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 网上突出的一件事是`install_method`。你可以选择源或发行版。每种方法都有其优缺点：
- en: Source is the simplest installation as it's done directly from the sources on
    the OpenStack official site and contains an environment that's compatible with
    all systems.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源是最简单的安装方式，因为它直接从OpenStack官方网站的源代码中完成，并包含与所有系统兼容的环境。
- en: The distro method is customized for the particular distribution you are installing
    on by using specific packages known to work and known as being stable. The major
    drawback of this is that updates are going to be much slower since not only OpenStack
    needs to be deployed but also information about all the packages on distributions,
    and that setup needs to be verified. As a result, expect long waits between when
    the upgrade reaches the *source* and gets to your *distro* installation. After
    installing, you must go with your primary choice; there is no mechanism for switching
    from one to the other.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发行版方法是根据你正在安装的特定发行版定制的，使用已知可行且稳定的特定软件包。这样做的主要缺点是更新速度会慢得多，因为不仅需要部署OpenStack，还需要关于发行版上所有软件包的信息，并且需要验证该设置。因此，预计在升级到*源*并到达*发行版*安装之间会有很长的等待时间。安装后，您必须选择您的首选项；没有从一个选择切换到另一个的机制。
- en: The last thing you need to do is open the `user_secrets.yml` file and assign
    passwords for all the services. You can either create your own passwords or use
    a script provided just for this purpose.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要做的最后一件事是打开`user_secrets.yml`文件，并为所有服务分配密码。您可以创建自己的密码，也可以使用专门为此目的提供的脚本。
- en: Running Ansible playbooks
  id: totrans-391
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行Ansible playbooks
- en: 'As we go through the deployment process, we will need to start a couple of
    Ansible playbooks. We need to use these three provided playbooks in this order:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行部署过程时，我们需要启动一些Ansible playbooks。我们需要按照以下顺序使用这三个提供的playbooks：
- en: '`setup-hosts.yml` : The initial Ansible playbook that we use to provision the
    necessary services on our OpenStack hosts.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setup-hosts.yml`：我们用来在OpenStack主机上提供必要服务的初始Ansible playbook。'
- en: '`setup-infrastructure.yml`: The Ansible playbook that deploys some more services,
    such as RabbitMQ, repository server, Memcached, and so on.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setup-infrastructure.yml`：部署一些其他服务的Ansible playbook，如RabbitMQ、仓库服务器、Memcached等等。'
- en: '`setup-openstack.yml`: The Ansible playbook that deploys the remaining services
    – Glance, Cinder, Nova, Keystone, Heat, Neutron, Horizon, and so on.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`setup-openstack.yml`：部署剩余服务的Ansible playbook——Glance、Cinder、Nova、Keystone、Heat、Neutron、Horizon等等。'
- en: 'All of these Ansible playbooks need to be finished successfully so that we
    can integrate Ansible with Openstack. So, the only thing left is to run the Ansible
    playbooks. We need to start with the following command:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些Ansible playbooks都需要成功完成，以便我们可以将Ansible与Openstack集成。所以，唯一剩下的就是运行Ansible
    playbooks。我们需要从以下命令开始：
- en: '[PRE5]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can find the appropriate files in `/opt/openstack-ansible/playbooks`. Now,
    run the remaining setups:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在`/opt/openstack-ansible/playbooks`中找到适当的文件。现在，运行剩下的设置：
- en: '[PRE6]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: All the playbooks should finish without unreachable or failed plays. And with
    that – congratulations! You have just installed OpenStack.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 所有playbooks都应该在没有不可达或失败的情况下完成。有了这个——恭喜！你刚刚安装了OpenStack。
- en: Summary
  id: totrans-401
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we spent a lot of time describing the architecture and inner
    workings of OpenStack. We discussed software-defined networking and its challenges,
    as well as different OpenStack services such as Nova, Swift, Glance, and so on.
    Then, we moved on to practical issues, such as deploying Packstack (let's just
    call that OpenStack for proof of concept), and full OpenStack. In the last part
    of this chapter, we discussed OpenStack-Ansible integration and what it might
    mean for us in larger environments.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们花了很多时间描述了OpenStack的架构和内部工作原理。我们讨论了软件定义的网络及其挑战，以及不同的OpenStack服务，如Nova、Swift、Glance等等。然后，我们转向实际问题，比如部署Packstack（让我们称之为OpenStack的概念验证），以及完整的OpenStack。在本章的最后部分，我们讨论了OpenStack-Ansible集成以及在更大的环境中可能对我们意味着什么。
- en: Now that we've covered the *private* cloud aspect, it's time to grow our environment
    and expand it to a more *public* or *hybrid*-based approach. In KVM-based infrastructures,
    this usually means connecting to AWS to convert your workloads and transfer them
    there (public cloud). If we're discussing the hybrid type of cloud functionality,
    then we have to introduce an application called Eucalyptus. For the hows and whys,
    check out the next chapter.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了*私有*云方面，是时候扩展我们的环境，将其扩展到更*公共*或*混合*的方式。在基于 KVM 的基础设施中，这通常意味着连接到 AWS，将您的工作负载转移到那里（公共云）。如果我们讨论混合类型的云功能，则必须介绍一个名为
    Eucalyptus 的应用程序。关于如何和为什么，请查看下一章。
- en: Questions
  id: totrans-404
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is the main problem with VLAN as a cloud overlay technology?
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: VLAN 作为云覆盖技术的主要问题是什么？
- en: Which types of cloud overlay networks are being used on the cloud market today?
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目前云市场上使用的云覆盖网络类型有哪些？
- en: How does VXLAN work?
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: VXLAN 是如何工作的？
- en: What are some of the most common problems with stretching Layer 2 networks across
    multiple sites?
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 跨多个站点延伸 Layer 2 网络的一些常见问题是什么？
- en: What is OpenStack?
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是 OpenStack？
- en: What are the architectural components of OpenStack?
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenStack 的架构组件是什么？
- en: What is OpenStack Nova?
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是 OpenStack Nova？
- en: What is OpenStack Swift?
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是 OpenStack Swift？
- en: What is OpenStack Glance?
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是 OpenStack Glance？
- en: What is OpenStack Horizon?
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是 OpenStack Horizon？
- en: What are OpenStack flavors?
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: OpenStack 的 flavors 是什么？
- en: What is OpenStack Neutron?
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是 OpenStack Neutron？
- en: Further reading
  id: totrans-417
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Please refer to the following links for more information regarding what was
    covered in this chapter:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下链接，了解本章涵盖的更多信息：
- en: 'OpenStack documentation: [https://docs.openstack.org](https://docs.openstack.org)'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenStack 文档：[https://docs.openstack.org](https://docs.openstack.org)
- en: 'Arista VXLAN overview: [https://www.arista.com/assets/data/pdf/Whitepapers/Arista_Networks_VXLAN_White_Paper.pdf](https://www.arista.com/assets/data/pdf/Whitepapers/Arista_Networks_VXLAN_White_Paper.pdf)'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arista VXLAN 概述：[https://www.arista.com/assets/data/pdf/Whitepapers/Arista_Networks_VXLAN_White_Paper.pdf](https://www.arista.com/assets/data/pdf/Whitepapers/Arista_Networks_VXLAN_White_Paper.pdf)
- en: 'Red Hat – What is GENEVE?: [https://www.redhat.com/en/blog/what-geneve](https://www.redhat.com/en/blog/what-geneve)'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 红帽 - 什么是 GENEVE？：[https://www.redhat.com/en/blog/what-geneve](https://www.redhat.com/en/blog/what-geneve)
- en: 'Cisco – Configuring Virtual Networks Using OpenStack: [https://www.cisco.com/c/en/us/td/docs/switches/datacenter/nexus1000/kvm/config_guide/network/5x/b_Cisco_N1KV_KVM_Virtual_Network_Config_5x/configuring_virtual_networks_using_openstack.pdf](https://www.cisco.com/c/en/us/td/docs/switches/datacenter/nexus1000/kvm/config_guide/network/5x/b_Cisco_N1KV_KVM_Virtual_Network_Config_5x/configuring_virtual_networks_using_openstack.pdf)'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 思科 - 使用 OpenStack 配置虚拟网络：[https://www.cisco.com/c/en/us/td/docs/switches/datacenter/nexus1000/kvm/config_guide/network/5x/b_Cisco_N1KV_KVM_Virtual_Network_Config_5x/configuring_virtual_networks_using_openstack.pdf](https://www.cisco.com/c/en/us/td/docs/switches/datacenter/nexus1000/kvm/config_guide/network/5x/b_Cisco_N1KV_KVM_Virtual_Network_Config_5x/configuring_virtual_networks_using_openstack.pdf)
- en: 'Packstack: [http://rdoproject.org](http://rdoproject.org)'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Packstack：[http://rdoproject.org](http://rdoproject.org)

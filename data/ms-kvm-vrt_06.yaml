- en: '*Chapter 4*: Libvirt Networking'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第4章*：Libvirt网络'
- en: Understanding how virtual networking works is really essential for virtualization.
    It would be very hard to justify the costs associated with a scenario in which
    we didn't have virtual networking. Just imagine having multiple virtual machines
    on a virtualization host and buying network cards so that every single one of
    those virtual machines can have their own dedicated, physical network port. By
    implementing virtual networking, we're also consolidating networking in a much
    more manageable way, both from an administration and cost perspective.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 了解虚拟网络如何工作对于虚拟化非常重要。很难证明在没有虚拟网络的情况下，我们可以承担与拥有多个虚拟机的虚拟化主机相关的成本。想象一下，在虚拟化网络中有多个虚拟机，并购买网络卡，以便每个虚拟机都可以拥有自己专用的物理网络端口。通过实施虚拟网络，我们也以更可管理的方式整合了网络，无论是从管理还是成本的角度来看。
- en: This chapter provides you with an insight into the overall concept of virtualized
    networking and Linux-based networking concepts. We will also discuss physical
    and virtual networking concepts, try to compare them, and find similarities and
    differences between them. Also covered in this chapter is the concept of virtual
    switching for a per-host concept and spanned-across-hosts concept, as well as
    some more advanced topics. These topics include single-root input/output virtualization,
    which allows for a much more direct approach to hardware for certain scenarios.
    We will come back to some of the networking concepts later in this book as we
    start discussing cloud overlay networks. This is because the basic networking
    concepts aren't scalable enough for large cloud environments.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章为您提供了对虚拟化网络和基于Linux的网络概念的整体概念。我们还将讨论物理和虚拟网络概念，尝试比较它们，并找出它们之间的相似之处和不同之处。本章还涵盖了虚拟交换的概念，用于主机概念和跨主机概念，以及一些更高级的主题。这些主题包括单根输入/输出虚拟化，它允许对某些场景的硬件采用更直接的方法。随着我们开始讨论云覆盖网络，我们将在本书的后面回顾一些网络概念。这是因为基本的网络概念对于大型云环境来说并不够可扩展。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding physical and virtual networking
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解物理和虚拟网络
- en: Using TAP/TUN
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TAP/TUN
- en: Implementing Linux bridging
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施Linux桥接
- en: Configuring Open vSwitch
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置Open vSwitch
- en: Understanding and configuring SR-IOV
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解和配置SR-IOV
- en: Understanding macvtap
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解macvtap
- en: Let's get started!
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Understanding physical and virtual networking
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解物理和虚拟网络
- en: Let's think about networking for a second. This is a subject that most system
    administrators nowadays understand pretty well. This might not up to the level
    many of us think we do, but still – if we were to try to find an area of system
    administration where we'd find the biggest common level of knowledge, it would
    be networking.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下网络。这是当今大多数系统管理员都相当了解的一个主题。这可能不是我们认为的那么高的水平，但是-如果我们试图找到一个系统管理领域，我们会发现最大的共同知识水平，那就是网络。
- en: So, what's the problem with that?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，问题出在哪里呢？
- en: 'Actually, nothing much. If we really understand physical networking, virtual
    networking is going to be a piece of cake for us. Spoiler alert: *it''s the same
    thing*. If we don''t, it''s going to be exposed rather quickly, because there''s
    no way around it. And the problems are going to get bigger and bigger as time
    goes by because environments evolve and – usually – grow. The bigger they are,
    the more problems they''re going to create, and the more time you''re going to
    spend in debugging mode.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，没有什么。如果我们真正理解物理网络，那么虚拟网络对我们来说将是小菜一碟。剧透警告：*它是一样的*。如果我们不理解，它将很快暴露出来，因为没有绕过它的办法。随着环境的发展和通常的增长，问题会越来越大，因为它们变得越大，它们将产生越多的问题，您将花费更多的时间处于调试模式。
- en: That being said, if you have a firm grasp of VMware or Microsoft-based virtual
    networking purely at a technological level, you're in the clear here as all of
    these concepts are very similar.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，如果您对基于VMware或Microsoft的虚拟网络在技术层面上有很好的掌握，那么这些概念对您来说都是非常相似的。
- en: With that out of the way, what's the whole hoopla about virtual networking?
    It's actually about understanding where things happen, how, and why. This is because,
    physically speaking, virtual networking is literally the same as physical networking.
    Logically speaking, there are some differences that relate more to the *topology*
    of things than to the principle or engineering side of things. And that's what
    usually throws people off a little bit – the fact that there are some weird, software-based
    objects that do the same job as the physical objects that most of us have grown
    used to managing via our favorite CLI-based or GUI-based utilities.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 说到这一点，虚拟网络到底是怎么回事？实际上，这是关于理解事情发生的地方，方式和原因。这是因为从物理上讲，虚拟网络与物理网络完全相同。从逻辑上讲，有一些差异更多地与事物的*拓扑*有关，而不是原则或工程方面的事物。这通常会让人们有点困惑-有一些奇怪的基于软件的对象，它们与大多数人已经习惯通过我们喜爱的基于CLI或GUI的实用程序来管理的物理对象做着相同的工作。
- en: 'First, let''s introduce the basic building block of virtualized networking
    – a virtual switch. A virtual switch is basically a software-based Layer 2 switch
    that you use to do two things:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们介绍虚拟化网络的基本构建块-虚拟交换机。虚拟交换机基本上是一个基于软件的第2层交换机，您可以使用它来做两件事：
- en: Hook up your virtual machines to it.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将您的虚拟机连接到它。
- en: Use its uplinks to connect them to physical server cards so that you can hook
    these physical network cards to a physical switch.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用其上行将它们连接到物理服务器卡，以便您可以将这些物理网络卡连接到物理交换机。
- en: So, let's deal with why we need these virtual switches from the virtual machine
    perspective. As we mentioned earlier, we use a virtual switch to connect virtual
    machines to it. Why? Well, if we didn't have some kind of software object that
    sits in-between our physical network card and our virtual machine, we'd have a
    big problem – we could only connect virtual machines for which we have physical
    network ports to our physical network, and that would be intolerable. First, it
    goes against some of the basic principles of virtualization, such as efficiency
    and consolidation, and secondly, it would cost a lot. Imagine having 20 virtual
    machines on your server. This means that, without a virtual switch, you'd have
    to have at least 20 physical network ports to connect to the physical network.
    On top of that, you'd actually use 20 physical ports on your physical switch as
    well, which would be a disaster.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们从虚拟机的角度来看为什么我们需要这些虚拟交换机。正如我们之前提到的，我们使用虚拟交换机将虚拟机连接到它。为什么呢？如果没有一种软件对象坐在我们的物理网络卡和虚拟机之间，我们会有一个大问题
    - 我们只能连接我们有物理网络端口的虚拟机到我们的物理网络，这是不可容忍的。首先，这违反了虚拟化的一些基本原则，如效率和整合，其次，这将花费很多。想象一下在您的服务器上有20台虚拟机。这意味着，如果没有虚拟交换机，您至少需要20个物理网络端口连接到物理网络。此外，您实际上还会在物理交换机上使用20个物理端口，这将是一场灾难。
- en: So, by introducing a virtual switch between a virtual machine and a physical
    network port, we're solving two problems at the same time – we're reducing the
    number of physical network adapters that we need per server, and we're reducing
    the number of physical switch ports that we need to use to connect our virtual
    machines to the network. We can actually argue that we're solving a third problem
    as well – efficiency – as there are many scenarios where one physical network
    card can handle being an uplink for 20 virtual machines connected to a virtual
    switch. Specifically, there are large parts of our environments that don't consume
    a lot of network traffic and for those scenarios, virtual networking is just amazingly
    efficient.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过在虚拟机和物理网络端口之间引入虚拟交换机，我们同时解决了两个问题 - 我们减少了每台服务器所需的物理网络适配器数量，减少了我们需要用来连接虚拟机到网络的物理交换机端口数量。我们实际上还可以说我们解决了第三个问题
    - 效率 - 因为有许多情况下，一个物理网络卡可以处理连接到虚拟交换机的20台虚拟机的上行流量。具体来说，我们的环境中有很大一部分并不消耗大量网络流量，对于这些情况，虚拟网络只是非常高效的。
- en: Virtual networking
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟网络
- en: Now, in order for that virtual switch to be able to connect to something on
    a virtual machine, we have to have an object to connect to – and that object is
    called a virtual network interface card, often referred to as a vNIC. Every time
    you configure a virtual machine with a virtual network card, you're giving it
    the ability to connect to a virtual switch that uses a physical network card as
    an uplink to a physical switch.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了使虚拟交换机能够连接到虚拟机上的某个东西，我们必须有一个对象来连接 - 这个对象被称为虚拟网络接口卡，通常称为vNIC。每次您配置一个虚拟机与虚拟网络卡，您都赋予它连接到使用物理网络卡作为上行连接到物理交换机的虚拟交换机的能力。
- en: Of course, there are some potential drawbacks to this approach. For example,
    if you have 50 virtual machines connected to the same virtual switch that uses
    the same physical network card as an uplink and that uplink fails (due to a network
    card issue, cable issue, switch port issue, or switch issue), your 50 virtual
    machines won't have access to the physical network. How do we get around this
    problem? By implementing a better design and following the basic design principles
    that we'd use on a physical network as well. Specifically, we'd use more than
    one physical uplink to the same virtual switch.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这种方法也存在一些潜在的缺点。例如，如果您有50台虚拟机连接到使用相同物理网络卡作为上行的同一个虚拟交换机，而该上行失败（由于网络卡问题、电缆问题、交换机端口问题或交换机问题），您的50台虚拟机将无法访问物理网络。我们如何解决这个问题？通过实施更好的设计，并遵循我们在物理网络上也会使用的基本设计原则。具体来说，我们会使用多个物理上行连接到同一个虚拟交换机。
- en: 'Linux has *a lot* of different types of networking interfaces, something like
    20 different types, some of which are as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Linux有*很多*不同类型的网络接口，大约有20种不同类型，其中一些如下：
- en: '**Bridge**: Layer 2 interface for (virtual machine) networking.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Bridge**: 用于（虚拟机）网络的第2层接口。'
- en: '**Bond**: For combining network interfaces to a single interface (for balancing
    and failover reasons) into one logical interface.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Bond**: 用于将网络接口组合成单个接口（用于平衡和故障转移原因）成为一个逻辑接口。'
- en: '**Team**: Different to bonding, teaming doesn''t create one logical interface,
    but can still do balancing and failover.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Team**: 与绑定不同，团队合作不会创建一个逻辑接口，但仍然可以进行平衡和故障转移。'
- en: '**MACVLAN**: Creates multiple MAC addresses on a single physical interface
    (creates subinterfaces) on Layer 2.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MACVLAN**: 在第二层上在单个物理接口上创建多个MAC地址（创建子接口）。'
- en: '**IPVLAN**: Unlike MACVLAN, IPVLAN uses the same MAC address and multiplexes
    on Layer 3.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IPVLAN**: 与MACVLAN不同，IPVLAN使用相同的MAC地址并在第3层上进行复用。'
- en: '**MACVTAP/IPVTAP**: Newer drivers that should simplify virtual networking by
    combining TUN, TAP, and bridge as a single module.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MACVTAP/IPVTAP**: 新的驱动程序，应该通过将TUN、TAP和桥接组合为单个模块来简化虚拟网络。'
- en: '**VXLAN**: A commonly used cloud overlay network concept that we will describe
    in detail in [*Chapter 12*](B14834_12_Final_ASB_ePub.xhtml#_idTextAnchor209),
    *Scaling Out KVM with OpenStack*.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VXLAN**: 一种常用的云覆盖网络概念，我们将在第12章中详细描述，*使用OpenStack扩展KVM*。'
- en: '**VETH**: A virtual Ethernet interface that can be used in a variety of ways
    for local tunneling.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VETH**: 一种可以用于本地隧道的虚拟以太网接口。'
- en: '**IPOIB**: IP over Infiniband. As Infiniband gains traction in HPC/low latency
    networks, this type of networking is also supported by the Linux kernel.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IPOIB**: Infiniband上的IP。随着Infiniband在HPC/低延迟网络中的普及，Linux内核也支持这种类型的网络。'
- en: 'There are a whole host of others. Then, on top of these network interface types,
    there are some 10 types of tunneling interfaces, some of which are as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 还有很多其他的。在这些网络接口类型之上，还有大约10种隧道接口类型，其中一些如下：
- en: '**GRETAP**, **GRE**: Generic Routing Encapsulation protocols for encapsulating
    Layer 2 and Layer 3 protocols, respectively.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GRETAP，GRE：用于封装第2层和第3层协议的通用路由封装协议。
- en: '**GENEVE**: A convergence protocol for cloud overlay networking that''s meant
    to fuse VXLAN, GRE, and others into one. This is why it''s supported in Open vSwitch,
    VMware NSX, and other products.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GENEVE：云覆盖网络的融合协议，旨在将VXLAN、GRE等融合为一个。这就是为什么它受到Open vSwitch、VMware NSX和其他产品的支持。
- en: '**IPIP**: IP over IP tunnel for connecting internal IPv4 subnets via a public
    network.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IPIP：通过公共网络连接内部IPv4子网的IP隧道。
- en: '**SIT**: Simple Internet Translation for interconnecting isolated IPv6 networks
    over IPv4.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SIT：用于在IPv4上互连孤立的IPv6网络的简单互联网翻译。
- en: '**ip6tnl**: IPv4/6 tunnel over IPv6 tunnel interface.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ip6tnl：IPv4/6隧道通过IPv6隧道接口。
- en: '**IP6GRE**, **IP6GRETAP**, and others.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IP6GRE，IP6GRETAP等。
- en: Getting your head around all of them is quite a complex and tedious process,
    so, in this book, we're only going to focus on the types of interfaces that are
    really important to us for virtualization and (later in this book) the cloud.
    This is why we will discuss VXLAN and GENEVE overlay networks in [*Chapter 12*](B14834_12_Final_ASB_ePub.xhtml#_idTextAnchor209),
    *Scaling Out KVM with OpenStack*, as we need to have a firm grip on **Software-Defined
    Networking** (**SDN**) as well.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 理解所有这些内容是一个相当复杂和繁琐的过程，因此在本书中，我们只会关注对虚拟化和（本书后面的内容）云非常重要的接口类型。这就是为什么我们将在[*第12章*](B14834_12_Final_ASB_ePub.xhtml#_idTextAnchor209)中讨论VXLAN和GENEVE覆盖网络，因为我们需要牢牢掌握**软件定义网络**（**SDN**）。
- en: So, specifically, as part of this chapter, we're going to cover TAP/TUN, bridging,
    Open vSwitch, and macvtap interfaces as these are fundamentally the most important
    networking concepts for KVM virtualization.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，具体来说，在本章的一部分中，我们将涵盖TAP/TUN、桥接、Open vSwitch和macvtap接口，因为这些基本上是KVM虚拟化最重要的网络概念。
- en: 'But before we dig deep into that, let''s explain a couple of basic virtual
    network concepts that apply to KVM/libvirt networking and other virtualization
    products (for example, VMware''s hosted virtualization products such as Workstation
    or Player use the same concept). When you start configuring libvirt networking,
    you can choose between three basic types: NAT, routed, and isolated. Let''s discuss
    what these networking modes do.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 但在深入研究之前，让我们解释一些适用于KVM/libvirt网络和其他虚拟化产品的基本虚拟网络概念（例如，VMware的托管虚拟化产品，如Workstation或Player，使用相同的概念）。当您开始配置libvirt网络时，您可以在NAT、路由和隔离之间进行选择。让我们讨论一下这些网络模式的作用。
- en: Libvirt NAT network
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Libvirt NAT网络
- en: In a `192.168.0.0/24` or something like that) for all the devices that we want
    to connect to the internet.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们想要连接到互联网的所有设备（例如`192.168.0.0/24`），我们需要一个NAT网络类型。
- en: Now, let's convert that into a virtualized network example. In our virtual machine
    scenario, this means that our virtual machine can communicate with anything that's
    connected to the physical network via host's IP address, but not the other way
    around. For something to communicate to our virtual machine behind a NAT'd switch,
    our virtual machine has to initiate that communication (or we have to set up some
    kind of port forwarding, but that's beside the point).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将其转换为虚拟化网络示例。在我们的虚拟机场景中，这意味着我们的虚拟机可以通过主机的IP地址与连接到物理网络的任何内容进行通信，但反之则不行。要使某物能够与我们的虚拟机在NAT交换机后面进行通信，我们的虚拟机必须启动该通信（或者我们必须设置某种端口转发，但这不是重点）。
- en: 'The following diagram might explain what we''re talking about a bit better:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表可能更好地解释了我们正在谈论的内容：
- en: '![Figure 4.1 – libvirt networking in NAT mode'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.1 - libvirt NAT模式下的网络'
- en: '](img/B14834_04_01.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_01.jpg)'
- en: Figure 4.1 – libvirt networking in NAT mode
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 - libvirt NAT模式下的网络
- en: From the virtual machine perspective, it's happily sitting in a completely separate
    network segment (hence the `192.168.122.210` and `220` IP addresses) and using
    a virtual network switch as its gateway to access external networks. It doesn't
    have to be concerned with any kind of additional routing as that's one of the
    reasons why we use NAT – to simplify endpoint routing.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从虚拟机的角度来看，它愉快地坐在一个完全独立的网络段中（因此有`192.168.122.210`和`220` IP地址），并使用虚拟网络交换机作为访问外部网络的网关。它不必担心任何额外的路由，因为这就是我们使用NAT的原因之一-简化端点路由。
- en: Libvirt routed network
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Libvirt路由网络
- en: 'The second network type is a routed network, which basically means that our
    virtual machine is directly connected to the physical network via a virtual switch.
    This means that our virtual machine is in the same Layer 2/3 network as the physical
    host. This type of network connection is used very often as, oftentimes, there
    is no need to have a separate NAT network to access your virtual machines in your
    environments. In a way, it just makes everything more complicated, especially
    because you have to configure routing to be aware of the NAT network that you''re
    using for your virtual machines. When using routed mode, the virtual machine sits
    *in the same* network segment as the next physical device. The following diagram
    tells a thousand words about routed networks:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种网络类型是路由网络，基本上意味着我们的虚拟机通过虚拟交换机直接连接到物理网络。这意味着我们的虚拟机与物理主机处于相同的第2/3层网络中。这种类型的网络连接经常被使用，因为通常情况下，没有必要在环境中访问虚拟机时使用单独的NAT网络。在某种程度上，这只会使一切变得更加复杂，特别是因为您必须配置路由以了解您用于虚拟机的NAT网络。在使用路由模式时，虚拟机位于与下一个物理设备*相同*的网络段中。以下图表对路由网络有很好的解释：
- en: '![Figure 4.2 – libvirt networking in routed mode'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.2 - libvirt路由模式下的网络'
- en: '](img/B14834_04_02.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_02.jpg)'
- en: Figure 4.2 – libvirt networking in routed mode
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 - libvirt路由模式下的网络
- en: Now that we've covered the two most commonly used types of virtual machine networking
    scenarios, it's time for the third one, which will seem a bit obscure. If we configure
    a virtual switch without any *uplinks* (which means it has no physical network
    cards attached to it), then that virtual switch can't send traffic to the physical
    network at all. All that's left is communication within the limits of that switch
    itself, hence the name *isolated*. Let's create that elusive isolated network
    now.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了最常用的两种虚拟机网络场景，是时候介绍第三种了，这种情况似乎有点模糊。如果我们配置一个没有*上行*（这意味着它没有物理网络卡连接到它）的虚拟交换机，那么该虚拟交换机根本无法将流量发送到物理网络。剩下的只是在该交换机本身的限制内进行通信，因此称为*隔离*。让我们现在创建这个难以捉摸的隔离网络。
- en: Libvirt isolated network
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Libvirt隔离网络
- en: In this scenario, virtual machines attached to the same isolated switch can
    communicate with each other, but they cannot communicate with anything outside
    the host that they're running on. We used the word *obscure* to describe this
    scenario earlier, but it really isn't – in some ways, it's actually an ideal way
    of *isolating* specific types of traffic so that it doesn't even get to the physical
    network.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，连接到同一隔离交换机的虚拟机可以彼此通信，但它们无法与它们运行的主机之外的任何东西通信。我们之前用“模糊”一词来描述这种情况，但实际上并不是
    - 在某些方面，这实际上是一种*隔离*特定类型的流量的理想方式，以至于它甚至不会到达物理网络。
- en: 'Think of it this way – let''s say that you have a virtual machine that hosts
    a web server, for example, running WordPress. You create two virtual switches:
    one running routed network (direct connection to the physical network) and another
    that''s isolated. Then, you can configure your WordPress virtual machine with
    two virtual network cards, with the first one connected to the routed virtual
    switch and the second one connected to the isolated virtual switch. WordPress
    needs a database, so you create another virtual machine and configure it to use
    an internal virtual switch only. Then, you use that isolated virtual switch to
    *isolate* traffic between the web server and the database server so that WordPress
    connects to the database server via that switch. What did you get by configuring
    your virtual machine infrastructure like this? You have a two-tier application,
    and the most important part of that web application (database) is inaccessible
    from the outside world. Doesn''t seem like that bad of an idea, right?'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这样想吧 - 假设你有一个托管Web服务器的虚拟机，例如运行WordPress。您创建了两个虚拟交换机：一个运行路由网络（直接连接到物理网络），另一个是隔离的。然后，您可以为WordPress虚拟机配置两个虚拟网络卡，第一个连接到路由虚拟交换机，第二个连接到隔离虚拟交换机。WordPress需要一个数据库，所以您创建另一个虚拟机并配置它仅使用内部虚拟交换机。然后，您使用该隔离虚拟交换机来*隔离*Web服务器和数据库服务器之间的流量，以便WordPress通过该交换机连接到数据库服务器。通过这样配置虚拟机基础设施，您得到了什么？您有一个双层应用程序，而该Web应用程序的最重要部分（数据库）无法从外部访问。看起来并不是一个坏主意，对吧？
- en: Isolated virtual networks are used in many other security-related scenarios,
    but this is just an example scenario that we can easily identify with.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 隔离的虚拟网络在许多其他与安全相关的场景中使用，但这只是一个我们可以轻松识别的示例场景。
- en: 'Let''s describe our isolated network with a diagram:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用图表描述我们的隔离网络：
- en: '![Figure 4.3 – libvirt networking in isolated mode'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.3 - libvirt隔离模式下的网络'
- en: '](img/B14834_04_03.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_03.jpg)'
- en: Figure 4.3 – libvirt networking in isolated mode
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 - libvirt隔离模式下的网络
- en: The previous chapter ([*Chapter 3*](B14834_03_Final_ASB_ePub.xhtml#_idTextAnchor049),
    *Installing KVM Hypervisor, libvirt, and ovirt*) of this book mentioned the *default*
    network, and we said that we're going to talk about that a bit later. This seems
    like an opportune moment to do so because now, we have more than enough information
    to describe what the default network configuration is.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的上一章（[*第3章*](B14834_03_Final_ASB_ePub.xhtml#_idTextAnchor049)，*安装KVM Hypervisor、libvirt和ovirt*）提到了*默认*网络，我们说我们稍后会谈论这个。现在似乎是一个合适的时机，因为现在我们已经有足够的信息来描述默认网络配置是什么。
- en: When we install all the necessary KVM libraries and utilities like we did in
    [*Chapter 3*](B14834_03_Final_ASB_ePub.xhtml#_idTextAnchor049), *Installing KVM
    Hypervisor, libvirt, and oVirt*, a default virtual switch gets configured out
    of the box. The reason for this is simple – it's more user-friendly to pre-configure
    something so that users can just start creating virtual machines and connecting
    them to the default network than expect users to configure that as well. VMware's
    vSphere hypervisor does the same thing (the default switch is called vSwitch0),
    and Hyper-V asks us during the deployment process to configure the first virtual
    switch (which we can actually skip and configure later). So, this is just a well-known,
    standardized, established scenario that enables us to start creating our virtual
    machines faster.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们像在[*第3章*](B14834_03_Final_ASB_ePub.xhtml#_idTextAnchor049)中所做的那样安装所有必要的KVM库和实用程序，*安装KVM
    Hypervisor、libvirt和oVirt*，默认的虚拟交换机会被自动配置。这样做的原因很简单 - 预先配置一些东西更加用户友好，这样用户就可以开始创建虚拟机并将它们连接到默认网络，而不是期望用户也配置这一点。VMware的vSphere
    hypervisor也是如此（默认交换机称为vSwitch0），Hyper-V在部署过程中要求我们配置第一个虚拟交换机（实际上我们可以跳过并稍后配置）。因此，这只是一个众所周知的、标准化的、已建立的场景，使我们能够更快地开始创建我们的虚拟机。
- en: The default virtual switch works in NAT mode with the DHCP server active, and
    again, there's a simple reason for that – guest operating systems are, by default
    pre-configured with DHCP networking configuration, which means that the virtual
    machine that we just created is going to poll the network for necessary IP configuration.
    This way, the VM gets all the necessary network configuration and we can start
    using it right away.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 默认虚拟交换机以NAT模式工作，DHCP服务器处于活动状态，再次，这样做的原因很简单 - 客户操作系统默认预配置了DHCP网络配置，这意味着我们刚刚创建的虚拟机将轮询网络以获取必要的IP配置。这样，虚拟机就可以获得所有必要的网络配置，我们可以立即开始使用它。
- en: 'The following diagram shows what the default KVM network does:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了默认的KVM网络的功能：
- en: '![Figure 4.4 – libvirt default network in NAT mode'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.4 - libvirt默认NAT模式网络'
- en: '](img/B14834_04_04.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_04.jpg)'
- en: Figure 4.4 – libvirt default network in NAT mode
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 - libvirt默认NAT模式网络
- en: 'Now, let''s learn how to configure these types of virtual networking concepts
    from the shell and from the GUI. We will treat this procedure as a procedure that
    needs to be done sequentially:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何从shell和GUI中配置这些类型的虚拟网络概念。我们将把这个过程视为一个需要按顺序完成的过程：
- en: Let's start by exporting the default network configuration to XML so that we
    can use it as a template to create a new network:![Figure 4.5 – Exporting the
    default virtual network configuration
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先将默认网络配置导出为XML，以便我们可以将其用作创建新网络的模板：![图4.5 - 导出默认虚拟网络配置
- en: '](img/B14834_04_05.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_05.jpg)'
- en: Figure 4.5 – Exporting the default virtual network configuration
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 - 导出默认虚拟网络配置
- en: 'Now, let''s copy that file to a new file called `packtnat.xml`, edit it, and
    then use it to create a new NAT virtual network. Before we do that, however, we
    need to generate two things – a new object UUID (for our new network) and a unique
    MAC address. A new UUID can be generated from the shell by using the `uuidgen`
    command, but generating a MAC address is a bit trickier. So, we can use the standard
    Red Hat-proposed method available on the Red Hat website: [https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/virtualization_administration_guide/sect-virtualization-tips_and_tricks-generating_a_new_unique_mac_address](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/virtualization_administration_guide/sect-virtualization-tips_and_tricks-generating_a_new_unique_mac_address).
    By using the first snippet of code available at that URL, create a new MAC address
    (for example, `00:16:3e:27:21:c1`).'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们将该文件复制到一个名为`packtnat.xml`的新文件中，编辑它，然后使用它来创建一个新的NAT虚拟网络。然而，在这之前，我们需要生成两样东西
    - 一个新的对象UUID（用于我们的新网络）和一个唯一的MAC地址。可以使用`uuidgen`命令从shell中生成一个新的UUID，但生成MAC地址有点棘手。因此，我们可以使用红帽网站上提供的标准红帽建议方法：[https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/virtualization_administration_guide/sect-virtualization-tips_and_tricks-generating_a_new_unique_mac_address](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/virtualization_administration_guide/sect-virtualization-tips_and_tricks-generating_a_new_unique_mac_address)。通过使用该URL上可用的第一段代码，创建一个新的MAC地址（例如，`00:16:3e:27:21:c1`）。
- en: 'By using `yum` command, install python2:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`yum`命令，安装python2：
- en: '[PRE0]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can now use the `virsh` command to import that configuration and create
    our new virtual network, start that network and make it available permanently,
    and check if everything loaded correctly:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用`virsh`命令导入该配置并创建我们的新虚拟网络，启动该网络并使其永久可用，并检查是否一切加载正确：
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Given that we didn''t delete our default virtual network, the last command
    should give us the following output:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们没有删除默认虚拟网络，最后一个命令应该给我们以下输出：
- en: '![Figure 4.7 – Using virsh net-list to check which virtual networks we have
    on the KVM host'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.7 - 使用virsh net-list检查KVM主机上有哪些虚拟网络'
- en: '](img/B14834_04_07.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_07.jpg)'
- en: Figure 4.7 – Using virsh net-list to check which virtual networks we have on
    the KVM host
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 - 使用virsh net-list检查KVM主机上有哪些虚拟网络
- en: 'Now, let''s create two more virtual networks – a bridged network and an isolated
    network. Again, let''s use files as templates to create both of these networks.
    Keep in mind that, in order to be able to create a bridged network, we are going
    to need a physical network adapter, so we need to have an available physical adapter
    in the server for that purpose. On our server, that interface is called `ens224`,
    while the interface called `ens192` is being used by the default libvirt network.
    So, let''s create two configuration files called `packtro.xml` (for our routed
    network) and `packtiso.xml` (for our isolated network):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建另外两个虚拟网络 - 一个桥接网络和一个隔离网络。同样，让我们使用文件作为模板来创建这两个网络。请记住，为了能够创建一个桥接网络，我们需要一个物理网络适配器，因此我们需要在服务器上有一个可用的物理适配器。在我们的服务器上，该接口被称为`ens224`，而名为`ens192`的接口被默认的libvirt网络使用。因此，让我们创建两个配置文件，分别称为`packtro.xml`（用于我们的路由网络）和`packtiso.xml`（用于我们的隔离网络）：
- en: '![Figure 4.8 – libvirt routed network definition'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.8 - libvirt路由网络定义'
- en: '](img/B14834_04_08.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_08.jpg)'
- en: Figure 4.8 – libvirt routed network definition
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 - libvirt路由网络定义
- en: 'In this specific configuration, we''re using `ens224` as an uplink to the routed
    virtual network, which would use the same subnet (`192.168.2.0/24`) as the physical
    network that `ens224` is connected to:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的配置中，我们使用`ens224`作为路由虚拟网络的上行链路，该虚拟网络将使用与`ens224`连接的物理网络相同的子网（`192.168.2.0/24`）：
- en: '![Figure 4.9 – libvirt isolated network definition'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.9 - libvirt隔离网络定义'
- en: '](img/B14834_04_09.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_09.jpg)'
- en: Figure 4.9 – libvirt isolated network definition
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 - libvirt隔离网络定义
- en: Just to cover our bases, we could have easily configured all of this by using
    the Virtual Machine Manager GUI, as that application has a wizard for creating
    virtual networks as well. But when we're talking about larger environments, importing
    XML is a much simpler process, even when we forget about the fact that a lot of
    KVM virtualization hosts don't have a GUI installed at all.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保万无一失，我们也可以使用虚拟机管理器GUI来配置所有这些，因为该应用程序也有一个用于创建虚拟网络的向导。但是当我们谈论更大的环境时，导入XML是一个更简单的过程，即使我们忘记了很多KVM虚拟化主机根本没有安装GUI。
- en: 'So far, we''ve discussed virtual networking from an overall host-level. However,
    there''s also a different approach to the subject – using a virtual machine as
    an object to which we can add a virtual network card and connect it to a virtual
    network. We can use `virsh` for that purpose. So, just as an example, we can connect
    our virtual machine called `MasteringKVM01` to an isolated virtual network:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了整体主机级别的虚拟网络。然而，还有一种不同的方法来处理这个主题 - 使用虚拟机作为我们可以向其添加虚拟网络适配器并将其连接到虚拟网络的对象。我们可以使用`virsh`来实现这一目的。因此，举个例子，我们可以将名为`MasteringKVM01`的虚拟机连接到一个隔离的虚拟网络：
- en: '[PRE2]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: There are other concepts that allow virtual machine connectivity to a physical
    network, and some of them we will discuss later in this chapter (such as SR-IOV).
    However, now that we've covered the basic approaches to connecting virtual machines
    to a physical network by using a virtual switch/bridge, we need to get a bit more
    technical. The thing is, there are more concepts involved in connecting a virtual
    machine to a virtual switch, such as TAP and TUN, which we will be covering in
    the following section.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他概念允许虚拟机连接到物理网络，其中一些我们将在本章后面讨论（如SR-IOV）。然而，现在我们已经介绍了通过虚拟交换/桥接将虚拟机连接到物理网络的基本方法，我们需要变得更加技术化。问题是，在连接虚拟机到虚拟交换中涉及更多的概念，比如TAP和TUN，我们将在接下来的部分中进行介绍。
- en: Using userspace networking with TAP and TUN devices
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TAP和TUN设备进行用户空间网络连接
- en: 'In [*Chapter 1*](B14834_01_Final_ASB_ePub.xhtml#_idTextAnchor016), *Understanding
    Linux Virtualization*, we used the `virt-host-validate` command to do some pre-flight
    checks in terms of the host''s preparedness for KVM virtualization. As a part
    of that process, some of the checks include checking if the following devices
    exist:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第1章*](B14834_01_Final_ASB_ePub.xhtml#_idTextAnchor016)，*理解Linux虚拟化*中，我们使用`virt-host-validate`命令对主机的KVM虚拟化准备情况进行了一些预检查。作为该过程的一部分，一些检查包括检查以下设备是否存在：
- en: '`/dev/kvm`: The KVM drivers create a `/dev/kvm` character device on the host
    to facilitate direct hardware access for virtual machines. Not having this device
    means that the VMs won''t be able to access physical hardware, although it''s
    enabled in the BIOS and this will reduce the VM''s performance significantly.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/dev/kvm`：KVM驱动程序在主机上创建了一个`/dev/kvm`字符设备，以便为虚拟机提供直接硬件访问。没有这个设备意味着虚拟机将无法访问物理硬件，尽管它在BIOS中已启用，这将显著降低虚拟机的性能。'
- en: '`/dev/vhost-net`: The `/dev/vhost-net` character device will be created on
    the host. This device serves as the interface for configuring the `vhost-net`
    instance. Not having this device significantly reduces the virtual machine''s
    network performance.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/dev/vhost-net`：在主机上将创建`/dev/vhost-net`字符设备。该设备用作配置`vhost-net`实例的接口。没有这个设备会显著降低虚拟机的网络性能。'
- en: '`/dev/net/tun`: This is another character special device used for creating
    TUN/TAP devices to facilitate network connectivity for a virtual machine. The
    TUN/TAP device will be explained in detail in future chapters. For now, just understand
    that having a character device is important for KVM virtualization to work properly.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/dev/net/tun`：这是另一个用于创建TUN/TAP设备以为虚拟机提供网络连接的字符特殊设备。TUN/TAP设备将在以后的章节中详细解释。现在只需理解，拥有一个字符设备对于KVM虚拟化正常工作是很重要的。'
- en: Let's focus on the last device, the TUN device, which is usually accompanied
    by a TAP device.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们专注于最后一个设备，TUN设备，通常会伴随着一个TAP设备。
- en: So far, all the concepts that we've covered include some kind of connectivity
    to a physical network card, with isolated virtual networks being an exception.
    But even an isolated virtual network is just a virtual network for our virtual
    machines. What happens when we have a situation where we need our communication
    to happen in the user space, such as between applications running on a server?
    It would be useless to patch them through some kind of virtual switch concept,
    or a regular bridge, as that would just bring additional overhead. This is where
    TUN/TAP devices come in, providing packet flow for user space programs. Easily
    enough, an application can open `/dev/net/tun` and use an `ioctl()` function to
    register a network device in the kernel, which, in turn, presents itself as a
    tunXX or tapXX device. When the application closes the file, the network devices
    and routes created by it disappear (as described in the kernel `tuntap.txt` documentation).
    So, it's just a type of virtual network interface for the Linux operating system
    supported by the Linux kernel – you can add an IP address and routes to it so
    that traffic from your application can route through it, and not via a regular
    network device.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所涵盖的所有概念都包括与物理网络卡的某种连接，隔离的虚拟网络是一个例外。但即使是隔离的虚拟网络对于我们的虚拟机来说也只是一个虚拟网络。当我们需要在用户空间进行通信时会发生什么，比如在服务器上运行的应用之间？将它们通过某种虚拟交换概念或常规桥接连接起来将会带来额外的开销。这就是TUN/TAP设备的作用，为用户空间程序提供数据包流。很容易，应用程序可以打开`/dev/net/tun`并使用`ioctl()`函数在内核中注册一个网络设备，然后它会呈现为一个tunXX或tapXX设备。当应用程序关闭文件时，它创建的网络设备和路由会消失（如内核`tuntap.txt`文档中所述）。因此，这只是Linux操作系统支持的一种虚拟网络接口类型，可以向其添加IP地址和路由，以便应用程序的流量可以通过它路由，而不是通过常规网络设备。
- en: TUN emulates an L3 device by creating a communication tunnel, something like
    a point-to-point tunnel. It gets activated when the tuntap driver gets configured
    in tun mode. When you activate it, any data that you receive from a descriptor
    (the application that configured it) will be data in the form of regular IP packages
    (as the most commonly used case). Also, when you send data, it gets written to
    the TUN device as regular IP packages. This type of interface is sometimes used
    in testing, development, and debugging for simulation purposes.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: TUN通过创建通信隧道来模拟L3设备，类似于点对点隧道。当tuntap驱动程序配置为tun模式时，它会被激活。激活后，从描述符（配置它的应用程序）接收到的任何数据都将以常规IP数据包的形式传输（作为最常用的情况）。同样，当发送数据时，它会被写入TUN设备作为常规IP数据包。这种类型的接口有时用于测试、开发和模拟调试目的。
- en: The TAP interface basically emulates an L2 Ethernet device. It gets activated
    when the tuntap driver gets configured in tap mode. When you activate it, unlike
    what happens with the TUN interface (Layer 3), you get Layer 2 raw Ethernet packages,
    including ARP/RARP packages and everything else. Basically, we're talking about
    a virtualized Layer 2 Ethernet connection.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: TAP接口基本上模拟L2以太网设备。当tuntap驱动程序以tap模式配置时，它会被激活。当您激活它时，与TUN接口（第3层）不同，您会获得第2层原始以太网数据包，包括ARP/RARP数据包和其他所有内容。基本上，我们谈论的是虚拟化的第2层以太网连接。
- en: These concepts (especially TAP) are usable on libvirt/QEMU as well because by
    using these types of configurations, we can create connections from the host to
    a virtual machine – without the libvirt bridge/switch, just as an example. We
    can actually configure all of the necessary details for the TUN/TAP interface
    and then start deploying virtual machines that are hooked up directly to those
    interfaces by using `kvm-qemu` options. So, it's a rather interesting concept
    that has its place in the virtualization world as well. This is especially interesting
    when we start creating Linux bridges.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念（特别是TAP）也可用于libvirt/QEMU，因为通过使用这些类型的配置，我们可以从主机到虚拟机创建连接 - 例如，没有libvirt桥/交换机。我们实际上可以配置TUN/TAP接口的所有必要细节，然后通过使用`kvm-qemu`选项将虚拟机连接到这些接口。因此，这是一个在虚拟化世界中有其位置的相当有趣的概念。当我们开始创建Linux桥接时，这尤其有趣。
- en: Implementing Linux bridging
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施Linux桥接
- en: 'Let''s create a bridge and then add a TAP device to it. Before we do that,
    we must make sure the bridge module is loaded into the kernel. Let''s get started:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个桥接，然后将TAP设备添加到其中。在这样做之前，我们必须确保桥接模块已加载到内核中。让我们开始吧：
- en: 'If it is not loaded, use `modprobe bridge` to load the module:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果未加载，请使用`modprobe bridge`加载模块：
- en: '[PRE3]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: brctl show command will list all the available bridges on the server, along
    with some basic information, such as the ID of the bridge, Spanning Tree Protocol
    (STP) status, and the interfaces attached to it. Here, the tester bridge does
    not have any interfaces attached to its virtual ports.
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: brctl show命令将列出服务器上所有可用的桥接以及一些基本信息，例如桥接的ID、生成树协议（STP）状态以及连接到其上的接口。在这里，测试器桥没有任何接口连接到其虚拟端口。
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'A Linux bridge will also be shown as a network device. To see the network details
    of the bridge tester, use the `ip` command:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Linux桥接也将显示为网络设备。要查看桥接测试器的网络详细信息，请使用`ip`命令：
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ifconfig tester
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ifconfig tester
- en: 'tester: flags=4098<BROADCAST,MULTICAST>mtu 1500'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 测试器：flags=4098<BROADCAST,MULTICAST>mtu 1500
- en: ether26:84:f2:f8:09:e0txqueuelen 1000 (Ethernet)
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ether26:84:f2:f8:09:e0txqueuelen 1000（以太网）
- en: RX packets 0 bytes 0 (0.0 B)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: RX数据包0 字节0（0.0 B）
- en: RX errors 0 dropped 0 overruns 0 frame 0
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: RX错误0 丢弃0 超限0 帧0
- en: TX packets 0 bytes 0 (0.0 B)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: TX数据包0 字节0（0.0 B）
- en: TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: TX错误0 丢弃0 超限0 载波0 冲突0
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'First, check if the TUN/TAP device module is loaded into the kernel. If not,
    you already know the drill:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，检查TUN/TAP设备模块是否加载到内核中。如果没有，您已经知道该怎么做：
- en: '[PRE7]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'tester and a tap device named vm-vnic. Let''s add vm-vnic to tester:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 测试器和名为vm-vnic的tap设备。让我们将vm-vnic添加到tester：
- en: '[PRE8]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, you can see that `vm-vnic` is an interface that was added to the `tester`
    bridge. Now, `vm-vnic` can act as the interface between your virtual machine and
    the `tester` bridge, which, in turn, enables the virtual machine to communicate
    with other virtual machines that are added to this bridge:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以看到`vm-vnic`是添加到`tester`桥的接口。现在，`vm-vnic`可以作为您的虚拟机和`tester`桥之间的接口，从而使虚拟机能够与添加到此桥的其他虚拟机进行通信：
- en: '![Figure 4.10 – Virtual machines connected to a virtual switch (bridge)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 - 连接到虚拟交换机（桥接）的虚拟机
- en: '](img/B14834_04_10.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_10.jpg)'
- en: Figure 4.10 – Virtual machines connected to a virtual switch (bridge)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 - 连接到虚拟交换机（桥接）的虚拟机
- en: 'You might also need to remove all the objects and configurations that were
    created in the previous procedure. Let''s do this step by step via the command
    line:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能还需要删除在上一个过程中创建的所有对象和配置。让我们通过命令行逐步进行：
- en: 'First, we need to remove the `vm-vnic` tap device from the `tester` bridge:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要从`tester`桥中删除`vm-vnic` tap设备：
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ip tuntap del dev vm-vnic mode tap
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ip tuntap del dev vm-vnic mode tap
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, remove the tester bridge:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，删除测试器桥：
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: These are the same steps that libvirt carried out in the backend while enabling
    or disabling networking for a virtual machine. We want you to understand this
    procedure thoroughly before moving ahead. Now that we've covered Linux bridging,
    it's time to move on to a more advanced concept called Open vSwitch.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是libvirt在后端执行的相同步骤，用于启用或禁用虚拟机的网络。在继续之前，我们希望您彻底了解此过程。现在我们已经介绍了Linux桥接，是时候转向一个更高级的概念，称为Open
    vSwitch。
- en: Configuring Open vSwitch
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置Open vSwitch
- en: Imagine for a second that you're working for a small company that has three
    to four KVM hosts, a couple of network-attached storage devices to host their
    15 virtual machines, and that you've been employed by the company from the very
    start. So, you've seen it all – the company buying some servers, network switches,
    cables, and storage devices, and you were a part of a small team of people that
    built that environment. After 2 years of that process, you're aware of the fact
    that everything works, it's simple to maintain, and doesn't give you an awful
    lot of grief.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你在一家小公司工作，有三到四个KVM主机，几个网络附加存储设备来托管他们的15台虚拟机，并且你从一开始就被公司雇佣。因此，您已经见证了一切 -
    公司购买了一些服务器、网络交换机、电缆和存储设备，并且您是建立该环境的一小部分人员团队。经过2年的过程，您已经意识到一切都运作正常，维护简单，并且没有给您带来太多烦恼。
- en: Now, imagine the life of a friend of yours working for a bigger enterprise company
    that has 400 KVM hosts and close to 2,000 virtual machines to manage, doing the
    same job as you're doing in a comfy chair of your office in your small company.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下，你的一个朋友在一家拥有400个KVM主机和近2000台虚拟机的大型企业公司工作，他们需要管理的工作与你在你的小公司的舒适椅子上所做的工作相同。
- en: Do you think that your friend can manage his or her environment by using the
    very same tools that you're using? XML files for network switch configuration,
    deploying servers from a bootable USB drive, manually configuring everything,
    and having the time to do so? Does that seem like a possibility to you?
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为你的朋友能否通过使用与你相同的工具来管理他或她的环境？使用XML文件进行网络交换机配置，从可引导的USB驱动器部署服务器，手动配置一切，并有时间这样做？这对你来说可能吗？
- en: 'There are two basic problems in this second situation:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种情况中有两个基本问题：
- en: 'The scale of the environment: This one is more obvious. Because of the environment
    size, you need some kind of concept that''s going to be managed centrally, instead
    of on a host-per-host level, such as the virtual switches we''ve discussed so
    far.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境的规模：这一点更为明显。由于环境的规模，您需要一种在中央进行管理的概念，而不是在主机级别进行管理，比如我们迄今讨论过的虚拟交换机。
- en: 'Company policies: These usually dictate some kind of compliance that comes
    from configuration standardization as much as possible. Now, we can agree that
    we could script some configuration updates via Ansible, Puppet, or something like
    that, but what''s the use? We''re going to have to create new config files, new
    procedures, and new workbooks every single time we need to introduce a change
    to KVM networking. And big companies frown upon that.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公司政策：这些通常规定尽可能从配置标准化中获得的一些合规性。现在，我们可以同意我们可以通过Ansible，Puppet或类似工具脚本化一些配置更新，但有什么用呢？每次我们需要对KVM网络进行更改时，我们都必须创建新的配置文件，新的流程和新的工作簿。大公司对此持负面态度。
- en: So, what we need is a centralized networking object that can span across multiple
    hosts and offer configuration consistency. In this context, configuration consistency
    offers us a huge advantage – every change that we introduce in this type of object
    will be replicated to all the hosts that are members of this centralized networking
    object. In other words, what we need is **Open vSwitch** (**OVS**). For those
    who are more versed in VMware-based networking, we can use an approximate metaphor
    – Open vSwitch is for KVM-based environments similar to what vSphere Distributed
    Switch is for VMware-based environments.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们需要的是一个可以跨越多个主机并提供配置一致性的集中式网络对象。在这种情况下，配置一致性为我们带来了巨大的优势 - 我们在这种类型的对象中引入的每个更改都将被复制到所有属于这个集中式网络对象的主机。换句话说，我们需要的是**Open
    vSwitch**（**OVS**）。对于那些更熟悉基于VMware的网络的人来说，我们可以使用一个近似的隐喻 - 对于基于KVM的环境，Open vSwitch类似于vSphere分布式交换机对于基于VMware的环境。
- en: 'In terms of technology, OVS supports the following:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在技术方面，OVS支持以下内容：
- en: VLAN isolation (IEEE 802.1Q)
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VLAN隔离（IEEE 802.1Q）
- en: Traffic filtering
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量过滤
- en: NIC bonding with or without LACP
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有或不具有LACP的NIC绑定
- en: Various overlay networks – VXLAN, GENEVE, GRE, STT, and so on
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种覆盖网络 - VXLAN，GENEVE，GRE，STT等
- en: 802.1ag support
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 802.1ag支持
- en: Netflow, sFlow, and so on
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Netflow，sFlow等
- en: (R)SPAN
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （R）SPAN
- en: OpenFlow
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenFlow
- en: OVSDB
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OVSDB
- en: Traffic queuing and shaping
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量排队和整形
- en: Linux, FreeBSD, NetBSD, Windows, and Citrix support (and a host of others)
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linux，FreeBSD，NetBSD，Windows和Citrix支持（以及其他许多）
- en: Now that we've listed some of the supported technologies, let's discuss the
    way in which Open vSwitch works.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经列出了一些支持的技术，让我们讨论一下Open vSwitch的工作方式。
- en: 'First, let''s talk about the Open vSwitch architecture. The implementation
    of Open vSwitch is broken down into two parts: the Open vSwitch kernel module
    (the data plane) and the user space tools (the control pane). Since the incoming
    data packets must be processed as fast as possible, the data plane of Open vSwitch
    was pushed to the kernel space:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们谈谈Open vSwitch的架构。 Open vSwitch的实现分为两部分：Open vSwitch内核模块（数据平面）和用户空间工具（控制平面）。由于传入的数据包必须尽快处理，因此Open
    vSwitch的数据平面被推到了内核空间：
- en: '![Figure 4.11 – Open vSwitch architecture'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.11 - Open vSwitch架构'
- en: '](img/B14834_04_11.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_11.jpg)'
- en: Figure 4.11 – Open vSwitch architecture
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11 - Open vSwitch架构
- en: The data path (OVS kernel module) uses the netlink socket to interact with the
    vswitchd daemon, which implements and manages any number of OVS switches on the
    local system.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 数据路径（OVS内核模块）使用netlink套接字与vswitchd守护程序进行交互，在本地系统上实现和管理任意数量的OVS交换机。
- en: Open vSwitch doesn't have a specific SDN controller that it uses for management
    purposes, in a similar fashion to VMware's vSphere distributed switch and NSX,
    which have vCenter and various NSX components to manage their capabilities. In
    OVS, the point is to use someone else's SDN controller, which then interacts with
    ovs-vswitchd using the OpenFlow protocol. The ovsdb-server maintains the switch
    table database and external clients can talk to the ovsdb-server using JSON-RPC;
    JSON is the data format. The ovsdb database currently contains around 13 tables
    and this database is persistent across restarts.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Open vSwitch没有特定的SDN控制器用于管理目的，类似于VMware的vSphere分布式交换机和NSX，它们有vCenter和各种NSX组件来管理它们的功能。在OVS中，重点是使用其他人的SDN控制器，然后使用OpenFlow协议与ovs-vswitchd进行交互。ovsdb-server维护交换机表数据库，外部客户端可以使用JSON-RPC与ovsdb-server进行通信；JSON是数据格式。ovsdb数据库目前包含大约13个表，并且此数据库在重新启动时是持久的。
- en: 'Open vSwitch works in two modes: normal and flow mode. This chapter will primarily
    concentrate on how to bring up a KVM VM connected to Open vSwitch''s bridge in
    standalone/normal mode and will a give brief introduction to flow mode using the
    OpenDaylight controller:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Open vSwitch有两种模式：正常模式和流模式。本章将主要集中讨论如何在独立/正常模式下启动连接到Open vSwitch桥的KVM VM，并简要介绍使用OpenDaylight控制器的流模式：
- en: '**Normal Mode**: Switching and forwarding are handled by OVS bridge. In this
    modem OVS acts as an L2 learning switch. This mode is specifically useful when
    configuring several overlay networks for your target rather than manipulating
    the switch''s flow.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正常模式**：交换和转发由OVS桥处理。在这种模式下，OVS充当L2学习交换机。当为目标配置多个覆盖网络而不是操纵交换机流时，此模式特别有用。'
- en: '`ctl` command. This mode allows a greater level of abstraction and automation;
    the SDN controller exposes the REST API. Our applications can make use of this
    API to directly manipulate the bridge''s flows to meet network needs.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ctl`命令。此模式允许更高级别的抽象和自动化；SDN控制器公开了REST API。我们的应用程序可以利用此API直接操纵桥接的流量以满足网络需求。'
- en: 'Let''s move on to the practical aspect and learn how to install Open vSwitch
    on CentOS 8:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续实际操作，学习如何在CentOS 8上安装Open vSwitch：
- en: 'The first thing that we must do is tell our system to use the appropriate repositories.
    In this case, we need to enable the repositories called `epel` and `centos-release-openstack-train`.
    We can do that by using a couple of `yum` commands:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们必须做的第一件事是告诉系统使用适当的存储库。在这种情况下，我们需要启用名为`epel`和`centos-release-openstack-train`的存储库。我们可以通过使用一些`yum`命令来实现：
- en: '[PRE13]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The next step will be installing `openvswitch` from Red Hat''s repository:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步将从Red Hat的存储库安装`openvswitch`：
- en: '[PRE14]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After the installation process, we need to check if everything is working by
    starting and enabling the Open vSwitch service and running the `ovs-vsctl -V`
    command:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装过程完成后，我们需要通过启动和启用Open vSwitch服务并运行`ovs-vsctl -V`命令来检查一切是否正常工作：
- en: '[PRE15]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now that we''ve successfully installed and started Open vSwitch, it''s time
    to configure it. Let''s choose a deployment scenario in which we''re going to
    use Open vSwitch as a new virtual switch for our virtual machines. In our server,
    we have another physical interface called `ens256`, which we''re going to use
    as an uplink for our Open vSwitch virtual switch. We''re also going to clear ens256
    configuration, configure an IP address for our OVS, and start the OVS by using
    the following commands:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经成功安装并启动了Open vSwitch，现在是时候对其进行配置了。让我们选择一个部署方案，在该方案中，我们将使用Open vSwitch作为虚拟机的新虚拟交换机。在我们的服务器中，我们还有另一个名为`ens256`的物理接口，我们将使用它作为Open
    vSwitch虚拟交换机的上行。我们还将清除ens256的配置，为我们的OVS配置IP地址，并使用以下命令启动OVS：
- en: '[PRE16]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now that everything has been configured but not persistently, we need to make
    the configuration persistent. This means configuring some network interface configuration
    files. So, go to `/etc/sysconfig/network-scripts` and create two files. Call one
    of them `ifcfg-ens256` (for our uplink interface):'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在一切都配置好了，但还没有持久化，我们需要使配置持久化。这意味着配置一些网络接口配置文件。因此，转到`/etc/sysconfig/network-scripts`并创建两个文件。将其中一个命名为`ifcfg-ens256`（用于我们的上行接口）：
- en: '[PRE17]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: DEVICE=ovs-br0
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: DEVICE=ovs-br0
- en: DEVICETYPE=ovs
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: DEVICETYPE=ovs
- en: TYPE=OVSBridge
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: TYPE=OVSBridge
- en: BOOTPROTO=static
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: BOOTPROTO=static
- en: IPADDR=10.10.10.1
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: IPADDR=10.10.10.1
- en: NETMASK=255.255.255.0
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: NETMASK=255.255.255.0
- en: GATEWAY=10.10.10.254
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: GATEWAY=10.10.10.254
- en: ONBOOT=yes
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ONBOOT=yes
- en: '[PRE18]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We didn''t configure all of this just for show, so we need to make sure that
    our KVM virtual machines are also able to use it. This means – again – that we
    need to create a KVM virtual network that''s going to use OVS. Luckily, we''ve
    dealt with KVM virtual network XML files before (check the *Libvirt isolated network*
    section), so this one isn''t going to be a problem. Let''s call our network `packtovs`
    and its corresponding XML file `packtovs.xml`. It should contain the following
    content:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不是为了展示而配置所有这些，因此我们需要确保我们的KVM虚拟机也能够使用它。这意味着我们需要创建一个将使用OVS的KVM虚拟网络。幸运的是，我们之前已经处理过KVM虚拟网络XML文件（查看*Libvirt隔离网络*部分），因此这不会成为问题。让我们将我们的网络命名为`packtovs`，其对应的XML文件命名为`packtovs.xml`。它应该包含以下内容：
- en: '[PRE19]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'So, now, we can perform our usual operations when we have a virtual network
    definition in an XML file, which is to define, start, and autostart the network:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在，当我们在XML文件中有一个虚拟网络定义时，我们可以执行我们通常的操作，即定义、启动和自动启动网络：
- en: '[PRE20]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If we left everything as it was when we created our virtual networks, the output
    from `virsh net-list` should look something like this:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在创建虚拟网络时保持一切不变，那么`virsh net-list`的输出应该是这样的：
- en: '![Figure 4.12 – Successful OVS configuration, and OVS+KVM configuration'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.12–成功的OVS配置和OVS+KVM配置'
- en: '](img/B14834_04_12.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_12.jpg)'
- en: Figure 4.12 – Successful OVS configuration, and OVS+KVM configuration
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12–成功的OVS配置和OVS+KVM配置
- en: 'So, all that''s left now is to hook up a VM to our newly defined OVS-based
    network called `packtovs` and we''re home free. Alternatively, we could just create
    a new one and pre-connect it to that specific interface using the knowledge we
    gained in [*Chapter 3*](B14834_03_Final_ASB_ePub.xhtml#_idTextAnchor049), *Installing
    KVM Hypervisor, libvirt, and oVirt*. So, let''s issue the following command, which
    has just two changed parameters (`--name` and `--network`):'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在剩下的就是将VM连接到我们新定义的基于OVS的网络`packtovs`，然后我们就可以放心了。或者，我们可以创建一个新的，并使用我们在[*第3章*](B14834_03_Final_ASB_ePub.xhtml#_idTextAnchor049)中获得的知识预先将其连接到特定接口。因此，让我们发出以下命令，其中只有两个更改的参数（`--name`和`--network`）：
- en: '[PRE21]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After the virtual machine installation completes, we''re connected to the OVS-based
    `packtovs` virtual network, and our virtual machine can use it. Let''s say that
    additional configuration is needed and that we got a request to tag traffic coming
    from this virtual machine with `VLAN ID 5`. Start your virtual machine and use
    the following set of commands:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟机安装完成后，我们连接到基于OVS的`packtovs`虚拟网络，并且我们的虚拟机可以使用它。假设需要进行额外配置，并且我们收到了一个请求，要求标记来自该虚拟机的流量为`VLAN
    ID 5`。启动虚拟机并使用以下一组命令：
- en: '[PRE22]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This command tells us that we''re using the `ens256` port as an uplink and
    that our virtual machine, `MasteringKVM03`, is using the virtual `vnet0` network
    port. We can apply VLAN tagging to that port by using the following command:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令告诉我们，我们正在使用`ens256`端口作为上行，并且我们的虚拟机`MasteringKVM03`正在使用虚拟`vnet0`网络端口。我们可以使用以下命令对该端口应用VLAN标记：
- en: '[PRE23]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We need to take note of some additional commands related to OVS administration
    and management since this is done via the CLI. So, here are some commonly used
    OVS CLI administration commands:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 由于OVS的管理和管理是通过CLI完成的，我们需要注意一些与OVS管理相关的附加命令。因此，以下是一些常用的OVS CLI管理命令：
- en: '`#ovs-vsctl show`: A very handy and frequently used command. It tells us what
    the current running configuration of the switch is.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`#ovs-vsctl show`：一个非常方便和经常使用的命令。它告诉我们交换机当前运行的配置是什么。'
- en: '`#ovs-vsctl list-br`: Lists bridges that were configured on Open vSwitch.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`#ovs-vsctl list-br`：列出在Open vSwitch上配置的桥接。'
- en: '`#ovs-vsctl list-ports <bridge>`: Shows the names of all the ports on `BRIDGE`.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`#ovs-vsctl list-ports <bridge>`：显示`BRIDGE`上所有端口的名称。'
- en: '`#ovs-vsctl list interface <bridge>`: Shows the names of all the interfaces
    on `BRIDGE`.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`#ovs-vsctl list interface <bridge>`：显示`BRIDGE`上所有接口的名称。'
- en: '`#ovs-vsctl add-br <bridge>`: Creates a bridge in the switch database.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`#ovs-vsctl add-br <bridge>`：在交换机数据库中创建一个桥接。'
- en: '`#ovs-vsctl add-port <bridge> : <interface>`: Binds an interface (physical
    or virtual) to the Open vSwitch bridge.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`#ovs-vsctl add-port <bridge> : <interface>`：将接口（物理或虚拟）绑定到Open vSwitch桥接。'
- en: '`#ovs-ofctl and ovs-dpctl`: These two commands are used for administering and
    monitoring flow entries. You learned that OVS manages two kinds of flows: OpenFlows
    and Datapath. The first is managed in the control plane, while the second one
    is a kernel-based flow.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`#ovs-ofctl和ovs-dpctl`：这两个命令用于管理和监视流条目。您了解到OVS管理两种流：OpenFlows和Datapath。第一种是在控制平面中管理的，而第二种是基于内核的流。'
- en: '`#ovs-ofctl`: This speaks to the OpenFlow module, whereas `ovs-dpctl` speaks
    to the Kernel module.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`#ovs-ofctl`：这是针对OpenFlow模块的，而`ovs-dpctl`则是针对内核模块的。'
- en: 'The following examples are the most used options for each of these commands:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例是每个命令的最常用选项：
- en: '`#ovs-ofctl show <BRIDGE>`: Shows brief information about the switch, including
    the port number to port name mapping.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`#ovs-ofctl show <BRIDGE>`：显示有关交换机的简要信息，包括端口号到端口名称的映射。'
- en: '`#ovs-ofctl dump-flows <Bridge>`: Examines OpenFlow tables.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`#ovs-ofctl dump-flows <Bridge>`：检查OpenFlow表。'
- en: '`#ovs-dpctl show`: Prints basic information about all the logical datapaths,
    referred to as *bridges*, present on the switch.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`#ovs-dpctl show`：打印有关交换机上存在的所有逻辑数据路径（称为*桥接*）的基本信息。'
- en: '`#ovs-dpctl dump-flows`: It shows the flow cached in datapath.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`#ovs-dpctl dump-flows`：显示在数据路径中缓存的流。'
- en: '`ovs-appctl`: This command offers a way to send commands to a running Open
    vSwitch and gathers information that is not directly exposed to the `ovs-ofctl`
    command. This is the Swiss Army knife of OpenFlow troubleshooting.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ovs-appctl`：此命令提供了一种向运行中的Open vSwitch发送命令并收集`ovs-ofctl`命令未直接暴露的信息的方法。这是OpenFlow故障排除的瑞士军刀。'
- en: '`#ovs-appctl bridge/dumpflows <br>`: Examines flow tables and offers direct
    connectivity for VMs on the same hosts.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`#ovs-appctl bridge/dumpflows <br>`：检查流表并为同一主机上的VM提供直接连接。'
- en: '`#ovs-appctl fdb/show <br>`: Lists MAC/VLAN pairs learned.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`#ovs-appctl fdb/show <br>`：列出学习到的MAC/VLAN对。'
- en: 'Also, you can always use the `ovs-vsctl show` command to get information about
    the configuration of your OVS switch:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还可以始终使用`ovs-vsctl show`命令获取有关OVS交换机配置的信息：
- en: '![Figure 4.13 – ovs-vsctl show output'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.13 – ovs-vsctl显示输出'
- en: '](img/B14834_04_13.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_13.jpg)'
- en: Figure 4.13 – ovs-vsctl show output
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13 – ovs-vsctl显示输出
- en: We are going to come back to the subject of Open vSwitch in [*Chapter 12*](B14834_12_Final_ASB_ePub.xhtml#_idTextAnchor209),
    *Scaling Out KVM with OpenStack* , as we go deeper into our discussion about spanning
    Open vSwitch across multiple hosts, especially while keeping in mind the fact
    that we want to be able to span our cloud overlay networks (based on GENEVE, VXLAN,
    GRE, or similar protocols) across multiple hosts and sites.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第12章*](B14834_12_Final_ASB_ePub.xhtml#_idTextAnchor209)中回到Open vSwitch的主题，*使用OpenStack扩展KVM*，当我们更深入地讨论跨多个主机跨Open
    vSwitch的情况时，特别是在考虑到我们希望能够跨多个主机和站点扩展我们的云覆盖网络（基于GENEVE、VXLAN、GRE或类似协议）的情况。
- en: Other Open vSwitch use cases
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他Open vSwitch用例
- en: As you might imagine, Open vSwitch isn't just a handy concept for libvirt or
    OpenStack – it can be used for a variety of other scenarios as well. Let's describe
    one of them as it might be important for people looking into VMware NSX or NSX-T
    integration.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能想象的那样，Open vSwitch不仅仅是libvirt或OpenStack的一个方便的概念——它也可以用于各种其他场景。让我们描述其中的一个，因为对于研究VMware
    NSX或NSX-T集成的人来说，这可能很重要。
- en: 'Let''s just describe a few basic terms and relationships here. VMware''s NSX
    is an SDN-based technology that can be used for a variety of use cases:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在这里只描述一些基本术语和关系。VMware的NSX是一种基于SDN的技术，可用于各种用例：
- en: Connecting data centers and extending cloud overlay networks across data center
    boundaries.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接数据中心，跨数据中心边界扩展云覆盖网络。
- en: A variety of disaster recover scenarios. NSX can be a big help for disaster
    recover, for multi-site environments, and for integration with a variety of external
    services and devices that can be a part of the scenario (Palo Alto PANs).
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种灾难恢复场景。NSX可以在灾难恢复、多站点环境以及与各种外部服务和设备集成方面提供大量帮助（Palo Alto PANs）。
- en: Consistent micro-segmentation, across sites, done *the right way* on the virtual
    machine network card level.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一致的微分段，跨站点，在虚拟机网络卡级别上以*正确的方式*完成。
- en: For security purposes, varying from different types of supported VPN technologies
    to connect sites and end users, to distributed firewalls, guest introspection
    options (antivirus and anti-malware), network introspection options (IDS/IPS),
    and more.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 出于安全目的，从不同类型的支持的VPN技术连接站点和终端用户，到分布式防火墙、客户端内省选项（防病毒和反恶意软件）、网络内省选项（IDS/IPS）等各种灾难恢复场景。
- en: For load balancing, up to Layer 7, with SSL offload, session persistence, high
    availablity, application rules, and more.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于负载平衡，直到第7层，具有SSL卸载、会话持久性、高可用性、应用规则等。
- en: Yes, VMware's take on SDN (NSX) and Open vSwitch seem like *competing technologies*
    on the market, but realistically, there are loads of clients who want to use both.
    This is where VMware's integration with OpenStack and NSX's integration with Linux-based
    KVM hosts (by using Open vSwitch and additional agents) comes in really handy.
    Just to further explain these points – there are things that NSX does that take
    *extensive* usage of Open vSwitch-based technologies – hardware VTEP integration
    via Open vSwitch Database, extending GENEVE networks to KVM hosts by using Open
    vSwitch/NSX integration, and much more.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，VMware对SDN（NSX）和Open vSwitch的看法在市场上看起来像是*竞争技术*，但实际上，有很多客户希望同时使用两者。这就是VMware与OpenStack集成以及NSX与基于Linux的KVM主机集成（通过使用Open
    vSwitch和额外的代理）非常方便的地方。再进一步解释一下这些观点 - NSX有一些需要*广泛*使用基于Open vSwitch的技术 - 通过Open
    vSwitch数据库进行硬件VTEP集成，通过使用Open vSwitch/NSX集成将GENEVE网络扩展到KVM主机，等等。
- en: Imagine that you're working for a service provider – a cloud service provider,
    an ISP; basically, any type of company that has large networks with a lot of network
    segmentation. There are loads of service providers using VMware's vCloud Director
    to provide cloud services to end users and companies. However, because of market
    needs, these environments often need to be extended to include AWS (for additional
    infrastructure growth scenarios via the public cloud) or OpenStack (to create
    hybrid cloud scenarios). If we didn't have a possibility to have interoperability
    between these solutions, there would be no way to use both of these offerings
    at the same time. But from a networking perspective, the network background for
    that is NSX or NSX-T (which actually *uses* Open vSwitch).
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你在为一个服务提供商工作 - 一个云服务提供商，一个ISP；基本上，任何具有大量网络分割的大型网络的公司。有很多服务提供商使用VMware的vCloud
    Director为最终用户和公司提供云服务。然而，由于市场需求，这些环境通常需要扩展到包括AWS（通过公共云进行额外基础设施增长场景）或OpenStack（创建混合云场景）。如果我们没有可能在这些解决方案之间实现互操作性，那么就没有办法同时使用这些提供。但从网络的角度来看，这个网络背景是NSX或NSX-T（实际上*使用*了Open
    vSwitch）。
- en: It's been clear for years that the future is all about multi-cloud environments,
    and these types of integrations will bring in more customers; they will want to
    take advantage of these options in their cloud service design. Future developments
    will also most probably include (and already partially include) integration with
    Docker, Kubernetes, and/or OpenShift to be able to manage containers in the same
    environment.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 多云环境的未来已经很清楚多年了，这些类型的集成将带来更多的客户；他们将希望在他们的云服务设计中利用这些选项。未来的发展也很可能包括（并且已经部分包括）与Docker、Kubernetes和/或OpenShift的集成，以便能够在同一环境中管理容器。
- en: There are also some more extreme examples of using hardware – in our example,
    we are talking about network cards on a PCI Express bus – in a *partitioned* way.
    For the time being, our explanation of this concept, called SR-IOV, is going to
    be limited to network cards, but we will expand on the same concept in [*Chapter
    6*](B14834_06_Final_ASB_ePub.xhtml#_idTextAnchor108), *Virtual Display Devices
    and Protocols*, when we start talking about partitioning GPUs for use in virtual
    machines. So, let's discuss a practical example of using SR-IOV on an Intel network
    card that supports it.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些更极端的例子使用硬件 - 在我们的例子中，我们谈论的是以*分区*方式使用PCI Express总线上的网络卡。目前，我们对这个概念SR-IOV的解释将局限于网络卡，但当我们开始讨论在虚拟机中使用分区GPU时，我们将在[*第6章*](B14834_06_Final_ASB_ePub.xhtml#_idTextAnchor108)中扩展相同的概念，*虚拟显示设备和协议*。因此，让我们讨论一下在支持它的Intel网络卡上使用SR-IOV的实际例子。
- en: Understanding and using SR-IOV
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解和使用SR-IOV
- en: 'The SR-IOV concept is something that we already mentioned in [*Chapter 2*](B14834_02_Final_ASB_ePub.xhtml#_idTextAnchor029),
    *KVM as a Virtualization Solution*. By utilizing SR-IOV, we can *partition* PCI
    resources (for example, network cards) into virtual PCI functions and inject them
    into a virtual machine. If we''re using this concept for network cards, we''re
    usually doing this with a single purpose – so that we can avoid using the operating
    system kernel and network stack while accessing a network interface card from
    our virtual machine. In order for us to be able to do this, we need to have hardware
    support, so we need to check if our network card actually supports it. On a physical
    server, we could use the `lspci` command to extract attribute information about
    our PCI devices and then `grep` out *Single Root I/O Virtualization* as a string
    to try to see if we have a device that''s compatible. Here''s an example from
    our server:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: SR-IOV的概念是我们在[*第2章*](B14834_02_Final_ASB_ePub.xhtml#_idTextAnchor029)中已经提到的，*KVM作为虚拟化解决方案*。通过利用SR-IOV，我们可以将PCI资源（例如，网络卡）*分区*为虚拟PCI功能，并将它们注入到虚拟机中。如果我们将这个概念用于网络卡，通常是出于一个目的
    - 那就是我们可以避免使用操作系统内核和网络堆栈，同时访问虚拟机中的网络接口卡。为了能够做到这一点，我们需要硬件支持，因此我们需要检查我们的网络卡是否实际支持它。在物理服务器上，我们可以使用`lspci`命令提取有关我们的PCI设备的属性信息，然后使用`grep`命令将*Single
    Root I/O Virtualization*作为一个字符串来尝试查看我们是否有兼容的设备。这是我们服务器的一个例子：
- en: '![Figure 4.14 – Checking if our system is SR-IOV compatible'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.14 – 检查我们的系统是否兼容SR-IOV'
- en: '](img/B14834_04_14.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_14.jpg)'
- en: Figure 4.14 – Checking if our system is SR-IOV compatible
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14 – 检查我们的系统是否兼容SR-IOV
- en: Important Note
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: 'Be careful when configuring SR-IOV. You need to have a server that supports
    it, a device that supports it, and you must make sure that you turn on SR-IOV
    functionality in BIOS. Then, you need to keep in mind that there are servers that
    only have specific slots assigned for SR-IOV. The server that we used (HP Proliant
    DL380p G8) has three PCI-Express slots assigned to CPU1, but SR-IOV worked only
    in slot #1\. When we connected our card to slot #2 or #3, we got a BIOS message
    that SR-IOV will not work in that slot and that we should move our card to a slot
    that supports SR-IOV. So, please, make sure that you read the documentation of
    your server thoroughly and connect a SR-IOV compatible device to a correct PCI-Express
    slot.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置SR-IOV时要小心。您需要具有支持它的服务器、支持它的设备，并且必须确保您在BIOS中打开了SR-IOV功能。然后，您需要记住，有些服务器只分配了特定的插槽用于SR-IOV。我们使用的服务器（HP
    Proliant DL380p G8）将三个PCI-Express插槽分配给CPU1，但是SR-IOV仅在插槽＃1中起作用。当我们将我们的卡连接到插槽＃2或＃3时，我们收到了一个BIOS消息，指出SR-IOV在该插槽中不起作用，并且我们应该将我们的卡移动到支持SR-IOV的插槽。因此，请务必彻底阅读服务器的文档，并将SR-IOV兼容设备连接到正确的PCI-Express插槽。
- en: 'In this specific case, it''s an Intel 10 Gigabit network adapter with two ports,
    which we could use to do SR-IOV. The procedure isn''t all that difficult, and
    it requires us to complete the following steps:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种特定情况下，这是一个具有两个端口的英特尔10千兆网络适配器，我们可以使用它来执行SR-IOV。该过程并不那么困难，它要求我们完成以下步骤：
- en: Unbind from the previous module.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从先前的模块中解绑。
- en: Register it to the vfio-pci module, which is available in the Linux kernel stack.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其注册到Linux内核堆栈中可用的vfio-pci模块。
- en: Configure a guest that's going to use it.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置将使用它的客户端。
- en: 'So, what you would do is unload the module that the network card is currently
    using by using `modprobe -r`. Then, you would load it again, but by assigning
    an additional parameter. On our specific server, the Intel dual-port adapter that
    we''re using (X540-AT2) was assigned to the `ens1f0` and `ens1f1` network devices.
    So, let''s use `ens1f0` as an example for SR-IOV configuration at boot time:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您要做的是通过使用`modprobe -r`卸载网卡当前正在使用的模块。然后，您会再次加载它，但是通过分配一个附加参数。在我们特定的服务器上，我们使用的英特尔双端口适配器（X540-AT2）被分配给了`ens1f0`和`ens1f1`网络设备。因此，让我们以`ens1f0`作为启动时SR-IOV配置的示例：
- en: 'The first thing that we need to do (as a general concept) is find out which
    kernel module our network card is using. To do that, we need to issue the following
    command:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事（作为一个一般概念）是找出我们的网卡正在使用哪个内核模块。为此，我们需要发出以下命令：
- en: '[PRE24]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'ixgbe module here, and we can do the following:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里使用ixgbe模块，我们可以执行以下操作：
- en: '[PRE25]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then, we can use the `modprobe` system to make these changes permanent across
    reboots by creating a file in `/etc/modprobe.d` called (for example) `ixgbe.conf`
    and adding the following line to it:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用`modprobe`系统通过在`/etc/modprobe.d`中创建一个名为（例如）`ixgbe.conf`的文件，并向其中添加以下行来使这些更改在重新启动时保持永久：
- en: '[PRE27]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This would give us up to four virtual functions that we can use inside our
    virtual machines. Now, the next issue that we need to solve is how to boot our
    server with SR-IOV active at boot time. There are quite a few steps involved here,
    so, let''s get started:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们提供最多四个虚拟功能，我们可以在虚拟机内使用。现在，我们需要解决的下一个问题是如何在服务器启动时激活SR-IOV。这里涉及了相当多的步骤，所以让我们开始吧：
- en: We need to add the `iommu` and `vfs` parameters to the default kernel boot line
    and the default kernel configuration. So, first, open `/etc/default/grub` and
    edit the `GRUB_CMDLINE_LINUX` line and add `intel_iommu=on` (or `amd_iommu=on`
    if you're using an AMD system) and `ixgbe.max_vfs=4` to it.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要将`iommu`和`vfs`参数添加到默认内核引导行和默认内核配置中。因此，首先打开`/etc/default/grub`并编辑`GRUB_CMDLINE_LINUX`行，添加`intel_iommu=on`（如果您使用的是AMD系统，则添加`amd_iommu=on`）和`ixgbe.max_vfs=4`。
- en: 'We need to reconfigure `grub` to use this change, so we need to use the following
    command:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要重新配置`grub`以使用此更改，因此我们需要使用以下命令：
- en: '[PRE28]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Sometimes, even that isn''t enough, so we need to configure the necessary kernel
    parameters, such as the maximum number of virtual functions and the `iommu` parameter
    to be used on the server. That leads us to the following command:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有时，即使这样还不够，因此我们需要配置必要的内核参数，例如虚拟功能的最大数量和服务器上要使用的`iommu`参数。这导致我们使用以下命令：
- en: '[PRE29]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'After reboot, we should be able to see our virtual functions. Type in the following
    command:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 重新启动后，我们应该能够看到我们的虚拟功能。输入以下命令：
- en: '[PRE30]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We should get an output that looks like this:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该得到以下类似的输出：
- en: '![Figure 4.15 –  Checking for virtual function visibility'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.15 - 检查虚拟功能可见性'
- en: '](img/B14834_04_15.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_15.jpg)'
- en: Figure 4.15 – Checking for virtual function visibility
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.15 - 检查虚拟功能可见性
- en: 'We should be able to see these virtual functions from libvirt, and we can check
    that via the `virsh` command. Let''s try this (we''re using `grep 04` because
    our device IDs start with 04, which is visible from the preceding image; we''ll
    shrink the output to important entries only):'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该能够从libvirt中看到这些虚拟功能，并且我们可以通过`virsh`命令进行检查。让我们尝试一下（我们使用`grep 04`，因为我们的设备ID以04开头，这在前面的图像中可见；我们将缩小输出以仅包含重要条目）：
- en: '[PRE31]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The first two devices are our physical functions. The remaining eight devices
    (two ports times four functions) are our virtual devices (from `pci_0000_04_10_0`
    to `pci_0000_04_10_7`). Now, let''s dump that device''s information by using the
    `virsh nodedev-dumpxml pci_0000_04_10_0` command:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个设备是我们的物理功能。其余的八个设备（两个端口乘以四个功能）是我们的虚拟设备（从`pci_0000_04_10_0`到`pci_0000_04_10_7`）。现在，让我们使用`virsh
    nodedev-dumpxml pci_0000_04_10_0`命令来转储该设备的信息：
- en: '![Figure 4.16 – Virtual function information from the perspective of virsh'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.16 - 从virsh的角度查看虚拟功能信息'
- en: '](img/B14834_04_16.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_16.jpg)'
- en: Figure 4.16 – Virtual function information from the perspective of virsh
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16 - 从virsh的角度查看虚拟功能信息
- en: 'So, if we have a running virtual machine that we''d like to reconfigure to
    use this, we''d have to create an XML file with definition that looks something
    like this (let''s call it `packtsriov.xml`):'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们有一个正在运行的虚拟机，我们想要重新配置以使用此功能，我们需要创建一个XML文件，其定义看起来像这样（让我们称其为`packtsriov.xml`）：
- en: '[PRE32]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Of course, the domain, bus, slot, and function need to point exactly to our
    VF. Then, we can use the `virsh` command to attach that device to our virtual
    machine (for example, `MasteringKVM03`):'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，域、总线、插槽和功能需要准确指向我们的VF。然后，我们可以使用`virsh`命令将该设备附加到我们的虚拟机（例如`MasteringKVM03`）：
- en: '[PRE33]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: When we use `virsh dumpxml`, we should now see a part of the output that starts
    with `<driver name='vfio'/>`, along with all the information that we configured
    in the previous step (address type, domain, bus, slot, function). Our virtual
    machine should have no problems using this virtual function as a network card.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用`virsh dumpxml`时，现在应该看到输出的一部分以`<driver name='vfio'/>`开头，以及我们在上一步中配置的所有信息（地址类型、域、总线、插槽、功能）。我们的虚拟机应该没有问题使用这个虚拟功能作为网络卡。
- en: 'Now, it''s time to cover another concept that''s very much useful in KVM networking:
    macvtap. It''s a newer driver that should simplify our virtualized networking
    by completely removing tun/tap and bridge drivers with a single module.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候介绍另一个在KVM网络中非常有用的概念了：macvtap。这是一个较新的驱动程序，应该通过一个模块完全消除tun/tap和桥接驱动程序来简化我们的虚拟化网络。
- en: Understanding macvtap
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解macvtap
- en: This module works like a combination of the tap and macvlan modules. We already
    explained what the tap module does. The macvlan module enables us to create virtual
    networks that are pinned to a physical network interface (usually, we call this
    interface a *lower* interface or device). Combining tap and macvlan enables us
    to choose between four different modes of operation, called **Virtual Ethernet
    Port Aggregator** (**VEPA**), bridge, private, and passthru.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模块的工作方式类似于tap和macvlan模块的组合。我们已经解释了tap模块的功能。macvlan模块使我们能够创建虚拟网络，这些网络固定在物理网络接口上（通常，我们称这个接口为*lower*接口或设备）。结合tap和macvlan使我们能够在**虚拟以太网端口聚合器**（**VEPA**）、桥接、私有和透传四种不同的操作模式之间进行选择。
- en: 'If we''re using the VEPA mode (default mode), the physical switch has to support
    VEPA by supporting `hairpin` mode (also called reflective relay). When a *lower*
    device receives data from a VEPA mode macvlan, this traffic is always sent out
    to the upstream device, which means that traffic is always going through an external
    switch. The advantage of this mode is the fact that network traffic between virtual
    machines becomes visible on the external network, which can be useful for a variety
    of reasons. You can check how network flow works in the following sequence of
    diagrams:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用VEPA模式（默认模式），物理交换机必须通过支持`hairpin`模式（也称为反射中继）来支持VEPA。当一个*lower*设备从VEPA模式macvlan接收数据时，这个流量总是发送到上游设备，这意味着流量总是通过外部交换机进行传输。这种模式的优势在于虚拟机之间的网络流量在外部网络上变得可见，这对于各种原因可能是有用的。您可以查看以下一系列图表中的网络流量是如何工作的：
- en: '![Figure 4.17 – macvtap VEPA mode, where traffic is forced to the external
    network'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.17 – macvtap VEPA模式，流量被强制发送到外部网络'
- en: '](img/B14834_04_17.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_17.jpg)'
- en: Figure 4.17 – macvtap VEPA mode, where traffic is forced to the external network
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17 – macvtap VEPA模式，流量被强制发送到外部网络
- en: 'In private mode, it''s similar to VEPA in that everything goes to an external
    switch, but unlike VEPA, traffic only gets delivered if it''s sent via an external
    router or switch. You can use this mode if you want to isolate virtual machines
    connected to the endpoints from one another, but not from the external network.
    If this sounds very much like a private VLAN scenario, you''re completely correct:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在私有模式下，它类似于VEPA，因为所有的东西都会发送到外部交换机，但与VEPA不同的是，只有通过外部路由器或交换机发送的流量才会被传送。如果您想要将连接到端点的虚拟机相互隔离，但不隔离外部网络，可以使用这种模式。如果这听起来非常像私有VLAN场景，那么您是完全正确的：
- en: '![Figure 4.18 – macvtap in private mode, using it for internal network isolation'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.18 – macvtap在私有模式下，用于内部网络隔离'
- en: '](img/B14834_04_18.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_18.jpg)'
- en: Figure 4.18 – macvtap in private mode, using it for internal network isolation
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.18 – macvtap在私有模式下，用于内部网络隔离
- en: 'In bridge mode, data received on your macvlan that''s supposed to go to another
    macvlan on the same lower device is sent directly to the target, not externally,
    and then routed back. This is very similar to what VMware NSX does when communication
    is supposed to happen between virtual machines on different VXLAN networks, but
    on the same host:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在桥接模式下，接收到的数据在您的macvlan上，应该发送到同一较低设备上的另一个macvlan，直接发送到目标，而不是外部发送，然后路由返回。这与VMware
    NSX在虚拟机应该在不同的VXLAN网络上进行通信时所做的非常相似，但是在同一主机上：
- en: '![Figure 4.19 – macvtap in bridge mode, providing a kind of internal routing'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.19 – macvtap在桥接模式下，提供一种内部路由'
- en: '](img/B14834_04_19.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_19.jpg)'
- en: Figure 4.19 – macvtap in bridge mode, providing a kind of internal routing
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.19 – macvtap在桥接模式下，提供一种内部路由
- en: 'In passthrough mode, we''re basically talking about the SR-IOV scenario, where
    we''re using a VF or a physical device directly to the macvtap interface. The
    key difference is that a single network interface can only be passed to a single
    guest (1:1 relationship):'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在透传模式下，我们基本上在谈论SR-IOV场景，我们将VF或物理设备直接传递给macvtap接口。关键区别在于单个网络接口只能传递给单个客户（1:1关系）：
- en: '![Figure 4.20 – macvtap in passthrough mode'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.20 – macvtap在透传模式下'
- en: '](img/B14834_04_20.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_04_20.jpg)'
- en: Figure 4.20 – macvtap in passthrough mode
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.20 – macvtap在透传模式下
- en: In [*Chapter 12*](B14834_12_Final_ASB_ePub.xhtml#_idTextAnchor209), *Scaling
    Out KVM with OpenStack* and [*Chapter 13*](B14834_13_Final_ASB_ePub.xhtml#_idTextAnchor238),
    *Scaling Out KVM with AWS,* we'll describe why virtualized and *overlay* networking
    (VXLAN, GRE, GENEVE) is even more important for cloud networking as we extend
    our local KVM-based environment to the cloud either via OpenStack or AWS.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第12章*](B14834_12_Final_ASB_ePub.xhtml#_idTextAnchor209)，*使用OpenStack扩展KVM*和[*第13章*](B14834_13_Final_ASB_ePub.xhtml#_idTextAnchor238)，*使用AWS扩展KVM*中，我们将描述为什么虚拟化和*覆盖*网络（VXLAN、GRE、GENEVE）对于云网络非常重要，因为我们将我们的本地KVM环境扩展到云端，无论是通过OpenStack还是AWS。
- en: Summary
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the basics of virtualized networking in KVM and
    explained why virtualized networking is such a huge part of virtualization. We
    went knee-deep into configuration files and their options as this will be the
    preferred method for administration in larger environments, especially when talking
    about virtualized networks.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了KVM中虚拟化网络的基础知识，并解释了为什么虚拟化网络是虚拟化的重要组成部分。我们深入研究了配置文件及其选项，因为这将是在较大环境中进行管理的首选方法，特别是在谈论虚拟化网络时。
- en: Pay close attention to all the configuration steps that we discussed through
    this chapter, especially the part related to using virsh commands to manipulate
    network configuration and to configure Open vSwitch and SR-IOV. SR-IOV-based concepts
    are heavily used in latency-sensitive environments to provide networking services
    with the lowest possible overhead and latency, which is why this principle is
    very important for various enterprise environments related to the financial and
    banking sector.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 请特别注意我们在本章中讨论的所有配置步骤，特别是与使用virsh命令来操作网络配置和配置Open vSwitch和SR-IOV相关的部分。基于SR-IOV的概念在延迟敏感的环境中被广泛使用，以提供具有最低可能开销和延迟的网络服务，这就是为什么这个原则对于与金融和银行业相关的各种企业环境非常重要。
- en: 'Now that we''ve covered all the necessary networking scenarios (some of which
    will be revisited later in this book), it''s time to start thinking about the
    next big topic of the virtualized world. We''ve already talked about CPU and memory,
    as well as networks, which means we''re left with the fourth pillar of virtualization:
    storage. We will tackle that subject in the next chapter.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经涵盖了所有必要的网络场景（其中一些将在本书的后面重新讨论），现在是时候开始考虑虚拟化世界的下一个重要主题了。我们已经讨论了CPU和内存，以及网络，这意味着我们剩下了虚拟化的第四支柱：存储。我们将在下一章中讨论这个主题。
- en: Questions
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Why is it important that virtual switches accept connectivity from multiple
    virtual machines at the same time?
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么虚拟交换机同时接受来自多个虚拟机的连接是重要的？
- en: How does a virtual switch work in NAT mode?
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虚拟交换机在NAT模式下是如何工作的？
- en: How does a virtual switch work in routed mode?
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 虚拟交换机在路由模式下是如何工作的？
- en: What is Open vSwitch and for what purpose can we use it in virtualized and cloud
    environments?
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Open vSwitch是什么，我们可以在虚拟化和云环境中用它来做什么？
- en: Describe the differences between TAP and TUN interfaces.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 描述TAP和TUN接口之间的区别。
- en: Further reading
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Please refer to the following links for more information regarding what was
    covered in this chapter:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 有关本章内容的更多信息，请参考以下链接：
- en: 'Libvirt networking: [https://wiki.libvirt.org/page/VirtualNetworking](https://wiki.libvirt.org/page/VirtualNetworking)'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Libvirt网络：[https://wiki.libvirt.org/page/VirtualNetworking](https://wiki.libvirt.org/page/VirtualNetworking)
- en: 'Network XML format: [https://libvirt.org/formatnetwork.html](https://libvirt.org/formatnetwork.html)'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络XML格式：[https://libvirt.org/formatnetwork.html](https://libvirt.org/formatnetwork.html)
- en: 'Open vSwitch: [https://www.openvswitch.org/](https://www.openvswitch.org/)'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Open vSwitch：[https://www.openvswitch.org/](https://www.openvswitch.org/)
- en: 'Open vSwitch and libvirt: [http://docs.openvswitch.org/en/latest/howto/libvirt/](http://docs.openvswitch.org/en/latest/howto/libvirt/)'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Open vSwitch和libvirt：[http://docs.openvswitch.org/en/latest/howto/libvirt/](http://docs.openvswitch.org/en/latest/howto/libvirt/)
- en: 'Open vSwitch Cheat Sheet: [https://adhioutlined.github.io/virtual/Openvswitch-Cheat-Sheet/](https://adhioutlined.github.io/virtual/Openvswitch-Cheat-Sheet/)'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Open vSwitch速查表：[https://adhioutlined.github.io/virtual/Openvswitch-Cheat-Sheet/](https://adhioutlined.github.io/virtual/Openvswitch-Cheat-Sheet/)

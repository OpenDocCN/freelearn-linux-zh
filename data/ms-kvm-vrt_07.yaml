- en: '*Chapter 5*: Libvirt Storage'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第5章*：Libvirt存储'
- en: This chapter provides you with an insight into the way that KVM uses storage.
    Specifically, we will cover both storage that's internal to the host where we're
    running virtual machines and *shared storage*. Don't let the terminology confuse
    you here – in virtualization and cloud technologies, the term *shared storage*
    means storage space that multiple hypervisors can have access to. As we will explain
    a bit later, the three most common ways of achieving this are by using block-level,
    share-level, or object-level storage. We will use NFS as an example of share-level
    storage, and **Internet Small Computer System Interface** (**iSCSI**) and **Fiber
    Channel** (**FC**) as examples of block-level storage. In terms of object-based
    storage, we will use Ceph. GlusterFS is also commonly used nowadays, so we'll
    make sure that we cover that, too. To wrap everything up in an easy-to-use and
    easy-to-manage box, we will discuss some open source projects that might help
    you while practicing with and creating testing environments.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章节为您提供了KVM使用存储的见解。具体来说，我们将涵盖运行虚拟机的主机内部存储和*共享存储*。不要让术语在这里让您困惑——在虚拟化和云技术中，术语*共享存储*表示多个虚拟化程序可以访问的存储空间。正如我们稍后将解释的那样，实现这一点的三种最常见方式是使用块级、共享级或对象级存储。我们将以NFS作为共享级存储的示例，以**Internet
    Small Computer System Interface**（**iSCSI**）和**Fiber Channel**（**FC**）作为块级存储的示例。在对象级存储方面，我们将使用Ceph。GlusterFS如今也被广泛使用，因此我们也会确保涵盖到它。为了将所有内容整合到一个易于使用和管理的框架中，我们将讨论一些可能在练习和创建测试环境时对您有所帮助的开源项目。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introduction to storage
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储介绍
- en: Storage pools
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储池
- en: NFS storage
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NFS存储
- en: iSCSI and SAN storage
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: iSCSI和SAN存储
- en: Storage redundancy and multipathing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储冗余和多路径处理
- en: Gluster and Ceph as a storage backend for KVM
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gluster和Ceph作为KVM的存储后端
- en: Virtual disk images and formats and basic KVM storage operations
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟磁盘映像和格式以及基本的KVM存储操作
- en: The latest developments in storage – NVMe and NVMeOF
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储的最新发展—NVMe和NVMeOF
- en: Introduction to storage
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储介绍
- en: 'Unlike networking, which is something that most IT people have at least a basic
    understanding of, storage tends to be quite different. In short, yes, it tends
    to be a bit more complex. There are loads of parameters involved, different technologies,
    and…let''s be honest, loads of different types of configuration options and people
    enforcing them. And a *lot* of questions. Here are some of them:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与网络不同，大多数IT人员至少对网络有基本的了解，而存储往往是完全不同的。简而言之，是的，它往往更加复杂。涉及许多参数、不同的技术，而且……坦率地说，有许多不同类型的配置选项和强制执行这些选项的人。还有*很多*问题。以下是其中一些：
- en: Should we configure one NFS share per storage device or two?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该为每个存储设备配置一个NFS共享还是两个？
- en: Should we create one iSCSI target per storage device or two?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该为每个存储设备创建一个iSCSI目标还是两个？
- en: Should we create one FC target or two?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该创建一个FC目标还是两个？
- en: How many **Logical Unit Numbers** (**LUNs**) per target?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个**逻辑单元号**（**LUN**）每个目标应该有多少个？
- en: What kind of cluster size should we use?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该使用何种集群大小？
- en: How should we carry out multipathing?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该如何进行多路径处理？
- en: Should we use block-level or share-level storage?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该使用块级还是共享级存储？
- en: Should we use block-level or object-level storage?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该使用块级还是对象级存储？
- en: Which technology or solution should we choose?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该选择哪种技术或解决方案？
- en: How should we configure caching?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该如何配置缓存？
- en: How should we configure zoning or masking?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该如何配置分区或掩码？
- en: How many switches should we use?
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该使用多少个交换机？
- en: Should we use some kind of clustering technology on a storage level?
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该在存储级别使用某种集群技术吗？
- en: As you can see, the questions just keep piling up, and we've barely touched
    the surface, because there are also questions about which filesystem to use, which
    physical controller we will use to access storage, and what type of cabling—it
    just becomes a big mashup of variables that has many potential answers. What makes
    it worse is the fact that many of those answers can be correct—not just one of
    them.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，问题不断堆积，而我们几乎只是触及了表面，因为还有关于使用哪种文件系统、使用哪种物理控制器来访问存储以及使用何种类型的布线等问题——这些问题变成了一个包含许多潜在答案的大杂烩。更糟糕的是，许多答案都可能是正确的，而不仅仅是其中一个。
- en: Let's get the basic-level mathematics out of the way. In an enterprise-level
    environment, shared storage is usually *the most expensive* part of the environment
    and can also have *the most significant negative impact* on virtual machine performance,
    while at the same time being *the most oversubscribed resource* in that environment.
    Let's think about this for a second—every powered-on virtual machine is constantly
    going to hammer our storage device with I/O operations. If we have 500 virtual
    machines running on a single storage device, aren't we asking a bit too much from
    that storage device?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先把基本的数学问题解决掉。在企业级环境中，共享存储通常是环境中*最昂贵*的部分，同时也可能对虚拟机性能产生*最显著的负面影响*，同时又是该环境中*最过度订阅的资源*。让我们想一想这个问题——每个开机的虚拟机都会不断地向我们的存储设备发送I/O操作。如果我们在单个存储设备上运行了500台虚拟机，那么我们是不是对存储设备要求过高了？
- en: At the same time, some kind of shared storage concept is a key pillar of virtualized
    environments. The basic principle is very simple – there are loads of advanced
    functionalities that will work so much better with shared storage. Also, many
    operations are much faster if shared storage is available. Even more so, there
    are so many simple options for high availability when we don't have our virtual
    machines stored in the same place where they are being executed.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，某种共享存储概念是虚拟化环境的关键支柱。基本原则非常简单——有许多高级功能可以通过共享存储更好地发挥作用。此外，如果有共享存储可用，许多操作会更快。更重要的是，如果我们的虚拟机存储和执行位置不在同一地方，那么高可用性的简单选项就有很多。
- en: 'As a bonus, we can easily avoid **Single Point Of Failure** (**SPOF**) scenarios
    if we design our shared storage environment correctly. In an enterprise-level
    environment, avoiding SPOF is one of the key design principles. But when we start
    adding switches and adapters and controllers to the *to buy* list, our managers''
    or clients'' heads usually starts to hurt. We talk about performance and risk
    management, while they talk about price. We talk about the fact that their databases
    and applications need to be properly fed in terms of I/O and bandwidth, and they
    feel that you can produce that out of thin air. Just wave your magic wand and
    there we are: unlimited storage performance.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个额外的好处，如果我们正确设计共享存储环境，我们可以轻松避免**单点故障**（**SPOF**）的情况。在企业级环境中，避免SPOF是关键设计原则之一。但是当我们开始将交换机、适配器和控制器添加到*购买*清单上时，我们的经理或客户通常会开始头痛。我们谈论性能和风险管理，而他们谈论价格。我们谈论他们的数据库和应用程序需要适当的I/O和带宽供应，而他们觉得你可以凭空产生这些。只需挥动魔术棒，我们就有了：无限的存储性能。
- en: But the best, and our all-time favorite, apples-to-oranges comparison that your
    clients are surely going to try to enforce on you goes something like this…*"the
    shiny new 1 TB NVMe SSD in my laptop has more than 1,000 times more IOPS and more
    than 5 times more performance than your $50,000 storage device, while costing
    100 times less! You have no idea what you're doing!"*
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，你的客户肯定会试图强加给你的最好的、我们永远喜欢的苹果和橙子比较是这样的……“我的闪亮新款1TB NVMe SSD笔记本电脑的IOPS比你的5万美元的存储设备多1000倍，性能比你的存储设备多5倍，而成本却少100倍！你根本不知道你在做什么！”
- en: If you've been there, we feel for you. Rarely will you see so many discussions
    and fights about a piece of hardware in a box. But it's such an essential piece
    of hardware in a box that it's a good fight to have. So, let's explain some key
    concepts that libvirt uses in terms of storage access and how to work with it.
    Then, let's use our knowledge to extract as much performance as possible out of
    our storage system and libvirt using it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经有过这种经历，我们为你感到难过。很少会有这么多关于盒子里的一块硬件的讨论和争论。但它是一个如此重要的盒子里的硬件，这是一场很好的争论。因此，让我们解释一些libvirt在存储访问方面使用的关键概念，以及如何利用我们的知识从我们的存储系统和使用它的libvirt中尽可能多地提取性能。
- en: In this chapter, we're basically going to cover almost all of these storage
    types via installation and configuration examples. Each and every one of these
    has its own use case, but generally, it's going to be up to you to choose what
    you're going to use.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们基本上将通过安装和配置示例涵盖几乎所有这些存储类型。每一种都有自己的用例，但一般来说，你将要选择你要使用的是什么。
- en: So, let's start our journey through these supported protocols and learn how
    to configure them. After we cover storage pools, we are going to discuss NFS,
    a typical share-level protocol for virtual machine storage. Then, we're going
    to move to block-level protocols such as iSCSI and FC. Then, we will move to redundancy
    and multipathing to increase the availability and bandwidth of our storage devices.
    We're also going to cover various use cases for not-so-common filesystems (such
    as Ceph, Gluster, and GFS) for KVM virtualization. We're also going to discuss
    the new developments that are de facto trends right now.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们开始我们通过这些支持的协议的旅程，并学习如何配置它们。在我们讨论存储池之后，我们将讨论NFS，这是一种典型的虚拟机存储的共享级协议。然后，我们将转向块级协议，如iSCSI和FC。然后，我们将转向冗余和多路径，以增加我们存储设备的可用性和带宽。我们还将讨论不太常见的文件系统（如Ceph、Gluster和GFS）在KVM虚拟化中的各种用例。我们还将讨论当前的事实趋势的新发展。
- en: Storage pools
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储池
- en: When you first start using storage devices—even if they're cheaper boxes—you're
    faced with some choices. They will ask you to do a bit of configuration—select
    the RAID level, configure hot-spares, SSD caching...it's a process. The same process
    applies to a situation in which you're building a data center from scratch or
    extending an existing one. You have to configure the storage to be able to use
    it.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当你第一次开始使用存储设备时，即使它们是更便宜的盒子，你也会面临一些选择。他们会要求你进行一些配置——选择RAID级别、配置热备份、SSD缓存……这是一个过程。同样的过程也适用于从头开始构建数据中心或扩展现有数据中心的情况。你必须配置存储才能使用它。
- en: Hypervisors are a bit *picky* when it comes to storage, as there are storage
    types that they support and storage types that they don't support. For example,
    Microsoft's Hyper-V supports SMB shares for virtual machine storage, but it doesn't
    really support NFS storage for virtual machine storage. VMware's vSphere Hypervisor
    supports NFS, but it doesn't support SMB. The reason is simple—a company developing
    a hypervisor chooses and qualifies technologies that its hypervisor is going to
    support. Then, it's up to various HBA/controller vendors (Intel, Mellanox, QLogic,
    and so on) to develop drivers for that hypervisor, and it's up to storage vendor
    to decide which types of storage protocols they're going to support on their storage
    device.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到存储时，虚拟化管理程序有点“挑剔”，因为它们支持一些存储类型，而不支持一些存储类型。例如，微软的Hyper-V支持SMB共享用于虚拟机存储，但实际上不支持NFS存储用于虚拟机存储。VMware的vSphere
    Hypervisor支持NFS，但不支持SMB。原因很简单——一家开发虚拟化管理程序的公司选择并验证其虚拟化管理程序将支持的技术。然后，各种HBA/控制器供应商（英特尔、Mellanox、QLogic等）开发该虚拟化管理程序的驱动程序，存储供应商决定他们的存储设备将支持哪些存储协议。
- en: 'From a CentOS perspective, there are many different storage pool types that
    are supported. Here are some of them:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 从CentOS的角度来看，有许多不同类型的存储池得到支持。以下是其中一些：
- en: '**Logical Volume Manager** (**LVM**)-based storage pools'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于**逻辑卷管理器**（**LVM**）的存储池
- en: Directory-based storage pools
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于目录的存储池
- en: Partition-based storage pools
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于分区的存储池
- en: GlusterFS-based storage pools
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于GlusterFS的存储池
- en: iSCSI-based storage pools
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于iSCSI的存储池
- en: Disk-based storage pools
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于磁盘的存储池
- en: HBA-based storage pools, which use SCSI devices
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于HBA的存储池，使用SCSI设备
- en: From the perspective of libvirt, a storage pool can be a directory, a storage
    device, or a file that libvirt manages. That leads us to 10+ different storage
    pool types, as you're going to see in the next section. From a virtual machine
    perspective, libvirt manages virtual machine storage, which virtual machines use
    so that they have the capacity to store data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从libvirt的角度来看，存储池可以是libvirt管理的目录、存储设备或文件。这导致了10多种不同的存储池类型，你将在下一节中看到。从虚拟机的角度来看，libvirt管理虚拟机存储，虚拟机使用它来存储数据。
- en: oVirt, on the other hand, sees things a bit differently, as it has its own service
    that works with libvirt to provide centralized storage management from a data
    center perspective. *Data center perspective* might seem like a term that's a
    bit odd. But think about it—a datacenter is some kind of *higher-level* object
    in which you can see all of your resources. A data center uses *storage* and *hypervisors*
    to provide us with all of the services that we need in virtualization—virtual
    machines, virtual networks, storage domains, and so on. Basically, from a data
    center perspective, you can see what's happening on all of your hosts that are
    members of that datacenter. However, from a host level, you can't see what's happening
    on another host. It's a hierarchy that's completely logical from both a management
    and a security perspective.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，oVirt看待事情有所不同，因为它有自己的服务与libvirt合作，从数据中心的角度提供集中的存储管理。*数据中心的角度*可能听起来有点奇怪。但想想看——数据中心是一种*更高级*的对象，你可以在其中看到所有的资源。数据中心使用*存储*和*虚拟化平台*为我们提供虚拟化所需的所有服务——虚拟机、虚拟网络、存储域等。基本上，从数据中心的角度来看，你可以看到所有属于该数据中心成员的主机上发生了什么。然而，从主机级别来看，你无法看到另一个主机上发生了什么。从管理和安全的角度来看，这是一个完全合乎逻辑的层次结构。
- en: 'oVirt can centrally manage these different types of storage pools (and the
    list can get bigger or smaller as the years go by):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: oVirt可以集中管理这些不同类型的存储池（随着时间的推移，列表可能会变得更长或更短）：
- en: '**Network File System** (**NFS**)'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络文件系统**（**NFS**）'
- en: '**Parallel NFS** (**pNFS**)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行NFS**（**pNFS**）'
- en: iSCSI
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: iSCSI
- en: FC
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FC
- en: Local storage (attached directly to KVM hosts)
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地存储（直接连接到KVM主机）
- en: GlusterFS exports
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GlusterFS导出
- en: POSIX-compliant file systems
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 符合POSIX的文件系统
- en: 'Let''s take care of some terminology first:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先搞清一些术语：
- en: '**Brtfs** is a type of filesystem that supports snapshots, RAID and LVM-like
    functionality, compression, defragmentation, online resizing, and many other advanced
    features. It was deprecated after it was discovered that its RAID5/6 can easily
    lead to a loss of data.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Brtfs**是一种文件系统，支持快照、RAID和类似LVM的功能、压缩、碎片整理、在线调整大小以及许多其他高级功能。在发现其RAID5/6很容易导致数据丢失后，它被弃用了。'
- en: '**ZFS** is a type of filesystem that supports everything that Brtfs does, plus
    read and write caching.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ZFS**是一种文件系统，支持Brtfs的所有功能，还支持读写缓存。'
- en: CentOS has a new way of dealing with storage pools. Although still in technology
    preview state, it's worth going through the complete configuration via this new
    tool, called **Stratis**. Basically, a couple of years ago, Red Hat finally deprecated
    the idea of pushing Brtfs for future releases and started working on Stratis.
    If you've ever used ZFS, that's where this is probably going—an easy-to-manage,
    ZFS-like, volume-managing set of utilities that Red Hat can stand behind in their
    future releases. Also, just like ZFS, a Stratis-based pool can use cache; so,
    if you have an SSD that you'd like to dedicate to pool cache, you can actually
    do that, as well. If you have been expecting Red Hat to support ZFS, there's a
    fundamental Red Hat policy that stands in the way. Specifically, ZFS is not a
    part of the Linux kernel, mostly because of licensing reasons. Red Hat has a policy
    for these situations—if it's not a part of the kernel (upstream), then they don't
    provide nor support it. As it stands, that's not going to happen anytime soon.
    These policies are also reflected in CentOS.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: CentOS有一种新的处理存储池的方式。虽然仍处于技术预览阶段，但通过这个名为**Stratis**的新工具进行完整配置是值得的。基本上，几年前，Red
    Hat最终放弃了推动Brtfs用于未来版本的想法，开始致力于Stratis。如果你曾经使用过ZFS，那么这可能是类似的——一套易于管理的、类似ZFS的卷管理工具，Red
    Hat可以在未来的发布中支持。此外，就像ZFS一样，基于Stratis的池可以使用缓存；因此，如果你有一块SSD想要专门用于池缓存，你也可以做到。如果你一直期待Red
    Hat支持ZFS，那么有一个基本的Red Hat政策阻碍了这一点。具体来说，ZFS不是Linux内核的一部分，主要是因为许可证的原因。Red Hat对这些情况有一个政策——如果它不是内核的一部分（上游），那么他们就不提供也不支持。就目前而言，这不会很快发生。这些政策也反映在了CentOS中。
- en: Local storage pools
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地存储池
- en: On the other hand, Stratis is available right now. We're going to use it to
    manage our local storage by creating storage pools. Creating a pool requires us
    to set up partitions or disks beforehand. After we create a pool, we can create
    a volume on top of it. We only have to be very careful about one thing—although
    Stratis can manage XFS filesystems, we shouldn't make changes to Stratis-managed
    XFS filesystems directly from the filesystem level. For example, do not reconfigure
    or reformat a Stratis-based XFS filesystem directly from XFS-based commands because
    you'll create havoc on your system.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Stratis现在就可以使用。我们将使用它来管理我们的本地存储，创建存储池。创建池需要我们事先设置分区或磁盘。创建池后，我们可以在其上创建卷。我们只需要非常小心一件事——虽然Stratis可以管理XFS文件系统，但我们不应该直接从文件系统级别对Stratis管理的XFS文件系统进行更改。例如，不要使用基于XFS的命令直接重新配置或重新格式化基于Stratis的XFS文件系统，因为这会在系统上造成混乱。
- en: 'Stratis supports various different types of block storage devices:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Stratis支持各种不同类型的块存储设备：
- en: Hard disks and SSDs
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬盘和固态硬盘
- en: iSCSI LUNs
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: iSCSI LUNs
- en: LVM
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LVM
- en: LUKS
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LUKS
- en: MD RAID
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MD RAID
- en: A device mapper multipath
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设备映射器多路径
- en: NVMe devices
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVMe设备
- en: 'Let''s start from scratch and install Stratis so that we can use it. Let''s
    use the following command:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从头开始安装Stratis，以便我们可以使用它。我们使用以下命令：
- en: '[PRE0]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first command installs the Stratis service and the corresponding command-line
    utilities. The second one will start and enable the Stratis service.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 第一条命令安装了Stratis服务和相应的命令行实用程序。第二条命令将启动并启用Stratis服务。
- en: 'Now, we are going to go through a complete example of how to use Stratis to
    configure your storage devices. We''re going to cover an example of this layered
    approach. So, what we are going to do is as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过一个完整的示例来介绍如何使用Stratis来配置您的存储设备。我们将介绍这种分层方法的一个示例。因此，我们将按照以下步骤进行：
- en: Create a software RAID10 + spare by using MD RAID.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MD RAID创建软件RAID10 +备用
- en: Create a Stratis pool out of that MD RAID device.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从MD RAID设备创建一个Stratis池。
- en: Add a cache device to the pool to use Stratis' cache capability.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向池中添加缓存设备以使用Stratis的缓存功能。
- en: Create a Stratis filesystem and mount it on our local server.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个Stratis文件系统并将其挂载在我们的本地服务器上
- en: 'The premise here is simple—the software RAID10+ spare via MD RAID is going
    to approximate the regular production approach, in which you''d have some kind
    of a hardware RAID controller presenting a single block device to the system.
    We''re going to add a cache device to the pool to verify the caching functionality,
    as this is something that we would most probably do if we were using ZFS, as well.
    Then, we are going to create a filesystem on top of that pool and mount it to
    a local directory with the help of the following commands:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的前提很简单——通过MD RAID的软件RAID10+备用将近似于常规的生产方法，其中您将有某种硬件RAID控制器向系统呈现单个块设备。我们将向池中添加缓存设备以验证缓存功能，因为这是我们在使用ZFS时很可能会做的事情。然后，我们将在该池上创建一个文件系统，并通过以下命令将其挂载到本地目录：
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This mounted filesystem is XFS-formatted. We could then easily use this filesystem
    via NFS export, which is exactly what we're going to do in the NFS storage lesson.
    But for now, this was just an example of how to create a pool by using Stratis.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这个挂载的文件系统是XFS格式的。然后我们可以通过NFS导出轻松地使用这个文件系统，这正是我们将在NFS存储课程中要做的。但现在，这只是一个使用Stratis创建池的示例。
- en: We've covered some basics of local storage pools, which brings us closer to
    our next subject, which is how to use pools from a libvirt perspective. So, that
    will be our next topic.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了本地存储池的一些基础知识，这使我们更接近我们下一个主题，即如何从libvirt的角度使用存储池。因此，这将是我们下一个主题。
- en: Libvirt storage pools
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Libvirt存储池
- en: 'Libvirt manages its own storage pools, which is done with one thing in mind—to
    provide different pools for virtual machine disks and related data. Keeping in
    mind that libvirt uses what the underlying operating system supports, it''s no
    wonder that it supports loads of different storage pool types. A picture is worth
    a thousand words, so here''s a screenshot of creating a libvirt storage pool from
    virt-manager:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Libvirt管理自己的存储池，这是出于一个目的——为虚拟机磁盘和相关数据提供不同的存储池。考虑到libvirt使用底层操作系统支持的内容，它支持多种不同的存储池类型并不奇怪。一幅图值千言，这里有一个从virt-manager创建libvirt存储池的截图：
- en: '![Figure 5.1 – Different storage pool types supported by libvirt'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.1 - libvirt支持的不同存储池类型'
- en: '](img/B14834_05_01.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_01.jpg)'
- en: Figure 5.1 – Different storage pool types supported by libvirt
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 - libvirt支持的不同存储池类型
- en: Out of the box, libvirt already has a predefined default storage pool, which
    is a directory storage pool on the local server. This default pool is located
    in the `/var/lib/libvirt/images` directory. This represents our default location
    where we'll save all the data from locally installed virtual machines.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: libvirt已经预先定义了一个默认存储池，这是本地服务器上的一个目录存储池。此默认池位于`/var/lib/libvirt/images`目录中。这代表了我们将保存所有本地安装的虚拟机的数据的默认位置。
- en: 'We''re going to create various different types of storage pools in the following
    sections—an NFS-based pool, an iSCSI and FC pool, and Gluster and Ceph pools:
    the whole nine yards. We''re also going to explain when to use each and every
    one of them as there will be different usage models involved.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将创建各种不同类型的存储池——基于NFS的存储池，基于iSCSI和FC的存储池，以及Gluster和Ceph存储池：全方位的。我们还将解释何时使用每一种存储池，因为涉及到不同的使用模型。
- en: NFS storage pool
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NFS存储池
- en: 'As a protocol, NFS has been around since the mid-80s. It was originally developed
    by Sun Microsystems as a protocol for sharing files, which is what it''s been
    used for up to this day. Actually, it''s still being developed, which is quite
    surprising for a technology that''s so *old*. For example, NFS version 4.2 came
    out in 2016\. In this version, NFS received a very big update, such as the following:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 作为协议，NFS自80年代中期以来就存在。最初由Sun Microsystems开发为共享文件的协议，直到今天仍在使用。实际上，它仍在不断发展，这对于一项如此“古老”的技术来说是相当令人惊讶的。例如，NFS
    4.2版本于2016年发布。在这个版本中，NFS得到了很大的更新，例如以下内容：
- en: '**Server-side copy**: A feature that significantly enhances the speed of cloning
    operations between NFS servers by carrying out cloning directly between NFS servers'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务器端复制：通过在NFS服务器之间直接进行克隆操作，显著提高了克隆操作的速度
- en: '**Sparse files** and **space reservation**: Features that enhance the way NFS
    works with files that have unallocated blocks, while keeping an eye on capacity
    so that we can guarantee space availability when we need to write data'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏文件和空间保留：增强了NFS处理具有未分配块的文件的方式，同时关注容量，以便在需要写入数据时保证空间可用性
- en: '**Application data block support**: A feature that helps applications that
    work with files as block devices (disks)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序数据块支持：一项帮助与文件作为块设备（磁盘）工作的应用程序的功能
- en: Better pNFS implementation
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的pNFS实现
- en: There are other bits and pieces that were enhanced in v4.2, but for now, this
    is more than enough. You can find even more information about this in IETF's RFC
    7862 document ([https://tools.ietf.org/html/rfc7862](https://tools.ietf.org/html/rfc7862)).
    We're going to focus our attention on the implementation of NFS v4.2 specifically,
    as it's the best that NFS currently has to offer. It also happens to be the default
    NFS version that CentOS 8 supports.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: v4.2中还有其他一些增强的部分，但目前这已经足够了。您可以在IETF的RFC 7862文档中找到更多关于此的信息（[https://tools.ietf.org/html/rfc7862](https://tools.ietf.org/html/rfc7862)）。我们将专注于NFS
    v4.2的实现，因为这是NFS目前提供的最好的版本。它也恰好是CentOS 8支持的默认NFS版本。
- en: 'The first thing that we have to do is install the necessary packages. We''re
    going to achieve that by using the following commands:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先要做的事情是安装必要的软件包。我们将使用以下命令来实现这一点：
- en: '[PRE2]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The first command installs the necessary utilities to run the NFS server. The
    second one is going to start it and permanently enable it so that the NFS service
    is available after reboot.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 第一条命令安装了运行NFS服务器所需的实用程序。第二条命令将启动它并永久启用它，以便在重新启动后NFS服务可用。
- en: 'Our next task is to configure what we''re going to share via the NFS server.
    For that, we need to *export* a directory and make it available to our clients
    over the network. NFS uses a configuration file, `/etc/exports`, for that purpose.
    Let''s say that we want to create a directory called `/exports`, and then share
    it to our clients in the `192.168.159.0/255.255.255.0` network, and we want to
    allow them to write data on that share. Our `/etc/exports` file should look like
    this:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来的任务是配置我们将通过NFS服务器共享的内容。为此，我们需要*导出*一个目录，并使其在网络上对我们的客户端可用。NFS使用一个配置文件`/etc/exports`来实现这个目的。假设我们想要创建一个名为`/exports`的目录，然后将其共享给我们在`192.168.159.0/255.255.255.0`网络中的客户端，并且我们希望允许他们在该共享上写入数据。我们的`/etc/exports`文件应该如下所示：
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: These configuration options tell our NFS server which directory to export (`/exports`),
    to which clients (`192.168.159.0/24`), and what options to use (`rw` means read-write).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这些配置选项告诉我们的NFS服务器要导出哪个目录（`/exports`），导出到哪些客户端（`192.168.159.0/24`），以及使用哪些选项（`rw`表示读写）。
- en: 'Some other available options include the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 其他可用选项包括以下内容：
- en: '`ro`: Read-only mode.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ro`：只读模式。'
- en: '`sync`: Synchronous I/O operations.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sync`：同步I/O操作。'
- en: '`root_squash`: All I/O operations from `UID 0` and `GID 0` are mapped to configurable
    anonymous UIDs and GIDs (the `anonuid` and `anongid` options).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`root_squash`：来自`UID 0`和`GID 0`的所有I/O操作都映射到可配置的匿名UID和GID（`anonuid`和`anongid`选项）。'
- en: '`all_squash`: All I/O operations from any UIDs and GIDs are mapped to anonymous
    UIDs and GIDs (`anonuid` and `anongid` options).'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`all_squash`：来自任何UID和GID的所有I/O操作都映射到匿名UID和GID（`anonuid`和`anongid`选项）。'
- en: '`no_root_squash`: All I/O operations from `UID 0` and `GID 0` are mapped to
    `UID 0` and `GID 0`.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`no_root_squash`：来自`UID 0`和`GID 0`的所有I/O操作都映射到`UID 0`和`GID 0`。'
- en: 'If you need to apply multiple options to the exported directory, you add them
    with a comma between them, as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要将多个选项应用到导出的目录中，可以在它们之间用逗号添加，如下所示：
- en: '[PRE4]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can use fully qualified domain names or short hostnames (if they''re resolvable
    by DNS or any other mechanism). Also, if you don''t like using prefixes (`24`),
    you can use regular netmasks, as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用完全合格的域名或短主机名（如果它们可以通过DNS或任何其他机制解析）。此外，如果您不喜欢使用前缀（`24`），您可以使用常规的网络掩码，如下所示：
- en: '[PRE5]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that we have configured the NFS server, let''s see how we''re going to
    configure libvirt to use that server as a storage pool. As always, there are a
    couple of ways to do this. We could just create an XML file with the pool definition
    and import it to our KVM host by using the `virsh pool-define --file` command.
    Here''s an example of that configuration file:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经配置了NFS服务器，让我们看看我们将如何配置libvirt来使用该服务器作为存储池。和往常一样，有几种方法可以做到这一点。我们可以只创建一个包含池定义的XML文件，并使用`virsh
    pool-define --file`命令将其导入到我们的KVM主机中。以下是该配置文件的示例：
- en: '![Figure 5.2 – Example XML configuration file for NFS pool'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.2 - NFS池的XML配置文件示例'
- en: '](img/B14834_05_02.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_02.jpg)'
- en: Figure 5.2 – Example XML configuration file for NFS pool
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 - NFS池的XML配置文件示例
- en: 'Let''s explain these configuration options:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解释一下这些配置选项：
- en: '`pool type`: `netfs` means that we are going to use an NFS file share.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`池类型`：`netfs`表示我们将使用NFS文件共享。'
- en: '`name`: The pool name, as libvirt uses pools as named objects, just like virtual
    networks.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`：池名称，因为libvirt使用池作为命名对象，就像虚拟网络一样。'
- en: '`host` : The address of the NFS server that we are connecting to.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`host`：我们正在连接的NFS服务器的地址。'
- en: '`dir path`: The NFS export path that we configured on the NFS server via `/etc/exports`.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dir path`：我们在NFS服务器上通过`/etc/exports`配置的NFS导出路径。'
- en: '`path`: The local directory on our KVM host where that NFS share is going to
    be mounted to.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`path`：我们的KVM主机上的本地目录，该NFS共享将被挂载到该目录。'
- en: '`permissions`: The permissions used for mounting this filesystem.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`permissions`：用于挂载此文件系统的权限。'
- en: '`owner` and `group`: The UID and GID used for mounting purposes (that''s why
    we exported the folder earlier with the `no_root_squash` option).'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`owner`和`group`：用于挂载目的的UID和GID（这就是为什么我们之前使用`no_root_squash`选项导出文件夹的原因）。'
- en: '`label`: The SELinux label for this folder—we''re going to discuss this in
    [*Chapter 16*](B14834_16_Final_ASB_ePub.xhtml#_idTextAnchor302), *Troubleshooting
    Guideline for the KVM Platform*.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label`：此文件夹的SELinux标签-我们将在[*第16章*](B14834_16_Final_ASB_ePub.xhtml#_idTextAnchor302)，*KVM平台故障排除指南*中讨论这个问题。'
- en: 'If we wanted, we could''ve easily done the same thing via the Virtual Machine
    Manager GUI. First, we would have to select the correct type (the NFS pool) and
    give it a name:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们愿意，我们本可以通过虚拟机管理器GUI轻松地完成相同的事情。首先，我们需要选择正确的类型（NFS池），并给它起一个名字：
- en: '![Figure 5.3 – Selecting the NFS pool type and giving it a name'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.3 - 选择NFS池类型并给它命名'
- en: '](img/B14834_05_03.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_03.jpg)'
- en: Figure 5.3 – Selecting the NFS pool type and giving it a name
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 - 选择NFS池类型并给它命名
- en: 'After we click **Forward**, we can move to the final configuration step, where
    we need to tell the wizard which server we''re mounting our NFS share from:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**前进**后，我们可以进入最后的配置步骤，需要告诉向导我们从哪个服务器挂载我们的NFS共享：
- en: '![Figure 5.4 – Configuring NFS server options'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.4–配置NFS服务器选项'
- en: '](img/B14834_05_04.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_04.jpg)'
- en: Figure 5.4 – Configuring NFS server options
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4–配置NFS服务器选项
- en: 'When we finish typing in these configuration options (**Host Name** and **Source
    Path**), we can press **Finish**, which will mean exiting the wizard. Also, our
    previous configuration screen, which only contained the **default** storage pool,
    now has our newly configured pool listed as well:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们完成输入这些配置选项（**主机名**和**源路径**）后，我们可以点击**完成**，这意味着退出向导。此外，我们之前的配置屏幕，只包含**默认**存储池，现在也列出了我们新配置的存储池：
- en: '![Figure 5.5 – Newly configured NFS pool visible on the list'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.5–新配置的NFS存储池在列表中可见'
- en: '](img/B14834_05_05.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_05.jpg)'
- en: Figure 5.5 – Newly configured NFS pool visible on the list
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5–新配置的NFS存储池在列表中可见
- en: When would we use NFS-based storage pools in libvirt, and for what? Basically,
    we can use them nicely for anything related to the storage of installation images—ISO
    files, virtual floppy disk files, virtual machine files, and so on.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们何时在libvirt中使用基于NFS的存储池，以及为什么？基本上，我们可以很好地用它们来存储安装映像的任何相关内容——ISO文件、虚拟软盘文件、虚拟机文件等等。
- en: Please remember that even though it seemed that NFS is almost gone from enterprise
    environments just a while ago, NFS is still around. Actually, with the introduction
    of NFS 4.1, 4.2, and pNFS, its future on the market actually looks even better
    than a couple of years ago. It's such a familiar protocol with a very long history,
    and it's still quite competitive in many scenarios. If you're familiar with VMware
    virtualization technology, VMware introduced a technology called Virtual Volumes
    in ESXi 6.0\. This is an object-based storage technology that can use both block-
    and NFS-based protocols for its basis, which is a really compelling use case for
    some scenarios. But for now, let's move on to block-level technologies, such as
    iSCSI and FC.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，尽管似乎NFS在企业环境中几乎已经消失了一段时间，但NFS仍然存在。实际上，随着NFS 4.1、4.2和pNFS的引入，它在市场上的未来实际上看起来比几年前更好。这是一个非常熟悉的协议，有着非常悠久的历史，在许多场景中仍然具有竞争力。如果您熟悉VMware虚拟化技术，VMware在ESXi
    6.0中引入了一种称为虚拟卷的技术。这是一种基于对象的存储技术，可以同时使用基于块和NFS的协议作为其基础，这对于某些场景来说是一个非常引人注目的用例。但现在，让我们转向块级技术，比如iSCSI和FC。
- en: iSCSI and SAN storage
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: iSCSI和SAN存储
- en: 'Using iSCSI for virtual machine storage has long been the regular thing to
    do. Even if you take into account the fact that iSCSI isn''t the most efficient
    way to approach storage, it''s still so widely accepted that you''ll find it everywhere.
    Efficiency is compromised for two reasons:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 长期以来，使用iSCSI进行虚拟机存储一直是常规做法。即使考虑到iSCSI并不是处理存储的最有效方式这一事实，它仍然被广泛接受，你会发现它无处不在。效率受到两个原因的影响：
- en: iSCSI encapsulates SCSI commands into regular IP packages, which means segmentation
    and overhead as IP packages have a pretty large header, which means less efficiency.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: iSCSI将SCSI命令封装成常规IP数据包，这意味着IP数据包有一个相当大的头部，这意味着分段和开销，这意味着效率较低。
- en: Even worse, it's TCP-based, which means that there are sequence numbers and
    retransmissions, which can lead to queueing and latency, and the bigger the environment
    is, the more you usually feel these effects affect your virtual machine performance.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更糟糕的是，它是基于TCP的，这意味着有序号和重传，这可能导致排队和延迟，而且环境越大，你通常会感觉到这些影响对虚拟机性能的影响越大。
- en: That being said, the fact that it's based on an Ethernet stack makes it easier
    to deploy iSCSI-based solutions, while at the same time offering some unique challenges.
    For example, sometimes it's difficult to explain to a customer that using the
    same network switch(es) for virtual machine traffic and iSCSI traffic is not the
    best idea. What makes it even worse is the fact that clients are sometimes so
    blinded by their desire to save money that they don't understand that they're
    working against their own best interest. Especially when it comes to network bandwidth.
    Most of us have been there, trying to work with clients' questions such as *"but
    we already have a Gigabit Ethernet switch, why would you need anything faster
    than that?"*
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，它基于以太网堆栈，使得部署基于iSCSI的解决方案更容易，同时也提供了一些独特的挑战。例如，有时很难向客户解释，在虚拟机流量和iSCSI流量使用相同的网络交换机并不是最好的主意。更糟糕的是，客户有时会因为渴望节省金钱而无法理解他们正在违背自己的最佳利益。特别是在涉及网络带宽时。我们大多数人都曾经历过这种情况，试图回答客户的问题，比如“但我们已经有了千兆以太网交换机，为什么你需要比这更快的东西呢？”
- en: The fact of the matter is, with iSCSI's intricacies, more is just – more. The
    more speed you have on the disk/cache/controller side and the more bandwidth you
    have on the networking side, the more chance you have of creating a storage system
    that's faster. All of that can have a big impact on our virtual machine performance.
    As you'll see in the *Storage redundancy and multipathing* section, you can actually
    build a very good storage system yourself—both for iSCSI and FC. This might come
    in real handy when you try to create some kind of a testing lab/environment to
    play with as you develop your KVM virtualization skills. You can apply that knowledge
    to other virtualized environments, as well.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 事实是，对于iSCSI的复杂性来说，更多就意味着更多。在磁盘/缓存/控制器方面拥有更快的速度，以及在网络方面拥有更多的带宽，就有更多的机会创建一个更快的存储系统。所有这些都可能对我们的虚拟机性能产生重大影响。正如您将在*存储冗余和多路径*部分中看到的那样，您实际上可以自己构建一个非常好的存储系统——无论是对于iSCSI还是FC。当您尝试创建某种测试实验室/环境来发展您的KVM虚拟化技能时，这可能会非常有用。您可以将这些知识应用到其他虚拟化环境中。
- en: The iSCSI and FC architectures are very similar—they both need a target (an
    iSCSI target and an FC target) and an initiator (an iSCS initiator and an FC initiator).
    In this terminology, the target is a *server* component, and the initiator is
    a *client* component. To put it simply, the initiator connects to a target to
    get access to block storage that's presented via that target. Then, we can use
    the initiator's identity to *limit* what the initiator is able to see on the target.
    This is where the terminology starts to get a bit different when comparing iSCSI
    and FC.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: iSCSI和FC架构非常相似 - 它们都需要一个目标（iSCSI目标和FC目标）和一个发起者（iSCS发起者和FC发起者）。在这个术语中，目标是*服务器*组件，发起者是*客户端*组件。简单地说，发起者连接到目标以访问通过该目标呈现的块存储。然后，我们可以使用发起者的身份来*限制*发起者在目标上能够看到的内容。这就是当比较iSCSI和FC时术语开始有点不同的地方。
- en: 'In iSCSI, the initiator''s identity can be defined by four different properties.
    They are as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在iSCSI中，发起者的身份可以由四个不同的属性来定义。它们如下：
- en: '**iSCSI Qualified Name** (**IQN**): This is a unique name that all initiators
    and targets have in iSCSI communication. We can compare this to a MAC or IP address
    in regular Ethernet-based networks. You can think of it this way—an IQN is for
    iSCSI what a MAC or IP address is for Ethernet-based networks.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**iSCSI合格名称**（**IQN**）：这是所有发起者和目标在iSCSI通信中都具有的唯一名称。我们可以将其与常规以太网网络中的MAC或IP地址进行比较。您可以这样想
    - 对于以太网网络来说，IQN就是iSCSI的MAC或IP地址。'
- en: '**IP address**: Every initiator will have a different IP address that it uses
    to connect to the target.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IP地址**：每个发起者都有一个不同的IP地址，用于连接到目标。'
- en: '**MAC address**: Every initiator has a different MAC address on Layer 2.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MAC地址**：每个发起者在第2层都有一个不同的MAC地址。'
- en: '**Fully Qualified Domain Name** (**FQDN**): This represents the name of the
    server as it''s resolved by a DNS service.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全合格的域名**（**FQDN**）：这代表了服务器的名称，它是由DNS服务解析的。'
- en: From the iSCSI target perspective—depending on its implementation—you can use
    any one of these properties to create a configuration that's going to tell the
    iSCSI target which IQNs, IP addresses, MAC addresses, or FQDNs can be used to
    connect to it. This is what's called *masking*, as we can *mask* what an initiator
    can *see* on the iSCSI target by using these identities and pairing them with
    LUNs. LUNs are just raw, block capacities that we export via an iSCSI target toward
    initiators. LUNs are *indexed*, or *numbered*, usually from 0 onward. Every LUN
    number represents a different storage capacity that an initiator can connect to.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 从iSCSI目标的角度来看 - 根据其实现方式 - 您可以使用这些属性中的任何一个来创建一个配置，该配置将告诉iSCSI目标可以使用哪些IQN、IP地址、MAC地址或FQDN来连接到它。这就是所谓的*掩码*，因为我们可以通过使用这些身份并将它们与LUN配对来*掩盖*发起者在iSCSI目标上可以*看到*的内容。LUN只是我们通过iSCSI目标向发起者导出的原始块容量。LUN通常是*索引*或*编号*的，通常从0开始。每个LUN编号代表发起者可以连接到的不同存储容量。
- en: For example, we can have an iSCSI target with three different LUNs—`LUN0` with
    20 GB, `LUN1` with 40 GB, and `LUN2` with 60 GB. These will all be hosted on the
    same storage system's iSCSI target. We can then configure the iSCSI target to
    accept an IQN to see all the LUNs, another IQN to only see `LUN1`, and another
    IQN to only see `LUN1` and `LUN2`. This is actually what we are going to configure
    right now.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以有一个iSCSI目标，其中包含三个不同的LUN - `LUN0`，容量为20 GB，`LUN1`，容量为40 GB，和`LUN2`，容量为60
    GB。这些都将托管在同一存储系统的iSCSI目标上。然后，我们可以配置iSCSI目标以接受一个IQN来查看所有LUN，另一个IQN只能看到`LUN1`，另一个IQN只能看到`LUN1`和`LUN2`。这实际上就是我们现在要配置的。
- en: 'Let''s start by configuring the iSCSI target service. For that, we need to
    install the `targetcli` package, and configure the service (called `target`) to
    run:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从配置iSCSI目标服务开始。为此，我们需要安装`targetcli`软件包，并配置服务（称为`target`）运行：
- en: '[PRE6]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Be careful about the firewall configuration; you might need to configure it
    to allow connectivity on port `3260/tcp`, which is the port that the iSCSI target
    portal uses. So, if your firewall has started, type in the following command:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 要注意防火墙配置；您可能需要配置它以允许在端口`3260/tcp`上进行连接，这是iSCSI目标门户使用的端口。因此，如果您的防火墙已启动，请输入以下命令：
- en: '[PRE7]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'There are three possibilities for iSCSI on Linux in terms of what storage backend
    to use. We could use a regular filesystem (such as XFS), a block device (a hard
    drive), or LVM. So, that''s exactly what we''re going to do. Our scenario is going
    to be as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux上，关于使用什么存储后端的iSCSI有三种可能性。我们可以使用常规文件系统（如XFS）、块设备（硬盘）或LVM。所以，这正是我们要做的。我们的情景将如下所示：
- en: '`LUN0` (20 GB): XFS-based filesystem, on the `/dev/sdb` device'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LUN0`（20 GB）：基于XFS的文件系统，位于`/dev/sdb`设备上'
- en: '`LUN1` (40 GB): Hard drive, on the `/dev/sdc` device'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LUN1`（40 GB）：硬盘驱动器，位于`/dev/sdc`设备上'
- en: '`LUN2` (60 GB): LVM, on the `/dev/sdd` device'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LUN2`（60 GB）：LVM，位于`/dev/sdd`设备上'
- en: 'So, after we install the necessary packages and configure the target service
    and firewall, we should start with configuring our iSCSI target. We''ll just start
    the `targetcli` command and check the state, which should be a blank slate as
    we''re just beginning the process:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在安装必要的软件包并配置目标服务和防火墙之后，我们应该开始配置我们的iSCSI目标。我们只需启动`targetcli`命令并检查状态，因为我们刚刚开始这个过程，状态应该是空白的：
- en: '![Figure 5.6 – The targetcli starting point – empty configuration'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.6 - targetcli的起点 - 空配置'
- en: '](img/B14834_05_06.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_06.jpg)'
- en: Figure 5.6 – The targetcli starting point – empty configuration
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 - targetcli的起点 - 空配置
- en: 'Let''s start with the step-by-step procedure:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从逐步的过程开始：
- en: So, let's configure the XFS-based filesystem and configure the `LUN0` file image
    to be saved there. First, we need to partition the disk (in our case, `/dev/sdb`):![Figure
    5.7 – Partitioning /dev/sdb for the XFS filesystem
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，让我们配置基于XFS的文件系统，并配置`LUN0`文件映像保存在那里。首先，我们需要对磁盘进行分区（在我们的情况下是`/dev/sdb`）：![图5.7
    - 为XFS文件系统分区`/dev/sdb`
- en: '](img/B14834_05_07.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_07.jpg)'
- en: Figure 5.7 – Partitioning /dev/sdb for the XFS filesystem
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 - 为XFS文件系统分区`/dev/sdb`
- en: The next step is to format this partition, create and use a directory called
    `/LUN0` to mount this filesystem, and serve our `LUN0` image, which we're going
    to configure in the next steps:![Figure 5.8 – Formatting the XFS filesystem, creating
    a directory, and mounting it to that directory
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是格式化这个分区，创建并使用一个名为`/LUN0`的目录来挂载这个文件系统，并提供我们的`LUN0`镜像，我们将在接下来的步骤中进行配置：![图5.8
    - 格式化XFS文件系统，创建目录，并将其挂载到该目录
- en: '](img/B14834_05_08.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_08.jpg)'
- en: Figure 5.8 – Formatting the XFS filesystem, creating a directory, and mounting
    it to that directory
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 - 格式化XFS文件系统，创建目录，并将其挂载到该目录
- en: The next step is configuring `targetcli` so that it creates `LUN0` and assigns
    an image file for `LUN0`, which will be saved in the `/LUN0` directory. First,
    we need to start the `targetcli` command:![Figure 5.9 – Creating an iSCSI target,
    LUN0, and hosting it as a file
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是配置`targetcli`，使其创建`LUN0`并为`LUN0`分配一个镜像文件，该文件将保存在`/LUN0`目录中。首先，我们需要启动`targetcli`命令：![图5.9
    - 创建iSCSI目标，LUN0，并将其作为文件托管
- en: '](img/B14834_05_09.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_09.jpg)'
- en: Figure 5.9 – Creating an iSCSI target, LUN0, and hosting it as a file
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 - 创建iSCSI目标，LUN0，并将其作为文件托管
- en: 'Next, let''s configure a block device-based LUN backend— `LUN2`—which is going
    to use `/dev/sdc1` (create the partition using the previous example) and check
    the current state:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们配置一个基于块设备的LUN后端— `LUN2`—它将使用`/dev/sdc1`（使用前面的示例创建分区）并检查当前状态：
- en: '![Figure 5.10 – Creating LUN1, hosting it directly from a block device'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.10 - 创建LUN1，直接从块设备托管'
- en: '](img/B14834_05_10.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_10.jpg)'
- en: Figure 5.10 – Creating LUN1, hosting it directly from a block device
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 - 创建LUN1，直接从块设备托管
- en: 'So, `LUN0` and `LUN1` and their respective backends are now configured. Let''s
    finish things off by configuring LVM:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`LUN0`和`LUN1`及其各自的后端现在已配置完成。让我们通过配置LVM来完成这些事情：
- en: First, we are going to prepare the physical volume for LVM, create a volume
    group out of that volume, and display all the information about that volume group
    so that we can see how much space we have for `LUN2`:![Figure 5.11 – Configuring
    the physical volume for LVM, building a volume group,
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将准备LVM的物理卷，从该卷创建一个卷组，并显示有关该卷组的所有信息，以便我们可以看到我们有多少空间可用于`LUN2`：![图5.11 - 为LVM配置物理卷，构建卷组，并显示有关该卷组的信息
- en: and displaying information about that volume group
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 和显示有关该卷组的信息
- en: '](img/B14834_05_11.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_11.jpg)'
- en: Figure 5.11 – Configuring the physical volume for LVM, building a volume group,
    and displaying information about that volume group
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 - 为LVM配置物理卷，构建卷组，并显示有关该卷组的信息
- en: The next step is to actually create the logical volume, which is going to be
    our block storage device backend for `LUN2` in the iSCSI target. We can see from
    the `vgdisplay` output that we have 15,359 4 MB blocks available, so let's use
    that to create our logical volume, called `LUN2`. Go to `targetcli` and configure
    the necessary settings for `LUN2`:![Figure 5.12 – Configuring LUN2 with the LVM
    backend
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是实际创建逻辑卷，这将是我们iSCSI目标中`LUN2`的块存储设备后端。我们可以从`vgdisplay`输出中看到我们有15,359个4MB块可用，所以让我们用它来创建我们的逻辑卷，称为`LUN2`。转到`targetcli`并配置`LUN2`的必要设置：![图5.12
    - 使用LVM后端配置LUN2
- en: '](img/B14834_05_12.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_12.jpg)'
- en: Figure 5.12 – Configuring LUN2 with the LVM backend
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 - 使用LVM后端配置LUN2
- en: 'Let''s stop here for a second and switch to the KVM host (the iSCSI initiator)
    configuration. First, we need to install the iSCSI initiator, which is part of
    a package called `iscsi-initiator-utils`. So, let''s use the `yum` command to
    install that:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们停在这里，转而转到KVM主机（iSCSI发起者）的配置。首先，我们需要安装iSCSI发起者，这是一个名为`iscsi-initiator-utils`的软件包的一部分。因此，让我们使用`yum`命令来安装它：
- en: '[PRE8]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we need to configure the IQN of our initiator. We usually want this name
    to be reminiscent of the hostname, so, seeing that our host''s FQDN is `PacktStratis01`,
    we''ll use that to configure the IQN. To do that, we need to edit the `/etc/iscsi/initiatorname.iscsi`
    file and configure the `InitiatorName` option. For example, let''s set it to `iqn.2019-12.com.packt:PacktStratis01`.
    The content of the `/etc/iscsi/initiatorname.iscsi` file should be as follows:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要配置我们发起者的IQN。通常我们希望这个名称能让人联想到主机名，所以，看到我们主机的FQDN是`PacktStratis01`，我们将使用它来配置IQN。为了做到这一点，我们需要编辑`/etc/iscsi/initiatorname.iscsi`文件并配置`InitiatorName`选项。例如，让我们将其设置为`iqn.2019-12.com.packt:PacktStratis01`。`/etc/iscsi/initiatorname.iscsi`文件的内容应该如下所示：
- en: '[PRE9]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now that this is configured, let's go back to the iSCSI target and create an
    **Access Control List** (**ACL**). The ACL is going to allow our KVM host's initiator
    to connect to the iSCSI target portal:![Figure 5.13 – Creating an ACL so that
    the KVM host's initiator can connect to the iSCSI target
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在这已经配置好了，让我们回到iSCSI目标并创建一个**访问控制列表**（**ACL**）。ACL将允许我们的KVM主机发起者连接到iSCSI目标门户：![图5.13
    - 创建ACL，以便KVM主机的发起者可以连接到iSCSI目标
- en: '](img/B14834_05_13.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_13.jpg)'
- en: Figure 5.13 – Creating an ACL so that the KVM host's initiator can connect to
    the iSCSI target
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 - 创建ACL，以便KVM主机的发起者可以连接到iSCSI目标
- en: 'Next, we need to publish our pre-created file-based and block-based devices
    to the iSCSI target LUNs. So, we need to do this:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要将我们预先创建的基于文件和基于块的设备发布到iSCSI目标LUNs。因此，我们需要这样做：
- en: '![Figure 5.14 – Adding our file-based and block-based devices to the iSCSI
    target LUNs 0, 1, and 2'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.14 - 将我们的基于文件和基于块的设备添加到iSCSI目标LUNs 0、1和2'
- en: '](img/B14834_05_14.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_14.jpg)'
- en: Figure 5.14 – Adding our file-based and block-based devices to the iSCSI target
    LUNs 0, 1, and 2
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 - 将我们的基于文件和基于块的设备添加到iSCSI目标LUNs 0、1和2
- en: 'The end result should look like this:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果应该如下所示：
- en: '![Figure 5.15 – The end result'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.15 - 最终结果'
- en: '](img/B14834_05_15.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_15.jpg)'
- en: Figure 5.15 – The end result
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15 - 最终结果
- en: 'At this point, everything is configured. We need to go back to our KVM host
    and define a storage pool that will use these LUNs. The easiest way to do that
    would be to use an XML configuration file for the pool. So, here''s our sample
    configuration XML file; we''ll call it `iSCSIPool.xml`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，一切都已配置好。我们需要回到我们的KVM主机，并定义一个将使用这些LUN的存储池。做到这一点最简单的方法是使用一个XML配置文件来定义池。因此，这是我们的示例配置XML文件；我们将称其为`iSCSIPool.xml`：
- en: '[PRE10]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s explain the file step by step:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地解释这个文件：
- en: '`pool type= ''iscsi''`: We''re telling libvirt that this is an iSCSI pool.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`池类型= ''iscsi''`：我们告诉libvirt这是一个iSCSI池。'
- en: '`name` : The pool name.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`名称`：池名称。'
- en: '`host name`: The IP address of the iSCSI target.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`主机名`：iSCSI目标的IP地址。'
- en: '`device path`: The IQN of the iSCSI target.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`设备路径`：iSCSI目标的IQN。'
- en: 'The IQN name in the initiator section: The IQN of the initiator.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发起者部分的IQN名称：发起者的IQN。
- en: '`target path`: The location where iSCSI target''s LUNs will be mounted.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`目标路径`：iSCSI目标的LUN将被挂载的位置。'
- en: 'Now, all that''s left for us to do is to define, start, and autostart our new
    iSCSI-backed KVM storage pool:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们所要做的就是定义、启动和自动启动我们的新的基于iSCSI的KVM存储池：
- en: '[PRE11]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The target path part of the configuration can be easily checked via `virsh`.
    If we type the following command into the KVM host, we will get the list of available
    LUNs from the `MyiSCSIPool` pool that we just configured:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 配置的目标路径部分可以通过`virsh`轻松检查。如果我们在KVM主机上输入以下命令，我们将得到刚刚配置的`MyiSCSIPool`池中可用LUN的列表：
- en: '[PRE12]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We get the following result for this command:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对此命令得到以下结果：
- en: '![Figure 5.16 – Runtime names for our iSCSI pool LUNs'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.16 - 我们iSCSI池LUN的运行时名称'
- en: '](img/B14834_05_16.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_16.jpg)'
- en: Figure 5.16 – Runtime names for our iSCSI pool LUNs
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.16 - 我们iSCSI池LUN的运行时名称
- en: If this output reminds you a bit of the VMware vSphere Hypervisor storage runtime
    names, you are definitely on the right track. We will be able to use these storage
    pools in [*Chapter 7*](B14834_07_Final_ASB_ePub.xhtml#_idTextAnchor125), *Virtual
    Machine – Installation, Configuration, and Life-Cycle Management*, when we start
    deploying our virtual machines.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个输出让你有点想起VMware vSphere Hypervisor存储运行时名称，那么你肯定是对的。当我们开始部署我们的虚拟机时，我们将能够在[*第7章*](B14834_07_Final_ASB_ePub.xhtml#_idTextAnchor125)，*虚拟机-安装、配置和生命周期管理*中使用这些存储池。
- en: Storage redundancy and multipathing
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储冗余和多路径
- en: Redundancy is one of the keywords of IT, where any single component failure
    could mean big problems for a company or its customers. The general design principle
    of avoiding SPOF is something that we should always stick to. At the end of the
    day, no network adapter, cable, switch, router, or storage controller is going
    to work forever. So, calculating redundancy into our designs helps our IT environment
    during its normal life cycle.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 冗余是IT的关键词之一，任何单个组件的故障都可能对公司或其客户造成重大问题。避免SPOF的一般设计原则是我们应该始终坚持的。归根结底，没有任何网络适配器、电缆、交换机、路由器或存储控制器会永远工作。因此，将冗余计算到我们的设计中有助于我们的IT环境在其正常生命周期内。
- en: At the same time, redundancy can be combined with multipathing to also ensure
    higher throughput. For example, when we connect our physical host to FC storage
    with two controllers with four FC ports each, we can use four paths (if the storage
    is active-passive) or eight paths (if it's active-active) to the same LUN(s) exported
    from this storage device to a host. This gives us multiple additional options
    for LUN access, on top of the fact that it gives us more availability, even in
    the case of failure.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，冗余可以与多路径结合，以确保更高的吞吐量。例如，当我们将物理主机连接到具有每个四个FC端口的两个控制器的FC存储时，我们可以使用四条路径（如果存储是主备的）或八条路径（如果是主动-主动的）连接到从存储设备导出给主机的相同LUN(s)。这为我们提供了多种额外的LUN访问选项，除了在故障情况下为我们提供更多的可用性外。
- en: Getting a regular KVM host to do, for example, iSCSI multipathing is quite a
    bit complex. There are multiple configuration issues and blank spots in terms
    of documentation, and supportability of such a configuration is questionable.
    However, there are products that use KVM that support it out of the box, such
    as oVirt (which we covered before) and **Red Hat Enterprise Virtualization Hypervisor**
    (**RHEV-H**). So, let's use oVirt for this example on iSCSI.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让一个普通的KVM主机执行，例如iSCSI多路径，是相当复杂的。在文档方面存在多个配置问题和空白点，这种配置的支持性是值得怀疑的。然而，有一些使用KVM的产品可以直接支持，比如oVirt（我们之前介绍过）和**Red
    Hat企业虚拟化Hypervisor**（**RHEV-H**）。因此，让我们在iSCSI的例子中使用oVirt。
- en: 'Before you do this, make sure that you have done the following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在你这样做之前，请确保你已经完成了以下工作：
- en: Your Hypervisor host is added to the oVirt inventory.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的Hypervisor主机已添加到oVirt清单中。
- en: Your Hypervisor host has two additional network cards, independent of the management
    network.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的Hypervisor主机有两个额外的网络卡，独立于管理网络。
- en: The iSCSI storage has two additional network cards in the same L2 networks as
    the two additional hypervisor network cards.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: iSCSI存储在与两个额外的Hypervisor网络卡相同的L2网络中有两个额外的网络卡。
- en: The iSCSI storage is configured so that it has at least a target and a LUN already
    configured in a way that will enable the hypervisor host to connect to it.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: iSCSI存储已经配置好，至少有一个目标和一个LUN已经配置好，这样就能使Hypervisor主机连接到它。
- en: 'So, as we''re doing this in oVirt, there are a couple of things that we need
    to do. First, from a networking perspective, it would be a good idea to create
    some storage networks. In our case, we''re going to assign two networks for iSCSI,
    and we will call them `iSCSI01` and `iSCSI02`. We need to open the oVirt administration
    panel, hover over `iSCSI01` (for the first one), uncheck the `iSCSI02` network:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们在oVirt中进行这项工作时，有一些事情是我们需要做的。首先，从网络的角度来看，为存储创建一些存储网络是一个好主意。在我们的情况下，我们将为iSCSI分配两个网络，并将它们称为`iSCSI01`和`iSCSI02`。我们需要打开oVirt管理面板，悬停在`iSCSI01`（第一个）上，取消选中`iSCSI02`网络：
- en: '![Figure 5.17 – Configuring networks for iSCSI bond'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.17 - 配置iSCSI绑定网络'
- en: '](img/B14834_05_17.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_17.jpg)'
- en: Figure 5.17 – Configuring networks for iSCSI bond
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.17 - 配置iSCSI绑定网络
- en: 'The next step is assigning these networks to host network adapters. Go to `compute/hosts`,
    double-click on the host that you added to oVirt''s inventory, select the `iSCSI01`
    on the second network interface and `iSCSI02` on the third network interface.
    The first network interface is already taken by the oVirt management network.
    It should look something like this:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将这些网络分配给主机网络适配器。转到`compute/hosts`，双击您添加到oVirt清单的主机，选择第二个网络接口上的`iSCSI01`和第三个网络接口上的`iSCSI02`。第一个网络接口已被oVirt管理网络占用。它应该看起来像这样：
- en: '![Figure 5.18 – Assigning virtual networks to the hypervisor''s physical adapters'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.18 - 将虚拟网络分配给hypervisor的物理适配器'
- en: '](img/B14834_05_18.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_18.jpg)'
- en: Figure 5.18 – Assigning virtual networks to the hypervisor's physical adapters
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.18 - 将虚拟网络分配给hypervisor的物理适配器
- en: 'Before you close the window down, make sure that you click on the *pencil*
    sign on both `iSCSI01` and `iSCSI02` to set up IP addresses for these two virtual
    networks. Assign network configuration that can connect you to your iSCSI storage
    on the same or different subnets:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在关闭窗口之前，请确保单击`iSCSI01`和`iSCSI02`上的*铅笔*图标，为这两个虚拟网络设置IP地址。分配可以将您连接到相同或不同子网上的iSCSI存储的网络配置：
- en: '![Figure 5.19 – Creating an iSCSI bond on the data center level'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.19 - 在数据中心级别创建iSCSI绑定'
- en: '](img/B14834_05_19.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_19.jpg)'
- en: Figure 5.19 – Creating an iSCSI bond on the data center level
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.19 - 在数据中心级别创建iSCSI绑定
- en: 'There you go, you have just configured an iSCSI bond. The last part of our
    configuration is enabling it. Again, in the oVirt GUI, go to **Compute** | **Data
    Centers**, select your datacenter with a double-click, and go to the **iSCSI Multipathing**
    tab:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 您刚刚配置了一个iSCSI绑定。我们配置的最后一部分是启用它。同样，在oVirt GUI中，转到**计算** | **数据中心**，双击选择您的数据中心，然后转到**iSCSI多路径**选项卡：
- en: '![Figure 5.20 – Configuring iSCSI multipathing on the data center level'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.20 - 在数据中心级别配置iSCSI多路径'
- en: '](img/B14834_05_20.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_20.jpg)'
- en: Figure 5.20 – Configuring iSCSI multipathing on the data center level
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.20 - 在数据中心级别配置iSCSI多路径
- en: Click on the `iSCSI01` and `iSCSI02` networks in the top part of the pop-up
    window, and the iSCSI target on the lower side.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在弹出窗口的顶部部分单击`iSCSI01`和`iSCSI02`网络，然后在底部单击iSCSI目标。
- en: Now that we have covered the basics of storage pools, NFS, and iSCSI, we can
    move on to a standard open source way of deploying storage infrastructure, which
    would be to use Gluster and/or Ceph.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了存储池、NFS和iSCSI的基础知识，我们可以继续使用标准的开源方式部署存储基础设施，即使用Gluster和/或Ceph。
- en: Gluster and Ceph as a storage backend for KVM
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gluster和Ceph作为KVM的存储后端
- en: There are other advanced types of filesystems that can be used as the libvirt
    storage backend. So, let's now discuss two of them—Gluster and Ceph. Later, we'll
    also check how libvirt works with GFS2.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他高级类型的文件系统可以用作libvirt存储后端。因此，让我们现在讨论其中的两种 - Gluster和Ceph。稍后，我们还将检查libvirt如何与GFS2一起使用。
- en: Gluster
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Gluster
- en: Gluster is a distributed filesystem that's often used for high-availability
    scenarios. Its main advantages over other filesystems are the fact that it's scalable,
    it can use replication and snapshots, it can work on any server, and it's usable
    as a basis for shared storage—for example, via NFS and SMB. It was developed by
    a company called Gluster Inc., which was acquired by RedHat in 2011\. However,
    unlike Ceph, it's a *file* storage service, while Ceph offers *block* and *object*-based
    storage. Object-based storage for block-based devices means direct, binary storage,
    directly to a LUN. There are no filesystems involved, which theoretically means
    less overhead as there's no filesystem, filesystem tables, and other constructs
    that might slow the I/O process down.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Gluster是一个经常用于高可用性场景的分布式文件系统。它相对于其他文件系统的主要优势在于，它是可扩展的，可以使用复制和快照，可以在任何服务器上工作，并且可用作共享存储的基础，例如通过NFS和SMB。它是由一家名为Gluster
    Inc.的公司开发的，该公司于2011年被RedHat收购。然而，与Ceph不同，它是一个*文件*存储服务，而Ceph提供*块*和*对象*为基础的存储。基于对象的存储对于基于块的设备意味着直接的二进制存储，直接到LUN。这里没有涉及文件系统，理论上意味着由于没有文件系统、文件系统表和其他可能减慢I/O过程的构造，因此开销更小。
- en: Let's first configure Gluster to show its use case with libvirt. In production,
    that means installing at least three Gluster servers so that we can make high
    availability possible. Gluster configuration is really straightforward, and in
    our example, we are going to create three CentOS 7 machines that we will use to
    host the Gluster filesystem. Then, we will mount that filesystem on our hypervisor
    host and use it as a local directory. We can use GlusterFS directly from libvirt,
    but the implementation is just not as refined as using it via the gluster client
    service, mounting it as a local directory, and using it directly as a directory
    pool in libvirt.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先配置Gluster以展示其在libvirt中的用途。在生产中，这意味着安装至少三台Gluster服务器，以便我们可以实现高可用性。Gluster配置非常简单，在我们的示例中，我们将创建三台CentOS
    7机器，用于托管Gluster文件系统。然后，我们将在我们的hypervisor主机上挂载该文件系统，并将其用作本地目录。我们可以直接从libvirt使用GlusterFS，但是实现方式并不像通过gluster客户端服务使用它、将其挂载为本地目录并直接在libvirt中使用它作为目录池那样精致。
- en: 'Our configuration will look like this:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的配置将如下所示：
- en: '![Figure 5.21 – Basic settings for our Gluster cluster'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.21 - 我们Gluster集群的基本设置'
- en: '](img/B14834_05_21.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_21.jpg)'
- en: Figure 5.21 – Basic settings for our Gluster cluster
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.21 - 我们Gluster集群的基本设置
- en: 'So, let''s put that into production. We have to issue a large sequence of commands
    on all of the servers before we configure Gluster and expose it to our KVM host.
    Let''s start with `gluster1`. First, we are going to do a system-wide update and
    reboot to prepare the core operating system for Gluster installation. Type the
    following commands into all three CentOS 7 servers:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们投入生产。在配置Gluster并将其暴露给我们的KVM主机之前，我们必须在所有服务器上发出一系列大量的命令。让我们从`gluster1`开始。首先，我们将进行系统范围的更新和重启，以准备Gluster安装的核心操作系统。在所有三台CentOS
    7服务器上输入以下命令：
- en: '[PRE13]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we can start deploying the necessary repositories and packages, format
    disks, configure the firewall, and so on. Type the following commands into all
    the servers:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以开始部署必要的存储库和软件包，格式化磁盘，配置防火墙等。在所有服务器上输入以下命令：
- en: '[PRE14]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We need to do a bit of networking configuration as well. It would be good if
    these three servers can *resolve* each other, which means either configuring a
    DNS server or adding a couple of lines to our `/etc/hosts` file. Let''s do the
    latter. Add the following lines to your `/etc/hosts` file:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要进行一些网络配置。如果这三台服务器可以*相互解析*，那将是很好的，这意味着要么配置一个DNS服务器，要么在我们的`/etc/hosts`文件中添加几行。我们选择后者。将以下行添加到您的`/etc/hosts`文件中：
- en: '[PRE15]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'For the next part of the configuration, we can just log in to the first server
    and use it as the de facto management server for our Gluster infrastructure. Type
    in the following commands:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置的下一部分，我们只需登录到第一台服务器，并将其用作我们的Gluster基础设施的事实管理服务器。输入以下命令：
- en: '[PRE16]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The first three commands should get you the `peer probe: success` status. The
    third one should return an output similar to this:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '前三个命令应该让您得到`peer probe: success`状态。第三个应该返回类似于这样的输出：'
- en: '![Figure 5.22 – Confirmation that the Gluster servers peered successfully'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.22 - 确认Gluster服务器成功对等'
- en: '](img/B14834_05_22.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_22.jpg)'
- en: Figure 5.22 – Confirmation that the Gluster servers peered successfully
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.22 - 确认Gluster服务器成功对等
- en: 'Now that this part of the configuration is done, we can create a Gluster-distributed
    filesystem. We can do this by typing the following sequence of commands:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 现在配置的这一部分已经完成，我们可以创建一个Gluster分布式文件系统。我们可以通过输入以下命令序列来实现这一点：
- en: '[PRE17]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, we could mount Gluster as an NFS directory for testing purposes. For
    example, we can create a distributed namespace called `kvmgluster` to all of the
    member hosts (`gluster1`, `gluster2`, and `gluster3`). We can do this by using
    the following commands:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将Gluster挂载为NFS目录进行测试。例如，我们可以为所有成员主机（`gluster1`、`gluster2`和`gluster3`）创建一个名为`kvmgluster`的分布式命名空间。我们可以通过使用以下命令来实现这一点：
- en: '[PRE18]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The Gluster part is now ready, so we need to go back to our KVM host and mount
    the Gluster filesystem to it by typing in the following commands:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: Gluster部分现在已经准备就绪，所以我们需要回到我们的KVM主机，并通过输入以下命令将Gluster文件系统挂载到它上面：
- en: '[PRE19]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We have to pay close attention to Gluster releases on the server and client,
    which is why we downloaded the Gluster repository information for CentOS 8 (we're
    using it on the KVM server) and installed the necessary Gluster client packages.
    That enabled us to mount the filesystem with the last command.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须密切关注服务器和客户端上的Gluster版本，这就是为什么我们下载了CentOS 8的Gluster存储库信息（我们正在KVM服务器上使用它），并安装了必要的Gluster客户端软件包。这使我们能够使用最后一个命令挂载文件系统。
- en: 'Now that we''ve finished our configuration, we just need to add this directory
    as a libvirt storage pool. Let''s do that by using an XML file with the storage
    pool definition, which contains the following entries:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了配置，我们只需要将这个目录作为libvirt存储池添加进去。让我们通过使用一个包含以下条目的存储池定义的XML文件来做到这一点：
- en: '[PRE20]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s say that we saved this file in the current directory, and that the file
    is called `gluster.xml`. We can import and start it in libvirt by using the following
    `virsh` commands:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们将这个文件保存在当前目录，并且文件名为`gluster.xml`。我们可以通过使用以下`virsh`命令将其导入并在libvirt中启动：
- en: '[PRE21]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We should mount this pool automatically on boot so that libvirt can use it.
    Therefore, we need to add the following line to `/etc/fstab`:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该在启动时自动挂载这个存储池，以便libvirt可以使用它。因此，我们需要将以下行添加到`/etc/fstab`中：
- en: '[PRE22]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Using a directory-based approach enables us to avoid two problems that libvirt
    (and its GUI interface, `virt-manager`) has with Gluster storage pools:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于目录的方法使我们能够避免libvirt（及其GUI界面`virt-manager`）在Gluster存储池方面存在的两个问题：
- en: We can use Gluster's failover capability, which will be managed automatically
    by the Gluster utilities that we installed directly, as libvirt doesn't support
    them yet.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用Gluster的故障转移功能，这将由我们直接安装的Gluster实用程序自动管理，因为libvirt目前还不支持它们。
- en: We will avoid creating virtual machine disks *manually*, which is another limitation
    of libvirt's implementation of Gluster support, while directory-based storage
    pools support it without any issues.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将避免*手动*创建虚拟机磁盘，这是libvirt对Gluster支持的另一个限制，而基于目录的存储池则可以无任何问题地支持它。
- en: 'It seems weird that we''re mentioning *failover*, as it seems as though we
    didn''t configure it as a part of any of the previous steps. Actually, we have.
    When we issued the last mount command, we used Gluster''s built-in modules to
    establish connectivity to the *first* Gluster server. That, in turn, means that
    after this connection, we got all of the details about the whole Gluster pool,
    which we configured so that it''s hosted on three servers. If any kind of failure
    happens—which we can easily simulate—this connection will continue working. We
    can simulate this scenario by turning off any of the Gluster servers, for example—`gluster1`.
    You''ll see that the local directory where we mounted Gluster directory still
    works, even though `gluster1` is down. Let''s see that in action (the default
    timeout period is 42 seconds):'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到*故障转移*似乎有点奇怪，因为似乎我们没有将它作为任何之前步骤的一部分进行配置。实际上，我们已经配置了。当我们发出最后一个挂载命令时，我们使用了Gluster的内置模块来建立与*第一个*Gluster服务器的连接。这反过来意味着在建立这个连接之后，我们得到了关于整个Gluster池的所有细节，我们配置了它以便它托管在三台服务器上。如果发生任何故障—我们可以很容易地模拟—这个连接将继续工作。例如，我们可以通过关闭任何一个Gluster服务器来模拟这种情况—比如`gluster1`。您会看到我们挂载Gluster目录的本地目录仍然可以工作，即使`gluster1`已经关闭。让我们看看它的运行情况（默认超时时间为42秒）：
- en: '![Figure 5.23 – Gluster failover working; the first node is down, but we''re
    still able to get our files'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.23 - Gluster故障转移工作；第一个节点已经关闭，但我们仍然能够获取我们的文件'
- en: '](img/B14834_05_23.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_23.jpg)'
- en: Figure 5.23 – Gluster failover working; the first node is down, but we're still
    able to get our files
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.23 - Gluster故障转移工作；第一个节点已经关闭，但我们仍然能够获取我们的文件
- en: 'If we want to be more aggressive, we can shorten this timeout period to—for
    example—2 seconds by issuing the following command on any of our Gluster servers:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想更积极一些，可以通过在任何一个Gluster服务器上发出以下命令来将此超时期缩短到——例如——2秒：
- en: '[PRE23]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `number` part is in seconds, and by assigning it a lower number, we can
    directly influence how aggressive the failover process is.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '`number`部分是以秒为单位的，通过分配一个较低的数字，我们可以直接影响故障切换过程的积极性。'
- en: So, now that everything is configured, we can start using the Gluster pool to
    deploy virtual machines, which we will discuss further in [*Chapter 7*](B14834_07_Final_ASB_ePub.xhtml#_idTextAnchor125),
    *Virtual Machine – Installation, Configuration, and Life-Cycle Management*.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在一切都配置好了，我们可以开始使用Gluster池部署虚拟机，我们将在[*第7章*](B14834_07_Final_ASB_ePub.xhtml#_idTextAnchor125)中进一步讨论，*虚拟机-安装、配置和生命周期管理*。
- en: Seeing as Gluster is a file-based backend that can be used for libvirt, it's
    only natural to describe how to use an advanced block-level and object-level storage
    backend. That's where Ceph comes in, so let's work on that now.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于Gluster是一个基于文件的后端，可以用于libvirt，自然而然地需要描述如何使用高级块级和对象级存储后端。这就是Ceph的用武之地，所以让我们现在来处理这个问题。
- en: Ceph
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ceph
- en: Ceph can act as file-, block-, and object-based storage. But for the most part,
    we're usually using it as either block- or object-based storage. Again, this is
    a piece of open source software that's designed to work on any server (or a virtual
    machine). In its core, Ceph runs an algorithm called **Controlled Replication
    Under Scalable Hashing** (**CRUSH**). This algorithm tries to distribute data
    across object devices in a pseudo-random manner, and in Ceph, it's managed by
    a cluster map (a CRUSH map). We can easily scale Ceph out by adding more nodes,
    which will redistribute data in a minimum fashion to ensure as small amount of
    replication as possible.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: Ceph可以作为文件、块和对象存储。但在大多数情况下，我们通常将其用作块或对象存储。同样，这是一款设计用于任何服务器（或虚拟机）的开源软件。在其核心，Ceph运行一个名为**可控复制下可扩展哈希**（**CRUSH**）的算法。该算法试图以伪随机的方式在对象设备之间分发数据，在Ceph中，它由一个集群映射（CRUSH映射）管理。我们可以通过添加更多节点轻松扩展Ceph，这将以最小的方式重新分发数据，以确保尽可能少的复制。
- en: An internal Ceph component called **Reliable Autonomic Distributed Object Store**
    (**RADOS**) is used for snapshots, replication, and thin provisioning. It's an
    open source project that was developed by the University of California.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 一个名为**可靠自主分布式对象存储**（**RADOS**）的内部Ceph组件用于快照、复制和薄配置。这是一个由加利福尼亚大学开发的开源项目。
- en: 'Architecture-wise, Ceph has three main services:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在架构上，Ceph有三个主要服务：
- en: '**ceph-mon** : Used for cluster monitoring, CRUSH maps, and **Object Storage
    Daemon** (**OSD**) maps.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ceph-mon**：用于集群监控、CRUSH映射和**对象存储守护程序**（**OSD**）映射。'
- en: '**ceph-osd**: This handles actual data storage, replication, and recovery.
    It requires at least two nodes; we''ll use three for clustering reasons.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ceph-osd**：处理实际数据存储、复制和恢复。至少需要两个节点；出于集群化的原因，我们将使用三个。'
- en: '**ceph-mds**: Metadata server, used when Ceph needs filesystem access.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ceph-mds**：元数据服务器，在Ceph需要文件系统访问时使用。'
- en: In accordance with best practices, make sure that you always design your Ceph
    environments with the key principles in mind—all of the data nodes need to have
    the same configuration. That means the same amount of memory, the same storage
    controllers (don't use RAID controllers, just plain HBAs without RAID firmware
    if possible), the same disks, and so on. That's the only way to ensure a constant
    level of Ceph performance in your environments.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 根据最佳实践，确保您始终在设计Ceph环境时牢记关键原则——所有数据节点需要具有相同的配置。这意味着相同数量的内存、相同的存储控制器（如果可能的话，不要使用RAID控制器，只使用普通的HBA而不带RAID固件）、相同的磁盘等。这是确保您的环境中Ceph性能保持恒定水平的唯一方法。
- en: 'One very important aspect of Ceph is data placement and how placement groups
    work. Placement groups offer us a chance to split the objects that we create and
    place them in OSDs in an optimal fashion. Translation: the bigger the number of
    placement groups we configure, the better balance we''re going to get.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: Ceph的一个非常重要的方面是数据放置和放置组的工作原理。放置组为我们提供了将创建的对象分割并以最佳方式放置在OSD中的机会。换句话说，我们配置的放置组数量越大，我们将获得的平衡就越好。
- en: So, let's configure Ceph from scratch. We're going to follow the best practices
    again and deploy Ceph by using five servers—one for administration, one for monitoring,
    and three OSDs.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们从头开始配置Ceph。我们将再次遵循最佳实践，并使用五台服务器部署Ceph——一台用于管理，一台用于监控，三个OSD。
- en: 'Our configuration will look like this:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的配置将如下所示：
- en: '![Figure 5.24 – Basic Ceph configuration for our infrastructure'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.24 - 我们基础设施的基本Ceph配置'
- en: '](img/B14834_05_24.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_24.jpg)'
- en: Figure 5.24 – Basic Ceph configuration for our infrastructure
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.24 - 我们基础设施的基本Ceph配置
- en: 'Make sure that these hosts can resolve each other via DNS or `/etc/hosts`,
    and that you configure all of them to use the same NTP source. Make sure that
    you update all of the hosts by using the following:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 确保这些主机可以通过DNS或`/etc/hosts`相互解析，并配置它们都使用相同的NTP源。确保通过以下方式更新所有主机：
- en: '[PRE24]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Also, make sure that you type the following commands into all of the hosts
    as the *root* user. Let''s start by deploying packages, creating an admin user,
    and giving them rights to `sudo`:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请确保您以*root*用户身份在所有主机上输入以下命令。让我们从部署软件包、创建管理员用户并赋予他们`sudo`权限开始：
- en: '[PRE25]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Disabling SELinux will make our life easier for this demonstration, as will
    getting rid of the firewall:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个演示来说，禁用SELinux会让我们的生活更轻松，摆脱防火墙也是如此。
- en: '[PRE26]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s add hostnames to `/etc/hosts` so that administration is easier for us:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将主机名添加到`/etc/hosts`中，以便我们更容易进行管理：
- en: '[PRE27]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Change the last `echo` part to suit your environment—hostnames and IP addresses.
    We''re just using this as an example from our environment. The next step is making
    sure that we can use our admin host to connect to all of the hosts. The easiest
    way to do that is by using SSH keys. So, on `ceph-admin`, log in as root and type
    in the `ssh-keygen` command, and then press the *Enter* key all the way through.
    It should look something like this:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 更改最后的`echo`部分以适应您的环境 - 主机名和IP地址。我们只是在这里举例说明。下一步是确保我们可以使用我们的管理主机连接到所有主机。最简单的方法是使用SSH密钥。因此，在`ceph-admin`上，以root身份登录并输入`ssh-keygen`命令，然后一直按*Enter*键。它应该看起来像这样：
- en: '![Figure 5.25 – Generating an SSH key for root for Ceph setup purposes'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.25-为Ceph设置目的为root生成SSH密钥'
- en: '](img/B14834_05_25.jpg)'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_25.jpg)'
- en: Figure 5.25 – Generating an SSH key for root for Ceph setup purposes
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.25-为Ceph设置目的为root生成SSH密钥
- en: 'We also need to copy this key to all of the hosts. So, again, on `ceph-admin`,
    use `ssh-copy-id` to copy the keys to all of the hosts:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要将此密钥复制到所有主机。因此，再次在`ceph-admin`上，使用`ssh-copy-id`将密钥复制到所有主机：
- en: '[PRE28]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Accept all of the keys when SSH asks you, and use `ceph123` as the password,
    which we selected in one of the earlier steps. After all of this is done, there''s
    one last step that we need to do on `ceph-admin` before we start deploying Ceph—we
    have to configure SSH to use the `cephadmin` user as a default user to log in
    to all of the hosts. We will do this by going to the `.ssh` directory as root
    on `ceph-admin`, and creating a file called `config` with the following content:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 当SSH询问您时，请接受所有密钥，并使用我们在较早步骤中选择的`ceph123`作为密码。完成所有这些之后，在我们开始部署Ceph之前，`ceph-admin`还有最后一步要做
    - 我们必须配置SSH以使用`cephadmin`用户作为默认用户登录到所有主机。我们将通过以root身份转到`ceph-admin`上的`.ssh`目录，并创建一个名为`config`的文件，并添加以下内容来完成这一步：
- en: '[PRE29]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'That was a long pre-configuration, wasn''t it? Now it''s time to actually start
    deploying Ceph. The first step is to configure `ceph-monitor`. So, on `ceph-admin`,
    type in the following commands:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 那是一个很长的预配置，不是吗？现在是时候真正开始部署Ceph了。第一步是配置`ceph-monitor`。因此，在`ceph-admin`上输入以下命令：
- en: '[PRE30]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Because of the fact that we selected a configuration in which we have three
    OSDs, we need to configure Ceph so that it uses these additional two hosts. So,
    in the `cluster` directory, edit the file called `ceph.conf` and add the following
    two lines at the end:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们选择了一个配置，其中有三个OSD，我们需要配置Ceph以便使用这另外两个主机。因此，在`cluster`目录中，编辑名为`ceph.conf`的文件，并在末尾添加以下两行：
- en: '[PRE31]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This will make sure that we can only use our example network (`192.168.159.0/24`)
    for Ceph, and that we have two additional OSDs on top of the original one.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这将确保我们只能使用我们的示例网络（`192.168.159.0/24`）进行Ceph，并且我们在原始的基础上有两个额外的OSD。
- en: 'Now that everything''s ready, we have to issue a sequence of commands to configure
    Ceph. So, again, on `ceph-admin`, type in the following commands:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切准备就绪，我们必须发出一系列命令来配置Ceph。因此，再次在`ceph-admin`上输入以下命令：
- en: '[PRE32]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let''s describe these commands one by one:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一描述这些命令：
- en: The first command starts the actual deployment process—for the admin, monitor,
    and OSD nodes, with the installation of all the necessary packages.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一条命令启动实际的部署过程 - 用于管理、监视和OSD节点的安装所有必要的软件包。
- en: The second and third commands configure the monitor host so that it's ready
    to accept external connections.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个和第三个命令配置监视主机，以便它准备好接受外部连接。
- en: The two disk commands are all about disk preparation—Ceph will clear the disks
    that we assigned to it (`/dev/sdb` per OSD host) and create two partitions on
    them, one for Ceph data and one for the Ceph journal.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这两个磁盘命令都是关于磁盘准备 - Ceph将清除我们分配给它的磁盘（每个OSD主机的`/dev/sdb`）并在上面创建两个分区，一个用于Ceph数据，一个用于Ceph日志。
- en: The last two commands prepare these filesystems for use and activate Ceph. If
    at any time your `ceph-deploy` script stops, check your DNS and `/etc/hosts` and
    `firewalld` configuration, as that's where the problems usually are.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后两个命令准备这些文件系统供使用并激活Ceph。如果您的`ceph-deploy`脚本在任何时候停止，请检查您的DNS和`/etc/hosts`和`firewalld`配置，因为问题通常出现在那里。
- en: 'We need to expose Ceph to our KVM host, which means that we have to do a bit
    of extra configuration. We''re going to expose Ceph as an object pool to our KVM
    host, so we need to create a pool. Let''s call it `KVMpool`. Connect to `ceph-admin`,
    and issue the following commands:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将Ceph暴露给我们的KVM主机，这意味着我们需要进行一些额外的配置。我们将Ceph公开为对象池给我们的KVM主机，因此我们需要创建一个池。让我们称之为`KVMpool`。连接到`ceph-admin`，并发出以下命令：
- en: '[PRE33]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This command will create a pool called `KVMpool`, with 128 placement groups.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将创建一个名为`KVMpool`的池，其中包含128个放置组。
- en: 'The next step involves approaching Ceph from a security perspective. We don''t
    want anyone connecting to this pool, so we''re going to create a key for authentication
    to Ceph, which we''re going to use on the KVM host for authentication purposes.
    We do that by typing the following command:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步涉及从安全角度接近Ceph。我们不希望任何人连接到这个池，因此我们将为Ceph创建一个用于身份验证的密钥，我们将在KVM主机上用于身份验证。我们通过输入以下命令来做到这一点：
- en: '[PRE34]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'It''s going to throw us a status message, something like this:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 它将向我们抛出一个状态消息，类似于这样：
- en: '[PRE35]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can then switch to the KVM host, where we need to do two things:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以切换到KVM主机，在那里我们需要做两件事：
- en: Define a secret—an object that's going to link libvirt to a Ceph user—and by
    doing that, we're going to create a secret object with its **Universally Unique
    Identifier** (**UUID**).
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义一个秘密 - 一个将libvirt链接到Ceph用户的对象 - 通过这样做，我们将创建一个带有其**通用唯一标识符**（**UUID**）的秘密对象。
- en: Use that secret's UUID to link it to the Ceph key when we define the Ceph storage
    pool.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在定义Ceph存储池时，使用该秘密的UUID将其与Ceph密钥进行关联。
- en: 'The easiest way to do these two steps would be by using two XML configuration
    files for libvirt. So, let''s create those two files. Let''s call the first one,
    `secret.xml`, and here are its contents:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这两个步骤的最简单方法是使用两个libvirt的XML配置文件。因此，让我们创建这两个文件。让我们称第一个为`secret.xml`，以下是其内容：
- en: '[PRE36]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Make sure that you save and import this XML file by typing in the following
    command:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您保存并导入此XML文件，输入以下命令：
- en: '[PRE37]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'After you press the *Enter* key, this command is going to throw out a UUID.
    Please copy and paste that UUID someplace safe, as we''re going to need it for
    the pool XML file. In our environment, this first `virsh` command threw out the
    following output:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 按下*Enter*键后，此命令将抛出一个UUID。请将该UUID复制并粘贴到一个安全的地方，因为我们将需要它用于池XML文件。在我们的环境中，这个第一个`virsh`命令抛出了以下输出：
- en: '[PRE38]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We need to assign a value to this secret so that when libvirt tries to use
    this secret, it knows which *password* to use. That''s actually the password that
    we created on the Ceph level, when we used `ceph auth get-create`, which threw
    us the key. So, now that we have both the secret UUID and the Ceph key, we can
    combine them to create a complete authentication object. On the KVM host, we need
    to type in the following command:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为这个秘密分配一个值，这样当libvirt尝试使用这个秘密时，它就知道要使用哪个*密码*。这实际上是我们在Ceph级别创建的密码，当我们使用`ceph
    auth get-create`时，它会给我们抛出密钥。因此，现在我们既有秘密UUID又有Ceph密钥，我们可以将它们结合起来创建一个完整的认证对象。在KVM主机上，我们需要输入以下命令：
- en: '[PRE39]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, we can create the Ceph pool file. Let''s call the config file `ceph.xml`,
    and here are its contents:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建Ceph池文件。让我们把配置文件命名为`ceph.xml`，以下是它的内容：
- en: '[PRE40]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'So, the UUID from the previous step was used in this file to reference which
    secret (identity) is going to be used for Ceph pool access. Now we need to do
    the standard procedure—import the pool, start it, and autostart it—if we want
    to use it permanently (after the KVM host reboot). So, let''s do that with the
    following sequence of commands on the KVM host:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，上一步的UUID被用于这个文件中，用来引用哪个秘密（身份）将被用于Ceph池访问。现在，如果我们想要永久使用它（在KVM主机重新启动后），我们需要执行标准程序——导入池，启动它，并自动启动它。因此，让我们在KVM主机上使用以下命令序列来执行：
- en: '[PRE41]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The last command should produce an output similar to this:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个命令应该产生类似于这样的输出：
- en: '![Figure 5.26 – Checking the state of our pools; the Ceph pool is configured
    and ready to be used'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.26-检查我们的池的状态；Ceph池已配置并准备好使用'
- en: '](img/B14834_05_26.jpg)'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_26.jpg)'
- en: Figure 5.26 – Checking the state of our pools; the Ceph pool is configured and
    ready to be used
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.26-检查我们的池的状态；Ceph池已配置并准备好使用
- en: Now that the Ceph object pool is available for our KVM host, we could install
    a virtual machine on it. We're going to work on that – again – in [*Chapter 7*](B14834_07_Final_ASB_ePub.xhtml#_idTextAnchor125),
    *Virtual Machine – Installation, Configuration, and Life-Cycle Management*.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Ceph对象池对我们的KVM主机可用，我们可以在其上安装虚拟机。我们将在[*第7章*](B14834_07_Final_ASB_ePub.xhtml#_idTextAnchor125)中再次进行这项工作——*虚拟机-安装、配置和生命周期管理*。
- en: Virtual disk images and formats and basic KVM storage operations
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 虚拟磁盘镜像和格式以及基本的KVM存储操作
- en: 'Disk images are standard files stored on the host''s filesystem. They are large
    and act as virtualized hard drives for guests. You can create such files using
    the `dd` command, as shown:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 磁盘镜像是存储在主机文件系统上的标准文件。它们很大，作为客人的虚拟硬盘。您可以使用`dd`命令创建这样的文件，如下所示：
- en: '[PRE42]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Here is the translation of this command for you:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是这个命令的翻译：
- en: Duplicate data (`dd`) from the input file (`if`) of `/dev/zero` (virtually limitless
    supply of zeros) into the output file (`of`) of `/vms/dbvm_disk2.img` (disk image)
    using blocks of 1 G size (`bs` = block size) and repeat this (`count`) just once
    (`10`).
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入文件（`if`）`/dev/zero`（几乎无限的零）复制数据（`dd`）到输出文件（`of`）`/vms/dbvm_disk2.img`（磁盘镜像），使用1G大小的块（`bs`
    = 块大小），并重复这个操作（`count`）只一次（`10`）。
- en: 'Important note:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示：
- en: '`dd` is known to be a resource-hungry command. It may cause I/O problems on
    the host system, so it''s good to first check the available free memory and I/O
    state of the host system, and only then run it. If the system is already loaded,
    lower the block size to MB and increase the count to match the size of the file
    you wanted (use `bs=1M`, `count=10000` instead of `bs=1G`, `count=10`).'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '`dd`被认为是一个耗费资源的命令。它可能会在主机系统上引起I/O问题，因此最好先检查主机系统的可用空闲内存和I/O状态，然后再运行它。如果系统已经加载，降低块大小到MB，并增加计数以匹配您想要的文件大小（使用`bs=1M`，`count=10000`，而不是`bs=1G`，`count=10`）。'
- en: '`/vms/dbvm_disk2.img` is the result of the preceding command. The image now
    has 10 GB preallocated and ready to use with guests either as the boot disk or
    second disk. Similarly, you can also create thin-provisioned disk images. Preallocated
    and thin-provisioned (sparse) are disk allocation methods, or you may also call
    it the format:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '`/vms/dbvm_disk2.img`是前面命令的结果。该镜像现在已经预分配了10GB，并准备好与客人一起使用，无论是作为引导磁盘还是第二个磁盘。同样，您也可以创建薄配置的磁盘镜像。预分配和薄配置（稀疏）是磁盘分配方法，或者您也可以称之为格式：'
- en: '**Preallocated**: A preallocated virtual disk allocates the space right away
    at the time of creation. This usually means faster write speeds than a thin-provisioned
    virtual disk.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预分配**：预分配的虚拟磁盘在创建时立即分配空间。这通常意味着比薄配置的虚拟磁盘写入速度更快。'
- en: '`seek` option with the `dd` command, as shown in the following command:'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dd`命令中的`seek`选项，如下所示：'
- en: '[PRE43]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Each comes with its own advantages and disadvantages. If you are looking for
    I/O performance, go for a preallocated format, but if you have a non-IO-intensive
    load, choose thin-provisioned.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 每种方法都有其优缺点。如果您正在寻求I/O性能，选择预分配格式，但如果您有非I/O密集型负载，请选择薄配置。
- en: 'Now, you might be wondering how you can identify what disk allocation method
    a certain virtual disk uses. There is a good utility for finding this out: `qemu-img`.
    This command allows you to read the metadata of a virtual image. It also supports
    creating a new disk and performing low-level format conversion.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可能想知道如何识别某个虚拟磁盘使用了什么磁盘分配方法。有一个很好的实用程序可以找出这一点：`qemu-img`。这个命令允许您读取虚拟镜像的元数据。它还支持创建新的磁盘和执行低级格式转换。
- en: Getting image information
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取镜像信息
- en: 'The `info` parameter of the `qemu-img` command displays information about a
    disk image, including the absolute path of the image, the file format, and the
    virtual and disk size. By looking at the virtual disk size from a QEMU perspective
    and comparing that to the image file size on the disk, you can easily identify
    what disk allocation policy is in use. As an example, let''s look at two of the
    disk images we created:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: “qemu-img”命令的“info”参数显示有关磁盘镜像的信息，包括镜像的绝对路径、文件格式和虚拟和磁盘大小。通过从QEMU的角度查看虚拟磁盘大小，并将其与磁盘上的镜像文件大小进行比较，您可以轻松地确定正在使用的磁盘分配策略。例如，让我们看一下我们创建的两个磁盘镜像：
- en: '[PRE44]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: See the `disk size` line of both the disks. It's showing `10G` for `/vms/dbvm_disk2.img`,
    whereas for `/vms/dbvm_disk2_seek.img`, it's showing `10M` MiB. This difference
    is because the second disk uses a thin-provisioning format. The virtual size is
    what guests see and the disk size is what space the disk reserved on the host.
    If both the sizes are the same, it means the disk is preallocated. A difference
    means that the disk uses the thin-provisioning format. Now, let's attach the disk
    image to a virtual machine; you can attach it using `virt-manager` or the CLI
    alternative, `virsh`.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 查看两个磁盘的“磁盘大小”行。对于“/vms/dbvm_disk2.img”，显示为“10G”，而对于“/vms/dbvm_disk2_seek.img”，显示为“10M”
    MiB。这种差异是因为第二个磁盘使用了薄配置格式。虚拟大小是客户看到的，磁盘大小是磁盘在主机上保留的空间。如果两个大小相同，这意味着磁盘是预分配的。差异意味着磁盘使用了薄配置格式。现在，让我们将磁盘镜像附加到虚拟机；您可以使用“virt-manager”或CLI替代方案“virsh”进行附加。
- en: Attaching a disk using virt-manager
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用virt-manager附加磁盘
- en: 'Start virt-manager from the host system''s graphical desktop environment. It
    can also be started remotely using SSH, as demonstrated in the following command:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 从主机系统的图形桌面环境启动virt-manager。也可以使用SSH远程启动，如以下命令所示：
- en: '[PRE45]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'So, let''s use the Virtual Machine Manager to attach the disk to the virtual
    machine:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们使用虚拟机管理器将磁盘附加到虚拟机：
- en: In the Virtual Machine Manager main window, select the virtual machine to which
    you want to add the secondary disk.
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在虚拟机管理器的主窗口中，选择要添加辅助磁盘的虚拟机。
- en: Go to the virtual hardware details window and click on the **Add Hardware**
    button located at the bottom-left side of the dialog box.
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到虚拟硬件详细信息窗口，然后单击对话框底部左侧的“添加硬件”按钮。
- en: In **Add New Virtual Hardware**, select **Storage** and select the **Create
    a disk image for the virtual machine** button and virtual disk size, as in the
    following screenshot:![Figure 5.27 – Adding a virtual disk in virt-manager
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在“添加新虚拟硬件”中，选择“存储”，然后选择“为虚拟机创建磁盘镜像”按钮和虚拟磁盘大小，如下面的屏幕截图所示：![图5.27 - 在virt-manager中添加虚拟磁盘
- en: '](img/B14834_05_27.jpg)'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_27.jpg)'
- en: Figure 5.27 – Adding a virtual disk in virt-manager
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.27 - 在virt-manager中添加虚拟磁盘
- en: If you want to attach the previously created `dbvm_disk2.img` image, choose
    `dbvm_disk2.img` file from the `/vms` directory or find it in the local storage
    pool, then select it and click `/dev/sdb`) or disk partition (`/dev/sdb1`), or
    LVM logical volume. We could have used any of the previously configured storage
    pools for storing this image either as a file or object or directly to a block
    device.
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果要附加先前创建的“dbvm_disk2.img”镜像，选择“/vms”目录中的“dbvm_disk2.img”文件或在本地存储池中找到它，然后选择它并单击“/dev/sdb”）或磁盘分区（“/dev/sdb1”）或LVM逻辑卷。我们可以使用任何先前配置的存储池来存储此镜像，无论是作为文件还是对象，还是直接到块设备。
- en: Clicking on the `virsh` command.
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“virsh”命令。
- en: Using virt-manager to create a virtual disk was easy enough—just a couple of
    clicks of a mouse and a bit of typing. Now, let's see how we can do that via the
    command line—namely, by using `virsh`.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 使用virt-manager创建虚拟磁盘非常简单——只需点击几下鼠标并输入一些内容。现在，让我们看看如何通过命令行来做到这一点，即使用“virsh”。
- en: Attaching a disk using virsh
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用virsh附加磁盘
- en: '`virsh` is a very powerful command-line alternative to virt-manager. You can
    perform an action in a second that would take minutes to perform through a graphical
    interface such as virt-manager. It provides an `attach-disk` option to attach
    a new disk device to a virtual machine. There are lots of switches provided with
    `attach-disk`:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '`virsh`是virt-manager的非常强大的命令行替代品。您可以在几秒钟内执行一个动作，而通过virt-manager等图形界面可能需要几分钟。它提供了“attach-disk”选项，用于将新的磁盘设备附加到虚拟机。与“attach-disk”一起提供了许多开关：'
- en: '[PRE46]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'However, in a normal scenario, the following are sufficient to perform hot-add
    disk attachment to a virtual machine:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在正常情况下，以下内容足以对虚拟机执行热添加磁盘附加：
- en: '[PRE47]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Here, `CentOS8` is the virtual machine to which a disk attachment is executed.
    Then, there is the path of the disk image. `vdb` is the target disk name that
    would be visible inside the guest operating system. `--live` means performing
    the action while the virtual machine is running, and `--config` means attaching
    it persistently across reboot. Not adding a `--config` switch will keep the disk
    attached only until reboot.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，“CentOS8”是执行磁盘附加的虚拟机。然后是磁盘镜像的路径。“vdb”是目标磁盘名称，在宿主操作系统中可见。“--live”表示在虚拟机运行时执行操作，“--config”表示在重新启动后持久地附加它。不添加“--config”开关将使磁盘仅在重新启动前附加。
- en: 'Important note:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示：
- en: 'Hot plugging support: The `acpiphp` kernel module should be loaded in a Linux
    guest operating system in order to recognize a hot-added disk; `acpiphp` provides
    legacy hot plugging support, whereas `pciehp` provides native hot plugging support
    . `pciehp` is dependent on `acpiphp`. Loading `acpiphp` will automatically load
    `pciehp` as a dependency.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 热插拔支持：在Linux宿主操作系统中加载“acpiphp”内核模块以识别热添加的磁盘；“acpiphp”提供传统的热插拔支持，而“pciehp”提供本地的热插拔支持。“pciehp”依赖于“acpiphp”。加载“acpiphp”将自动加载“pciehp”作为依赖项。
- en: 'You can use the `virsh domblklist <vm_name>` command to quickly identify how
    many vDisks are attached to a virtual machine. Here is an example:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用“virsh domblklist <vm_name>”命令快速识别附加到虚拟机的vDisks数量。以下是一个示例：
- en: '[PRE48]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This clearly indicates that the two vDisks connected to the virtual machine
    are both file images. They are visible to the guest operating system as `vda`
    and `vdb`, respectively, and in the last column of the disk images path on the
    host system.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 这清楚地表明连接到虚拟机的两个vDisks都是文件映像。它们分别显示为客户操作系统的`vda`和`vdb`，并且在主机系统上的磁盘映像路径的最后一列中可见。
- en: Next, we are going to see how to create an ISO library.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到如何创建ISO库。
- en: Creating an ISO image library
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建ISO镜像库
- en: Although a guest operating system on the virtual machine can be installed from
    physical media by carrying out a passthrough the host's CD/DVD drive to the virtual
    machine, it's not the most efficient way. Reading from a DVD drive is slow compared
    to reading ISO from a hard disk, so a better way is to store ISO files (or logical
    CDs) used to install operating systems and applications for the virtual machines
    in a file-based storage pool and create an ISO image library.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟机上的客户操作系统虽然可以通过将主机的CD/DVD驱动器传递到虚拟机来从物理媒体安装，但这并不是最有效的方法。从DVD驱动器读取比从硬盘读取ISO文件慢，因此更好的方法是将用于安装操作系统和虚拟机应用程序的ISO文件（或逻辑CD）存储在基于文件的存储池中，并创建ISO镜像库。
- en: 'To create an ISO image library, you can either use virt-manager or a `virsh`
    command. Let''s see how to create an ISO image library using the `virsh` command:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建ISO镜像库，可以使用virt-manager或`virsh`命令。让我们看看如何使用`virsh`命令创建ISO镜像库：
- en: 'First, create a directory on the host system to store the `.iso` images:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，在主机系统上创建一个目录来存储`.iso`镜像：
- en: '[PRE49]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Set the correct permissions. It should be owned by a root user with permission
    set to `700`. If SELinux is in enforcing mode, the following context needs to
    be set:'
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置正确的权限。它应该由root用户拥有，权限设置为`700`。如果SELinux处于强制模式，则需要设置以下上下文：
- en: '[PRE50]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Define the ISO image library using the `virsh` command, as shown in the following
    code block:'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`virsh`命令定义ISO镜像库，如下面的代码块所示：
- en: '[PRE51]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Verify that the pool (ISO image library) was created:'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证是否已创建池（ISO镜像库）：
- en: '[PRE52]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Now you can copy or move the `.iso` images to the `/iso_lib` directory.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在可以将`.iso`镜像复制或移动到`/iso_lib`目录中。
- en: 'Upon copying the `.iso` files into the `/iso_lib` directory, refresh the pool
    and then check its contents:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`.iso`文件复制到`/iso_lib`目录后，刷新池，然后检查其内容：
- en: '[PRE53]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: This will list all the ISO images stored in the directory, along with their
    path. These ISO images can now be used directly with a virtual machine for guest
    operating system installation, software installation, or upgrades.
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将列出存储在目录中的所有ISO镜像，以及它们的路径。这些ISO镜像现在可以直接与虚拟机一起用于客户操作系统的安装、软件安装或升级。
- en: Creating an ISO image library is the de facto norm in today's enterprises. It's
    better to have a centralized place where all your ISO images are, and it makes
    it easier to implement some kind of synchronization method (for example, `rsync`)
    if you need to synchronize across different locations.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 在今天的企业中，创建ISO镜像库是一种事实上的规范。最好有一个集中的地方存放所有的ISO镜像，并且如果需要在不同位置进行同步（例如`rsync`），这样做会更容易。
- en: Deleting a storage pool
  id: totrans-418
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除存储池
- en: Deleting a storage pool is fairly easy. Please note that deleting a storage
    domain will not remove any file/block devices. It just disconnects the storage
    from virt-manager. The file/block device has to be removed manually.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 删除存储池相当简单。请注意，删除存储域不会删除任何文件/块设备。它只是将存储从virt-manager中断开。文件/块设备必须手动删除。
- en: 'We can delete a storage pool via virt-manager or by using the `virsh` command.
    Let''s first check how to do it via virt-manager:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过virt-manager或使用`virsh`命令删除存储池。让我们首先看看如何通过virt-manager进行操作：
- en: '![Figure 5.28 – Deleting a pool'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.28–删除存储池'
- en: '](img/B14834_05_28.jpg)'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_28.jpg)'
- en: Figure 5.28 – Deleting a pool
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.28–删除存储池
- en: First, select the red stop button to stop the pool, and then click on the red
    circle with an **X** to delete the pool.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，选择红色停止按钮停止池，然后单击带有**X**的红色圆圈以删除池。
- en: 'If you want to use `virsh`, it''s even simpler. Let''s say that we want to
    delete the storage pool called `MyNFSpool` in the previous screenshot. Just type
    in the following commands:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要使用`virsh`，那就更简单了。假设我们要删除上一个截图中名为`MyNFSpool`的存储池。只需输入以下命令：
- en: '[PRE54]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The next logical step after creating a storage pool is to create a storage volume.
    From a logical standpoint, the storage volume slices a storage pool into smaller
    parts. Let's learn how to do that now.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 创建存储池后的下一个逻辑步骤是创建存储卷。从逻辑上讲，存储卷将存储池划分为较小的部分。现在让我们学习如何做到这一点。
- en: Creating storage volumes
  id: totrans-428
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建存储卷
- en: 'Storage volumes are created on top of storage pools and attached as virtual
    disks to virtual machines. In order to create a storage volume, start the Storage
    Management console, navigate to virt-manager, then click **Edit** | **Connection
    Details** | **Storage** and select the storage pool where you want to create a
    new volume. Click on the create new volume button (**+**):'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 存储卷是在存储池之上创建的，并作为虚拟磁盘附加到虚拟机。为了创建存储卷，启动存储管理控制台，导航到virt-manager，然后单击**编辑** | **连接详细信息**
    | **存储**，并选择要创建新卷的存储池。单击创建新卷按钮（**+**）：
- en: '![Figure 5.29 – Creating a storage volume for the virtual machine'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.29–为虚拟机创建存储卷'
- en: '](img/B14834_05_29.jpg)'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_29.jpg)'
- en: Figure 5.29 – Creating a storage volume for the virtual machine
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.29–为虚拟机创建存储卷
- en: Next, provide the name of the new volume, choose the disk allocation format
    for it, and click on the `virsh` command. There are several disk formats that
    are supported by libvirt (`raw`, `cow`, `qcow`, `qcow2`, `qed`, and `vmdk`). Use
    the disk format that suits your environment and set the proper size in the `Max
    Capacity` and `Allocation` fields to decide whether you wish to go with preallocated
    disk allocation or thin-provisioned. If you keep the disk size the same in `qcow2`
    format does not support the thick disk allocation method.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，提供新卷的名称，选择磁盘分配格式，并单击`virsh`命令。libvirt支持几种磁盘格式（`raw`、`cow`、`qcow`、`qcow2`、`qed`和`vmdk`）。使用适合您环境的磁盘格式，并在`最大容量`和`分配`字段中设置适当的大小，以决定您是否希望选择预分配的磁盘分配或薄置备。如果在`qcow2`格式中保持磁盘大小不变，则不支持厚磁盘分配方法。
- en: In [*Chapter 8*](B14834_08_Final_ASB_ePub.xhtml#_idTextAnchor143), *Creating
    and Modifying VM Disks, Templates, and Snapshots*, all the disk formats are explained
    in detail. For now, just understand that `qcow2` is a specially designed disk
    format for KVM virtualization. It supports the advanced features needed for creating
    internal snapshots.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第8章*]（B14834_08_Final_ASB_ePub.xhtml#_idTextAnchor143）*创建和修改VM磁盘、模板和快照*中，详细解释了所有磁盘格式。现在，只需了解`qcow2`是为KVM虚拟化专门设计的磁盘格式。它支持创建内部快照所需的高级功能。
- en: Creating volumes using the virsh command
  id: totrans-435
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用virsh命令创建卷
- en: 'The syntax to create a volume using the `virsh` command is as follows:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`virsh`命令创建卷的语法如下：
- en: '[PRE55]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Here, `dedicated_storage` is the storage pool, `vm_vol1` is the volume name,
    and 10 GB is the size:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`dedicated_storage`是存储池，`vm_vol1`是卷名称，10 GB是大小。
- en: '[PRE56]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The `virsh` command and arguments to create a storage volume are almost the
    same regardless of the type of storage pool it is created on. Just enter the appropriate
    input for a `--pool` switch. Now, let's see how to delete a volume using the `virsh`
    command.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '`virsh`命令和参数用于创建存储卷，几乎不管它是在哪种类型的存储池上创建的，都几乎相同。只需输入适当的输入以使用`--pool`开关。现在，让我们看看如何使用`virsh`命令删除卷。'
- en: Deleting a volume using the virsh command
  id: totrans-441
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用virsh命令删除卷
- en: 'The syntax to delete a volume using the `virsh` command is as follows:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`virsh`命令删除卷的语法如下：
- en: '[PRE57]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Executing this command will remove the `vm_vol2` volume from the `dedicated_storage`
    storage pool.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此命令将从`dedicated_storage`存储池中删除`vm_vol2`卷。
- en: The next step in our storage journey is about looking a bit into the future
    as all of the concepts that we mentioned in this chapter have been well known
    for years, some even for decades. The world of storage is changing and moving
    into new and interesting directions, so let's discuss that for a bit next.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 我们存储之旅的下一步是展望未来，因为本章提到的所有概念多年来都广为人知，甚至有些已经有几十年的历史了。存储世界正在改变，朝着新的有趣方向发展，让我们稍微讨论一下。
- en: The latest developments in storage – NVMe and NVMeOF
  id: totrans-446
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储的最新发展 - NVMe和NVMeOF
- en: In the past 20 or so years, by far the biggest disruption in the storage world
    in terms of technology has been the introduction of **Solid State Drives** (**SSDs**).
    Now, we know that a lot of people have gotten quite used to having them in their
    computers—laptops, workstations, whichever type of device we use. But again, we're
    discussing storage for virtualization, and enterprise storage concepts overall,
    and that means that our regular SATA SSDs aren't going to make the cut. Although
    a lot of people use them in mid-range storage devices and/or handmade storage
    devices that host ZFS pools (for cache), some of these concepts have a life of
    their own in the latest generations of storage devices. These devices are fundamentally
    changing the way technology is working and redoing parts of modern IT history
    in terms of which protocols are used, how fast they are, how much lower latencies
    they have, and how they approach storage tiering—tiering being a concept that
    differentiates different storage devices or their storage pools based on a capability,
    usually speed.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的20年左右，就技术而言，存储世界最大的颠覆是**固态硬盘**（**SSD**）的引入。现在，我们知道很多人已经习惯在他们的计算机上使用它们 -
    笔记本电脑、工作站，无论我们使用哪种类型的设备。但是，我们正在讨论虚拟化的存储和企业存储概念，这意味着我们常规的SATA SSD不够用。尽管很多人在中档存储设备和/或手工制作的存储设备中使用它们来托管ZFS池（用于缓存），但这些概念在最新一代存储设备中有了自己的生命。这些设备从根本上改变了技术的工作方式，并在现代IT历史的某些部分进行了重塑，包括使用的协议、速度有多快、延迟有多低，以及它们如何处理存储分层
    - 分层是一个区分不同存储设备或它们的存储池的概念，通常是速度的能力。
- en: Let's briefly explain what we're discussing here by using an example of where
    the storage world is heading. Along with that, the storage world is taking the
    virtualization, cloud, and HPC world along for the ride, so these concepts are
    not outlandish. They already exist, in readily available storage devices that
    you can buy today.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要解释一下我们正在讨论的内容，通过一个存储世界的发展方向的例子。除此之外，存储世界正在带动虚拟化、云和HPC世界一起前进，因此这些概念并不离奇。它们已经存在于现成的存储设备中，您今天就可以购买到。
- en: The introduction of SSDs brought a significant change in the way we access our
    storage devices. It's all about performance and latency, and older concepts such
    as **Advanced Host Controller Interface** (**AHCI**), which we're still actively
    using with many SSDs on the market today, are just not good enough to handle the
    performance that SSDs have. AHCI is a standard way in which a regular hard disk
    (mechanical disk or regular spindle) talks via software to SATA devices. However,
    the key part of that is *hard disk*, which means cylinders, heads sectors—things
    that SSDs just don't have, as they don't spin around and don't need that kind
    of paradigm. That meant that another standard had to be created so that we can
    use SSDs in a more native fashion. That's what **Non-Volatile Memory Express**
    (**NVMe**) is all about—bridging the gap between what SSDs are capable of doing
    and what they can actually do, without using translations from SATA to AHCI to
    PCI Express (and so on).
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: SSD的引入显著改变了我们访问存储设备的方式。这一切都关乎性能和延迟，而像**高级主机控制器接口**（**AHCI**）这样的旧概念，我们今天市场上仍在积极使用，已经不足以处理SSD的性能。AHCI是常规硬盘（机械硬盘或常规磁头）通过软件与SATA设备通信的标准方式。然而，关键部分是*硬盘*，这意味着圆柱、磁头扇区—这些SSD根本没有，因为它们不会旋转，也不需要那种范式。这意味着必须创建另一个标准，以便我们可以更本地地使用SSD。这就是**非易失性内存扩展**（**NVMe**）的全部内容—弥合SSD的能力和实际能力之间的差距，而不使用从SATA到AHCI到PCI
    Express（等等）的转换。
- en: The fast development pace of SSDs and the integration of NVMe made huge advancements
    in enterprise storage possible. That means that new controllers, new software,
    and completely new architectures had to be invented to support this paradigm shift.
    As more and more storage devices integrate NVMe for various purposes—primarily
    for caching, then for storage capacity as well—it's becoming clear that there
    are other problems that need to be solved as well. The first of which is the way
    in which we're going to connect storage devices offering such a tremendous amount
    of capability to our virtualized, cloud, or HPC environments.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: SSD的快速发展速度和NVMe的整合使企业存储取得了巨大的进步。这意味着必须发明新的控制器、新的软件和完全新的架构来支持这种范式转变。随着越来越多的存储设备为各种目的集成NVMe—主要是用于缓存，然后也用于存储容量—变得清楚的是，还有其他问题需要解决。其中第一个问题是我们将如何连接提供如此巨大能力的存储设备到我们的虚拟化、云或HPC环境。
- en: 'In the past 10 or so years, many people argued that FC is going to disappear
    from the market, and a lot of companies hedged their bets on different standards—iSCSI,
    iSCSI over RDMA, NFS over RDMA, and so on. The reasoning behind that seemed solid
    enough:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的10年左右，许多人争论说FC将从市场上消失，许多公司对不同的标准进行了押注—iSCSI、iSCSI over RDMA、NFS over RDMA等。这背后的推理似乎足够坚实：
- en: FC is expensive—it requires separate physical switches, separate cabling, and
    separate controllers, all of which cost a lot of money.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FC很昂贵——它需要单独的物理交换机、单独的布线和单独的控制器，所有这些都需要花费大量的钱。
- en: There's licensing involved—when you buy, for example, a Brocade switch that
    has 40 FC ports, that doesn't mean that you can use all of them out of the box,
    as there are licenses to get more ports (8-port, 16-port, and so on).
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 涉及许可证—当你购买一个拥有40个FC端口的Brocade交换机时，并不意味着你可以立即使用所有端口，因为需要许可证来获取更多端口（8端口、16端口等）。
- en: FC storage devices are expensive and often require more expensive disks (with
    FC connectors).
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FC存储设备昂贵，并且通常需要更昂贵的磁盘（带有FC连接器）。
- en: Configuring FC requires extensive knowledge and/or training, as you can't simply
    go and configure a stack of FC switches for an enterprise-level company without
    knowing the concepts, and the CLI from the switch vendor, on top of knowing what
    that enterprise's needs are.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置FC需要广泛的知识和/或培训，因为你不能简单地去配置一堆FC交换机给一个企业级公司，而不知道概念和交换机供应商的CLI，还要知道企业的需求。
- en: The ability of FC as a protocol to speed up its development to reach new speeds
    has been really bad. In simple terms, during the time it took FC to go from 8
    Gbit/s to 32 Gbit/s, Ethernet went from 1 Gbit/s to 25, 40, 50, and 100 Gbit/s
    bandwidth. There's already talk about 400 Gbit/s Ethernet, and there are the first
    devices that support that standard as well. That usually makes customers concerned
    as higher numbers mean better throughput, at least in most people's minds.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为一种协议，FC加速发展以达到新的速度的能力一直很差。简单来说，在FC从8 Gbit/s加速到32 Gbit/s的时间内，以太网从1 Gbit/s加速到25、40、50和100
    Gbit/s的带宽。已经有关于400 Gbit/s以太网的讨论，也有第一个支持该标准的设备。这通常会让客户感到担忧，因为更高的数字意味着更好的吞吐量，至少在大多数人的想法中是这样。
- en: But what's happening on the market *now* tells us a completely different story—not
    just that FC is back, but that it's back with a mission. The enterprise storage
    companies have embraced that and started introducing storage devices with *insane*
    levels of performance (with the aid of NVMe SSDs, as a first phase). That performance
    needs to be transferred to our virtualized, cloud, and HPC environments, and that
    requires the best possible protocol, in terms of lowest latency, its design, and
    the quality and reliability, and FC has all of that.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 但市场上*现在*发生的事情告诉我们一个完全不同的故事—不仅FC回来了，而且它回来了有使命。企业存储公司已经接受了这一点，并开始推出具有*疯狂*性能水平的存储设备（首先是NVMe
    SSD的帮助）。这种性能需要转移到我们的虚拟化、云和HPC环境中，这需要最佳的协议，以实现最低的延迟、设计、质量和可靠性，而FC具备所有这些。
- en: That leads to the second phase, where NVMe SSDs aren't just being used as cache
    devices, but as capacity devices as well.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了第二阶段，NVMe SSD不仅被用作缓存设备，而且也被用作容量设备。
- en: Take note of the fact that, right now, there's a big fight brewing on the storage
    memory/storage interconnects market. There are multiple different standards trying
    to compete with Intel's **Quick Path Interconnect** (**QPI**), a technology that's
    been used in Intel CPUs for more than a decade. If this is a subject that's interesting
    to you, there is a link at the end of this chapter, in the *Further reading* section,
    where you can find more information. Essentially, QPI is a point-to-point interconnection
    technology with low latency and high bandwidth that's at the core of today's servers.
    Specifically, it handles communication between CPUs, CPUs and memory, CPUs and
    chipsets, and so on. It's a technology that Intel developed after it got rid of
    the **Front Side Bus** (**FSB**) and chipset-integrated memory controllers. FSB
    was a shared bus that was shared between memory and I/O requests. That approach
    had much higher latency, didn't scale well, and had lower bandwidth and problems
    with situations in which there's a large amount of I/O happening on the memory
    and I/O side. After switching to an architecture where the memory controller was
    a part of the CPU (therefore, memory directly connects to it), it was essential
    for Intel to finally move to this kind of concept.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，目前存储内存/存储互连市场上正在酝酿一场大战。有多种不同的标准试图与英特尔的**快速路径互连**（**QPI**）竞争，这项技术已经在英特尔CPU中使用了十多年。如果这是你感兴趣的话题，本章末尾有一个链接，在*进一步阅读*部分，你可以找到更多信息。基本上，QPI是一种点对点互连技术，具有低延迟和高带宽，是当今服务器的核心。具体来说，它处理CPU之间、CPU和内存、CPU和芯片组等之间的通信。这是英特尔在摆脱**前端总线**（**FSB**）和芯片组集成内存控制器后开发的技术。FSB是一个在内存和I/O请求之间共享的总线。这种方法具有更高的延迟，不易扩展，带宽较低，并且在内存和I/O端发生大量I/O的情况下存在问题。在切换到内存控制器成为CPU的一部分的架构后（因此，内存直接连接到它），对于英特尔最终转向这种概念是至关重要的。
- en: If you're more familiar with AMD CPUs, QPI is to Intel what HyperTransport bus
    on a CPU with built-in memory controller is to AMD CPUs.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更熟悉AMD CPU，QPI对英特尔来说就像内置内存控制器的CPU上的HyperTransport总线对AMD CPU来说一样。
- en: 'As NVMe SSDs became faster, the PCI Express standard also needed to be updated,
    which is the reason why the latest version (PCIe 4.0 – the first products started
    shipping recently) was so eagerly anticipated. But now, the focus has switched
    to two other problems that need resolving for storage systems to work. Let''s
    describe them briefly:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 随着NVMe SSD变得更快，PCI Express标准也需要更新，这就是为什么最新版本（PCIe 4.0 - 最新产品最近开始发货）如此受期待的原因。但现在，焦点已经转移到需要解决的另外两个问题。让我们简要描述一下：
- en: Problem number one is simple. For a regular computer user, one or two NVMe SSDs
    will be enough in 99% of scenarios or more. Realistically, the only real reason
    why regular computer users need a faster PCIe bus is for a faster graphics cards.
    But for storage manufacturers, it's completely different. They want to produce
    enterprise storage devices that will have 20, 30, 50, 100, 500 NVMe SSDs in a
    single storage system—and they want that now, as SSDs are mature as a technology
    and are widely available.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个问题很简单。对于普通计算机用户，在99%或更多的情况下，一两个NVMe SSD就足够了。实际上，普通计算机用户需要更快的PCIe总线的唯一真正原因是为了更快的显卡。但对于存储制造商来说，情况完全不同。他们希望生产企业存储设备，其中将有20、30、50、100、500个NVMe
    SSD在一个存储系统中-他们希望现在就能做到这一点，因为SSD作为一种技术已经成熟并且广泛可用。
- en: Problem number two is more complex. To add insult to injury, the latest generation
    of SSDs (for example, based on Intel Optane) can offer even lower latency and
    higher throughput. That's only going to get *worse* (even lower latencies, higher
    throughput) as technology evolves. For today's services—virtualization, cloud,
    and HPC—it's essential that the storage system is able to handle any load that
    we can throw at it. These technologies are a real game-changer in terms of how
    much faster storage devices can become, only if interconnects can handle it (QPI,
    FC, and many more). Two of these concepts derived from Intel Optane—**Storage
    Class Memory** (**SCM**) and **Persistent Memory** (**PM**) are the latest technologies
    that storage companies and customers want adopted into their storage systems,
    and fast.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个问题更为复杂。更令人沮丧的是，最新一代的SSD（例如基于英特尔Optane的SSD）可以提供更低的延迟和更高的吞吐量。随着技术的发展，这种情况只会变得更糟（更低的延迟，更高的吞吐量）。对于今天的服务-虚拟化、云和HPC-存储系统能够处理我们可能投入其中的任何负载是至关重要的。这些技术在存储设备变得更快的程度上是真正的游戏改变者，只要互连能够处理它（QPI、FC等）。从英特尔Optane衍生出的两个概念-**存储级内存**（**SCM**）和**持久内存**（**PM**）是存储公司和客户希望快速采用到他们的存储系统中的最新技术。
- en: The third problem is how to transfer all of that bandwidth and I/O capability
    to the servers and infrastructures using them. This is why the concept of **NVMe
    over Fabrics** (**NVMe-OF**) was created, to try to work on the storage infrastructure
    stack to make NVMe much more efficient and faster for its consumers.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个问题是如何将所有这些带宽和I/O能力传输到使用它们的服务器和基础设施。这就是为什么创建了**NVMe over Fabrics**（**NVMe-OF**）的概念，试图在存储基础设施堆栈上工作，使NVMe对其消费者更加高效和快速。
- en: If you take a look at these advancements from a conceptual point of view, it
    was clear for decades that RAM-like memory is the fastest, lowest latency technology
    that we've had for the past couple of decades. It's also logical that we're moving
    workloads to RAM, as much as possible. Think of in-memory databases (such as Microsoft
    SQL, SAP Hana, and Oracle). They've been around the block for years.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上看，几十年来，RAM样的内存是我们拥有的最快、最低延迟的技术。逻辑上，我们正在尽可能地将工作负载转移到RAM。想想内存数据库（如Microsoft
    SQL、SAP Hana和Oracle）。它们已经存在多年了。
- en: These technologies fundamentally change the way we think about storage. Basically,
    no longer are we discussing storage tiering based on technology (SSD versus SAS
    versus SATA), or outright speed, as the speed is unquestionable. The latest storage
    technologies discuss storage tiering in terms of *latency*. The reason is very
    simple—let's say that you're a storage company and that you build a storage system
    that uses 50 SCM SSDs for capacity. For cache, the only reasonable technology
    would be RAM, hundreds of gigabytes of it. The only way you'd be able to work
    with storage tiering on a device like that is by basically *emulating* it in software,
    by creating additional technologies that will produce tiering-like services based
    on queueing, handling priority in cache (RAM), and similar concepts. Why? Because
    if you're using the same SCM SSDs for capacity, and they offer the same speed
    and I/O, you just don't have a way of tiering based on technology or capability.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术从根本上改变了我们对存储的看法。基本上，我们不再讨论基于技术（SSD与SAS与SATA）或纯粹速度的存储分层，因为速度是不容置疑的。最新的存储技术讨论存储分层是基于*延迟*。原因非常简单——假设你是一个存储公司，你建立了一个使用50个SCM
    SSD作为容量的存储系统。对于缓存，唯一合理的技术将是RAM，数百GB的RAM。你能够在这样的设备上使用存储分层的唯一方法就是通过在软件中*模拟*它，通过创建额外的技术来产生基于排队、处理缓存（RAM）中的优先级和类似概念的分层式服务。为什么？因为如果你使用相同的SCM
    SSD作为容量，并且它们提供相同的速度和I/O，你就无法基于技术或能力进行分层。
- en: 'Let''s further describe this by using an available storage system to explain.
    The best device to make our point is Dell/EMC''s PowerMax series of storage devices.
    If you load them with NVMe and SCM SSDs, the biggest model (8000) can scale to
    15 million IOPS(!), 350 GB/s throughput at lower than 100 microseconds latency
    and up to 4 PB capacity. Think about those numbers for a second. Then add another
    number—on the frontend, it can have up to 256 FC/FICON/iSCSI ports. Just recently,
    Dell/EMC released new 32 Gbit/s FC modules for it. The smaller PowerMax model
    (2000) can do 7.5 million IOPS, sub-100 microsecond latency, and scale to 1 PB.
    It can also do all of the *usual EMC stuff*—replication, compression, deduplication,
    snapshots, NAS features, and so on. So, this is not just marketing talk; these
    devices are already out there, being used by enterprise customers:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用一个可用的存储系统来进一步解释这一点。最好的设备来阐明我们的观点是戴尔/EMC的PowerMax系列存储设备。如果你用NVMe和SCM SSD装载它们，最大型号（8000）可以扩展到1500万IOPS(!)，350GB/s吞吐量，低于100微秒的延迟，容量高达4PB。想一想这些数字。然后再加上另一个数字——在前端，它可以有高达256个FC/FICON/iSCSI端口。就在最近，戴尔/EMC发布了新的32
    Gbit/s FC模块。较小的PowerMax型号（2000）可以做到750万IOPS，低于100微秒的延迟，并扩展到1PB。它还可以做所有*通常的EMC功能*——复制、压缩、去重、快照、NAS功能等等。所以，这不仅仅是市场宣传；这些设备已经存在，并被企业客户使用：
- en: '![Figure 3.30 – PowerMax 2000 – it seems small, but it packs a lot of punch'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.30 – PowerMax 2000 – 看起来很小，但功能强大'
- en: '](img/B14834_05_30.jpg)'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14834_05_30.jpg)'
- en: Figure 3.30 – PowerMax 2000 – it seems small, but it packs a lot of punch
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.30 – PowerMax 2000 – 看起来很小，但功能强大
- en: These are very important concepts for the future, as more and more manufacturers
    produce similar devices (and they are on the way). We fully expect the KVM-based
    world to embrace these concepts in large-scale environments, especially for infrastructures
    with OpenStack and OpenShift.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 这些对于未来非常重要，因为越来越多的制造商生产类似的设备（它们正在途中）。我们完全期待基于KVM的世界在大规模环境中采用这些概念，特别是对于具有OpenStack和OpenShift基础设施的情况。
- en: Summary
  id: totrans-472
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced and configured various Open Source storage concepts
    for libvirt. We also discussed industry-standard approaches, such as iSCSI and
    NFS, as they are often used in infrastructures that are not based on KVM. For
    example, VMware vSphere-based environments can use FC, iSCSI, and NFS, while Microsoft-based
    environments can only use FC and iSCSI, from the list of subjects we covered in
    this chapter.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍并配置了libvirt的各种开源存储概念。我们还讨论了行业标准的方法，比如iSCSI和NFS，因为它们经常在不基于KVM的基础设施中使用。例如，基于VMware
    vSphere的环境可以使用FC、iSCSI和NFS，而基于Microsoft的环境只能使用FC和iSCSI，从我们在本章中涵盖的主题列表中选择。
- en: The next chapter will cover subjects related to virtual display devices and
    protocols. We'll provide an in-depth introduction to VNC and SPICE protocols.
    We will also provide a description of other protocols that are used for virtual
    machine connection. All that will help us to understand the complete stack of
    fundamentals that we need to work with our virtual machines, which we covered
    in the past three chapters.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将涵盖与虚拟显示设备和协议相关的主题。我们将深入介绍VNC和SPICE协议。我们还将描述其他用于虚拟机连接的协议。所有这些将帮助我们理解我们在过去三章中涵盖的与虚拟机一起工作所需的完整基础知识栈。
- en: Questions
  id: totrans-475
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What is a storage pool?
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是存储池？
- en: How does NFS storage work with libvirt?
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NFS存储如何与libvirt一起工作？
- en: How does iSCSI work with libvirt?
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: iSCSI如何与libvirt一起工作？
- en: How do we achieve redundancy on storage connections?
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何在存储连接上实现冗余？
- en: What can we use for virtual machine storage except NFS and iSCSI?
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了NFS和iSCSI，我们可以用什么来作为虚拟机存储？
- en: Which storage backend can we use for object-based storage with libvirt?
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用哪种存储后端来进行基于对象的存储与libvirt的连接？
- en: How can we create a virtual disk image to use with a KVM virtual machine?
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何创建一个虚拟磁盘映像以供KVM虚拟机使用？
- en: How does using NVMe SSDs and SCM devices change the way that we create storage
    tiers?
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用NVMe SSD和SCM设备如何改变我们创建存储层的方式？
- en: What are the fundamental problems of delivering tier-0 storage services for
    virtualization, cloud, and HPC environments?
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为虚拟化、云和HPC环境提供零层存储服务的基本问题是什么？
- en: Further reading
  id: totrans-485
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Please refer to the following links for more information regarding what was
    covered in this chapter:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 有关本章涵盖内容的更多信息，请参考以下链接：
- en: 'What''s new with RHEL8 file systems and storage: [https://www.redhat.com/en/blog/whats-new-rhel-8-file-systems-and-storage](https://www.redhat.com/en/blog/whats-new-rhel-8-file-systems-and-storage)'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RHEL8文件系统和存储的新功能：[https://www.redhat.com/en/blog/whats-new-rhel-8-file-systems-and-storage](https://www.redhat.com/en/blog/whats-new-rhel-8-file-systems-and-storage)
- en: 'oVirt storage: [https://www.ovirt.org/documentation/administration_guide/#chap-Storage](https://www.ovirt.org/documentation/administration_guide/#chap-Storage)'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: oVirt存储：[https://www.ovirt.org/documentation/administration_guide/#chap-Storage](https://www.ovirt.org/documentation/administration_guide/#chap-Storage)
- en: 'RHEL 7 storage administration guide: [https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/index](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/index)'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RHEL 7存储管理指南：[https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/index](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/index)
- en: 'RHEL 8 managing storage devices: [https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_storage_devices/index](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_storage_devices/index)'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RHEL 8管理存储设备：[https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_storage_devices/index](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_storage_devices/index)
- en: 'OpenFabrics CCIX, Gen-Z, OpenCAPI (overview and comparison): [https://www.openfabrics.org/images/eventpresos/2017presentations/213_CCIXGen-Z_BBenton.pdf](https://www.openfabrics.org/images/eventpresos/2017presentations/213_CCIXGen-Z_BBenton.pdf)'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenFabrics CCIX，Gen-Z，OpenCAPI（概述和比较）：[https://www.openfabrics.org/images/eventpresos/2017presentations/213_CCIXGen-Z_BBenton.pdf](https://www.openfabrics.org/images/eventpresos/2017presentations/213_CCIXGen-Z_BBenton.pdf)

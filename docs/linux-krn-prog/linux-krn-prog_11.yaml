- en: Kernel Memory Allocation for Module Authors - Part 2
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 模块作者的内核内存分配-第2部分
- en: The previous chapter covered the basics (and a lot more!) on using the available
    APIs for memory allocation via both the page (BSA) and the slab allocators within
    the kernel. In this chapter, we will delve further into this large and interesting
    topic. We cover the creation of custom slab caches, the `vmalloc` interfaces,
    and very importantly, given the wealth of choice, which APIs to use in which situation.
    Internal kernel details regarding the dreaded **Out Of Memory** (**OOM**) killer and
    demand paging help round off these important topics.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章详细介绍了通过内核中的页面（BSA）和slab分配器进行内存分配的可用API的基础知识（以及更多！）。在本章中，我们将进一步深入探讨这个广泛而有趣的主题。我们将涵盖创建自定义slab缓存、`vmalloc`接口，以及非常重要的是，鉴于选择的丰富性，应在哪种情况下使用哪些API。关于令人恐惧的**内存不足**（**OOM**）杀手和需求分页的内部内核细节有助于完善这些重要主题。
- en: These areas tend to be one of the key aspects to understand when working with
    kernel modules, especially device drivers. A Linux system project's sudden crash
    with merely a `Killed` message on the console requires some explanation, yes!?
    The OOM killer's the sweet chap behind this...
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这些领域往往是在处理内核模块时理解的关键方面之一，特别是设备驱动程序。一个Linux系统项目突然崩溃，控制台上只有一个`Killed`消息，需要一些解释，对吧！？OOM杀手就是背后的甜蜜家伙...
- en: 'Briefly, within this chapter, these are the main areas covered:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，在本章中，主要涵盖了以下主要领域：
- en: Creating a custom slab cache
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建自定义slab缓存
- en: Debugging at the slab layer
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在slab层进行调试
- en: Understanding and using the kernel vmalloc() API
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解和使用内核vmalloc()API
- en: Memory allocation in the kernel – which APIs to use when
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核中的内存分配-何时使用哪些API
- en: Stayin' alive - the OOM killer
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持存活- OOM杀手
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: I assume that you have gone through [Chapter 1](ad75db43-a1a2-4f3f-92c7-a544f47baa23.xhtml),
    *Kernel Workspace Setup*, and have appropriately prepared a guest VM running Ubuntu
    18.04 LTS (or a later stable release) and installed all the required packages.
    If not, I highly recommend you do this first.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设您已经阅读了[第1章](ad75db43-a1a2-4f3f-92c7-a544f47baa23.xhtml)，*内核工作空间设置*，并已经适当地准备了一个运行Ubuntu
    18.04 LTS（或更高稳定版本）的虚拟机，并安装了所有必需的软件包。如果没有，我强烈建议您首先这样做。
- en: Also, the last section of this chapter has you deliberately run a *very *memory-intensive
    app; so intensive that the kernel will take some drastic action! Obviously, I
    highly recommend you try out stuff like this on a safe, isolated system, preferably a
    Linux test VM (with no important data on it).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本章的最后一节让您故意运行一个*非常*占用内存的应用程序；如此占用内存以至于内核将采取一些极端的行动！显然，我强烈建议您在一个安全的、隔离的系统上尝试这样的东西，最好是一个Linux测试虚拟机（上面没有重要数据）。
- en: To get the most out of this book, I strongly recommend you first set up the
    workspace
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用本书，我强烈建议您首先设置工作空间
- en: environment, including cloning this book's GitHub repository for the code, and
    work on it in a hands-on fashion. The GitHub  repository can be found at [https://github.com/PacktPublishing/Linux-Kernel-Programming](https://github.com/PacktPublishing/Linux-Kernel-Programming).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 环境，包括克隆本书的GitHub存储库以获取代码，并以实际操作的方式进行工作。GitHub存储库可以在[https://github.com/PacktPublishing/Linux-Kernel-Programming](https://github.com/PacktPublishing/Linux-Kernel-Programming)找到。
- en: Creating a custom slab cache
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建自定义slab缓存
- en: 'As explained in detail in the previous chapter, a key design concept behind
    slab caches is the powerful idea of object caching. By caching frequently used
    objects – data structures, really – performance receives a boost. So, think about
    this: what if we''re writing a driver, and in that driver, a certain data structure
    (an object) is very frequently allocated and freed? Normally, we would use the
    usual `kzalloc()` (or `kmalloc()`) followed by the `kfree()` APIs to allocate
    and free this object. The good news, though: the Linux kernel sufficiently exposes the
    slab layer API to us as module authors, allowing us to create *our own custom
    slab caches*. In this section, you''ll learn how you can leverage this powerful
    feature.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章节中详细解释的，slab缓存背后的关键设计概念是对象缓存的强大理念。通过缓存频繁使用的对象-实际上是数据结构-性能得到提升。因此，想象一下：如果我们正在编写一个驱动程序，在该驱动程序中，某个数据结构（对象）被非常频繁地分配和释放？通常，我们会使用通常的`kzalloc()`（或`kmalloc()`）然后是`kfree()`API来分配和释放这个对象。不过好消息是：Linux内核充分地向我们模块作者公开了slab层API，允许我们创建*我们自己的自定义slab缓存*。在本节中，您将学习如何利用这一强大功能。
- en: Creating and using a custom slab cache within a kernel module
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在内核模块中创建和使用自定义slab缓存
- en: 'In this section, we''re about to create, use, and subsequently destroy a custom
    slab cache. At a broad level, we''ll be performing the following steps:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建，使用和随后销毁自定义slab缓存。在广义上，我们将执行以下步骤：
- en: Creating a custom slab cache of a given size with the `kmem_cache_create()` API.
    This is often done as part of the init code path of the kernel module (or within the
    probe method when in a driver).
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`kmem_cache_create()`API创建给定大小的自定义slab缓存。这通常作为内核模块的初始化代码路径的一部分进行（或者在驱动程序中的探测方法中进行）。
- en: 'Using the slab cache. Here we will do the following:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用slab缓存。在这里我们将做以下事情：
- en: Issue the `kmem_cache_alloc()` API to allocate a single instance of the custom
    object(s) within your slab cache.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`kmem_cache_alloc()`API来分配自定义对象的单个实例在您的slab缓存中。
- en: Use the object.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用对象。
- en: Free it back to the cache with the `kmem_cache_free()` API.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`kmem_cache_free()`API将其释放回缓存。
- en: Destroying the custom slab cache when done with `kmem_cache_destroy()`. This
    is often done as part of the cleanup code path of the kernel module (or within the
    remove/detach/disconnect method when in a driver).
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`kmem_cache_destroy()`在完成后销毁自定义slab缓存。这通常作为内核模块的清理代码路径的一部分进行（或者在驱动程序中的删除/分离/断开方法中进行）。
- en: Let's explore each of these APIs in a bit of detail. We start with the creation
    of a custom (slab) cache.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微详细地探讨这些API中的每一个。我们从创建自定义（slab）缓存开始。
- en: Creating a custom slab cache
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建自定义slab缓存
- en: 'First, of course, let''s learn how to create the custom slab cache. The signature
    of the `kmem_cache_create()` kernel API is as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，当然，让我们学习如何创建自定义的slab缓存。`kmem_cache_create()`内核API的签名如下：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first parameter is the name of the cache - as will be revealed by `proc` (and
    hence by other wrapper utilities over `proc`, such as `vmstat(8)`, `slabtop(1)`,
    and so on). It usually matches the name of the data structure or object being
    cached (but does not have to).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是缓存的*名称* - 将由`proc`（因此也由`proc`上的其他包装工具，如`vmstat(8)`，`slabtop(1)`等）显示。它通常与被缓存的数据结构或对象的名称匹配（但不一定要匹配）。
- en: 'The second parameter, `size`, is really the key one – it''s the size in bytes
    for each object within the new cache. Based on this object size (using a best-fit
    algorithm), the kernel''s slab layer constructs a cache of objects. The actual
    size of each object within the cache will be (slightly) larger than what''s requested,
    due to three reasons:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数`size`实际上是关键的参数-它是新缓存中每个对象的字节大小。基于此对象大小（使用最佳适配算法），内核的slab层构造了一个对象缓存。由于三个原因，缓存内每个对象的实际大小将比请求的稍大：
- en: One, we can always provide more, but never less, than the memory requested.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一，我们总是可以提供更多，但绝不会比请求的内存少。
- en: Two, some space for metadata (housekeeping information) is required.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二，需要一些用于元数据（管理信息）的空间。
- en: Three, the kernel is limited in being able to provide a cache of the exact size
    required. It uses the memory of the closest possible matching size (recall from
    [Chapter 8](e78245d1-5a99-4b9e-a98c-cb16b15f3bee.xhtml), *Kernel Memory Allocation
    for Module Authors – Part 1*, in the *Caveats when using the slab allocator* section,
    where we clearly saw that more (sometimes a lot!) memory could actually be used).
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三，内核在能够提供所需确切大小的缓存方面存在限制。它使用最接近的可能匹配大小的内存（回想一下[第8章](e78245d1-5a99-4b9e-a98c-cb16b15f3bee.xhtml)，*模块作者的内核内存分配-第1部分*，在*使用slab分配器时的注意事项*部分，我们清楚地看到实际上可能使用更多（有时是很多！）内存）。
- en: 'Recall from [Chapter 8](e78245d1-5a99-4b9e-a98c-cb16b15f3bee.xhtml), *Kernel
    Memory Allocation for Module Authors – Part 1*, that the `ksize()` API can be
    used to query the actual size of the allocated object. There is another API with
    which we can query the size of the individual objects within the new slab cache:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第8章](e78245d1-5a99-4b9e-a98c-cb16b15f3bee.xhtml)，*模块作者的内核内存分配-第1部分*，`ksize()`API可用于查询分配对象的实际大小。还有另一个API，我们可以查询新slab缓存中个别对象的大小：
- en: '`unsigned int kmem_cache_size(struct kmem_cache *s);`. You shall see this being
    used shortly.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`unsigned int kmem_cache_size(struct kmem_cache *s);`。您很快将看到这个被使用。'
- en: The third parameter, `align`, is the *alignment* required for the objects within
    the cache. If unimportant, just pass it as `0`. Quite often though, there are
    very particular alignment requirements, for example, ensuring that the object
    is aligned to the size of a word on the machine (32 or 64 bits). To do so, pass
    the value as `sizeof(long)` (the unit for this parameter is bytes, not bits).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个参数`align`是缓存内对象所需的*对齐*。如果不重要，只需将其传递为`0`。然而，通常有非常特定的对齐要求，例如，确保对象对齐到机器上的字大小（32位或64位）。为此，将值传递为`sizeof(long)`（此参数的单位是字节，而不是位）。
- en: 'The fourth parameter, `flags`, can either be `0` (implying no special behavior),
    or the bitwise-OR operator of the following flag values. For clarity, we directly
    reproduce the information on the following flags from the comments within the
    source file, `mm/slab_common.c`:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个参数`flags`可以是`0`（表示没有特殊行为），也可以是以下标志值的按位或运算符。为了清晰起见，我们直接从源文件`mm/slab_common.c`的注释中复制以下标志的信息：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let''s quickly check the flags out:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速检查一下标志：
- en: The first of the flags, `SLAB_POISON`, provides slab poisoning, that is, initializing
    the cache memory to a previously known value (`0xa5a5a5a5`). Doing this can help
    during debug situations.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个标志`SLAB_POISON`提供了slab毒化，即将缓存内存初始化为先前已知的值（`0xa5a5a5a5`）。这样做可以在调试情况下有所帮助。
- en: The second flag, `SLAB_RED_ZONE`, is interesting, inserting red zones (analogous
    to guard pages) around the allocated buffer. This is a common way of checking
    for buffer overflow errors. It's almost always used in a debug context (typically
    during development).
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个标志`SLAB_RED_ZONE`很有趣，它在分配的缓冲区周围插入红色区域（类似于保护页面）。这是检查缓冲区溢出错误的常见方法。它几乎总是在调试环境中使用（通常在开发过程中）。
- en: The third possible flag, `SLAB_HWCACHE_ALIGN`, is very commonly used and is
    in fact recommended for performance. It guarantees that all the cache objects
    are aligned to the hardware (CPU) cacheline size. This is precisely how the memory
    allocated via the popular `k[m|z]alloc()` APIs are aligned to the hardware (CPU)
    cacheline.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个可能的标志`SLAB_HWCACHE_ALIGN`非常常用，实际上也是性能推荐的。它保证所有缓存对象都对齐到硬件（CPU）缓存行大小。这正是通过流行的`k[m|z]alloc()`API分配的内存如何对齐到硬件（CPU）缓存行的。
- en: 'Finally, the fifth parameter to `kmem_cache_create()` is very interesting too:
    a function pointer, `void (*ctor)(void *);`. It is modeled as a constructor function (as
    in object orientation and OOP languages). It conveniently allows you to initialize
    the slab object from the custom slab cache the moment it''s allocated! As one
    example of this feature in action within the kernel, see the code of the **Linux
    Security Module** (**LSM**) called `integrity `here:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`kmem_cache_create()`的第五个参数也非常有趣：一个函数指针，`void (*ctor)(void *);`。它被建模为一个*构造函数*（就像面向对象和OOP语言中的构造函数）。它方便地允许您在分配时从自定义slab缓存初始化slab对象！作为内核中此功能的一个示例，请参阅名为`integrity`的**Linux安全模块**（**LSM**）的代码：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'It invokes the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 它调用以下内容：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `init_once()` function initializes the cached object instance (that was
    just allocated). Remember, the constructor function is called whenever new pages
    are allocated by this cache.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`init_once()`函数初始化了刚刚分配的缓存对象实例。请记住，构造函数在此缓存分配新页面时被调用。'
- en: 'Though it may seem counter-intuitive, the fact is that the modern Linux kernel
    is quite object-oriented in design terms. The code, of course, is mostly plain
    old C, a traditional procedural language. Nevertheless, a vast number of architecture
    implementations within the kernel (the driver model being a big one) are quite
    object-oriented in design: method dispatch via virtual function pointer tables
    - the strategy design pattern, and so on. See a two-part article on LWN depicting
    this in some detail here: *Object-oriented design patterns in the kernel, part
    1, June 2011* ([https://lwn.net/Articles/444910/](https://lwn.net/Articles/444910/)).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这似乎有些违直觉，但事实是现代Linux内核在设计方面相当面向对象。当然，代码大多是传统的过程式语言C。然而，在内核中有大量的架构实现（驱动程序模型是其中之一）在设计上是面向对象的：通过虚拟函数指针表进行方法分派
    - 策略设计模式等。在LWN上有一篇关于此的两部分文章，详细介绍了这一点：*内核中的面向对象设计模式，第1部分，2011年6月*（[https://lwn.net/Articles/444910/](https://lwn.net/Articles/444910/)）。
- en: The return value from the `kmem_cache_create()` API is a pointer to the newly
    created custom slab cache on success, and `NULL` on failure. This pointer is usually
    kept global, as you will require access to it in order to actually allocate objects
    from it (our next step).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`kmem_cache_create()` API的返回值在成功时是指向新创建的自定义slab缓存的指针，失败时是`NULL`。通常会将此指针保持为全局，因为您将需要访问它以实际从中分配对象（我们的下一步）。'
- en: It's important to understand that the `kmem_cache_create()` API can only be
    called from process context. A fair bit of kernel code (including many drivers)
    create and use their own custom slab caches. For example, in the 5.4.0 Linux kernel,
    there are over 350 instances of this API being invoked.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解`kmem_cache_create()` API只能从进程上下文中调用。许多内核代码（包括许多驱动程序）创建并使用自己的自定义slab缓存。例如，在5.4.0
    Linux内核中，有超过350个实例调用了此API。
- en: All right, now that you have a custom (slab) cache available, how exactly do
    you use it to allocate memory objects? Read on; the next section covers precisely
    this.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，现在您有了一个自定义（slab）缓存，那么您究竟如何使用它来分配内存对象呢？接下来的部分将详细介绍这一点。
- en: Using the new slab cache's memory
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用新的slab缓存的内存
- en: 'So, okay, we created a custom slab cache. To make use of it, you must issue
    the `kmem_cache_alloc()` API. Its job: given the pointer to a slab cache (which
    you just created), it allocates a single instance of an object on that slab cache
    (in fact, this is really how the `k[m|z]alloc()` APIs work under the hood). Its
    signature is as follows (of course, remember to always include the `<linux/slab.h>` header
    for all slab-based APIs):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们创建了一个自定义的slab缓存。要使用它，您必须发出`kmem_cache_alloc()` API。它的作用是：给定一个slab缓存的指针（您刚刚创建的），它在该slab缓存上分配一个对象的单个实例（实际上，这确实是`k[m|z]alloc()`
    API在底层是如何工作的）。它的签名如下（当然，记得始终为所有基于slab的API包含`<linux/slab.h>`头文件）：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s look at its parameters:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它的参数：
- en: The first parameter to `kmem_cache_alloc()` is the pointer to the (custom) cache
    that we created in the previous step (the pointer being the return value from
    the `kmem_cache_create()`API).
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kmem_cache_alloc()`的第一个参数是指向我们在上一步中创建的（自定义）缓存的指针（从`kmem_cache_create()` API的返回值）。'
- en: 'The second parameter is the usual GFP flags to pass along (remember the essential
    rule: use `GFP_KERNEL` for normal process-context allocations, else `GFP_ATOMIC` if
    in any kind of atomic or interrupt context).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个参数是要传递的通常的GFP标志（记住基本规则：对于正常的进程上下文分配，请使用`GFP_KERNEL`，否则如果处于任何类型的原子或中断上下文中，请使用`GFP_ATOMIC`）。
- en: As with the now-familiar `k[m|z]alloc()` APIs, the return value is a pointer
    to the newly allocated memory chunk – a kernel logical address (it's a KVA of
    course).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 与现在熟悉的`k[m|z]alloc()` API一样，返回值是指向新分配的内存块的指针 - 内核逻辑地址（当然是KVA）。
- en: 'Use the newly allocated memory object, and when done, do not forget to free
    it with the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用新分配的内存对象，并在完成后，不要忘记使用以下方法释放它：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, take note of the following with respect to the `kmem_cache_free()` API:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，关于`kmem_cache_free()` API，请注意以下内容：
- en: The first parameter to `kmem_cache_free()` is, again, the pointer to the (custom)
    slab cache that you created in the previous step (the return value from `kmem_cache_create()`).
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kmem_cache_free()`的第一个参数再次是指向您在上一步中创建的（自定义）slab缓存的指针（从`kmem_cache_create()`的返回值）。'
- en: The second parameter is the pointer to the memory object you wish to free – the
    object instance that you were just allocated with `kmem_cache_alloc()` – and thus
    have it return to the cache specified by the first parameter!
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个参数是指向您希望释放的内存对象的指针 - 刚刚使用`kmem_cache_alloc()`分配的对象实例 - 因此，它将返回到由第一个参数指定的缓存！
- en: Similar to the `k[z]free()` APIs, there is no return value.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与`k[z]free()` API类似，没有返回值。
- en: Destroying the custom cache
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 销毁自定义缓存
- en: 'When completely done (often in the cleanup or exit code path of the kernel
    module, or your driver''s `remove` method), you must destroy the custom slab cache
    that you created earlier using the following line:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当完全完成时（通常在内核模块的清理或退出代码路径中，或者您的驱动程序的`remove`方法中），您必须销毁先前创建的自定义slab缓存，使用以下行：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The parameter, of course, is the pointer to the (custom) cache that you created
    in the previous step (the return value from the `kmem_cache_create()` API).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 参数当然是指向您在上一步中创建的（自定义）缓存的指针（从`kmem_cache_create()` API的返回值）。
- en: Now that you have understood the procedure and its related APIs, let's get hands
    on with a kernel module that creates its own custom slab cache, uses it, and then
    destroys it.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了该过程及其相关的API，让我们来使用一个创建自己的自定义slab缓存的内核模块，并在完成后销毁它。
- en: Custom slab – a demo kernel module
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义slab - 演示内核模块
- en: Time to get our hands dirty with some code! Let's look at a simple demonstration
    of using the preceding APIs to create our very own custom slab cache. As usual,
    we show only relevant code here. I urge you to clone the book's GitHub repository
    and try it out yourself! You can find the code for this file at `ch9/slab_custom/slab_custom.c`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候动手写一些代码了！让我们看一个简单的演示，使用前面的API来创建我们自己的自定义slab缓存。像往常一样，我们这里只显示相关的代码。我建议您克隆本书的GitHub存储库并自己尝试一下！您可以在`ch9/slab_custom/slab_custom.c`中找到此文件的代码。
- en: 'In our init code path, we first call the following function to create our custom
    slab cache:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的初始化代码路径中，我们首先调用以下函数来创建我们的自定义slab缓存：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In the preceding code, we declare a (global) pointer (`gctx_cachep`) to the
    to-be-created custom slab cache – which will hold objects; namely, our fictional
    often allocated data structure, `myctx`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们声明了一个（全局）指针（`gctx_cachep`）指向即将创建的自定义slab缓存 - 它将保存对象；即我们虚构的经常分配的数据结构`myctx`。
- en: 'In the following, see the code that creates the custom slab cache:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，看看创建自定义slab缓存的代码：
- en: '[PRE8]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Hey, that''s interesting: notice that our cache creation API supplies a constructor
    function to help initialize any newly allocated object; here it is:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 嘿，这很有趣：注意我们的缓存创建API提供了一个构造函数来帮助初始化任何新分配的对象；在这里：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The comments in the preceding code are self-explanatory; do take a look. The
    constructor routine, if set up (depending on the value of our `use_ctor` module
    parameter; it's `1` by default), will be auto-invoked by the kernel whenever a
    new memory object is allocated to our cache.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码中的注释是不言自明的；请仔细查看。构造函数例程，如果设置（取决于我们`use_ctor`模块参数的值；默认为`1`），将在内核每当为我们的缓存分配新内存对象时自动调用。
- en: 'Within the init code path, we call a `use_our_cache()` function. It allocates
    an instance of our `myctx` object via the `kmem_cache_alloc()` API, and if our
    custom constructor routine is enabled, it runs, initializing the object. We then
    dump its memory to show that it was indeed initialized as coded, freeing it when
    done (for brevity, we''ll leave out showing the error code paths):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化代码路径中，我们调用`use_our_cache()`函数。它通过`kmem_cache_alloc()`API分配了我们的`myctx`对象的一个实例，如果我们的自定义构造函数例程已启用，它会运行，初始化对象。然后我们将其内存转储以显示它确实按照编码进行了初始化，并在完成时释放它（为简洁起见，我们将不显示错误代码路径）：
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, in the exit code path, we destroy our custom slab cache:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在退出代码路径中，我们销毁我们的自定义slab缓存：
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following output from a sample run helps us understand how it works. The
    following is just a partial screenshot showing the output on our x86_64 Ubuntu
    18.04 LTS guest running the Linux 5.4 kernel:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 来自一个样本运行的以下输出帮助我们理解它是如何工作的。以下只是部分截图，显示了我们的x86_64 Ubuntu 18.04 LTS客户机上运行Linux
    5.4内核的输出：
- en: '![](img/2a679dea-0e4b-4978-9faf-e3dd880fa594.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a679dea-0e4b-4978-9faf-e3dd880fa594.png)'
- en: Figure 9.1 – Output of our slab_custom kernel module on an x86_64 VM
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 - 在x86_64 VM上的slab_custom内核模块的输出
- en: 'Great! Hang on though, a couple of key points to take note of here:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！等一下，这里有几个要注意的关键点：
- en: As our constructor routine is enabled by default (the value of our `use_ctor`
    module parameter is `1`), it runs whenever a new object instance is allocated
    by the kernel slab layer to our new cache. Here, we performed just a single `kmem_cache_alloc()`,
    yet our constructor routine has run 21 times, implying that the kernel's slab
    code (pre)allocated 21 objects to our brand new cache! Of course, this number
    varies.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们的构造函数例程默认启用（我们的`use_ctor`模块参数的值为`1`），每当内核slab层为我们的新缓存分配新对象实例时，它都会运行。在这里，我们只执行了一个`kmem_cache_alloc()`，但我们的构造函数例程已经运行了21次，这意味着内核的slab代码（预）分配了21个对象给我们的全新缓存！当然，这个数字会有所变化。
- en: 'Two, something very important to notice! As seen in the preceding screenshot,
    the *size* of each object is seemingly 328 bytes (as shown by all these three
    APIs: `sizeof()`, `kmem_cache_size()`, and `ksize()`). However, again, this is
    not really true! The actual size of the object as allocated by the kernel is larger; we
    can see this via `vmstat(8)`:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二，非常重要的一点要注意！如前面的截图所示，每个对象的*大小*似乎是328字节（由`sizeof()`、`kmem_cache_size()`和`ksize()`显示）。然而，再次强调，这并不是真的！内核分配的对象的实际大小更大；我们可以通过`vmstat(8)`看到这一点。
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As highlighted in the preceding code, the actual size of each allocated object
    is not 328 bytes but 768 bytes (the exact number varies; in one case I saw it
    as 448 bytes).Just as we saw earlier, this is important for you to realize, and
    indeed check for. We show another way to quite easily check this in the *Debugging
    at the slab layer *section that follows.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的那样，每个分配的对象的实际大小不是328字节，而是768字节（确切的数字会有所变化；在一个案例中，我看到它是448字节）。这对您来说是很重要的，确实需要检查。我们在接下来的*在slab层调试*部分中展示了另一种相当容易检查这一点的方法。
- en: FYI, you can always check out the man page of `vmstat(8)` for the precise meaning
    of each column seen earlier.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: FYI，您可以随时查看`vmstat(8)`的man页面，以了解先前看到的每一列的确切含义。
- en: We'll round off the discussion on creating and using custom slab caches with
    the slab shrinker interface.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用slab收缩器接口结束关于创建和使用自定义slab缓存的讨论。
- en: Understanding slab shrinkers
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解slab收缩器
- en: Caches are good for performance. Visualize reading the content of a large file
    from disk as opposed to reading its content from RAM. There's no question that
    the RAM-based I/O is much faster! As can be imagined, the Linux kernel leverages
    these ideas and thus maintains several caches – the page cache, dentry cache,
    inode cache, slab caches, and so on. These caches indeed greatly help performance,
    but, thinking about it, are not actually a mandatory requirement. When memory
    pressure reaches high levels (implying that too much memory is in use and too
    little is free), the Linux kernel has mechanisms to intelligently free up caches
    (aka memory reclamation - it's a continuous ongoing process; kernel threads (typically
    named `kswapd*`) reclaim memory as part of their housekeeping chores; more on
    this in the *Reclaiming memory – a kernel housekeeping task and* *OOM* section).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存对性能有利。想象一下从磁盘读取大文件的内容与从RAM读取其内容的情况。毫无疑问，基于RAM的I/O要快得多！可以想象，Linux内核利用这些想法，因此维护了几个缓存-页面缓存、目录项缓存、索引节点缓存、slab缓存等等。这些缓存确实极大地提高了性能，但是，仔细想想，实际上并不是强制性要求。当内存压力达到较高水平时（意味着使用的内存过多，可用内存过少），Linux内核有机制智能地释放缓存（也称为内存回收-这是一个持续进行的过程；内核线程（通常命名为`kswapd*`）作为其管理任务的一部分回收内存；在*回收内存-内核管理任务和*OOM*部分中会更多地介绍）。
- en: In the case of the slab cache(s), the fact is that some kernel subsystems and
    drivers create their own custom slab caches as we covered earlier in this chapter.
    For the purpose of integrating well and cooperating with the kernel, best practice
    demands that your custom slab cache code is expected to register a shrinker interface.
    When this is done, and when memory pressure gets high enough, the kernel might
    well invoke several slab shrinker callbacks, which are expected to ease the memory
    pressure by freeing up (shrinking) slab objects.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在slab缓存的情况下，事实上是一些内核子系统和驱动程序会像我们在本章前面讨论的那样创建自己的自定义slab缓存。为了与内核良好集成并合作，最佳实践要求您的自定义slab缓存代码应该注册一个shrinker接口。当这样做时，当内存压力足够高时，内核可能会调用多个slab收缩器回调，预期通过释放（收缩）slab对象来缓解内存压力。
- en: 'The API to register a shrinker function with the kernel is the `register_shrinker()` API. The
    single parameter to it (as of Linux 5.4) is a pointer to a `shrinker` structure.
    This structure contains (among other housekeeping members) two callback routines:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 与内核注册shrinker函数的API是`register_shrinker()`API。它的单个参数（截至Linux 5.4）是指向`shrinker`结构的指针。该结构包含（除其他管理成员外）两个回调例程：
- en: The first routine, `count_objects()`, merely counts and returns the number of
    objects that would be freed (when it is actually invoked). If it returns `0`,
    this implies that the number of freeable memory objects cannot be determined now,
    or that we should not even attempt to free any right now.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个例程`count_objects()`仅计算并返回将要释放的对象的数量（当实际调用时）。如果返回`0`，这意味着现在无法确定可释放的内存对象的数量，或者我们现在甚至不应该尝试释放任何对象。
- en: The second routine, `scan_objects()`, is invoked only if the first callback
    routine returns a non-zero value; it's the one that, when invoked by the slab
    cache layer, actually frees up, or shrinks, the slab cache in question. It returns
    the actual number of objects freed up in this reclaim cycle, or `SHRINK_STOP` if
    the reclaim attempt could not progress (due to possible deadlocks).
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个例程`scan_objects()`仅在第一个回调例程返回非零值时调用；当slab缓存层调用它时，它实际上释放或收缩了相关的slab缓存。它返回在此回收周期中实际释放的对象数量，或者如果回收尝试无法进行（可能会导致死锁）则返回`SHRINK_STOP`。
- en: We'll now wrap up the discussion on the slab layer with a quick summation of
    the pros and cons of using this layer for memory (de)allocation—very important
    for you as a kernel/driver author to be keenly aware of!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将通过快速总结使用此层进行内存（解）分配的利弊来结束对slab层的讨论-对于您作为内核/驱动程序作者来说，这是非常重要的，需要敏锐意识到！
- en: The slab allocator – pros and cons – a summation
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: slab分配器-利弊-总结
- en: In this section, we very briefly summarize things you have already learned by
    now. This is intended as a way for you to quickly look up and recollect these
    key points!
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们非常简要地总结了您现在已经学到的内容。这旨在让您快速查阅和回顾这些关键要点！
- en: 'The pros of using the slab allocator (or slab cache) APIs to allocate and free
    kernel memory are as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用slab分配器（或slab缓存）API来分配和释放内核内存的优点如下：
- en: (Very) fast (as it uses pre-cached memory objects).
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （非常）快速（因为它使用预缓存的内存对象）。
- en: A physically contiguous memory chunk is guaranteed.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保证物理上连续的内存块。
- en: Hardware (CPU) cacheline-aligned memory is guaranteed when the `SLAB_HWCACHE_ALIGN` flag
    is used when creating the cache. This is the case for `kmalloc()`, `kzalloc()`,
    and so on.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当创建缓存时使用`SLAB_HWCACHE_ALIGN`标志时，保证硬件（CPU）缓存行对齐的内存。这适用于`kmalloc()`、`kzalloc()`等。
- en: You can create your own custom slab cache for particular (frequently alloc-ed/freed)
    objects.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以为特定（频繁分配/释放）对象创建自定义的slab缓存。
- en: 'The cons of using the slab allocator (or slab cache) APIs are the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用slab分配器（或slab缓存）API的缺点如下：
- en: A limited amount of memory can be allocated at a time; typically, just 8 KB
    directly via the slab interfaces, or up to 4 MB indirectly via the page allocator
    on most current platforms (of course, the precise upper limits are arch-dependent).
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一次只能分配有限数量的内存；通常，通过slab接口直接分配8 KB，或者通过大多数当前平台上的页面分配器间接分配高达4 MB的内存（当然，精确的上限取决于架构）。
- en: 'Using the `k[m|z]alloc()` APIs incorrectly: asking for too much memory, or
    asking for a memory size just over a threshold value (discussed in detail in [Chapter
    8](e78245d1-5a99-4b9e-a98c-cb16b15f3bee.xhtml), *Kernel Memory Allocation for
    Module Authors – Part 1*, under the *Size limitations of the kmalloc API* section),
    can certainly lead to internal fragmentation (wastage). It''s designed to only
    really optimize for the common case – for allocations of a size less than one
    page.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`k[m|z]alloc()`API不正确：请求过多的内存，或者请求一个略高于阈值的内存大小（在[第8章](e78245d1-5a99-4b9e-a98c-cb16b15f3bee.xhtml)中详细讨论，*内核内存分配给模块作者-第1部分*，在*kmalloc
    API的大小限制*部分），肯定会导致内部碎片（浪费）。它的设计只是真正优化常见情况-分配小于一页大小的内存。
- en: Now, let's move on to another really key aspect for the kernel/driver developer
    – effectively debugging when things go wrong with respect to memory allocations/freeing,
    particularly within the slab layer.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续讨论另一个对于内核/驱动程序开发人员来说非常关键的方面-当内存分配/释放出现问题时，特别是在slab层内部。
- en: Debugging at the slab layer
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在slab层调试
- en: Memory corruption is unfortunately a very common root cause of bugs. Being able
    to debug them is a key skill. We'll now look at a few ways to go about this. Before
    diving into the details, remember that the following discussion is with respect
    to the *SLUB* (the unqueued allocator) implementation of the slab layer. This
    is the default on most Linux installations (we mentioned in [Chapter 8](e78245d1-5a99-4b9e-a98c-cb16b15f3bee.xhtml),
    *Kernel Memory Allocation for Module Authors – Part 1*, under the *Slab layer
    implementations within the kernel* section, that current Linux kernels have three
    mutually exclusive implementations of the slab layer).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 内存损坏不幸地是错误的一个非常常见的根本原因。能够调试它们是一个关键的技能。我们现在将看一下一些处理这个问题的方法。在深入细节之前，请记住，以下讨论是关于*SLUB*（未排队的分配器）实现的slab层。这是大多数Linux安装的默认设置（我们在[第8章](e78245d1-5a99-4b9e-a98c-cb16b15f3bee.xhtml)中提到，内核内存分配给模块作者-第1部分，*内核内存分配给模块作者-第1部分*，在*内核中的slab层实现*部分，当前的Linux内核有三个互斥的slab层实现）。
- en: 'Also, our intention here is not to discuss in-depth kernel debug tools with
    respect to memory debugging—that is a large topic by itself that unfortunately
    lies beyond the scope of this book. Nevertheless, I will say that you would do
    well to gain familiarity with the powerful frameworks/tools that have been mentioned,
    particularly the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的意图并不是深入讨论关于内存调试的内核调试工具-这本身就是一个庞大的话题，不幸的是超出了本书的范围。尽管如此，我会说你最好熟悉已经提到的强大框架/工具，特别是以下内容：
- en: '**KASAN** (the **Kernel Address Sanitizer**; available for x86_64 and AArch64,
    4.x kernels onward)'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KASAN**（**内核地址消毒剂**；从x86_64和AArch64，4.x内核开始可用）'
- en: SLUB debug techniques (covered here)
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SLUB调试技术（在这里介绍）
- en: '`kmemleak` (though KASAN is superior)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kmemleak`（尽管KASAN更好）'
- en: '`kmemcheck` (note though that `kmemcheck` was removed in Linux 4.15)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kmemcheck`（请注意，`kmemcheck`在Linux 4.15中被移除）'
- en: Don't forget to look for links to these in the *Further reading *section. Okay,
    let's get down to a few useful ways to help a developer debug code at the slab
    layer.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记在*进一步阅读*部分寻找这些链接。好的，让我们来看看一些有用的方法，帮助开发人员在slab层调试代码。
- en: Debugging through slab poisoning
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过slab毒害调试
- en: 'One very useful feature is so-called slab poisoning. The term *poisoning* in
    this context implies poking memory with certain signature bytes or a pattern that
    is easily recognizable. The prerequisite to using this, though, is that the `CONFIG_SLUB_DEBUG` kernel
    configuration option is on.How can you check? Simple:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常有用的功能是所谓的slab毒害。在这种情况下，“毒害”一词意味着用特定的签名字节或易于识别的模式刺激内存。然而，使用这个的前提是`CONFIG_SLUB_DEBUG`内核配置选项是开启的。你怎么检查？简单：
- en: '[PRE13]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `=y` seen in the preceding code indicates that it''s indeed on. Now (assuming
    it''s turned on) if you create a slab cache with the `SLAB_POISON` flag (we covered
    the creation of a slab cache in the *Creating a custom slab cache* section), then,
    when the memory is allocated, it''s always initialized to the special value or
    memory pattern `0x5a5a5a5a` – it''s poisoned (it''s quite intentional: the hex
    value `0x5a` is the ASCII character `Z` for zero)! So, think about it, if you
    spot this value in a kernel diagnostic message or dump, also called an *Oops,*
    there''s a good chance that this is an (unfortunately pretty typical) uninitialized
    memory bug or **UMR** (short for **Uninitialized Memory Read**), perhaps.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中看到的`=y`表示它确实是开启的。现在（假设它已经开启），如果你使用`SLAB_POISON`标志创建一个slab缓存（我们在*创建自定义slab缓存*部分中介绍了创建slab缓存），那么当内存被分配时，它总是被初始化为特殊值或内存模式`0x5a5a5a5a`-它被毒害了（这是非常有意义的：十六进制值`0x5a`是ASCII字符`Z`代表零）！所以，想一想，如果你在内核诊断消息或转储中看到这个值，也称为*Oops*，那么很有可能这是一个（不幸地相当典型的）未初始化内存错误或**UMR**（未初始化内存读取）。
- en: Why use the word *perhaps* in the preceding sentence? Well, simply because debugging
    deeply hidden bugs is a really difficult thing to do! The symptoms that might
    present themselves are not necessarily *the root cause* of the issue at hand.
    Thus, hapless developers are fairly often led down the proverbial garden path
    by various red herrings! The reality is that debugging is both an art and a science;
    deep knowledge of the ecosystem (here, the Linux kernel) goes a really long way
    in helping you effectively debug difficult situations.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么在前面的句子中使用*也许*这个词？嗯，简单地因为调试深藏的错误是一件非常困难的事情！可能出现的症状并不一定是问题的根本原因。因此，不幸的开发人员经常被各种红鲱引入歧途！现实是调试既是一门艺术又是一门科学；对生态系统（这里是Linux内核）的深入了解在帮助你有效调试困难情况方面起到了很大作用。
- en: If the `SLAB_POISON` flag is unset, uninitialized slab memory is set to the `0x6b6b6b6b` memory
    pattern (hex `0x6b` is ASCII character `k` (see Figure 9.2)). Similarly, when
    the slab cache memory is freed up and `CONFIG_SLUB_DEBUG` is on, the kernel writes
    the same memory pattern (`0x6b6b6b6b ; 'k'`) into it. This can be very useful
    too, allowing us to spot (what the kernel thinks is) uninitialized or free memory.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未设置`SLAB_POISON`标志，则未初始化的slab内存将设置为`0x6b6b6b6b`内存模式（十六进制`0x6b`是ASCII字符`k`（见图9.2））。同样，当slab高速缓存内存被释放并且`CONFIG_SLUB_DEBUG`打开时，内核将相同的内存模式（`0x6b6b6b6b；'k'`）写入其中。这也非常有用，可以让我们发现（内核认为的）未初始化或空闲内存。
- en: 'The poison values are defined in `include/linux/poison.h` as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 毒值在`include/linux/poison.h`中定义如下：
- en: '[PRE14]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'With respect to the kernel''s SLUB implementation of the slab allocator, let''s
    check out a summary view of **how and when** (the specific circumstances are determined
    by the following `if` part) the *slab poisoning occurs,* along with its type in
    the following pseudocode:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 关于内核SLUB实现的slab分配器，让我们来看一下**何时**（具体情况由以下`if`部分确定）以及*slab中毒发生的类型*的摘要视图，以及以下伪代码中的类型：
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then the slab poisoning occurs as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然后毒化slab发生如下：
- en: The slab memory is set to `POISON_INUSE (0x5a = ASCII 'Z')` upon initialization;
    the code for this is here: `mm/slub.c:setup_page_debug()`.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: slab内存在初始化时设置为`POISON_INUSE（0x5a = ASCII 'Z'）`；此代码在此处：`mm/slub.c:setup_page_debug()`。
- en: The slab object is set to `POISON_FREE (0x6b = ASCII 'k')` upon initialization
    in `mm/slub.c:init_object()`.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: slab对象在`mm/slub.c:init_object()`中初始化为`POISON_FREE（0x6b = ASCII 'k'）`。
- en: The slab object's last byte is set to `POISON_END (0xa5)` upon initialization
    in `mm/slub.c:init_object()`.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: slab对象的最后一个字节在`mm/slub.c:init_object()`中初始化为`POISON_END（0xa5）`。
- en: (So, because of the way the slab layer performs these slab memory initializations,
    we end up with the value `0x6b` (ASCII `k`) as the initial value of just-allocated
    slab memory). Notice that for this to work, you shouldn't install a custom constructor
    function. Also, you can ignore the `it's-type-safe-by-RCU` directive for now;
    it's usually the case (that is, the "is type-safe-by-RCU" is true; FYI, RCU (Read
    Copy Update) is an advanced synchronization technology that's beyond this book's
    scope). As can be seen from how slabs are initialized when running in SLUB debug
    mode, the memory content is effectively initialized to the value `POISON_FREE
    (0x6b = ASCII 'k')`. Thus, if this value ever changes after the memory is freed,
    the kernel can detect this and trigger a report (via printk). This, of course,
    is a case of the well-known **Use After Free** (**UAF**) memory bug! Similarly,
    writing before or after the redzone regions (these are in effect guard regions
    and are typically initialized to `0xbb`) will trigger a write buffer under/overflow
    bug, which the kernel reports. Useful!
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: （因此，由于slab层执行这些slab内存初始化的方式，我们最终得到值`0x6b`（ASCII `k`）作为刚分配的slab内存的初始值）。请注意，为了使其工作，您不应安装自定义构造函数。此外，您现在可以忽略`it's-type-safe-by-RCU`指令；通常情况下是这样（即，“is
    type-safe-by-RCU”为真；FYI，RCU（Read Copy Update）是一种高级同步技术，超出了本书的范围）。从在SLUB调试模式下运行时slab的初始化方式可以看出，内存内容实际上被初始化为值`POISON_FREE（0x6b
    = ASCII 'k'）`。因此，如果内存释放后此值发生变化，内核可以检测到并触发报告（通过printk）。当然，这是一个众所周知的**使用后释放**（**UAF**）内存错误的案例！类似地，在红色区域之前或之后写入（这些实际上是保护区域，通常初始化为`0xbb`）将触发写入缓冲区下/溢出错误，内核将报告。有用！
- en: Trying it out – triggering a UAF bug
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 试一下-触发UAF错误
- en: 'To help you understand this better, we''ll show an example via screenshots
    in this section. Implement the following steps:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助您更好地理解这一点，我们将在本节的屏幕截图中展示一个示例。执行以下步骤：
- en: Firstly, ensure you enable the `CONFIG_SLUB_DEBUG` kernel config (it should
    be set to `y`; this is typically the case on distro kernels)
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先确保启用了`CONFIG_SLUB_DEBUG`内核配置（应设置为`y`；这通常是发行版内核的情况）
- en: 'Next, boot the system while including the kernel command-line `slub_debug=`
    directive (this turns on full SLUB debug; or you could pass a finer granularity
    variant such as `slub_debug=FZPU` (see the kernel documentation here for an explanation
    of each field: [https://www.kernel.org/doc/Documentation/vm/slub.txt](https://www.kernel.org/doc/Documentation/vm/slub.txt));
    as a demo, on my Fedora 31 guest VM, I passed the kernel command line as follows
    - the important thing here, the `slub_debug=FZPU` is highlighted in bold font:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在包括内核命令行`slub_debug=`指令的情况下启动系统（这将打开完整的SLUB调试；或者您可以传递更精细的变体，例如`slub_debug=FZPU`（请参阅此处的内核文档以了解每个字段的解释：[https://www.kernel.org/doc/Documentation/vm/slub.txt](https://www.kernel.org/doc/Documentation/vm/slub.txt)）；作为演示，在我的Fedora
    31虚拟机上，我传递了以下内核命令行-这里重要的是，`slub_debug=FZPU`以粗体字体突出显示：
- en: '[PRE16]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: (More detail on the `slub_debug` parameter is in the next section *​SLUB debug
    options at boot and runtime*).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: （有关`slub_debug`参数的更多详细信息，请参阅下一节*​引导和运行时的SLUB调试选项*）。
- en: 'Write a kernel module that creates a new custom slab cache (which of course
    has a memory bug!). Ensure no constructor function is specified (sample code is
    here: `ch9/poison_test`; I''ll leave it as an exercise for you to browse through
    the code and test it).'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个创建新的自定义slab高速缓存的内核模块（当然其中存在内存错误！）。确保未指定构造函数（示例代码在此处：`ch9/poison_test`；我将留给您浏览代码并测试的练习）。
- en: 'We try it out here: allocate some slab memory via `kmem_cache_alloc()` (or
    equivalent). Here''s a screenshot (Figure 9.2) showing the allocated memory, and
    the same region after performing a quick `memset()` setting the first 16 bytes
    to `z` (`0x7a`):'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在这里尝试一下：通过`kmem_cache_alloc()`（或等效方法）分配一些slab内存。下面是一个屏幕截图（图9.2），显示分配的内存，以及在执行快速的`memset()`将前16个字节设置为`z`（`0x7a`）后的相同区域：
- en: '![](img/2a38ad65-0115-41bb-bc1c-8de17c81ead1.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a38ad65-0115-41bb-bc1c-8de17c81ead1.png)'
- en: Figure 9.2 – Slab memory after allocation and memset() of the first 16 bytes
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2-分配和memset()后的slab内存）。
- en: 'Now, for the bug! In the cleanup method, we free the allocated slab and then
    reuse it by attempting to do another `memset()` upon it, *thus triggering the
    UAF bug*. Again, we show the kernel log via another screenshot (Figure 9.3):'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，来说说bug！在清理方法中，我们释放了分配的slab，然后尝试对其进行另一个`memset()`，*从而触发了UAF bug*。同样，我们通过另一张屏幕截图（图9.3）显示内核日志：
- en: '![](img/3a1f3f3a-ecf0-46a2-a5c4-be69803a03b4.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3a1f3f3a-ecf0-46a2-a5c4-be69803a03b4.png)'
- en: Figure 9.3 – The kernel reporting the UAF bug!
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 - 内核报告UAF bug！
- en: 'Notice how the kernel reports this (the first text in red in the preceding
    figure) as a `Poison overwritten` bug. This is indeed the case: we overwrote the
    `0x6b` poison value with `0x21 `(which, quite intentionally is the ASCII character
    `!`). After freeing a buffer that originated from the slab cache, if the kernel
    detects any value other than the poison value (`POISON_FREE = 0x6b = ASCII ''k''`)
    within the payload, it triggers the bug. (Also notice, the redzone - guard - areas
    are initialized to the value `0xbb`).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意内核如何报告这一点（前面图中红色的第一段文字）作为`Poison overwritten` bug。事实上就是这样：我们用`0x21`（故意是ASCII字符`!`）覆盖了`0x6b`毒值。在释放了来自slab缓存的缓冲区后，如果内核在有效负载中检测到毒值之外的任何值（`POISON_FREE
    = 0x6b = ASCII 'k'`），就会触发bug。（还要注意，红区 - 保护区 - 的值初始化为`0xbb`）。
- en: The next section provides a few more details on the SLUB layer debug options
    available.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将提供有关可用的SLUB层调试选项的更多细节。
- en: SLUB debug options at boot and runtime
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引导和运行时的SLUB调试选项
- en: 'Debugging kernel-level slab issues when using the SLUB implementation (the
    default) is very powerful as the kernel has full debugging information available.
    It''s just that it''s turned off by default. There are various ways (viewports)
    via which we can turn on and look at slab debug-level information; a wealth of
    details is available! Some of the ways to do so include the following:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用SLUB实现（默认）时，调试内核级slab问题非常强大，因为内核具有完整的调试信息。只是默认情况下它是关闭的。有各种方式（视口）可以打开和查看slab调试级别的信息；有大量的细节可用！其中一些方法包括以下内容：
- en: Passing the `slub_debug=` string on the kernel command line (via the bootloader
    of course). This turns on full SLUB kernel-level debugging.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在内核命令行上传递`slub_debug=`字符串（当然是通过引导加载程序）。这会打开完整的SLUB内核级调试。
- en: 'The specific debug information to be seen can be fine-tuned via options passed
    to the `slub_debug=` string (passing nothing after the `=` implies that all SLUB
    debug options are enabled); for example, passing `slub_debug=FZ` turns on the
    following options:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要查看的特定调试信息可以通过传递给`slub_debug=`字符串的选项进行微调（在`=`后面不传递任何内容意味着启用所有SLUB调试选项）；例如，传递`slub_debug=FZ`会启用以下选项：
- en: '`F`: Sanity checks on (enables `SLAB_DEBUG_CONSISTENCY_CHECKS`); note that
    turning this on can slow down the system.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`F`: 对齐检查（启用`SLAB_DEBUG_CONSISTENCY_CHECKS`）；请注意，打开此选项可能会减慢系统速度。'
- en: '`Z`: Red zoning.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Z`: 红色分区。'
- en: 'Even if the SLUB debug feature has not been turned on via the kernel command
    line, we can still enable/disable it by writing `1` (as root) to suitable pseudo-files
    under `/sys/kernel/slab/<slab-name>`:'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使没有通过内核命令行打开SLUB调试功能，我们仍然可以通过在`/sys/kernel/slab/<slab-name>`下的适当伪文件中写入`1`（作为root用户）来启用/禁用它：
- en: 'Recall our earlier demo kernel module (`ch9/slab_custom`); once loaded into
    the kernel, see the theoretical and actual size of each allocated object like
    this:'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回想一下我们之前的演示内核模块（`ch9/slab_custom`）；一旦加载到内核中，可以像这样查看每个分配对象的理论和实际大小：
- en: '[PRE17]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Several other pseudo-files are present as well; doing `ls(1)` on `/sys/kernel/slab/<name-of-slab>/`
    will reveal them. For example, look up the constructor function to our `ch9/slab_custom` slab
    cache by performing `cat` on the pseudo-file at `/sys/kernel/slab/our_ctx/ctor`:'
  id: totrans-158
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还有其他几个伪文件；在`/sys/kernel/slab/<name-of-slab>/`上执行`ls(1)`将会显示它们。例如，通过在`/sys/kernel/slab/our_ctx/ctor`上执行`cat`来查找到我们的`ch9/slab_custom`
    slab缓存的构造函数：
- en: '[PRE18]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You can find quite some relevant details in this (very useful!) document here: *Short
    users guide for SLUB *([https://www.kernel.org/doc/Documentation/vm/slub.txt](https://www.kernel.org/doc/Documentation/vm/slub.txt)).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里可以找到一些相关的详细信息（非常有用！）：*SLUB的简短用户指南*（[https://www.kernel.org/doc/Documentation/vm/slub.txt](https://www.kernel.org/doc/Documentation/vm/slub.txt)）。
- en: Also, a quick look under the kernel source tree's `tools/vm` folder will reveal
    some interesting programs (`slabinfo.c` being the relevant one here) and a script
    to generate graphs (via `gnuplot(1)`). The document mentioned in the preceding
    paragraph provides usage details on plot generation as well.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，快速查看内核源树的`tools/vm`文件夹将会发现一些有趣的程序（这里相关的是`slabinfo.c`）和一个用于生成图表的脚本（通过`gnuplot(1)`）。前面段落提到的文档提供了有关生成图表的使用细节。
- en: As an important aside, the kernel has an enormous (and useful!) number of *kernel
    parameters* that can be optionally passed to it at boot (via the bootloader).
    See the complete list here in the documentation: *The kernel’s command-line parameters* ([https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html](https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html)).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个重要的附带说明，内核有大量（而且有用！）的*内核参数*可以在引导时（通过引导加载程序）选择性地传递给它。在这里的文档中可以看到完整的列表：*内核的命令行参数*（[https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html](https://www.kernel.org/doc/html/latest/admin-guide/kernel-parameters.html)）。
- en: 'Well, this (finally) concludes our coverage of the slab allocator (from the
    previous chapter continuing into this one). You have learned that it''s layered
    above the page allocator and solves two key things: one, it allows the kernel
    to create and maintain object caches so that the allocation and freeing of some
    important kernel data structures can be performed very efficiently; two, this
    includes generic memory caches allowing you to allocate small amounts of RAM -
    fragments of a page - with very little overhead (unlike the binary buddy system
    allocator). The fact is simply this: the slab APIs are the really commonly employed
    ones by drivers; not only that, modern driver authors exploit the resource-managed
    `devm_k{m,z}alloc()` APIs; we encourage you to do so. Be careful though: we examined
    in detail how more memory than you think might actually be allocated (use `ksize()`
    to figure out just how much). You also learned how to create a custom slab cache,
    and, importantly, how to go about debugging at the slab layer.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，这（终于）结束了我们对slab分配器的覆盖（从上一章延续到这一章）。您已经了解到它是在页面分配器之上的一层，解决了两个关键问题：一是允许内核创建和维护对象缓存，以便非常高效地执行一些重要的内核数据结构的分配和释放；二是包括通用内存缓存，允许您以非常低的开销（与二进制伙伴系统分配器不同）分配小量的RAM——页面的片段。事实就是这样：slab
    API是驱动程序中真正常用的API；不仅如此，现代驱动程序作者还利用了资源管理的`devm_k{m,z}alloc()` API；我们鼓励您这样做。不过要小心：我们详细讨论了实际分配的内存可能比您想象的要多（使用`ksize()`来找出实际分配了多少）。您还学会了如何创建自定义的slab缓存，以及如何进行slab层的调试。
- en: Now let's learn what the `vmalloc()` API is, how and when to use it for kernel
    memory allocation.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们学习`vmalloc()` API是什么，如何以及何时用于内核内存分配。
- en: Understanding and using the kernel vmalloc() API
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解并使用内核vmalloc() API
- en: As we have learned in the previous chapter, ultimately there is just one engine
    for memory allocation within the kernel – the page (or buddy system) allocator.
    Layered on top is the slab allocator (or slab cache) machinery. In addition, there
    is another completely virtual address space within the kernel's address space
    from where virtual pages can be allocated at will – this is called the kernel `vmalloc`
    region.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们已经学到，内核内存分配的最终引擎只有一个——页面（或伙伴系统）分配器。在其上层是slab分配器（或slab缓存）机制。此外，内核地址空间中还有另一个完全虚拟的地址空间，可以随意分配虚拟页面，这就是所谓的内核`vmalloc`区域。
- en: Of course, ultimately, once a virtual page is actually used (by something in
    the kernel or in user space via a process or thread) - it's physical page frame
    that it's mapped to is really allocated via the page allocator (this is ultimately
    true of all user space memory frames as well, though in an indirect fashion; more
    on this later in the *Demand paging and OOM* section).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 当虚拟页面实际被使用时（由内核中的某个东西或通过进程或线程的用户空间使用），它实际上是通过页面分配器分配的物理页面帧（这对所有用户空间内存帧也是最终真实的，尽管是间接的方式；这一点我们稍后在*需求分页和OOM*部分会详细介绍）。
- en: Within the kernel segment or VAS (we covered all this in some detail in [Chapter
    7](06ee05b5-3e71-482d-93b8-235c27ce23bc.xhtml), *Memory Management Internals -
    Essentials,* under the *Examining the kernel segment* section), is the *vmalloc *address
    space, extending from `VMALLOC_START` to `VMALLOC_END-1`. It's a completely virtual
    region to begin with, that is, its virtual pages initially are not mapped to any
    physical page frames.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核段或VAS（我们在[第7章](06ee05b5-3e71-482d-93b8-235c27ce23bc.xhtml)中详细介绍了这些内容，*内存管理内部-基础*，在*检查内核段*部分），是*vmalloc*地址空间，从`VMALLOC_START`到`VMALLOC_END-1`。它起初是一个完全虚拟的区域，也就是说，它的虚拟页面最初并未映射到任何物理页面帧上。
- en: For a quick refresher, revisit the diagram of the user and kernel segments –
    in effect, the complete VAS – by re-examining *Figure 7.12*. You will find this
    in [Chapter 7](06ee05b5-3e71-482d-93b8-235c27ce23bc.xhtml), *Memory Management
    Internals - Essentials,* under the *Trying it out – viewing kernel segment details* section.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 要快速复习一下，可以重新查看用户和内核段的图表——实际上是完整的VAS——通过重新查看*图7.12*。您可以在[第7章](06ee05b5-3e71-482d-93b8-235c27ce23bc.xhtml)中的*内存管理内部-基础*部分的*尝试-查看内核段详细信息*部分找到这个图表。
- en: In this book, our purpose is not to delve into the gory internal details regarding
    the kernel's `vmalloc` region. Instead, we present enough information for you,
    the module or driver author, to use this region for the purpose of allocating
    virtual memory at runtime.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们的目的不是深入研究内核的`vmalloc`区域的内部细节。相反，我们提供足够的信息，让您作为模块或驱动程序的作者，在运行时使用这个区域来分配虚拟内存。
- en: Learning to use the vmalloc family of APIs
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习使用vmalloc系列API
- en: 'You can allocate virtual memory (in kernel space of course) from the  kernel''s `vmalloc` region using
    the `vmalloc()` API:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`vmalloc()` API从内核的`vmalloc`区域中分配虚拟内存（当然是在内核空间中）：
- en: '[PRE19]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Some key points to note on the vmalloc:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 关于vmalloc的一些关键点：
- en: The `vmalloc()`API allocates contiguous virtual memory to the caller. There
    is no guarantee that the allocated region will be physically contiguous; it may
    or may not be (in fact, the larger the allocation, the less the chance that it's
    physically contiguous).
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vmalloc()`API将连续的虚拟内存分配给调用者。并不保证分配的区域在物理上是连续的；可能是连续的，也可能不是（事实上，分配越大，物理上连续的可能性就越小）。'
- en: The content of the virtual pages allocated is, in theory, random; in practice,
    it appears to be arch-dependent (the x86_64, at least, seems to zero out the memory
    region); of course, (at the risk of a slight performance hit) you're recommended
    to ensure memory zeroing out by employing the `vzalloc()` wrapper API
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理论上分配的虚拟页面的内容是随机的；实际上，它似乎是与架构相关的（至少在x86_64上，似乎会将内存区域清零）；当然，（尽管可能会稍微影响性能）建议您通过使用`vzalloc()`包装API来确保内存清零。
- en: The `vmalloc()` (and friends) APIs must only ever be invoked from a process
    context (as it might cause the caller to sleep).
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vmalloc()`（以及相关函数）API只能在进程上下文中调用（因为它可能导致调用者休眠）。'
- en: The return value of `vmalloc()` is the KVA (within the kernel vmalloc region)
    on success or `NULL` on failure.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vmalloc()`的返回值是成功时的KVA（在内核vmalloc区域内），失败时为`NULL`。'
- en: The start of the vmalloc memory just allocated is guaranteed to be on a page
    boundary (in other words, it's always page-aligned).
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刚刚分配的vmalloc内存的起始位置保证在页面边界上（换句话说，它总是页面对齐的）。
- en: The actual allocated memory (from the page allocator) might well be larger than
    what's requested (as again, it internally allocates sufficient pages to cover
    the size requested)
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际分配的内存（来自页面分配器）可能比请求的大小要大（因为它在内部分配足够的页面来覆盖请求的大小）
- en: It will strike you that this API seems very similar to the familiar user space `malloc(3)`.
    Indeed it is at first glance, except that, of course, it's a kernel space allocation
    (and again, remember that there is no direct correlation between the two).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现，这个API看起来非常类似于熟悉的用户空间`malloc(3)`。事实上，乍一看确实如此，只是当然，它是内核空间的分配（还要记住，两者之间没有直接的对应关系）。
- en: This being the case, how is `vmalloc()` helpful to us module or driver authors?
    When you require a large virtually contiguous buffer of a size greater than the
    slab APIs (that is, `k{m|z}alloc()` and friends) can provide – recall that it's
    typically 4 MB with a single allocation on both ARM and x86[_64]) – then you should
    use `vmalloc`!
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`vmalloc()`对我们模块或驱动程序的作者有什么帮助呢？当你需要一个大的虚拟连续缓冲区，其大小大于slab API（即`k{m|z}alloc()`和友元）可以提供的大小时——请记住，在ARM和x86[_64]上，单个分配通常为4MB——那么你应该使用`vmalloc`！
- en: 'FYI, the kernel uses `vmalloc()` for various reasons, some of them as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，内核出于各种原因使用`vmalloc()`，其中一些如下：
- en: Allocating space for the (static) memory of kernel modules when they are loaded
    into the kernel (in `kernel/module.c:load_module()`).
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在加载内核模块时为内核模块的（静态）内存分配空间（在`kernel/module.c:load_module()`中）。
- en: If `CONFIG_VMAP_STACK` is defined, then `vmalloc()` is used for the allocation
    of the kernel-mode stack of every thread (in `kernel/fork.c:alloc_thread_stack_node()`).
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果定义了`CONFIG_VMAP_STACK`，那么`vmalloc()`用于为每个线程的内核模式堆栈分配内存（在`kernel/fork.c:alloc_thread_stack_node()`中）。
- en: Internally, while servicing an operation called `ioremap()`.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在内部，为了处理一个叫做`ioremap()`的操作。
- en: Within the Linux socket filter (bpf) code paths, and so on.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Linux套接字过滤器（bpf）代码路径中等。
- en: 'For convenience, the kernel provides the `vzalloc()` wrapper API (analogous
    to `kzalloc()`) to allocate and zero out the memory region – a good coding practice,
    no doubt, but one that might hurt time-critical code paths slightly:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，内核提供了`vzalloc()`包装API（类似于`kzalloc()`）来分配并清零内存区域——这是一个良好的编码实践，但可能会稍微影响时间关键的代码路径：
- en: '[PRE20]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Once you are done with using the allocated virtual buffer, you must of course
    free it:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你使用了分配的虚拟缓冲区，当然你必须释放它：
- en: '[PRE21]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As expected, the parameter to `vfree()` is the return address from `v[m|z]alloc()` (or
    even the underlying `__vmalloc()` API that these invoke). Passing `NULL` causes
    it to just harmlessly return.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，传递给`vfree()`的参数是`v[m|z]alloc()`的返回地址（甚至是这些调用的底层`__vmalloc()` API）。传递`NULL`会导致它只是无害地返回。
- en: In the following snippet, we show some sample code from our `ch9/vmalloc_demo` kernel
    module. As usual, I urge you to clone the book's GitHub repository and try it
    out yourself (for brevity, we don't show the whole of the source code in the following
    snippet; we show the primary `vmalloc_try()` function invoked by the module's
    init code).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的片段中，我们展示了我们的`ch9/vmalloc_demo`内核模块的一些示例代码。和往常一样，我建议你克隆本书的GitHub存储库并自己尝试一下（为了简洁起见，我们没有在下面的片段中显示整个源代码；我们显示了模块初始化代码调用的主要`vmalloc_try()`函数）。
- en: 'Here is the first part of the code. If the `vmalloc()` API fails by any chance,
    we generate a warning via the kernel''s `pr_warn()` helper. Do note that the following `pr_warn()`
    helper isn''t really required; being pedantic here, we keep it... ditto for the
    remaining cases, as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这是代码的第一部分。如果`vmalloc()` API出现任何问题，我们通过内核的`pr_warn()`辅助程序生成警告。请注意，以下的`pr_warn()`辅助程序实际上并不是必需的；在这里我有点迂腐，我们保留它……其他情况也是如此，如下所示：
- en: '[PRE22]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `vmalloc()` API in the preceding code block allocates a contiguous kernel
    virtual memory region of (at least) 10,000 bytes; in reality, the memory is page-aligned!
    We employ the kernel's `print_hex_dump_bytes()` helper routine to dump the first
    16 bytes of this region.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码块中，`vmalloc()` API分配了一个至少有10,000字节的连续内核虚拟内存区域；实际上，内存是页面对齐的！我们使用内核的`print_hex_dump_bytes()`辅助例程来转储这个区域的前16个字节。
- en: 'Moving on, see the following code employ the `vzalloc()` API to again allocate
    another contiguous kernel virtual memory region of (at least) 10,000 bytes (it''s
    page-aligned memory though); this time, the memory contents are set to zeroes:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，看一下以下代码如何使用`vzalloc()` API再次分配另一个至少有10,000字节的连续内核虚拟内存区域（尽管它是页面对齐的内存）；这次，内存内容被设置为零：
- en: '[PRE23]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'A couple of points regarding the following code: one, notice the error handling
    with `goto` (at the target labels of multiple `goto` instances, where we use `vfree()` to
    free up previously allocated memory buffers as required), typical of kernel code.
    Two, for now, please ignore the `kvmalloc()`, `kcalloc()`, and `__vmalloc()` friend
    routines; we''ll cover them in the *Friends of vmalloc()* section:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 关于以下代码的一些要点：首先，注意使用`goto`进行错误处理（在多个`goto`实例的目标标签处，我们使用`vfree()`根据需要释放先前分配的内存缓冲区），这是典型的内核代码。其次，暂时忽略`kvmalloc()`、`kcalloc()`和`__vmalloc()`等友元例程；我们将在*vmalloc的友元*部分介绍它们：
- en: '[PRE24]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the cleanup code path of our kernel module, we of course free the allocated
    memory regions:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们内核模块的清理代码路径中，我们当然释放了分配的内存区域：
- en: '[PRE25]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We'll leave it to you to try out and verify this demo kernel module.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将让你自己尝试并验证这个演示内核模块。
- en: Now, let's delve briefly into another really key aspect – how exactly does a
    user space `malloc()`, or a kernel space `vmalloc()`, memory allocation become
    physical memory? Do read on to find out!
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们简要地探讨另一个非常关键的方面——用户空间的`malloc()`或内核空间的`vmalloc()`内存分配如何变成物理内存？继续阅读以了解更多！
- en: A brief note on memory allocations and demand paging
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于内存分配和需求分页的简要说明
- en: Without delving into deep detail regarding the internal workings of `vmalloc()` (or
    the user space `malloc()`), we'll nevertheless cover some crucial points that
    a competent kernel/driver developer like you must understand.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 不深入研究`vmalloc()`（或用户空间`malloc()`）的内部工作细节，我们仍然会涵盖一些关键点，这些关键点是像你这样的有能力的内核/驱动程序开发人员必须理解的。
- en: First and foremost, vmalloc-ed virtual memory has to, at some point (when used),
    become physical memory. This physical memory is allocated via the one and only
    way that it can be in the kernel – via the page (or buddy system) allocator. How
    this happens is a bit indirect and is briefly explained as follows.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，vmalloc-ed虚拟内存必须在某个时候（在使用时）变成物理内存。这种物理内存是通过内核中唯一的方式分配的 - 通过页面（或伙伴系统）分配器。这是一个有点间接的过程，简要解释如下。
- en: 'When using `vmalloc()`, a key point should be understood: `vmalloc()` only
    causes virtual memory pages to be allocated (they are merely marked as reserved
    by the OS). No physical memory is actually allocated at this time. The actual
    physical page frames corresponding to the virtual ones only get allocated – that
    too on a page-by-page basis – when these virtual pages are touched in any manner,
    such as for reads, writes, or executions. This key principle of not actually allocating
    physical memory until the program or process actually attempts to use it is referred
    to by various names – *demand paging, lazy allocation, on-demand allocation*,
    and so on. In fact, the documentation states this very fact:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`vmalloc()`时，一个关键点应该被理解：`vmalloc()`只会导致虚拟内存页面被分配（它们只是被操作系统标记为保留）。此时实际上并没有分配物理内存。实际的物理页面框架只有在这些虚拟页面被触摸时才会被分配
    - 而且也是逐页进行 - 无论是读取、写入还是执行。直到程序或进程实际尝试使用它之前，实际上并没有分配物理内存的这一关键原则被称为各种名称 - *需求分页、延迟分配、按需分配*等等。事实上，文档中明确说明了这一点：
- en: '"vmalloc space is lazily synchronized into the different PML4/PML5 pages of
    the processes using the page fault handler ..."'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '"vmalloc空间被懒惰地同步到使用页面错误处理程序的进程的不同PML4/PML5页面中..."'
- en: 'It''s quite enlightening to clearly understand how memory allocation really
    works for `vmalloc()` and friends, and indeed, for the user space glibc `malloc()` family
    of routines – it''s all via demand paging! Meaning, the successful return of these
    APIs really does not mean anything in terms of *physical* memory allocation. When
    `vmalloc()`, or indeed a user space `malloc()`, returns success, all that has
    really happened so far is that a virtual memory region has been reserved; no physical
    memory has actually been allocated yet! *The actual allocation of a physical page
    frame only happens on a per-page basis as and when the virtual page is accessed
    (for anything: reading, writing, or execution)*.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 清楚地了解`vmalloc()`和相关内容以及用户空间glibc `malloc()`系列例程的内存分配实际工作原理是非常有启发性的 - 这一切都是通过需求分页！这意味着这些API的成功返回实际上并不意味着*物理*内存分配。当`vmalloc()`或者用户空间的`malloc()`返回成功时，到目前为止实际上只是保留了一个虚拟内存区域；实际上还没有分配物理内存！*实际的物理页面框架分配只会在虚拟页面被访问时（无论是读取、写入还是执行）逐页进行*。
- en: 'But how does this happen internally? The answer, in brief: whenever the kernel
    or a process accesses a virtual address, the virtual address is interpreted by
    the **Memory Management Unit** (**MMU**), which is a part of the silicon on the
    CPU core. The MMU''s **Translation Lookaside Buffer** (**TLB**) *(*we don''t have
    the luxury of being able to delve into all of this here, sorry!*)* will now be
    checked for a *hit*. If so, the memory translation (virtual-to-physical address)
    is already available; if not, we have a TLB-miss. If so, the MMU will now *walk*
    the paging tables of the process, effectively translating the virtual address
    and thus obtaining the *physical address. *It puts this on the address bus, and
    the CPU goes on its merry way.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 但这是如何在内部发生的呢？简而言之，答案是：每当内核或进程访问虚拟地址时，虚拟地址都会被CPU核心上的硅片的一部分**内存管理单元**（MMU）解释。MMU的**转换旁路缓冲器**（TLB）*（我们没有能力在这里深入研究所有这些，抱歉！）*现在将被检查是否*命中*。如果是，内存转换（虚拟到物理地址）已经可用；如果不是，我们有一个TLB缺失。如果是这样，MMU现在将*遍历*进程的分页表，有效地转换虚拟地址，从而获得*物理地址。*它将这个地址放在地址总线上，CPU就可以继续进行。
- en: But, think on this, what if the MMU cannot find a matching physical address?
    This can happen for a number of reasons, one of them being our case here – we
    don't (yet) *have *a physical page frame, only a virtual page. At this point,
    the MMU essentially gives up as it cannot handle it. Instead, it *invokes the
    OS's page fault handler code* – an exception or fault handler that runs in the
    process's context – in the context of `current`. This page fault handler actually
    resolves the situation; in our case, with `vmalloc()` (or indeed even the user
    space `malloc()`!), it requests the page allocator for a single physical page
    frame (at order `0`) and maps it to the virtual page.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，想一想，如果MMU找不到匹配的物理地址会怎么样？这可能是由于许多原因之一，其中之一就是我们这里的情况 - 我们（还）*没有*物理页面框架，只有一个虚拟页面。在这一点上，MMU基本上放弃了，因为它无法处理。相反，它*调用操作系统的页面错误处理程序代码*
    - 在进程的上下文中运行的异常或错误处理程序 - 在`current`的上下文中。这个页面错误处理程序实际上解决了这种情况；在我们的情况下，使用`vmalloc()`（或者甚至是用户空间的`malloc()`！），它请求页面分配器为单个物理页面框架（在order
    `0`处）并将其映射到虚拟页面。
- en: It's equally important to realize that this demand paging (or lazy allocation)
    is *not the case for kernel memory allocations* carried out via the page (buddy
    system) and the slab allocator. There, when memory is allocated, understand that
    actual physical page frames are allocated *immediately*. (In reality on Linux,
    it's all very fast because, recall, the buddy system freelists have already mapped all
    system physical RAM into the kernel *lowmem* region and can therefore use it at
    will.)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是要意识到，通过页面（伙伴系统）和slab分配器进行的内核内存分配并不是懒惰分页（或延迟分配）的情况。在那里，当分配内存时，要理解实际的物理页面框架是立即分配的。（在Linux上，实际上一切都非常快，因为伙伴系统的空闲列表已经将所有系统物理RAM映射到内核的lowmem区域，因此可以随意使用。）
- en: Recall what we did in an earlier program, `ch8/lowlevel_mem`; there, we used
    our `show_phy_pages()` library routine to display the virtual address, the physical
    address, and **Page Frame Number** (**PFN**) for a given memory range, thereby
    verifying that the low-level page allocator routines really do allocate physically
    contiguous memory chunks. Now, you might think, why not call this same function
    in this `vmalloc_demo` kernel module? If the PFNs of the allocated (virtual) pages
    are not consecutive, we again prove that, indeed, it's only virtually contiguous.
    It sounds tempting to try, but it doesn't work!Why? Simply because, as stated
    earlier (in [Chapter 8](e78245d1-5a99-4b9e-a98c-cb16b15f3bee.xhtml), *Kernel Memory
    Allocation for Module Authors – Part 1*): do not attempt to translate from virtual
    to physical any addresses other than direct-mapped (identity-mapped / lowmem region)
    ones – the ones the page or slab allocators supply. It just doesn't work with
    `vmalloc`.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们在之前的程序`ch8/lowlevel_mem`中所做的事情；在那里，我们使用我们的`show_phy_pages()`库例程来显示给定内存范围的虚拟地址、物理地址和**页面帧号**（PFN），从而验证低级页面分配器例程确实分配了物理连续的内存块。现在，您可能会想，为什么不在这个`vmalloc_demo`内核模块中调用相同的函数？如果分配的（虚拟）页面的PFN不是连续的，我们再次证明，确实只是虚拟连续的。尝试听起来很诱人，但是不起作用！为什么？因为，正如之前所述（在[第8章](e78245d1-5a99-4b9e-a98c-cb16b15f3bee.xhtml)中，*模块作者的内核内存分配-第1部分*）：除了直接映射（身份映射/低内存区域）的地址之外，不要尝试将任何其他地址从虚拟转换为物理-页面或slab分配器提供的地址。它在`vmalloc`中根本不起作用。
- en: A few more points on `vmalloc` and some associated information follow; do read
    on.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`vmalloc`和一些相关信息的一些附加点将在下文中介绍；请继续阅读。'
- en: Friends of vmalloc()
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: vmalloc()的朋友
- en: 'In many cases, the precise API (or memory layer) used to perform a memory allocation
    does not really matter to the caller. So, a pattern of usage that emerged in a
    lot of in-kernel code paths went something like the following pseudocode:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，执行内存分配的精确API（或内存层）对调用者并不真正重要。因此，在许多内核代码路径中出现了以下伪代码的使用模式：
- en: '[PRE26]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The cleaner alternative to this kind of code is the `kvmalloc()` API. Internally,
    it attempts to allocate the requested `n` bytes of memory like this: first, via
    the more efficient `kmalloc()`; if it succeeds, fine, we have quickly obtained
    physically contiguous memory and are done; if not, it falls back to allocating
    the memory via the slower but surer `vmalloc()` (thus obtaining  virtually contiguous
    memory). Its signature is as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这种代码的更清晰的替代方案是`kvmalloc()`API。在内部，它尝试以以下方式分配所请求的`n`字节的内存：首先，通过更有效的`kmalloc()`；如果成功，很好，我们很快就获得了物理连续的内存并完成了；如果没有成功，它会回退到通过更慢但更可靠的`vmalloc()`分配内存（从而获得虚拟连续的内存）。它的签名如下：
- en: '[PRE27]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '(Remember to include the header file.) Note that for the (internal) `vmalloc()` to
    go through (if it comes to that), only the `GFP_KERNEL` flag must be supplied.
    As usual, the return value is a pointer (a kernel virtual address) to the allocated
    memory, or `NULL` on failure. Free the memory obtained with `kvfree`:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: （记得包含头文件。）请注意，对于（内部的）`vmalloc()`要通过（如果需要的话），只需提供`GFP_KERNEL`标志。与往常一样，返回值是指向分配内存的指针（内核虚拟地址），或者在失败时为`NULL`。释放使用`kvfree`获得的内存：
- en: '[PRE28]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Here, the parameter of course is the return address from `kvmalloc()`.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，参数当然是从`kvmalloc()`返回的地址。
- en: 'Similarly, and analogous to the `{k|v}zalloc()` APIs, we also have the `kvzalloc()` API,
    which of course *z*eroes the memory content. I''d suggest you use it in preference
    to the `kvmalloc()` API (with the usual caveat: it''s safer but a bit slower).'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，与`{k|v}zalloc()`API类似，我们还有`kvzalloc()`API，它当然*将*内存内容设置为零。我建议您优先使用它而不是`kvmalloc()`API（通常的警告：它更安全但速度稍慢）。
- en: 'Further, you can use the `kvmalloc_array()` API to allocate virtual contiguous
    memory for an array of items. It allocates `n` elements of `size` bytes each.
    Its implementation is shown as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您可以使用`kvmalloc_array()`API为*数组*分配虚拟连续内存。它分配`n`个`size`字节的元素。其实现如下所示：
- en: '[PRE29]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'A key point here: notice how a validity check for the dangerous **integer overflow**
    (**IoF**) bug is made; that''s important and interesting; do write robust code
    by performing similar validity checks in your code where required.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个关键点：注意对危险的整数溢出（IoF）错误进行有效性检查；这很重要和有趣；在代码中进行类似的有效性检查，以编写健壮的代码。
- en: 'Next, the `kvcalloc()` API is functionally equivalent to the `calloc(3)` user
    space API, and is just a simple wrapper over the `kvmalloc_array()` API:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，`kvcalloc()`API在功能上等同于用户空间API`calloc(3)`，只是`kvmalloc_array()`API的简单包装器：
- en: '[PRE30]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We also mention that for code requiring *NUMA awareness *(we covered NUMA and
    associated topics in [Chapter 7](06ee05b5-3e71-482d-93b8-235c27ce23bc.xhtml), *Memory
    Management Internals – Essentials*, under the *Physical RAM organization *section),
    the following APIs are available, with which we can specify the particular NUMA
    node to allocate the memory from as a parameter (this being the point to NUMA
    systems; do see the information box that follows shortly):'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提到，对于需要NUMA意识的代码（我们在第7章“内存管理内部-基本知识”中涵盖了NUMA和相关主题，*物理RAM组织*部分），可以使用以下API，我们可以指定要从特定NUMA节点分配内存的参数（这是指向NUMA系统的要点；请看后面不久会出现的信息框）：
- en: '[PRE31]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Similarly, we have the `kzalloc_node()` API as well, which sets the memory content
    to zero.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们也有`kzalloc_node()`API，它将内存内容设置为零。
- en: 'In fact, generically, most of the kernel-space memory APIs we have seen ultimately
    boil down to one *that takes a NUMA node as a parameter*. For example, take the
    call chain for one of the primary page allocator APIs, the `__get_free_page()` API:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，通常我们看到的大多数内核空间内存API最终都归结为一个*以NUMA节点作为参数*的API。例如，对于主要的页面分配器API之一，`__get_free_page()`API的调用链如下：
- en: '`__get_free_page() -> __get_free_pages() -> alloc_pages() -> alloc_pages_current()'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '`__get_free_page() -> __get_free_pages() -> alloc_pages() -> alloc_pages_current()'
- en: '-> __alloc_pages_nodemask() `. The **`__alloc_pages_nodemask()`** API is considered
    to be the *heart* of the zoned buddy allocator; notice its fourth parameter, the
    (NUMA) nodemask:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: -> __alloc_pages_nodemask() `. **`__alloc_pages_nodemask()`** API被认为是分区伙伴分配器的*核心*；请注意它的第四个参数，（NUMA）nodemask：
- en: '`mm/page_alloc.c:struct page *`'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`mm/page_alloc.c:struct page *`'
- en: '`__alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order,'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`__alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order,'
- en: int preferred_nid, nodemask_t *nodemask);`
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: int preferred_nid, nodemask_t *nodemask);`
- en: Of course, you must free the memory you take; for the preceding `kv*()` APIs
    (and the `kcalloc()` API), free the memory obtained with `kvfree()`.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您必须释放您获取的内存；对于前面的`kv*()`API（和`kcalloc()`API），请使用`kvfree()`释放获得的内存。
- en: 'Another internal detail worth knowing about, and a reason the `k[v|z]malloc[_array]()`APIs
    are useful: with a regular `kmalloc()`, the kernel will indefinitely retry allocating
    the memory requested if it''s small enough (this number currently being defined
    as `CONFIG_PAGE_ALLOC_COSTLY_ORDER`, which is `3`, implying 8 pages or less);
    this can actually hurt performance! With the `kvmalloc()` API, this indefinite
    retrying is not done (this behavior is specified via the GFP flags `__GFP_NORETRY|__GFP_NOWARN`),
    thus speeding things up. An LWN article goes into detail regarding the rather
    weird indefinite-retry semantics of the slab allocator: *The "too small to fail"
    memory-allocation rule, Jon Corbet, December 2014* ([https://lwn.net/Articles/627419/](https://lwn.net/Articles/627419/)).'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得了解的内部细节，以及`k[v|z]malloc[_array]()`API有用的原因：对于常规的`kmalloc()`，如果请求的内存足够小（当前定义为`CONFIG_PAGE_ALLOC_COSTLY_ORDER`，即`3`，表示8页或更少），内核将无限重试分配内存；这实际上会影响性能！使用`kvmalloc()`API，不会进行无限重试（此行为通过GFP标志`__GFP_NORETRY|__GFP_NOWARN`指定），从而加快速度。LWN的一篇文章详细介绍了slab分配器的相当奇怪的无限重试语义：*“太小而无法失败”的内存分配规则，Jon
    Corbet，2014年12月*（[https://lwn.net/Articles/627419/](https://lwn.net/Articles/627419/)）。
- en: 'With regard to the `vmalloc_demo` kernel module we saw in this section, take
    a quick look at the code again (`ch9/vmalloc_demo/vmalloc_demo.c`). We use `kvmalloc()` as
    well as `kcalloc()` (*steps 3* and *4* in the comments). Let''s run it on an x86_64
    Fedora 31 guest system and see the output:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 关于我们在本节中看到的`vmalloc_demo`内核模块，再快速看一下代码（`ch9/vmalloc_demo/vmalloc_demo.c`）。我们使用`kvmalloc()`以及`kcalloc()`（*注释中的步骤3和4*）。让我们在x86_64
    Fedora 31客户系统上运行它并查看输出：
- en: '![](img/71b63bcd-29aa-476e-bcb1-052ffead0f5c.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/71b63bcd-29aa-476e-bcb1-052ffead0f5c.png)'
- en: Figure 9.4 – Output on loading our vmalloc_demo.ko kernel module
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4-加载我们的vmalloc_demo.ko内核模块时的输出
- en: 'We can see the actual return (kernel virtual) address from the APIs in the
    preceding output - note that they all belong within the kernel''s vmalloc region.
    Notice the return address of `kvmalloc()`(step 3 in Figure 9.4); let''s search
    for it under `proc`:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从前面的输出中的API中看到实际的返回（内核虚拟）地址-请注意它们都属于内核的vmalloc区域。注意`kvmalloc()`（图9.4中的步骤3）的返回地址；让我们在`proc`下搜索一下：
- en: '[PRE32]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: There it is! We can clearly see how using the `kvmalloc()` API for a large quantity
    of memory (5 MB) resulted in the `vmalloc()` API being internally invoked (the
    `kmalloc()` API would have failed and would not have emitted a warning, nor retried)
    and thus, as you can see, the hit under `/proc/vmallocinfo`.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！我们可以清楚地看到，使用`kvmalloc()`API为大量内存（5 MB）分配导致内部调用了`vmalloc()`API（`kmalloc()`API将失败并且不会发出警告，也不会重试），因此，正如您所看到的，命中了`/proc/vmallocinfo`。
- en: To interpret the preceding fields of `/proc/vmallocinfo`, refer to the kernel
    documentation here: [https://www.kernel.org/doc/Documentation/filesystems/proc.txt](https://www.kernel.org/doc/Documentation/filesystems/proc.txt).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 要解释`/proc/vmallocinfo`的前面字段，请参阅这里的内核文档：[https://www.kernel.org/doc/Documentation/filesystems/proc.txt](https://www.kernel.org/doc/Documentation/filesystems/proc.txt)。
- en: 'Something for you to try out here: in our `ch9/vmalloc_demo` kernel module,
    change the amount of memory to be allocated via `kvmalloc()` by passing `kvnum=<#
    bytes to alloc>` as a module parameter.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`ch9/vmalloc_demo`内核模块中，通过将`kvnum=<# bytes to alloc>`作为模块参数传递来更改通过`kvmalloc()`分配的内存量。
- en: FYI, the kernel provides an internal helper API, the `vmalloc_exec()` - it's
    (again) a wrapper over the `vmalloc()` API, and is used to allocate a virtually
    contiguous memory region that has execute permissions set upon it. An interesting
    user is the kernel module allocation code path (`kernel/module.c:module_alloc()`);
    the space for the kernel module's (executable section) memory is allocated via
    this routine. This routine isn't exported though.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: FYI，内核提供了一个内部辅助API，`vmalloc_exec()`-它（再次）是`vmalloc()`API的包装器，并用于分配具有执行权限的虚拟连续内存区域。一个有趣的用户是内核模块分配代码路径（`kernel/module.c:module_alloc()`）；内核模块的（可执行部分）内存空间是通过这个例程分配的。不过，这个例程并没有被导出。
- en: The other helper routine we mention is `vmalloc_user()`; it's (yet again) a
    wrapper over  the `vmalloc()` API, and is used to allocate a zeroed-out virtually
    contiguous memory region suitable for mapping into user VAS. This routine is exported; it's
    used, for example, by several device drivers as well as the kernel's performance
    events ring buffer.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提到的另一个辅助例程是`vmalloc_user()`；它（再次）是`vmalloc()`API的包装器，并用于分配适合映射到用户VAS的零内存的虚拟连续内存区域。这个例程是公开的；例如，它被几个设备驱动程序以及内核的性能事件环缓冲区使用。
- en: Specifying the memory protections
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指定内存保护
- en: 'What if you intend to specify certain specific memory protections (a combination
    of read, write, and execute protections) for the memory pages you allocate? In
    this case, use the underlying `__vmalloc()` API (it is exported). Consider the
    following comment in the kernel source (`mm/vmalloc.c`):'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您打算为您分配的内存页面指定特定的内存保护（读、写和执行保护的组合），该怎么办？在这种情况下，使用底层的`__vmalloc()`API（它是公开的）。请参考内核源代码中的以下注释（`mm/vmalloc.c`）：
- en: '[PRE33]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The signature of the `__vmalloc()` API shows how we can achieve this:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '`__vmalloc()`API的签名显示了我们如何实现这一点：'
- en: '[PRE34]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: FYI, from the 5.8 kernel, the `__vmalloc()` function's third parameter -  `pgprot_t
    prot` - has been removed (as there weren't any users for page permissions besides
    the usual ones; [https://github.com/torvalds/linux/commit/88dca4ca5a93d2c09e5bbc6a62fbfc3af83c4fca](https://github.com/torvalds/linux/commit/88dca4ca5a93d2c09e5bbc6a62fbfc3af83c4fca)).
    Tells us another thing regarding the kernel community - if a feature isn't being
    used by anyone, it's simply removed.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，从5.8内核开始，`__vmalloc()`函数的第三个参数——`pgprot_t prot`已被移除（因为除了通常的用户之外，没有其他用户需要页面权限；[https://github.com/torvalds/linux/commit/88dca4ca5a93d2c09e5bbc6a62fbfc3af83c4fca](https://github.com/torvalds/linux/commit/88dca4ca5a93d2c09e5bbc6a62fbfc3af83c4fca)）。这告诉我们关于内核社区的另一件事——如果一个功能没有被任何人使用，它就会被简单地移除。
- en: 'The first two parameters are the usual suspects – the size of the memory required
    in bytes and the GFP flags for the allocation. The third parameter is the one
    of interest here: `prot`represents the memory protection bitmask that we can specify
    for the memory pages. For example, to allocate 42 pages that are set to be read-only
    (`r--`), we could do the following:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个参数是通常的嫌疑犯——以字节为单位的内存大小和用于分配的GFP标志。第三个参数在这里是感兴趣的：`prot`代表我们可以为内存页面指定的内存保护位掩码。例如，要分配42个设置为只读（`r--`）的页面，我们可以这样做：
- en: '[PRE35]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: And subsequently, of course, call `vfree()` to free the memory back to the system.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当然，调用`vfree()`来将内存释放回系统。
- en: Testing it – a quick **Proof of Concept**
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试它——一个快速的**概念验证**
- en: We'll try a quick Proof of Concept in our `vmalloc_demo` kernel module. We allocate
    a region of memory specifying the page protection to be read-only (or *RO*) via
    the `__vmalloc()` kernel API. We then test it by reading *and writing *to the
    read-only memory region. A code snippet from it is seen as follows.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在我们的`vmalloc_demo`内核模块中尝试一个快速的概念验证。我们通过`__vmalloc()`内核API分配了一个内存区域，指定页面保护为只读（或*RO*）。然后我们通过读取和写入只读内存区域来测试它。以下是其中的一部分代码片段。
- en: 'Note that we have kept the (silly) `WR2ROMEM_BUG` macro in the following code
    undefined by default, so that you, innocent reader, don''t have our evil `vmalloc_demo` kernel
    module simply crash on you. So in order to try this PoC, please un-comment the
    define statement (as shown here), thus allowing the buggy code to execute:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们默认情况下未定义以下代码中的（愚蠢的）`WR2ROMEM_BUG`宏，这样你，无辜的读者，就不会让我们邪恶的`vmalloc_demo`内核模块在你身上崩溃。因此，为了尝试这个PoC，请取消注释定义语句（如下所示），从而允许错误的代码执行：
- en: '[PRE36]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Upon running, at the point where we attempt to write to the read-only memory,
    it crashes! See the following partial screenshot (Figure 9.5; from running it
    on our x86_64 Fedora guest):'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时，在我们尝试写入只读内存的地方，它会崩溃！请参见以下部分截图（图9.5；在我们的x86_64 Fedora客户机上运行）：
- en: '![](img/bfbfd566-9506-4aa4-b538-05d438fed559.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bfbfd566-9506-4aa4-b538-05d438fed559.png)'
- en: Figure 9.5 – The kernel Oops that occurs when we try and write to a read-only
    memory region!
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5——当我们尝试写入只读内存区域时发生的内核Oops！
- en: 'This proves that, indeed, the `__vmalloc()` API we performed had successfully
    set the memory region to read-only. Again, the details on the interpretation of
    the preceding (partially seen) kernel diagnostics or *Oops* messagelie beyond
    this book''s scope. Nevertheless, it''s quite easy to see the root cause of the
    issue highlighted in the preceding figure: the following lines literally pinpoint
    the reason for this bug:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这证明了我们执行的`__vmalloc()` API成功地将内存区域设置为只读。再次强调，对于前面（部分可见）的内核诊断或*Oops*消息的解释细节超出了本书的范围。然而，很容易看出在前面的图中突出显示的问题的根本原因：以下行文字确切地指出了这个错误的原因：
- en: '[PRE37]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: In user space applications, performing a similar memory protection setting upon
    an arbitrary memory region can be done via the `mprotect(2)` system call; do look
    up its man page for usage details (it even kindly provides example code!).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在用户空间应用程序中，可以通过`mprotect(2)`系统调用对任意内存区域执行类似的内存保护设置；请查阅其手册以获取使用详情（它甚至友好地提供了示例代码！）。
- en: Why make memory read-only?
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么要将内存设置为只读？
- en: 'Specifying memory protections at allocation time to, say, read-only may appear
    to be a pretty useless thing to do: how would you then initialize that memory
    to some meaningful content? Well, think about it – **guard pages** are the perfect
    use case for this scenario (similar to the redzone pages that the SLUB layer keeps
    when in debug mode); it is useful indeed.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在分配时指定内存保护，比如只读，可能看起来是一个相当无用的事情：那么你怎么初始化那块内存为一些有意义的内容呢？嗯，想一想——**guard pages**就是这种情况的完美用例（类似于SLUB层在调试模式下保留的redzone页面）；它确实是有用的。
- en: 'What if we wanted read-only pages for some purpose other than guard pages?
    Well, instead of using `__vmalloc()`, we might avail of some alternate means:
    perhaps memory mapping some kernel memory into user space via an `mmap()` method,
    and using the `mprotect(2)` system call from a user space app to set up appropriate
    protections (or even setting up protections through well-known and tested LSM
    frameworks, such as SELinux, AppArmor, Integrity, and so on).'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要为某些目的而使用只读页面呢？那么，我们可以使用一些替代方法，而不是使用`__vmalloc()`，也许是通过`mmap()`方法将一些内核内存映射到用户空间，然后使用用户空间应用程序的`mprotect(2)`系统调用来设置适当的保护（甚至通过著名且经过测试的LSM框架，如SELinux、AppArmor、Integrity等来设置保护）。
- en: 'We conclude this section with a quick comparison between the typical kernel
    memory allocator APIs: `kmalloc()` and `vmalloc()`.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用一个快速比较来结束本节：典型的内核内存分配器API：`kmalloc()`和`vmalloc()`。
- en: The kmalloc() and vmalloc() APIs – a quick comparison
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`kmalloc()`和`vmalloc()` API——一个快速比较'
- en: 'A quick comparison between the `kmalloc()` (or `kzalloc()`) and `vmalloc()` (or
    `vzalloc()`) APIs is presented in the following table:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格中简要比较了`kmalloc()`（或`kzalloc()`）和`vmalloc()`（或`vzalloc()`）API：
- en: '| **Characteristic** | **`kmalloc()` or `kzalloc()`** | **`vmalloc()` or `vzalloc()`**
    |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| **特征** | **`kmalloc()`或`kzalloc()`** | **`vmalloc()`或`vzalloc()`** |'
- en: '| **Memory allocated is ** | Physically contiguous | Virtually (logically)
    contiguous |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| **分配的内存是** | 物理连续的 | 虚拟（逻辑）连续的 |'
- en: '| **Memory alignment** | Aligned to hardware (CPU) cacheline | Page-aligned
    |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: 内存对齐 | 对硬件（CPU）缓存行对齐 | 页面对齐
- en: '| **Minimum granularity** | Arch-dependent; as low as 8 bytes on x86[_64] |
    1 page |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: 最小粒度 | 与架构相关；在x86[_64]上最低为8字节 | 1页
- en: '| **Performance** | Much faster (physical RAM allocated) for small memory allocations
    (the typical case); ideal for allocations < 1 page | Slower, demand-paged (only
    virtual memory allocated; lazy allocation of RAM involving the page fault handler);
    can service large (virtual) allocations |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: 性能 | 对于小内存分配（典型情况下）更快（分配物理RAM）；适用于小于1页的分配 | 较慢，按需分页（只分配虚拟内存；涉及页面错误处理程序的延迟分配RAM）；可以为大（虚拟）分配提供服务
- en: '| **Size limitation** | Limited (to typically 4 MB) | Very large (the kernel
    vmalloc region can even be several terabytes on 64-bit systems, though much less
    on 32-bit) |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: 大小限制 | 有限（通常为4 MB） | 非常大（64位系统上内核vmalloc区域甚至可以达到数TB，但32位系统上要少得多）
- en: '| **Suitability** | Suitable for almost all use cases where performance matters,
    the memory required is small, including DMA (still, use the DMA API); can work
    in atomic/interrupt contexts | Suitable for large software (virtually) contiguous
    buffers; slower, cannot be allocated in atomic/interrupt contexts |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: 适用性 | 适用于几乎所有性能要求较高的用例，所需内存较小，包括DMA（仍然，请使用DMA API）；可以在原子/中断上下文中工作 | 适用于大型软件（几乎）连续的缓冲区；较慢，不能在原子/中断上下文中分配
    |
- en: 'This does not imply that one is superior to the other. Their usage depends
    upon the circumstances. This leads us into our next – indeed very important –
    topic: how do you decide which memory allocation API to use when? Making the right
    decision is actually critical for the best possible system performance and stability
    – do read on to find out how to make that choice!'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不意味着其中一个优于另一个。它们的使用取决于具体情况。这将引出我们下一个 - 确实非常重要的 - 话题：在何时决定使用哪种内存分配API？做出正确的决定对于获得最佳系统性能和稳定性非常关键
    - 请继续阅读以了解如何做出选择！
- en: Memory allocation in the kernel – which APIs to use when
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内核中的内存分配 - 何时使用哪些API
- en: 'A really quick summation of what we have learned so far: the kernel''s underlying
    engine for memory allocation (and freeing) is called the page (or buddy system)
    allocator. Ultimately, every single memory allocation (and subsequent free) goes
    through this layer. It has its share of problems though, the chief one being internal
    fragmentation or wastage (due to its minimum granularity being a page). Thus we
    have the slab allocator (or slab cache) layered above it, providing the power
    of object caching and caching fragments of a page (helping alleviate the page
    allocator''s wastage issues). Also, don''t forget that you can create your own
    custom slab caches, and, as we have just seen, the kernel has a `vmalloc` region and
    APIs to allocate *virtual* pages from within it.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止我们学到的东西的一个非常快速的总结：内核内存分配（和释放）的基础引擎称为页面（或伙伴系统）分配器。最终，每个内存分配（和随后的释放）都经过这一层。然而，它也有自己的问题，其中主要问题是内部碎片或浪费（由于其最小粒度是一个页面）。因此，我们有了位于其上面的slab分配器（或slab缓存），它提供了对象缓存的功能，并缓存页面的片段（有助于减轻页面分配器的浪费问题）。此外，不要忘记您可以创建自己的自定义slab缓存，并且正如我们刚刚看到的，内核有一个`vmalloc`区域和API来从中分配*虚拟*页面。
- en: With this information in mind, let's move along. To understand which API to
    use when, let's first look at the kernel memory allocation API set.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些信息，让我们继续。要了解何时使用哪种API，让我们首先看看内核内存分配API集。
- en: Visualizing the kernel memory allocation API set
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化内核内存分配API集
- en: 'The following conceptual diagram shows us the Linux kernel''s memory allocation
    layers as well as the prominent APIs within them; note the following:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 以下概念图向我们展示了Linux内核的内存分配层以及其中的显著API；请注意以下内容：
- en: Here we only show the (typically used) APIs exposed by the kernel to module/driver
    authors (with the exception being the one that ultimately performs the allocations
    – the `__alloc_pages_nodemask()` API right at the bottom!).
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里，我们只展示了内核向模块/驱动程序作者公开的（通常使用的）API（除了最终执行分配的`__alloc_pages_nodemask()` API在底部！）。
- en: For brevity, we haven't shown the corresponding memory-freeing APIs.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为简洁起见，我们没有展示相应的内存释放API。
- en: 'The following is a diagram showing several of the (exposed to module / driver
    authors) kernel memory allocation APIs:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个图表，显示了几个（向模块/驱动程序作者公开的）内核内存分配API：
- en: '![](img/60fd6661-248f-4bc6-824a-4271596f2180.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/60fd6661-248f-4bc6-824a-4271596f2180.png)'
- en: Figure 9.6 – Conceptual diagram showing the kernel's memory allocation API set
    (for module / driver authors)
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 - 概念图显示内核的内存分配API集（用于模块/驱动程序作者）
- en: Now that you have seen the wealth of (exposed) memory allocation APIs available,
    the following sections delve into helping you make the right decision as to which
    to use under what circumstances.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 既然您已经看到了（公开的）可用内存分配API的丰富选择，接下来的部分将深入探讨如何帮助您在何种情况下做出正确的选择。
- en: Selecting an appropriate API for kernel memory allocation
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择适当的内核内存分配API
- en: 'With all this choice of APIs, how do we choose? Though we have already talked
    about this very case in this chapter as well as the previous one, we''ll again
    summarize it as it''s very important. Broadly speaking, there are two ways to
    look at it – the API to use depends upon the following:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这么多选择的API，我们该如何选择？虽然我们在本章以及上一章已经讨论过这个问题，但我们会再次总结，因为这非常重要。大体上来说，有两种看待它的方式 -
    使用的API取决于以下因素：
- en: The amount of memory required
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所需内存的数量
- en: The type of memory required
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所需的内存类型
- en: We will illustrate both cases in this section.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节中说明这两种情况。
- en: 'First, to decide which API to use by the type, amount, and contiguity of the
    memory to be allocated, scan through the following flowchart (starting at the
    upper right from the label Start here):'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过扫描以下流程图（从标签“从这里开始”右上方开始），决定使用哪种API来分配内存的类型、数量和连续性：
- en: '![](img/0a9000d5-5341-4715-80e7-746aa1d675e7.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a9000d5-5341-4715-80e7-746aa1d675e7.png)'
- en: Figure 9.7 – Decision flowchart for which kernel memory allocation API(s) to
    use for a module/driver
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 - 决定为模块/驱动程序使用哪种内核内存分配API的决策流程图
- en: 'Of course, it''s not trivial; not only that, I''d like to remind you to recall
    the detailed discussions we covered earlier in this chapter, including the GFP
    flags to use (and the *do not sleep in atomic context* rule); in effect, the following:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这并不是微不足道的；不仅如此，我想提醒您回顾一下我们在本章早些时候讨论过的详细内容，包括要使用的GFP标志（以及*不要在原子上下文中休眠*的规则）；实际上，以下内容：
- en: When in any atomic context, including interrupt contexts, ensure you only use
    the `GFP_ATOMIC` flag.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在任何原子上下文中，包括中断上下文，确保只使用`GFP_ATOMIC`标志。
- en: Else (process context), you decide whether to use the `GFP_ATOMIC` or `GFP_KERNEL`
    flag; use `GFP_KERNEL` when it's safe to sleep
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则（进程上下文），您可以决定是否使用`GFP_ATOMIC`或`GFP_KERNEL`标志；当可以安全休眠时，请使用`GFP_KERNEL`
- en: Then, as covered under the *Caveats when using the slab allocator* section: when
    using the `k[m|z]alloc()` API and friends, make sure to check the actual allocated
    memory with `ksize()`.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，如在*使用slab分配器时的注意事项*部分所述：在使用`k[m|z]alloc()` API和相关函数时，请确保使用`ksize()`检查实际分配的内存。
- en: 'Next, to decide which API to use by the type of memory to be allocated, scan
    through the following table:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，根据要分配的内存类型决定使用哪个API，扫描以下表：
- en: '| **Type of memory required** | **Allocation method** | **APIs** |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '**所需内存类型** | **分配方法** | **API**'
- en: '| Kernel modules, typical case: regular usage for small amounts (less than
    one page), physically contiguous | Slab allocator | `k[m&#124;z]alloc()`, `kcalloc()`,
    and  `krealloc()` |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: 内核模块，典型情况：小量（少于一页），物理上连续的常规用法 | Slab分配器 | `k[m|z]alloc()`，`kcalloc()`和`krealloc()`
    |
- en: '| Device drivers: regular usage for small amounts (< 1 page), physically contiguous; suitable
    for driver `probe()` or init methods; recommended for drivers | Resource-managed
    APIs | `devm_kzalloc()` and `devm_kmalloc()` |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: 设备驱动程序：小量（<1页），物理上连续的常规用法；适用于驱动程序`probe()`或init方法；建议驱动程序使用 | 资源管理API | `devm_kzalloc()`和`devm_kmalloc()`
    |
- en: '| Physically contiguous, general-purpose usage | Page allocator | `__get_free_page[s]()`,
    `get_zeroed_page()`, and'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 物理上连续，通用用途 | 页面分配器 | `__get_free_page[s]()`, `get_zeroed_page()`和
- en: '`alloc_page[s][_exact]()` |'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '`alloc_page[s][_exact]()` |'
- en: '| Physically contiguous, for **Direct Memory Access** (**DMA**) | Purpose-built
    DMA API layer, with CMA (or slab/page allocator) | (not covered here: `dma_alloc_coherent(),
    dma_map_[single&#124;sg]()`, Linux DMA Engine APIs, and so on) |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: 对于**直接内存访问**（**DMA**），物理上连续的情况下，可以使用专门的DMA API层，带有CMA（或slab/page分配器） | （这里不涵盖：`dma_alloc_coherent(),
    dma_map_[single|sg]()`, Linux DMA引擎API等）
- en: '| Virtually contiguous (for large software-only buffers) | Indirect via page
    allocator | `v[m&#124;z]alloc()` |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: 对于大型软件缓冲区，虚拟上连续的情况下，可以通过页面分配器间接使用 | `v[m|z]alloc()` |
- en: '| Virtually or physically contiguous, when unsure of runtime size | Either
    slab or vmalloc region | `kvmalloc[_array]()` |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: 在运行时大小不确定时，虚拟或物理上连续的情况下，可以使用slab或vmalloc区域 | `kvmalloc[_array]()` |
- en: '| Custom data structures (objects) | Creates and uses a custom slab cache |
    `kmem_cache_[create&#124;destroy]()` and `   kmem_cache_[alloc&#124;free]()` |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: 自定义数据结构（对象） | 创建并使用自定义slab缓存 | `kmem_cache_[create|destroy]()`和`kmem_cache_[alloc|free]()`
    |
- en: (Of course, there is some overlap with this table and the flowchart in *Figure
    9.7*). As a generic rule of thumb, your first choice should be the slab allocator
    APIs, that is via `kzalloc()` or `kmalloc()`; these are the most efficient for
    typical allocations of less than a page in size. Also, recall that when unsure
    of the runtime size required, you could use the `kvmalloc()` API. Again, if the
    size required happens to be a perfectly rounded power-of-2 number of pages (2⁰,
    2¹, ..., 2^(MAX_ORDER-1) *pages*), then using the page allocator APIs will be
    optimal.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: （当然，这个表格与*图9.7*中的流程图有一些重叠）。作为一个通用的经验法则，您的首选应该是slab分配器API，即通过`kzalloc()`或`kmalloc()`；这些对于典型小于一页的分配来说是最有效的。此外，请记住，当运行时所需大小不确定时，您可以使用`kvmalloc()`
    API。同样，如果所需大小恰好是完全舍入的2的幂页数（2⁰、2¹、...、2^(MAX_ORDER-1) *页*），那么使用页面分配器API将是最佳的。
- en: A word on DMA and CMA
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于DMA和CMA的说明
- en: On the topic of DMA, though its study and usage is beyond the scope of this
    book, I would nevertheless like to mention that Linux has a purpose-built set
    of APIs for DMA christened the *DMA Engine.* Driver authors performing DMA operations
    are very much expected to use these APIs and *not* directly use the slab or page
    allocator APIs (subtle hardware issues do turn up).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 关于DMA的话题，虽然其研究和使用超出了本书的范围，但我仍然想提一下，Linux有一套专门为DMA设计的API，称为*DMA引擎*。执行DMA操作的驱动程序作者非常希望使用这些API，而不是直接使用slab或页面分配器API（微妙的硬件问题确实会出现）。
- en: Further, several years back, Samsung engineers successfully merged a patch into
    the mainline kernel calledthe **Contiguous Memory Allocator** (**CMA**). Essentially,
    it allows the allocation of *large physically contiguous memory* chunks (of a
    size over the typical 4 MB limit!). This is required for DMA on some memory-hungry
    devices (you want to stream that ultra-HD quality movie on a big-screen tablet
    or TV?). The cool thing is that the CMA code is transparently built into the DMA
    Engine and DMA APIs. Thus, as usual, driver authors performing DMA operations
    should just stick to using the Linux DMA Engine layer.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，几年前，三星工程师成功地将一个补丁合并到主线内核中，称为**连续内存分配器**（**CMA**）。基本上，它允许分配*大的物理上连续的内存*块（超过典型的4
    MB限制！）。这对于一些内存需求量大的设备的DMA是必需的（你想在大屏平板电脑或电视上播放超高清质量的电影吗？）。很酷的是，CMA代码被透明地构建到DMA引擎和DMA
    API中。因此，像往常一样，执行DMA操作的驱动程序作者应该坚持使用Linux DMA引擎层。
- en: If you are interested in learning more about DMA and CMA, see the links provided
    in the Further reading section for this chapter.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有兴趣了解DMA和CMA，请参阅本章的进一步阅读部分提供的链接。
- en: Also, realize that our discussion has mostly been with regard to the typical
    kernel module or device driver author. Within the OS itself, the demand for single
    pages tends to be quite high (due to the OS servicing demand paging via the page
    fault handler – what are called *minor* faults). Thus, under the hood, the memory
    management subsystem tends to issue the `__get_free_page[s]()` APIs quite frequently.
    Also, to service the memory demand for the *page cache *(and other internal caches),
    the page allocator plays an important role.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，要意识到我们的讨论大多是关于典型的内核模块或设备驱动程序作者。在操作系统本身，对单页的需求往往非常高（由于操作系统通过页面错误处理程序服务需求分页
    - 即所谓的*次要*错误）。因此，在底层，内存管理子系统往往频繁地发出`__get_free_page[s]()`API。此外，为了满足*页面缓存*（和其他内部缓存）的内存需求，页面分配器发挥着重要作用。
- en: All right, well done, with this you have (almost!) completed our two chapters
    of coverage on the various kernel memory allocation layers and APIs (for module/driver
    authors)! Let's finish off this large topic with a remaining important area –
    the Linux kernel's (fairly controversial) OOM killer; do read on!
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，干得好，通过这个你（几乎！）完成了我们对各种内核内存分配层和API（用于模块/驱动程序作者）的两章覆盖。让我们用一个重要的剩余领域来结束这个大主题
    - Linux内核（相当有争议的）OOM killer；继续阅读吧！
- en: Stayin' alive – the OOM killer
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保持活力 - OOM killer
- en: Let's first cover a few background details regarding kernel memory management,
    particularly the reclaiming of free memory. This will put you in a position to
    understand what the kernel *OOM killer *component is, how to work with it, and
    even how to deliberately invoke it.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先介绍一些关于内核内存管理的背景细节，特别是有关回收空闲内存的内容。这将使您能够理解内核*OOM killer*组件是什么，如何与它一起工作，甚至如何故意调用它。
- en: Reclaiming memory – a kernel housekeeping task and OOM
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回收内存 - 内核的例行公事和OOM
- en: As you will be aware, the kernel tries, for optimal performance, to keep the
    working set of memory pages as high up as possible in the memory pyramid (or hierarchy).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所知，内核会尽量将内存页面的工作集保持在内存金字塔（或层次结构）的最高位置，以实现最佳性能。
- en: 'The so-called memory pyramid (or memory hierarchy) on a system consists of
    (in order, from smallest size but fastest speed to largest size but slowest):
    CPU registers, CPU caches (LI, L2, L3, ...), RAM, and swap (raw disk/flash/SSD
    partition). In our following discussion, we ignore CPU registers as their size
    is minuscule.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 系统上所谓的内存金字塔（或内存层次结构）包括（按顺序，从最小但速度最快到最大但速度最慢）：CPU寄存器、CPU缓存（L1、L2、L3...）、RAM和交换空间（原始磁盘/闪存/SSD分区）。在我们的后续讨论中，我们忽略CPU寄存器，因为它们的大小微不足道。
- en: So, the processor uses its hardware caches (L1, L2, and so on) to hold the working
    set of pages. But of course, CPU cache memory is very limited, thus it will soon
    run out, causing the memory to spill over into the next hierarchical level – RAM.
    On modern systems, even many embedded ones, there's quite a bit of RAM; still,
    if and when the OS does run low on RAM, it spills over the memory pages that can
    no longer fit in RAM into a raw disk partition – *swap*. Thus the system continues
    to work well, albeit at a significant performance cost once swap is (often) used.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，处理器使用其硬件缓存（L1、L2等）来保存页面的工作集。但当然，CPU缓存内存非常有限，因此很快就会用完，导致内存溢出到下一个分层级别 - RAM。在现代系统中，甚至是许多嵌入式系统，都有相当多的RAM；但是，如果操作系统的RAM不足，它会将无法放入RAM的内存页面溢出到原始磁盘分区
    - *交换空间*。因此，系统继续正常工作，尽管一旦使用交换空间，性能成本就会显著增加。
- en: The Linux kernel, in an effort to ensure that a given minimum amount of free
    memory pages are available at all times within RAM, continually performs background
    page reclamation work – indeed, you can think of this as routine housekeeping.
    Who actually performs this work? The `kswapd`kernel thread(s) are continually
    monitoring memory usage on the system and invoke a page reclaim mechanism when
    they sense that memory is running low.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保RAM中始终有一定数量的空闲内存页面可用，Linux内核不断进行后台页面回收工作 - 实际上，您可以将其视为例行公事。谁实际执行这项工作？`kswapd`内核线程不断监视系统上的内存使用情况，并在它们感觉到内存不足时调用页面回收机制。
- en: This page reclamation work is done on a per *node:zone* basis. The kernel uses
    so-called *watermark levels* – min, low, and high – per *node:zone* to determine
    when to reclaim memory pages in an intelligent fashion. You can always look up `/proc/zoneinfo` to
    see the current watermark levels. (Note that the unit of watermark levels is pages.)
    Also, as we mentioned earlier, caches are typically the first victims and are
    shrunk down as memory pressure increases.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这项页面回收工作是基于每个*节点:区域*的基础进行的。内核使用所谓的*水印级别* - 最小、低和高 - 每个*节点:区域*来智能地确定何时回收内存页面。您可以随时查看`/proc/zoneinfo`以查看当前的水印级别。（请注意，水印级别的单位是页面。）此外，正如我们之前提到的，缓存通常是第一个受害者，并且在内存压力增加时会被缩小。
- en: 'But let''s play devil''s advocate: what if all of this memory reclamation work
    doesn''t help, and memory pressure keeps increasing to the point where the complete
    memory pyramid is exhausted, where a kernel allocation of even a few pages fails
    (or infinitely retries, which, frankly, is just as useless, perhaps worse)? What
    if all CPU caches, RAM, and swap are (almost completely) full!? Well, most systems
    just die at this point (actually, they don''t die, they just become so slow that
    it appears as though they''re permanently hung). The Linux kernel, though, being
    Linux, tends to be aggressive in these situations; it invokes a component aptly
    named the OOM killer*. *The OOM killer''s job – you guessed it! – is to identify
    and summarily kill the memory-hogger process (by sending it the fatal `SIGKILL` signal;
    it could even end up killing a whole bunch of processes).'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 但让我们假设反面：如果所有这些内存回收工作都没有帮助，内存压力继续增加，直到完整的内存金字塔耗尽，即使是几页的内核分配也失败（或者无限重试，坦率地说，这也是无用的，也许更糟糕）？如果所有CPU缓存、RAM和交换空间（几乎完全）都满了呢？嗯，大多数系统在这一点上就死了（实际上，它们并没有死，它们只是变得非常慢，看起来好像它们永远挂起）。然而，作为Linux的Linux内核在这些情况下往往是积极的；它调用一个名为OOM
    killer的组件。OOM killer的工作 - 你猜对了！ - 是识别并立即杀死内存占用进程（通过发送致命的SIGKILL信号；它甚至可能会杀死一大堆进程）。
- en: As you might imagine, it has had its fair share of controversy. Early versions
    of the OOM killer have been (quite rightly) criticized. Recent versions use superior
    heuristics that work quite well.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您可能想象的那样，它也经历了自己的争议。早期版本的OOM killer已经（完全正确地）受到了批评。最近的版本使用了更好的启发式方法，效果相当不错。
- en: You can find more information on the improved OOM killer work (the kick-in strategy
    and the OOM reaper thread) in this LWN article (December 2015): *Toward more predictable
    and reliable out-of-memory handling:* [https://lwn.net/Articles/668126/](https://lwn.net/Articles/668126/).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此LWN文章（2015年12月）中找到有关改进的OOM killer工作（启动策略和OOM reaper线程）的更多信息：*Towards more
    predictable and reliable out-of-memory handling:* [https://lwn.net/Articles/668126/](https://lwn.net/Articles/668126/)。
- en: Deliberately invoking the OOM killer
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 故意调用OOM killer
- en: To test the kernel OOM killer, we shall have to put enormous memory pressure
    on the system. Thus, the kernel will unleash its weapon – the OOM killer, which,
    once invoked, will identify and kill some process (or processes). Hence, obviously,
    I highly recommend you try out stuff like this on a safe isolated system, preferably
    a test Linux VM (with no important data on it).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试内核OOM killer，我们必须对系统施加巨大的内存压力。因此，内核将释放其武器 - OOM killer，一旦被调用，将识别并杀死一些进程。因此，显然，我强烈建议您在一个安全的隔离系统上尝试这样的东西，最好是一个测试Linux
    VM（上面没有重要数据）。
- en: Invoking the OOM killer via Magic SysRq
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过Magic SysRq调用OOM killer
- en: The kernel provides an interesting feature dubbed *Magic SysRq*:essentially,
    certain keyboard key combinations (or accelerators) result in a callback to some
    kernel code. For example, assuming it's enabled, pressing the `Alt-SysRq-b` key
    combination on an x86[_64] system results in a cold reboot! Take care, don't just
    type anything, do read the relevant documentation here: [https://www.kernel.org/doc/Documentation/admin-guide/sysrq.rst](https://www.kernel.org/doc/Documentation/admin-guide/sysrq.rst).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 内核提供了一个有趣的功能，称为Magic SysRq：基本上，某些键盘组合（或加速器）会导致回调到一些内核代码。例如，假设它已启用，在x86[_64]系统上按下`Alt-SysRq-b`组合键将导致冷启动！小心，不要随便输入任何内容，确保阅读相关文档：[https://www.kernel.org/doc/Documentation/admin-guide/sysrq.rst](https://www.kernel.org/doc/Documentation/admin-guide/sysrq.rst)。
- en: 'Let''s try some interesting things; we run the following on our Fedora Linux
    VM:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一些有趣的事情；我们在我们的Fedora Linux VM上运行以下命令：
- en: '[PRE38]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This shows that the Magic SysRq feature is partially enabled (the kernel documentation
    mentioned at the start of this section gives the details). To fully enable it,
    we run the following:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明Magic SysRq功能部分启用（本节开头提到的内核文档给出了详细信息）。要完全启用它，我们运行以下命令：
- en: '[PRE39]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Okay, so to get to the point here: you can use Magic SysRq to invoke the OOM
    killer!'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，为了到达这里的要点：您可以使用Magic SysRq来调用OOM killer！
- en: Careful! Invoking the OOM killer, via Magic SysRq or otherwise, *will* cause
    some process – typically the *heavy* one(s) – to unconditionally die!
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 小心！通过Magic SysRq或其他方式调用OOM killer *将*导致一些进程 - 通常是*重*进程 - 无条件死亡！
- en: 'How? As root, just type the following:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 如何？以root身份，只需输入以下内容：
- en: '[PRE40]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Look up the kernel log to see whether anything interesting occurred!
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 查看内核日志，看看是否发生了什么有趣的事情！
- en: Invoking the OOM killer with a crazy allocator program
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过一个疯狂的分配器程序调用OOM killer
- en: We'll also demonstrate in the following section a more hands-on and interesting
    way by which you can (most probably) invite the OOM killer in. Write a simple
    user space C program that behaves as a crazy allocator, performing (typically)
    tens of thousands of memory allocations, writing something to each page, and,
    of course, never freeing up the memory, thus putting tremendous pressure on memory
    resources.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们还将演示一种更加实用和有趣的方式，通过这种方式，您可以（很可能）邀请OOM killer。编写一个简单的用户空间C程序，作为一个疯狂的分配器，执行（通常）成千上万的内存分配，向每个页面写入一些内容，当然，永远不释放内存，从而对内存资源施加巨大压力。
- en: 'As usual, we show only the most relevant parts of the source code in the following
    snippet; please refer to and clone the book''s GitHub repo for the full code;
    remember, this is a user-mode app not a kernel module:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们在以下片段中只显示源代码的最相关部分；请参考并克隆本书的GitHub存储库以获取完整的代码；请记住，这是一个用户模式应用程序，而不是内核模块：
- en: '[PRE41]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'In the following code block, we show some output obtained when running our
    *crazy allocator* program on an x86_64 Fedora 31 VM running our custom 5.4.0 Linux
    kernel:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码块中，我们展示了在x86_64 Fedora 31 VM上运行我们的自定义5.4.0 Linux内核的*crazy allocator*程序时获得的一些输出：
- en: '[PRE42]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The `Killed` message is the giveaway! The user mode process has been killed
    by the kernel. The reason becomes obvious once we glance at the kernel log – it's
    the OOM killer, of course (we show the kernel log in the *Demand paging and OOM* section).
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '`Killed`消息是一个线索！用户模式进程已被内核终止。一旦我们瞥一眼内核日志，原因就显而易见了——当然是OOM杀手（我们在*需求分页和OOM*部分展示了内核日志）。'
- en: Understanding the rationale behind the OOM killer
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解OOM杀手背后的原理
- en: 'Glance at the preceding output of our `oom_killer_try` app: (in this particular
    run) 33 periods (`.`) appear before the dreaded `Killed` message. In our code,
    we emit a `.` (via `printf`) every 5,000 times we make an allocation (of 2 pages
    or 8 KB). Thus, here, we have 33 times 5 periods, meaning 33 * 5 = 165 times =>
    165 * 5000 * 8K ~= 6,445 MB. Thus, we can conclude that, after our process (virtually)
    allocated approximately 6,445 MB (~ 6.29 GB) of memory, the OOM killer terminated
    our process! You now need to understand why this occurred at this particular number.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 瞥一眼我们的`oom_killer_try`应用程序的前面输出：（在这次运行中）出现了33个周期（`.`）之后是可怕的`Killed`消息。在我们的代码中，我们每次分配（2页或8KB）时发出一个`.`（通过`printf`）。因此，在这里，我们有33次5个周期，意味着33
    * 5 = 165次=> 165 * 5000 * 8K ~= 6,445MB。因此，我们可以得出结论，我们的进程（虚拟地）分配了大约6,445MB（约6.29GB）的内存后，OOM杀手终止了我们的进程！现在您需要理解为什么会在这个特定的数字发生这种情况。
- en: On this particular Fedora Linux VM, the RAM is 2 GB *and the* *swap space *is
    2 GB; thus, the total available memory in the *memory* *pyramid* = (CPU caches
    +) RAM + swap.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的Fedora Linux VM上，RAM为2GB，交换空间为2GB；因此，在*内存金字塔*中，总可用内存=（CPU缓存+）RAM + 交换空间。
- en: 'This is 4 GB (to keep it simple, let''s just ignore the fairly insignificant
    amount of memory within the CPU caches). But then, it begs the question, why didn''t
    the kernel invoke the OOM killer at the 4 GB point (or lower)? Why only at around
    6 GB?  This is an interesting point: the Linux kernel follows a **VM overcommit** policy,
    deliberately over-committing memory (to a certain extent). To understand this,
    see the current `vm.overcommit` setting:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这是4GB（为了简单起见，让我们忽略CPU缓存中的相当微不足道的内存量）。但是，这就引出了一个问题，为什么内核在4GB点（或更低）没有调用OOM杀手呢？为什么只在大约6GB时？这是一个有趣的观点：Linux内核遵循**VM过度承诺**策略，故意过度承诺内存（在一定程度上）。要理解这一点，请查看当前的`vm.overcommit`设置：
- en: '[PRE43]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This is indeed the default (`0`). The permissible values (settable only by
    root) are as follows:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实是默认值（`0`）。可设置的值（仅由root设置）如下：
- en: '`0`: Allow memory overcommitting using a heuristic algorithm (see more in the
    following section); *the default.*'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`：允许使用启发式算法进行内存过度承诺；*默认设置*。'
- en: '`1`: Always overcommit; in other words, never refuse any `malloc(3)`; useful
    for some types of scientific apps that use sparse memory.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`1`：总是过度承诺；换句话说，从不拒绝任何`malloc(3)`；对于某些使用稀疏内存的科学应用程序很有用。'
- en: '`2`: The following notes are direct quotes from the kernel documentation ([https://www.kernel.org/doc/html/v4.18/vm/overcommit-accounting.html#overcommit-accounting](https://www.kernel.org/doc/html/v4.18/vm/overcommit-accounting.html#overcommit-accounting)):'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`2`：以下注释直接引用自内核文档（[https://www.kernel.org/doc/html/v4.18/vm/overcommit-accounting.html#overcommit-accounting](https://www.kernel.org/doc/html/v4.18/vm/overcommit-accounting.html#overcommit-accounting)）：'
- en: '*"Don''t overcommit. The total address space commit for the system is not permitted
    to exceed swap plus a configurable amount (default is 50%) of physical RAM. Depending
    on the amount you use, in most situations this means a process will not be killed
    while accessing pages but will receive errors on memory allocation as appropriate.
    Useful for applications that want to guarantee their memory allocations will be
    available in the future without having to initialize every page"*'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '*"不要过度承诺。系统的总地址空间承诺不得超过交换空间加上可配置数量（默认为物理RAM的50%）。根据您使用的数量，在大多数情况下，这意味着进程在访问页面时不会被终止，但将在适当的内存分配错误时收到错误。适用于希望保证其内存分配将来可用而无需初始化每个页面的应用程序"*'
- en: 'The overcommit extent is determined by the overcommit ratio:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 过度承诺程度由过度承诺比率确定：
- en: '[PRE44]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We'll examine two cases in the following sections.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在以下部分中检查两种情况。
- en: Case 1 – vm.overcommit set to 2, overcommit turned off
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 情况1——vm.overcommit设置为2，关闭过度承诺
- en: 'Firstly, remember, this is *not *the default. With the `overcommit_memory` tunable
    set to `2`, the formula used to calculate the total (possibly overcommitted) available
    memory is as follows:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请记住，这*不是*默认设置。当`tunable`设置为`2`时，用于计算总（可能过度承诺的）可用内存的公式如下：
- en: '*Total available memory = (RAM + swap) * (overcommit_ratio/100);   *'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '*总可用内存=（RAM + 交换空间）*（过度承诺比率/100）;*'
- en: This formula only applies when `vm.overcommit == 2`.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式仅适用于`vm.overcommit == 2`时。
- en: 'On our Fedora 31 VM, with `vm.overcommit == 2` and 2 GB each of RAM and swap,
    this yields the following (in gigabytes):'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的Fedora 31 VM上，`vm.overcommit == 2`，RAM和交换空间各为2GB，这将产生以下结果（以GB为单位）：
- en: '*Total available memory = (2 + 2) * (50/100) = 4 * 0.5 = 2 GB*'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '*总可用内存=（2 + 2）*（50/100）= 4 * 0.5 = 2GB*'
- en: This value – the (over)commit limit – is also seen in `/proc/meminfo` as the `CommitLimit` field.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 这个值——（过度）承诺限制——也可以在`/proc/meminfo`中看到，作为`CommitLimit`字段。
- en: Case 2 – vm.overcommit set to 0, overcommit on, the default
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 情况2——vm.overcommit设置为0，过度承诺开启，为默认设置
- en: 'This *is* the default. `vm.overcommit` is set to `0` (not `2`): with this,
    the kernel effectively calculates the total (over)committed memory size as follows:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 这*是*默认设置。`vm.overcommit`设置为`0`（而不是`2`）：使用此设置，内核有效地计算总（过度）承诺的内存大小如下：
- en: '*Total available memory = (RAM + swap) * (overcommit_ratio + 100)%;   *'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '*总可用内存=（RAM + 交换空间）*（过度承诺比率+100）%;*'
- en: This formula only applies when `vm.overcommit == 0`.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式仅适用于`vm.overcommit == 0`时。
- en: 'On our Fedora 31 VM, with `vm.overcommit == 0` and 2 GB each of RAM and swap,
    this formula yields the following (in gigabytes):'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的Fedora 31 VM上，`vm.overcommit == 0`，RAM和交换空间各为2GB，这个公式将产生以下结果（以GB为单位）：
- en: '*Total available memory = (2 + 2) * (50+100)% = 4 * 150% = 6 GB*'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '*总可用内存=（2 + 2）*（50+100）% = 4 * 150% = 6GB*'
- en: 'So the system effectively pretends that there is a grand total of 6 GB of memory
    available. So now we understand: when our `oom_killer_try` process allocated huge
    amounts of memory and this limit (6 GB) was exceeded, the OOM killer jumped in!'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，系统有效地*假装*有总共6GB的内存可用。现在我们明白了：当我们的`oom_killer_try`进程分配了大量内存并且超出了这个限制（6GB）时，OOM
    killer就会介入！
- en: We now understand that the kernel provides several VM overcommit tunables under `/proc/sys/vm`,
    allowing the system administrator (or root) to fine-tune it (including switching
    it off by setting `vm.overcommit` to the value `2`). At first glance, it may appear
    tempting to do so, to simply turn it off. Do pause though and think it through;
    leaving the VM overcommit at the kernel defaults is best on most workloads.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在明白，内核在`/proc/sys/vm`下提供了几个VM过度承诺的可调参数，允许系统管理员（或root）对其进行微调（包括通过将`vm.overcommit`设置为值`2`来关闭它）。乍一看，关闭它似乎很诱人。不过，请暂停一下，仔细考虑一下；在大多数工作负载上，保持内核默认的VM过度承诺是最好的。
- en: (For example, setting the `vm.overcommit` value to `2` on my Fedora 31 guest
    VM caused the effective available memory to change to just 2 GB. The typical memory
    usage, especially with the GUI running, far exceeded this, causing the system
    to be unable to even log in the user in GUI mode!) The following links help throw
    more light on the subject: Linux kernel documentation: [https://www.kernel.org/doc/Documenta](https://www.kernel.org/doc/Documentation/vm/overcommit-accounting)[tion/vm/overcommit-accounting](https://www.kernel.org/doc/Documentation/vm/overcommit-accounting) and
    *What are the disadvantages of disabling memory overcommit in Linux?* : [https://www.quora.com/What-are-the-disadvantages-of-disabling-memory-overcommit-in-Linux](https://www.quora.com/What-are-the-disadvantages-of-disabling-memory-overcommit-in-Linux) . (Do
    see the *Further reading *section for more.)
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我的Fedora 31客户端VM上将`vm.overcommit`值设置为`2`会导致有效可用内存变为只有2GB。典型的内存使用，特别是在GUI运行时，远远超过了这个值，导致系统甚至无法在GUI模式下登录用户！以下链接有助于更好地了解这个主题：Linux内核文档：[https://www.kernel.org/doc/Documentation/vm/overcommit-accounting](https://www.kernel.org/doc/Documentation/vm/overcommit-accounting)和*在Linux中禁用内存过度承诺的缺点是什么？*：[https://www.quora.com/What-are-the-disadvantages-of-disabling-memory-overcommit-in-Linux](https://www.quora.com/What-are-the-disadvantages-of-disabling-memory-overcommit-in-Linux)。（请查看*更多阅读*部分了解更多信息。）
- en: Demand paging and OOM
  id: totrans-384
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 需求分页和OOM
- en: 'Recall the really important fact we learned earlier in the chapter, in the
    *A brief note on memory allocations and demand paging *section: because of the demand
    paging (or lazy allocation) policy that the OS uses, when a memory page is allocated
    by `malloc(3)`(and friends), it only actually causes virtual memory space to be
    reserved in a region of the process VAS; no physical memory is allocated at this
    time. Only when you perform some action on any byte(s) of the virtual page – a
    read, write, or execute – does the MMU raise a page fault (a minor fault) and
    the OS''s page fault handler runs as a result. If it deems that this memory access
    is legal, it allocates a physical frame (via the page allocator).'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们在本章早些时候学到的真正重要的事实，在*内存分配和需求分页简要说明*部分：由于操作系统使用的需求分页（或延迟分配）策略，当通过`malloc(3)`（和其他函数）分配内存页面时，实际上只会在进程VAS的某个区域保留虚拟内存空间，此时并不会分配物理内存。只有当你对虚拟页面的任何字节执行某些操作
    - 读取、写入或执行 - 时，MMU才会引发页面错误（一个*次要*错误），并且操作系统的页面错误处理程序会相应地运行。如果它认为这个内存访问是合法的，它会通过页面分配器分配一个物理帧。
- en: 'In our simple `oom_killer_try` app, we manipulate this very idea via it''s
    third parameter, `force_page_fault`: when set as `1`, we emulate precisely this
    situation by writing something, anything really, into a byte - any byte - of each
    of the two pages allocated per loop iteration (peek at the code again if you need
    to).'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们简单的`oom_killer_try`应用程序中，我们通过它的第三个参数`force_page_fault`来操纵这个想法：当设置为`1`时，我们通过在每个循环迭代中写入任何东西来精确模拟这种情况（如果需要，请再次查看代码）。
- en: 'So, now that you know this, let''s re-run our app with the third parameter, `force_page_fault`,
    set to `1`, to indeed force page faults! Here''s the output that resulted when
    I ran this on my Fedora 31 VM (on our custom 5.4.0 kernel):'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在你知道了这一点，让我们将我们的应用程序重新运行，将第三个参数`force_page_fault`设置为`1`，确实强制发生页面错误！这是我在我的Fedora
    31 VM上运行此操作（在我们自定义的5.4.0内核上）时产生的输出：
- en: '[PRE45]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This time, you can literally feel the system struggle as it fights for memory.
    This time, it runs out of memory much sooner *as actual physical memory was allocated*. (From
    the preceding output, we see in this particular case 15 x 5 + 1 dots (`. `or periods);
    that is, 15 times 5 dots + 1 dot => = 76 times => 76 * 5000 loop iterations *
    8K per iteration ~= 2969 MB virtually *and physically* allocated!)
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，你可以真切地感觉到系统在为内存而奋斗。这一次，它很快就耗尽了内存，*实际物理内存已经分配*。（从前面的输出中，我们在这个特定情况下看到了15 x
    5 + 1个点（`. `或句号）；也就是说，15乘以5个点加1个点=> = 76次=> 76 * 5000个循环迭代* 8K每次迭代~= 2969 MB虚拟*和物理*分配！）
- en: 'Apparently, at this point, one of two things occurred:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，在这一点上，发生了两件事中的一件：
- en: The system ran out of both RAM and swap, thus failing to allocate a page and
    thus inviting the OOM killer in.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统的RAM和交换空间都用完了，因此无法分配页面，从而引发了OOM killer。
- en: The calculated (artificial) kernel VM commit limit was exceeded.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算出的（人为的）内核VM提交限制已超出。
- en: 'We can easily look up this kernel VM commit value (again on the Fedora 31 VM
    where I ran this):'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松查找这个内核VM提交值（再次在我运行此操作的Fedora 31 VM上）：
- en: '[PRE46]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This works out to about 3,108 MB (well over our calculation of 2,969 MB). So
    here, it's likely that with all the RAM and swap space being used to run the GUI
    and existing apps, the first case came into play.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当于约3108 MB（远远超过我们计算的2969 MB）。因此，在这种情况下，很可能是由于所有的RAM和交换空间都被用来运行GUI和现有的应用程序，第一种情况发生了。
- en: Also notice how, before running our program, the amount of memory used by the
    larger system caches (the page and buffer caches) is significant. The column entitled
    `buff/cache` in the output of the `free(1)` utility shows this. Before running
    our crazy allocator app, 866 MB out of 2 GB was being used for the page cache.
    Once our program runs, though, it applies so much memory pressure on the OS that
    tremendous amounts of swapping – the paging out of RAM pages to the raw disk partition
    called "swap" – is performed and literally all caches are freed up. Inevitably
    (as we refuse to free any memory), the OOM killer jumps in and kills us, causing
    large amounts of memory to be reclaimed. The free memory and the cache usage right
    after the OOM killer cleans up are 1.5 GB and 192 MB respectively. (The cache
    usage right now is low; it will increase as the system runs.)
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，在运行我们的程序之前，较大系统缓存（页面和缓冲缓存）使用的内存量是相当可观的。`free(1)`实用程序的输出中的名为`buff/cache`的列显示了这一点。在运行我们疯狂的分配器应用程序之前，2GB中的866MB用于页面缓存。然而，一旦我们的程序运行，它对操作系统施加了如此大的内存压力，以至于大量的交换
    - 将RAM页面换出到名为“swap”的原始磁盘分区 - 被执行，并且所有缓存都被释放。不可避免地（因为我们拒绝释放任何内存），OOM killer介入并杀死我们，导致大量内存被回收。OOM
    killer清理后的空闲内存和缓存使用量分别为1.5GB和192MB。（当前缓存使用量较低；随着系统运行，它将增加。）
- en: 'Looking up the kernel log reveals that indeed, the OOM killer has paid us a
    visit! Note that the following partial screenshot shows only the stack dump on
    the x86_64 Fedora 31 VM running the 5.4.0 kernel:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 查看内核日志后，确实发现OOM killer来过了！请注意，以下部分截图仅显示了在运行5.4.0内核的x86_64 Fedora 31虚拟机上的堆栈转储：
- en: '![](img/f228f9f4-25a4-498f-af3d-fb4ae0d50eca.png)'
  id: totrans-398
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f228f9f4-25a4-498f-af3d-fb4ae0d50eca.png)'
- en: Figure 9.8 – The kernel log after the OOM killer, showing the kernel call stack
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 - OOM killer后的内核日志，显示内核调用堆栈
- en: 'Read the kernel-mode stack in *Figure 9.8* in a bottom-up fashion (ignoring
    the frames that start with `?`): clearly, a page fault occurred; you can see the
    call frames: `page_fault()` | `do_page_fault()` | `[ ... ]` | `__hande_mm_fault()` | `__do_fault()` | `[
    ... ]` | `__alloc_pages_nodemask()` .'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 以自下而上的方式阅读*图9.8*中的内核模式堆栈（忽略以`?`开头的帧）：显然，发生了页错误；您可以看到调用帧：`page_fault()` | `do_page_fault()`
    | `[ ... ]` | `__hande_mm_fault()` | `__do_fault()` | `[ ... ]` | `__alloc_pages_nodemask()`。
- en: Think about it, this is completely normal: the fault was raised by the MMU as
    it was trying to service a virtual page with no physical counterpart. The OS's
    fault handling code runs (in process context, implying that `current` runs its
    code!); it ultimately leads to the OS invoking the page allocator routine's `__alloc_pages_nodemask()` function,
    which as we learned earlier is literally the heart of the zoned buddy system (or
    page) allocator – the engine of memory allocation!
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 想一想，这是完全正常的：MMU在尝试为没有物理对应物的虚拟页面提供服务时引发了错误。操作系统的错误处理代码运行（在进程上下文中，意味着`current`运行其代码！）；最终导致操作系统调用页面分配器例程的`__alloc_pages_nodemask()`函数，正如我们之前所了解的，这实际上是分区伙伴系统（或页面）分配器的核心
    - 内存分配的引擎！
- en: What isn't normal, is that this time it (the `__alloc_pages_nodemask()` function) failed!
    This is deemed a critical issue and  caused the OS to invoke the OOM killer (you
    can see the `out_of_memory` call frame in the preceding figure).
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 不正常的是，这一次它（`__alloc_pages_nodemask()`函数）失败了！这被认为是一个关键问题，导致操作系统调用OOM killer（您可以在前面的图中看到`out_of_memory`调用帧）。
- en: 'Toward the latter part of its diagnostic dump, the kernel tries hard to justify
    its reason for killing a given process. It shows a table of all threads, their
    memory usage (and various other statistics). Actually, these statistics being
    displayed occurs due to `sysctl : /proc/sys/vm/oom_dump_tasks`being on (`1`) by
    default. Here''s a sampling (in the following output, we have eliminated the leftmost
    timestamp column of `dmesg` to make the data more readable):'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 在诊断转储的后期，内核努力为杀死给定进程提供理由。它显示了所有线程的表格，它们的内存使用情况（以及各种其他统计数据）。实际上，由于`sysctl：/proc/sys/vm/oom_dump_tasks`默认为`1`，这些统计数据被显示出来。以下是一个示例（在以下输出中，我们已经删除了`dmesg`的最左边的时间戳列，以使数据更易读）：
- en: '[PRE47]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In the preceding output, we have highlighted in bold the `rss` (*Resident Set
    Size*) column as it's a good indication of physical memory usage by the process
    in question (the unit is KB). Clearly, our `oom_killer_try` process is using an
    enormous amount of physical memory. Also, notice how its number of swap entries
    (`swapents`) is very high. Modern kernels (4.6 onward) use a specialized `oom_reaper` kernel
    thread to perform the work of reaping (killing) the victim process (the last line
    of the preceding output shows that this kernel thread reaped our wonderful `oom_killer_try`
    process!). Interestingly, the Linux kernel's OOM can be thought of as a (last)
    defense against fork bombs and similar **(Distributed) Denial of Service** (**(D)DoS**)
    attacks.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述输出中，我们已经用粗体突出显示了`rss`（*Resident Set Size*）列，因为这是对所讨论进程的物理内存使用的良好指示（单位为KB）。显然，我们的`oom_killer_try`进程使用了大量的物理内存。还要注意它的交换条目（`swapents`）数量非常高。现代内核（4.6及更高版本）使用专门的`oom_reaper`内核线程来执行收割（杀死）受害进程的工作（上述输出的最后一行显示了这个内核线程收割了我们美妙的`oom_killer_try`进程！）。有趣的是，Linux内核的OOM可以被认为是（最后的）防御措施，用来防止分叉炸弹和类似的（分布式）拒绝服务（D）DoS攻击。
- en: Understanding the OOM score
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解OOM分数
- en: In order to speed up the discovery of what the memory-hogging process is at
    crunch time (when the OOM killer is invoked), the kernel assigns and maintains
    an *OOM score* on a per-process basis (you can always look up the value in the `/proc/<pid>/oom_score` pseudo-file).
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加快在关键时刻（当OOM killer被调用时）发现内存占用过多的进程，内核会为每个进程分配和维护一个*OOM分数*（您可以随时在`/proc/<pid>/oom_score`伪文件中查找该值）。
- en: 'The OOM score range is `0` to `1000`:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: OOM分数范围是`0`到`1000`：
- en: An OOM score of `0` implies that the process is not using any memory available
    to it
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OOM分数为`0`意味着该进程没有使用任何可用内存
- en: An OOM score of `1000` implies the process is using 100 percent of the memory
    available to it
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OOM分数为`1000`意味着该进程使用了其可用内存的100％
- en: 'Obviously, the process with the highest OOM score wins. Its reward – it is
    instantly killed by the OOM killer (talk about dry humor). Not so fast though:
    the kernel has heuristics to protect important tasks. For example, the baked-in
    heuristics imply that the OOM killer will not select as its victim any root-owned
    process, a kernel thread, or a task that has a hardware device open.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，具有最高OOM分数的进程获胜。它的奖励-它会被OOM killer立即杀死（说到干燥的幽默）。不过，内核有启发式方法来保护重要任务。例如，内置的启发式方法意味着OOM
    killer不会选择任何属于root的进程、内核线程或具有硬件设备打开的任务作为其受害者。
- en: 'What if we would like to ensure that a certain process will *never be killed*
    by the OOM killer? It''s quite possible to do so, though it does require root
    access. The kernel provides a tunable, `/proc/<pid>/oom_score_adj`, an OOM adjustment
    value (with the default being `0`). The *net *OOM score is the sum of the `oom_score` value and
    the adjustment value:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想确保某个进程永远不会被OOM killer杀死怎么办？虽然需要root访问权限，但这是完全可能的。内核提供了一个可调节的`/proc/<pid>/oom_score_adj`，即OOM调整值（默认为`0`）。*net*
    OOM分数是`oom_score`值和调整值的总和：
- en: '[PRE48]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Thus, setting the `oom_score_adj` value of a process to `1000` pretty much guarantees
    that it will be killed, whereas setting it to `-1000` has exactly the opposite
    effect – it will never be selected as a victim.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将进程的`oom_score_adj`值设置为`1000`几乎可以保证它会被杀死，而将其设置为`-1000`则产生完全相反的效果-它永远不会被选为受害者。
- en: 'A quick way to query (and even set) a process''s OOM score (as well as it''s
    OOM adjustment value) is via the `choom(1)` utility. For example, to query the
    OOM score and OOM adjustment value of the systemd process, just do `choom -p 1`.
    We did the obvious thing - wrote a simple script (that internally uses `choom(1)`)
    to query the OOM score of all processes currently alive on the system (it''s here:
    `ch9/query_process_oom.sh`; do try it out on your box). Quick tip: the (ten) processes
    with the highest OOM score on the system can quickly be seen with (the third column
    is the net OOM score):'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 快速查询（甚至设置）进程的OOM分数（以及OOM调整值）的方法是使用`choom(1)`实用程序。例如，要查询systemd进程的OOM分数和OOM调整值，只需执行`choom
    -p 1`。我们做了显而易见的事情-编写了一个简单的脚本（内部使用`choom(1)`）来查询系统上当前所有进程的OOM分数（在这里：`ch9/query_process_oom.sh`；请在您的系统上尝试一下）。快速提示：系统上OOM分数最高的（十个）进程可以通过以下方式快速查看（第三列是net
    OOM分数）：
- en: '[PRE49]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: With this, we conclude this section and indeed this chapter.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 随此，我们结束了本节，也结束了本章。
- en: Summary
  id: totrans-418
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we continued where we left off in the previous chapter. We
    covered, in a good amount of detail, how you can create and use your own custom
    slab caches (useful when your driver or module very frequently allocates and frees
    a certain data structure), and how to use some kernel infrastructure to help you
    debug slab (SLUB) memory issues. We then learned about and used the kernel `vmalloc`
    APIs (and friends), including how to set up given memory protections on memory
    pages. With the wealth of memory APIs and strategies available to you, how do
    you select which one to use in a given situation? We covered this important concern
    with a useful *decision chart* and table. Finally, we delved into understanding
    what exactly the kernel's *OOM killer *component is and how to work with it.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们延续了上一章的内容。我们详细介绍了如何创建和使用自定义的slab缓存（在您的驱动程序或模块非常频繁地分配和释放某个数据结构时非常有用），以及如何使用一些内核基础设施来帮助您调试slab（SLUB）内存问题。然后，我们了解并使用了内核的`vmalloc`
    API（和相关内容），包括如何在内存页面上设置给定的内存保护。有了丰富的内存API和可用的策略，您如何选择在特定情况下使用哪一个呢？我们通过一个有用的*决策图*和表格来解决了这个重要问题。最后，我们深入了解了内核的*OOM
    killer*组件以及如何与其一起工作。
- en: As I have mentioned before, sufficiently deep knowledge of the Linux memory
    management internals and exported API set will go a long way in helping you as
    a kernel module and/or device driver author. The reality, as we well know, is
    that a significant amount of time is spent by developers on troubleshooting and
    debugging code; the intricate knowledge and skills gained here will help you better
    navigate these mazes.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，对Linux内存管理内部和导出API集的深入了解将对您作为内核模块和/或设备驱动程序作者有很大帮助。事实是，我们都知道，开发人员花费了大量时间在故障排除和调试代码上；在这里获得的复杂知识和技能将帮助您更好地应对这些困难。
- en: This completes the explicit coverage of Linux kernel memory management in this
    book. Though we have covered many areas, we have also left out or only skimmed
    over some of them.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了本书对Linux内核内存管理的明确覆盖。尽管我们涵盖了许多领域，但也留下或只是粗略地涉及了一些领域。
- en: The fact is that Linux memory management is a huge and complex topic, well worth
    understanding for the purposes of learning, writing more efficient code, and debugging
    complex situations.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，Linux内存管理是一个庞大而复杂的主题，值得为了学习、编写更高效的代码和调试复杂情况而加以理解。
- en: Learning the (basic) usage of the powerful `crash(1)` utility (used to look
    deep within the kernel, via either a live session or a kernel dumpfile), and then
    re-looking at this and the previous chapter's content armed with this knowledge
    is indeed a powerful way to learn!
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 学习强大的`crash(1)`实用程序的（基本）用法（用于深入查看内核，通过实时会话或内核转储文件），然后利用这些知识重新查看本章和上一章的内容，确实是一种强大的学习方式！
- en: Great job on having covered Linux memory management! The next two chapters will
    have you learning about another core OS topic – how *CPU scheduling* is performed
    on the Linux OS. Take a breather, work on the following assignments and questions,
    and browse through the *Further reading *materials that capture your interest.
    Then, revitalized, jump into the next exciting area with me!
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成了Linux内存管理的学习之后，接下来的两章将让您了解另一个核心操作系统主题-在Linux操作系统上如何执行*CPU调度*。休息一下，完成以下作业和问题，浏览引起您兴趣的*进一步阅读*材料。然后，精力充沛地跟我一起进入下一个令人兴奋的领域！
- en: Questions
  id: totrans-425
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'As we conclude, here is a list of questions for you to test your knowledge
    regarding this chapter''s material: [https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions).
    You will find some of the questions answered in the book''s GitHub repo: [https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn).'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这里有一些问题供您测试对本章材料的了解：[https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/questions)。您会在本书的GitHub存储库中找到一些问题的答案：[https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn](https://github.com/PacktPublishing/Linux-Kernel-Programming/tree/master/solutions_to_assgn)。
- en: Further reading
  id: totrans-427
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: To help you delve deeper into the subject with useful materials, we provide
    a rather detailed list of online references and links (and at times, even books)
    in a Further reading document in this book's GitHub repository. The *Further reading*
    document is available here: [https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md](https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md).
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助您深入研究这一主题并获取有用的材料，我们在本书的GitHub存储库中提供了一个相当详细的在线参考和链接列表（有时甚至包括书籍）的“进一步阅读”文档。*进一步阅读*文档在这里可用：[https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md](https://github.com/PacktPublishing/Linux-Kernel-Programming/blob/master/Further_Reading.md)。

["```\n#include <linux/dma-mapping.h> \n```", "```\nvoid *dma_alloc_coherent(struct device *dev, size_t size, \n                     dma_addr_t *dma_handle, gfp_t flag)  \n```", "```\nvoid dma_free_coherent(struct device *dev, size_t size, \n                 void *cpu_addr, dma_addr_t dma_handle); \n```", "```\nenum dma_data_direction { \n   DMA_BIDIRECTIONAL = 0, \n   DMA_TO_DEVICE = 1, \n   DMA_FROM_DEVICE = 2, \n   DMA_NONE = 3, \n}; \n```", "```\ndma_addr_t dma_map_single(struct device *dev, void *ptr, \n         size_t size, enum dma_data_direction direction); \n```", "```\nvoid dma_unmap_single(struct device *dev, dma_addr_t dma_addr, \n              size_t size, enum dma_data_direction direction); \n```", "```\nstruct scatterlist { \n   unsigned long page_link; \n   unsigned int offset; \n   unsigned int length; \n   dma_addr_t dma_address; \n   unsigned int dma_length; \n}; \n```", "```\nu32 *wbuf, *wbuf2, *wbuf3; \nwbuf = kzalloc(SDMA_BUF_SIZE, GFP_DMA); \nwbuf2 = kzalloc(SDMA_BUF_SIZE, GFP_DMA); \nwbuf3 = kzalloc(SDMA_BUF_SIZE/2, GFP_DMA); \n\nstruct scatterlist sg[3]; \nsg_init_table(sg, 3); \nsg_set_buf(&sg[0], wbuf, SDMA_BUF_SIZE); \nsg_set_buf(&sg[1], wbuf2, SDMA_BUF_SIZE); \nsg_set_buf(&sg[2], wbuf3, SDMA_BUF_SIZE/2); \nret = dma_map_sg(NULL, sg, 3, DMA_MEM_TO_MEM); \n```", "```\nvoid dma_sync_sg_for_cpu(struct device *dev, \n                         struct scatterlist *sg, \n                         int nents, \n                         enum dma_data_direction direction); \nvoid dma_sync_sg_for_device(struct device *dev, \n                          struct scatterlist *sg, int nents, \n                          enum dma_data_direction direction); \n\nvoid dma_sync_single_for_cpu(struct device *dev, dma_addr_t addr, \n                           size_t size, \n                           enum dma_data_direction dir) \n\nvoid dma_sync_single_for_device(struct device *dev, \n                           dma_addr_t addr, size_t size, \n                           enum dma_data_direction dir) \n```", "```\n<linux/completion.h>  \n```", "```\n DECLARE_COMPLETION(my_comp); \n```", "```\nstruct completion my_comp; \ninit_completion(&my_comp); \n```", "```\nvoid wait_for_completion(struct completion *comp); \n```", "```\nvoid complete(struct completion *comp); \nvoid complete_all(struct completion *comp); \n```", "```\n #include <linux/dmaengine.h> \n```", "```\nstruct dma_chan *dma_request_channel(const dma_cap_mask_t *mask, \n                          dma_filter_fn fn, void *fn_param); \n```", "```\nenum dma_transaction_type { \n    DMA_MEMCPY,     /* Memory to memory copy */ \n    DMA_XOR,        /* Memory to memory XOR*/ \n    DMA_PQ,         /* Memory to memory P+Q computation */ \n    DMA_XOR_VAL,    /* Memory buffer parity check using XOR */ \n    DMA_PQ_VAL,     /* Memory buffer parity check using P+Q */ \n    DMA_INTERRUPT,  /* The device is able to generrate dummy transfer that will generate interrupts */ \n    DMA_SG,         /* Memory to memory scatter gather */ \n    DMA_PRIVATE,    /* channels are not to be used for global memcpy. Usually used with DMA_SLAVE */ \n    DMA_SLAVE,      /* Memory to device transfers */ \n    DMA_CYCLIC,     /* Device is ableto handle cyclic tranfers */ \n    DMA_INTERLEAVE, /* Memoty to memory interleaved transfer */ \n} \n```", "```\ndma_cap_mask my_dma_cap_mask; \nstruct dma_chan *chan; \ndma_cap_zero(my_dma_cap_mask); \ndma_cap_set(DMA_MEMCPY, my_dma_cap_mask); /* Memory to memory copy */ \nchan = dma_request_channel(my_dma_cap_mask, NULL, NULL); \n```", "```\ntypedef bool (*dma_filter_fn)(struct dma_chan *chan, \n                void *filter_param); \n```", "```\nvoid dma_release_channel(struct dma_chan *chan) \n```", "```\nint dmaengine_slave_config(struct dma_chan *chan, \nstruct dma_slave_config *config) \n```", "```\n/* \n * Please refer to the complete description in \n * include/linux/dmaengine.h \n */ \nstruct dma_slave_config { \n   enum dma_transfer_direction direction; \n   phys_addr_t src_addr; \n   phys_addr_t dst_addr; \n   enum dma_slave_buswidth src_addr_width; \n   enum dma_slave_buswidth dst_addr_width; \n   u32 src_maxburst; \n   u32 dst_maxburst; \n   [...] \n}; \n```", "```\n/* dma transfer mode and direction indicator */ \nenum dma_transfer_direction { \n    DMA_MEM_TO_MEM, /* Async/Memcpy mode */ \n    DMA_MEM_TO_DEV, /* From Memory to Device */ \n    DMA_DEV_TO_MEM, /* From Device to Memory */ \n    DMA_DEV_TO_DEV, /* From Device to Device */ \n    [...] \n}; \n```", "```\nenum dma_slave_buswidth { \n        DMA_SLAVE_BUSWIDTH_UNDEFINED = 0, \n        DMA_SLAVE_BUSWIDTH_1_BYTE = 1, \n        DMA_SLAVE_BUSWIDTH_2_BYTES = 2, \n        DMA_SLAVE_BUSWIDTH_3_BYTES = 3, \n        DMA_SLAVE_BUSWIDTH_4_BYTES = 4, \n        DMA_SLAVE_BUSWIDTH_8_BYTES = 8, \n        DMA_SLAVE_BUSWIDTH_16_BYTES = 16, \n        DMA_SLAVE_BUSWIDTH_32_BYTES = 32, \n        DMA_SLAVE_BUSWIDTH_64_BYTES = 64, \n}; \n```", "```\nstruct dma_chan *my_dma_chan; \ndma_addr_t dma_src, dma_dst; \nstruct dma_slave_config my_dma_cfg = {0}; \n\n/* No filter callback, neither filter param */ \nmy_dma_chan = dma_request_channel(my_dma_cap_mask, 0, NULL); \n\n/* scr_addr and dst_addr are ignored in this structure for mem to mem copy */ \nmy_dma_cfg.direction = DMA_MEM_TO_MEM; \nmy_dma_cfg.dst_addr_width = DMA_SLAVE_BUSWIDTH_32_BYTES; \n\ndmaengine_slave_config(my_dma_chan, &my_dma_cfg); \n\nchar *rx_data, *tx_data; \n/* No error check */ \nrx_data = kzalloc(BUFFER_SIZE, GFP_DMA); \ntx_data = kzalloc(BUFFER_SIZE, GFP_DMA); \n\nfeed_data(tx_data); \n\n/* get dma addresses */ \ndma_src_addr = dma_map_single(NULL, tx_data, \nBUFFER_SIZE, DMA_MEM_TO_MEM); \ndma_dst_addr = dma_map_single(NULL, rx_data, \nBUFFER_SIZE, DMA_MEM_TO_MEM); \n```", "```\nstruct dma_device *dma_dev = my_dma_chan->device; \nstruct dma_async_tx_descriptor *tx = NULL; \n\ntx = dma_dev->device_prep_dma_memcpy(my_dma_chan, dma_dst_addr, \ndma_src_addr, BUFFER_SIZE, 0); \n\nif (!tx) { \n    printk(KERN_ERR \"%s: Failed to prepare DMA transfer\\n\", \n               __FUNCTION__); \n    /* dma_unmap_* the buffer */ \n} \n```", "```\nstruct dma_async_tx_descriptor *(*device_prep_dma_memcpy)( \n         struct dma_chan *chan, dma_addr_t dst, dma_addr_t src, \n         size_t len, unsigned long flags) \n```", "```\nstruct dma_async_tx_descriptor *tx = NULL; \ntx = dma_dev->device_prep_dma_memcpy(my_dma_chan, dma_dst_addr, \ndma_src_addr, BUFFER_SIZE, 0); \nif (!tx) { \n    printk(KERN_ERR \"%s: Failed to prepare DMA transfer\\n\", \n               __FUNCTION__); \n    /* dma_unmap_* the buffer */ \n} \n```", "```\ndma_cookie_t dmaengine_submit(struct dma_async_tx_descriptor *desc) \n```", "```\nstruct completion transfer_ok; \ninit_completion(&transfer_ok); \ntx->callback = my_dma_callback; \n\n/* Submit our dma transfer */ \ndma_cookie_t cookie = dmaengine_submit(tx); \n\nif (dma_submit_error(cookie)) { \n    printk(KERN_ERR \"%s: Failed to start DMA transfer\\n\", __FUNCTION__); \n    /* Handle that */ \n[...] \n} \n```", "```\nvoid dma_async_issue_pending(struct dma_chan *chan); \n```", "```\ndma_async_issue_pending(my_dma_chan); \nwait_for_completion(&transfer_ok); \n\ndma_unmap_single(my_dma_chan->device->dev, dma_src_addr, \nBUFFER_SIZE, DMA_MEM_TO_MEM); \ndma_unmap_single(my_dma_chan->device->dev, dma_src_addr, \n               BUFFER_SIZE, DMA_MEM_TO_MEM); \n\n/* Process buffer through rx_data and tx_data virtualaddresses. */ \n```", "```\nstatic void my_dma_callback() \n{ \n    complete(transfer_ok); \n    return; \n} \n```", "```\nenum sdma_peripheral_type { \n    IMX_DMATYPE_SSI,    /* MCU domain SSI */ \n    IMX_DMATYPE_SSI_SP, /* Shared SSI */ \n    IMX_DMATYPE_MMC,    /* MMC */ \n    IMX_DMATYPE_SDHC,   /* SDHC */ \n    IMX_DMATYPE_UART,   /* MCU domain UART */ \n    IMX_DMATYPE_UART_SP,    /* Shared UART */ \n    IMX_DMATYPE_FIRI,   /* FIRI */ \n    IMX_DMATYPE_CSPI,   /* MCU domain CSPI */ \n    IMX_DMATYPE_CSPI_SP,    /* Shared CSPI */ \n    IMX_DMATYPE_SIM,    /* SIM */ \n    IMX_DMATYPE_ATA,    /* ATA */ \n    IMX_DMATYPE_CCM,    /* CCM */ \n    IMX_DMATYPE_EXT,    /* External peripheral */ \n    IMX_DMATYPE_MSHC,   /* Memory Stick Host Controller */ \n    IMX_DMATYPE_MSHC_SP, /* Shared Memory Stick Host Controller */ \n    IMX_DMATYPE_DSP,    /* DSP */ \n    IMX_DMATYPE_MEMORY, /* Memory */ \n    IMX_DMATYPE_FIFO_MEMORY,/* FIFO type Memory */ \n    IMX_DMATYPE_SPDIF,  /* SPDIF */ \n    IMX_DMATYPE_IPU_MEMORY, /* IPU Memory */ \n    IMX_DMATYPE_ASRC,   /* ASRC */ \n    IMX_DMATYPE_ESAI,   /* ESAI */ \n    IMX_DMATYPE_SSI_DUAL,   /* SSI Dual FIFO */ \n    IMX_DMATYPE_ASRC_SP,    /* Shared ASRC */ \n    IMX_DMATYPE_SAI,    /* SAI */ \n}; \n```", "```\nstruct imx_dma_data { \n    int dma_request; /* DMA request line */ \n    int dma_request2; /* secondary DMA request line */ \n    enum sdma_peripheral_type peripheral_type; \n    int priority; \n}; \n\nenum imx_dma_prio { \n    DMA_PRIO_HIGH = 0, \n    DMA_PRIO_MEDIUM = 1, \n    DMA_PRIO_LOW = 2 \n}; \n```", "```\n#include <linux/module.h> \n#include <linux/slab.h>     /* for kmalloc */ \n#include <linux/init.h> \n#include <linux/dma-mapping.h> \n#include <linux/fs.h> \n#include <linux/version.h> \n#if (LINUX_VERSION_CODE >= KERNEL_VERSION(3,0,35)) \n#include <linux/platform_data/dma-imx.h> \n#else \n#include <mach/dma.h> \n#endif \n\n#include <linux/dmaengine.h> \n#include <linux/device.h> \n\n#include <linux/io.h> \n#include <linux/delay.h> \n\nstatic int gMajor; /* major number of device */ \nstatic struct class *dma_tm_class; \nu32 *wbuf;  /* source buffer */ \nu32 *rbuf;  /* destinationn buffer */ \n\nstruct dma_chan *dma_m2m_chan;  /* our dma channel */ \nstruct completion dma_m2m_ok;   /* completion variable used in the DMA callback */ \n#define SDMA_BUF_SIZE  1024 \n```", "```\nstatic bool dma_m2m_filter(struct dma_chan *chan, void *param) \n{ \n    if (!imx_dma_is_general_purpose(chan)) \n        return false; \n    chan->private = param; \n    return true; \n} \n```", "```\nint sdma_open(struct inode * inode, struct file * filp) \n{ \n    dma_cap_mask_t dma_m2m_mask; \n    struct imx_dma_data m2m_dma_data = {0}; \n\n    init_completion(&dma_m2m_ok); \n\n    dma_cap_zero(dma_m2m_mask); \n    dma_cap_set(DMA_MEMCPY, dma_m2m_mask); /* Set channel capacities */ \n    m2m_dma_data.peripheral_type = IMX_DMATYPE_MEMORY; /* choose the dma device type. This is proper to i.MX */ \n    m2m_dma_data.priority = DMA_PRIO_HIGH;  /* we need high priority */ \n\n    dma_m2m_chan = dma_request_channel(dma_m2m_mask, dma_m2m_filter, &m2m_dma_data); \n    if (!dma_m2m_chan) { \n        printk(\"Error opening the SDMA memory to memory channel\\n\"); \n        return -EINVAL; \n    } \n\n    wbuf = kzalloc(SDMA_BUF_SIZE, GFP_DMA); \n    if(!wbuf) { \n        printk(\"error wbuf !!!!!!!!!!!\\n\"); \n        return -1; \n    } \n\n    rbuf = kzalloc(SDMA_BUF_SIZE, GFP_DMA); \n    if(!rbuf) { \n        printk(\"error rbuf !!!!!!!!!!!\\n\"); \n        return -1; \n    } \n\n    return 0; \n} \n```", "```\nint sdma_release(struct inode * inode, struct file * filp) \n{ \n    dma_release_channel(dma_m2m_chan); \n    dma_m2m_chan = NULL; \n    kfree(wbuf); \n    kfree(rbuf); \n    return 0; \n} \n```", "```\nssize_t sdma_read (struct file *filp, char __user * buf, \nsize_t count, loff_t * offset) \n{ \n    int i; \n    for (i=0; i<SDMA_BUF_SIZE/4; i++) { \n        if (*(rbuf+i) != *(wbuf+i)) { \n            printk(\"Single DMA buffer copy falled!,r=%x,w=%x,%d\\n\", *(rbuf+i), *(wbuf+i), i); \n            return 0; \n        } \n    } \n    printk(\"buffer copy passed!\\n\"); \n    return 0; \n} \n```", "```\nstatic void dma_m2m_callback(void *data) \n{ \n    printk(\"in %s\\n\",__func__); \n    complete(&dma_m2m_ok); \n    return ; \n} \n```", "```\nssize_t sdma_write(struct file * filp, const char __user * buf, \n\n                         size_t count, loff_t * offset) \n{ \n    u32 i; \n    struct dma_slave_config dma_m2m_config = {0}; \n    struct dma_async_tx_descriptor *dma_m2m_desc; /* transaction descriptor */ \n    dma_addr_t dma_src, dma_dst; \n\n    /* No copy_from_user, we just fill the source buffer with predefined data */ \n    for (i=0; i<SDMA_BUF_SIZE/4; i++) { \n        *(wbuf + i) = 0x56565656; \n    } \n\n    dma_m2m_config.direction = DMA_MEM_TO_MEM; \n    dma_m2m_config.dst_addr_width = DMA_SLAVE_BUSWIDTH_4_BYTES; \n    dmaengine_slave_config(dma_m2m_chan, &dma_m2m_config); \n\n    dma_src = dma_map_single(NULL, wbuf, SDMA_BUF_SIZE, DMA_TO_DEVICE); \n    dma_dst = dma_map_single(NULL, rbuf, SDMA_BUF_SIZE, DMA_FROM_DEVICE); \n    dma_m2m_desc = dma_m2m_chan->device->device_prep_dma_memcpy(dma_m2m_chan, dma_dst, dma_src, SDMA_BUF_SIZE,0); \n    if (!dma_m2m_desc) \n        printk(\"prep error!!\\n\"); \n    dma_m2m_desc->callback = dma_m2m_callback; \n    dmaengine_submit(dma_m2m_desc); \n    dma_async_issue_pending(dma_m2m_chan); \n    wait_for_completion(&dma_m2m_ok); \n    dma_unmap_single(NULL, dma_src, SDMA_BUF_SIZE, DMA_TO_DEVICE); \n    dma_unmap_single(NULL, dma_dst, SDMA_BUF_SIZE, DMA_FROM_DEVICE); \n\n    return 0; \n} \n\nstruct file_operations dma_fops = { \n    open: sdma_open, \n    release: sdma_release, \n    read: sdma_read, \n    write: sdma_write, \n}; \n```", "```\nuart1: serial@02020000 { \n    compatible = \"fsl,imx6sx-uart\", \"fsl,imx21-uart\"; \n    reg = <0x02020000 0x4000>; \n    interrupts = <GIC_SPI 26 IRQ_TYPE_LEVEL_HIGH>; \n    clocks = <&clks IMX6SX_CLK_UART_IPG>, \n                <&clks IMX6SX_CLK_UART_SERIAL>; \n    clock-names = \"ipg\", \"per\"; \n    dmas = <&sdma 25 4 0>, <&sdma 26 4 0>; \n    dma-names = \"rx\", \"tx\"; \n    status = \"disabled\"; \n}; \n```", "```\nstatic int imx_uart_dma_init(struct imx_port *sport) \n{ \n    struct dma_slave_config slave_config = {}; \n    struct device *dev = sport->port.dev; \n    int ret; \n\n    /* Prepare for RX : */ \n    sport->dma_chan_rx = dma_request_slave_channel(dev, \"rx\"); \n    if (!sport->dma_chan_rx) { \n        [...] /* cannot get the DMA channel. handle error */ \n    } \n\n    slave_config.direction = DMA_DEV_TO_MEM; \n    slave_config.src_addr = sport->port.mapbase + URXD0; \n    slave_config.src_addr_width = DMA_SLAVE_BUSWIDTH_1_BYTE; \n    /* one byte less than the watermark level to enable the aging timer */ \n    slave_config.src_maxburst = RXTL_DMA - 1; \n    ret = dmaengine_slave_config(sport->dma_chan_rx, &slave_config); \n    if (ret) { \n        [...] /* handle error */ \n    } \n\n    sport->rx_buf = kzalloc(PAGE_SIZE, GFP_KERNEL); \n    if (!sport->rx_buf) { \n        [...] /* handle error */ \n    } \n\n    /* Prepare for TX : */ \n    sport->dma_chan_tx = dma_request_slave_channel(dev, \"tx\"); \n    if (!sport->dma_chan_tx) { \n        [...] /* cannot get the DMA channel. handle error */ \n    } \n\n    slave_config.direction = DMA_MEM_TO_DEV; \n    slave_config.dst_addr = sport->port.mapbase + URTX0; \n    slave_config.dst_addr_width = DMA_SLAVE_BUSWIDTH_1_BYTE; \n    slave_config.dst_maxburst = TXTL_DMA; \n    ret = dmaengine_slave_config(sport->dma_chan_tx, &slave_config); \n    if (ret) { \n        [...] /* handle error */ \n    } \n    [...] \n} \n```"]